diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 0abb7889f5..2bd2eea443 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -1912,7 +1912,7 @@ public static enum ConfVars {
         "final aggregations in single reduce task. If this is set true, Hive delegates final aggregation\n" +
         "stage to fetch task, possibly decreasing the query time."),
 
-    HIVEOPTIMIZEMETADATAQUERIES("hive.compute.query.using.stats", false,
+    HIVEOPTIMIZEMETADATAQUERIES("hive.compute.query.using.stats", true,
         "When set to true Hive will answer a few queries like count(1) purely using stats\n" +
         "stored in metastore. For basic stats collection turn on the config hive.stats.autogather to true.\n" +
         "For more advanced stats collection need to run analyze table queries."),
diff --git a/ql/src/test/queries/clientnegative/insert_into6.q b/ql/src/test/queries/clientnegative/insert_into6.q
index 0feb00eb76..588cf86470 100644
--- a/ql/src/test/queries/clientnegative/insert_into6.q
+++ b/ql/src/test/queries/clientnegative/insert_into6.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 DROP TABLE IF EXISTS insert_into6_neg;
 
diff --git a/ql/src/test/queries/clientnegative/lockneg_query_tbl_in_locked_db.q b/ql/src/test/queries/clientnegative/lockneg_query_tbl_in_locked_db.q
index 4966f2b9b2..6ccdae30da 100644
--- a/ql/src/test/queries/clientnegative/lockneg_query_tbl_in_locked_db.q
+++ b/ql/src/test/queries/clientnegative/lockneg_query_tbl_in_locked_db.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 create database lockneg1;
 use lockneg1;
 
diff --git a/ql/src/test/queries/clientpositive/alter_merge_orc.q b/ql/src/test/queries/clientpositive/alter_merge_orc.q
index 9b836a65de..aac98a82d2 100644
--- a/ql/src/test/queries/clientpositive/alter_merge_orc.q
+++ b/ql/src/test/queries/clientpositive/alter_merge_orc.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 create table src_orc_merge_test(key int, value string) stored as orc;
 
diff --git a/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q b/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q
index dc48ee6df9..a87fa1af0f 100644
--- a/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q
+++ b/ql/src/test/queries/clientpositive/bucketizedhiveinputformat.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set mapred.max.split.size = 32000000;
 
diff --git a/ql/src/test/queries/clientpositive/cbo_udf_udaf.q b/ql/src/test/queries/clientpositive/cbo_udf_udaf.q
index 34d5985453..8534cecd37 100644
--- a/ql/src/test/queries/clientpositive/cbo_udf_udaf.q
+++ b/ql/src/test/queries/clientpositive/cbo_udf_udaf.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.cbo.enable=true;
 set hive.exec.check.crossproducts=false;
diff --git a/ql/src/test/queries/clientpositive/dynamic_partition_pruning.q b/ql/src/test/queries/clientpositive/dynamic_partition_pruning.q
index 9e60fe857b..d28da6e12a 100644
--- a/ql/src/test/queries/clientpositive/dynamic_partition_pruning.q
+++ b/ql/src/test/queries/clientpositive/dynamic_partition_pruning.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.optimize.ppd=true;
diff --git a/ql/src/test/queries/clientpositive/dynpart_sort_opt_vectorization.q b/ql/src/test/queries/clientpositive/dynpart_sort_opt_vectorization.q
index 48a2f87dae..a300f9199f 100644
--- a/ql/src/test/queries/clientpositive/dynpart_sort_opt_vectorization.q
+++ b/ql/src/test/queries/clientpositive/dynpart_sort_opt_vectorization.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.optimize.sort.dynamic.partition=true;
diff --git a/ql/src/test/queries/clientpositive/dynpart_sort_optimization.q b/ql/src/test/queries/clientpositive/dynpart_sort_optimization.q
index b4ff0e881b..5ef8ead00d 100644
--- a/ql/src/test/queries/clientpositive/dynpart_sort_optimization.q
+++ b/ql/src/test/queries/clientpositive/dynpart_sort_optimization.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.optimize.sort.dynamic.partition=true;
diff --git a/ql/src/test/queries/clientpositive/escape1.q b/ql/src/test/queries/clientpositive/escape1.q
index 967db784c4..a28dba8578 100644
--- a/ql/src/test/queries/clientpositive/escape1.q
+++ b/ql/src/test/queries/clientpositive/escape1.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.exec.dynamic.partition=true;
 set hive.exec.max.dynamic.partitions.pernode=200;
diff --git a/ql/src/test/queries/clientpositive/escape2.q b/ql/src/test/queries/clientpositive/escape2.q
index 416d2e4a44..5814650889 100644
--- a/ql/src/test/queries/clientpositive/escape2.q
+++ b/ql/src/test/queries/clientpositive/escape2.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.exec.dynamic.partition=true;
 set hive.exec.max.dynamic.partitions.pernode=200;
diff --git a/ql/src/test/queries/clientpositive/orc_llap_counters.q b/ql/src/test/queries/clientpositive/orc_llap_counters.q
index 1bd55d3410..cc0e991f80 100644
--- a/ql/src/test/queries/clientpositive/orc_llap_counters.q
+++ b/ql/src/test/queries/clientpositive/orc_llap_counters.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 SET hive.optimize.index.filter=true;
 SET hive.cbo.enable=false;
diff --git a/ql/src/test/queries/clientpositive/orc_merge1.q b/ql/src/test/queries/clientpositive/orc_merge1.q
index d0f0b283e6..f704a1c8c1 100644
--- a/ql/src/test/queries/clientpositive/orc_merge1.q
+++ b/ql/src/test/queries/clientpositive/orc_merge1.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.merge.orcfile.stripe.level=false;
diff --git a/ql/src/test/queries/clientpositive/orc_merge10.q b/ql/src/test/queries/clientpositive/orc_merge10.q
index 98d3aa3560..b84ed80a3f 100644
--- a/ql/src/test/queries/clientpositive/orc_merge10.q
+++ b/ql/src/test/queries/clientpositive/orc_merge10.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.merge.orcfile.stripe.level=false;
diff --git a/ql/src/test/queries/clientpositive/orc_merge_diff_fs.q b/ql/src/test/queries/clientpositive/orc_merge_diff_fs.q
index 1787f08dff..94c0e6a35b 100644
--- a/ql/src/test/queries/clientpositive/orc_merge_diff_fs.q
+++ b/ql/src/test/queries/clientpositive/orc_merge_diff_fs.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.merge.orcfile.stripe.level=false;
diff --git a/ql/src/test/queries/clientpositive/orc_ppd_basic.q b/ql/src/test/queries/clientpositive/orc_ppd_basic.q
index c367848a35..43f2c85888 100644
--- a/ql/src/test/queries/clientpositive/orc_ppd_basic.q
+++ b/ql/src/test/queries/clientpositive/orc_ppd_basic.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 SET hive.fetch.task.conversion=none;
 SET hive.optimize.index.filter=true;
diff --git a/ql/src/test/queries/clientpositive/partition_coltype_literals.q b/ql/src/test/queries/clientpositive/partition_coltype_literals.q
index b918dd3f71..0c2365a1d1 100644
--- a/ql/src/test/queries/clientpositive/partition_coltype_literals.q
+++ b/ql/src/test/queries/clientpositive/partition_coltype_literals.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 drop table if exists partcoltypenum;
 create table partcoltypenum (key int, value string) partitioned by (tint tinyint, sint smallint, bint bigint);
 
diff --git a/ql/src/test/queries/clientpositive/stats_aggregator_error_1.q b/ql/src/test/queries/clientpositive/stats_aggregator_error_1.q
index 5e6b0aaa12..d6f84ed7e4 100644
--- a/ql/src/test/queries/clientpositive/stats_aggregator_error_1.q
+++ b/ql/src/test/queries/clientpositive/stats_aggregator_error_1.q
@@ -10,6 +10,7 @@ set hive.stats.dbclass=custom;
 set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
 set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;
 set hive.stats.reliable=false;
+set hive.compute.query.using.stats=false;
 
 set hive.test.dummystats.aggregator=connect;
 
diff --git a/ql/src/test/queries/clientpositive/stats_publisher_error_1.q b/ql/src/test/queries/clientpositive/stats_publisher_error_1.q
index 513b8e75a0..50751f7170 100644
--- a/ql/src/test/queries/clientpositive/stats_publisher_error_1.q
+++ b/ql/src/test/queries/clientpositive/stats_publisher_error_1.q
@@ -10,6 +10,7 @@ set hive.stats.dbclass=custom;
 set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
 set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;
 set hive.stats.reliable=false;
+set hive.compute.query.using.stats=false;
 
 set hive.test.dummystats.publisher=connect;
 
diff --git a/ql/src/test/queries/clientpositive/symlink_text_input_format.q b/ql/src/test/queries/clientpositive/symlink_text_input_format.q
index d89aad41ea..d7759c61e2 100644
--- a/ql/src/test/queries/clientpositive/symlink_text_input_format.q
+++ b/ql/src/test/queries/clientpositive/symlink_text_input_format.q
@@ -1,4 +1,6 @@
 set hive.mapred.mode=nonstrict;
+set hive.compute.query.using.stats=false;
+
 DROP TABLE IF EXISTS symlink_text_input_format;
 
 EXPLAIN
diff --git a/ql/src/test/queries/clientpositive/tez_union.q b/ql/src/test/queries/clientpositive/tez_union.q
index c49c96df53..fba543c1ad 100644
--- a/ql/src/test/queries/clientpositive/tez_union.q
+++ b/ql/src/test/queries/clientpositive/tez_union.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.auto.convert.join=true;
diff --git a/ql/src/test/queries/clientpositive/vector_complex_all.q b/ql/src/test/queries/clientpositive/vector_complex_all.q
index 1f23b60c01..91a736806f 100644
--- a/ql/src/test/queries/clientpositive/vector_complex_all.q
+++ b/ql/src/test/queries/clientpositive/vector_complex_all.q
@@ -1,3 +1,5 @@
+set hive.compute.query.using.stats=false;
+set hive.compute.query.using.stats=false;
 set hive.cli.print.header=true;
 set hive.explain.user=false;
 set hive.fetch.task.conversion=none;
diff --git a/ql/src/test/queries/clientpositive/vectorization_short_regress.q b/ql/src/test/queries/clientpositive/vectorization_short_regress.q
index 3772329977..114a3e2155 100644
--- a/ql/src/test/queries/clientpositive/vectorization_short_regress.q
+++ b/ql/src/test/queries/clientpositive/vectorization_short_regress.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 SET hive.vectorized.execution.enabled=true;
diff --git a/ql/src/test/queries/clientpositive/vectorized_dynamic_partition_pruning.q b/ql/src/test/queries/clientpositive/vectorized_dynamic_partition_pruning.q
index 2dc1271ae0..2d3788d238 100644
--- a/ql/src/test/queries/clientpositive/vectorized_dynamic_partition_pruning.q
+++ b/ql/src/test/queries/clientpositive/vectorized_dynamic_partition_pruning.q
@@ -1,3 +1,4 @@
+set hive.compute.query.using.stats=false;
 set hive.mapred.mode=nonstrict;
 set hive.explain.user=false;
 set hive.optimize.ppd=true;
diff --git a/ql/src/test/results/clientpositive/alter_merge_2_orc.q.out b/ql/src/test/results/clientpositive/alter_merge_2_orc.q.out
index 7e30942696..caa41b25cf 100644
--- a/ql/src/test/results/clientpositive/alter_merge_2_orc.q.out
+++ b/ql/src/test/results/clientpositive/alter_merge_2_orc.q.out
@@ -64,12 +64,10 @@ POSTHOOK: Lineage: src_orc_merge_test_part PARTITION(ds=2012-01-03,ts=2012-01-03
 PREHOOK: query: select count(1) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_orc_merge_test_part
-PREHOOK: Input: default@src_orc_merge_test_part@ds=2012-01-03/ts=2012-01-03+14%3A46%3A31
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc_merge_test_part
-POSTHOOK: Input: default@src_orc_merge_test_part@ds=2012-01-03/ts=2012-01-03+14%3A46%3A31
 #### A masked pattern was here ####
 610
 PREHOOK: query: select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31'
diff --git a/ql/src/test/results/clientpositive/alter_merge_orc.q.out b/ql/src/test/results/clientpositive/alter_merge_orc.q.out
index b5a6d04d9c..aa83fce02b 100644
--- a/ql/src/test/results/clientpositive/alter_merge_orc.q.out
+++ b/ql/src/test/results/clientpositive/alter_merge_orc.q.out
@@ -179,12 +179,10 @@ minFileSize:2515
 PREHOOK: query: select count(1) from src_orc_merge_test_part
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src_orc_merge_test_part
-PREHOOK: Input: default@src_orc_merge_test_part@ds=2011
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from src_orc_merge_test_part
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc_merge_test_part
-POSTHOOK: Input: default@src_orc_merge_test_part@ds=2011
 #### A masked pattern was here ####
 1500
 PREHOOK: query: select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part
diff --git a/ql/src/test/results/clientpositive/alter_partition_coltype.q.out b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out
index d6f607c4dc..703a8e3782 100644
--- a/ql/src/test/results/clientpositive/alter_partition_coltype.q.out
+++ b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out
@@ -40,13 +40,11 @@ PREHOOK: query: -- select with paritition predicate.
 select count(*) from alter_coltype where dt = '100'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_coltype
-PREHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 POSTHOOK: query: -- select with paritition predicate.
 select count(*) from alter_coltype where dt = '100'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_coltype
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 25
 PREHOOK: query: -- alter partition key column data type for dt column.
@@ -74,15 +72,11 @@ PREHOOK: query: -- make sure the partition predicate still works.
 select count(*) from alter_coltype where dt = '100'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_coltype
-PREHOOK: Input: default@alter_coltype@dt=100/ts=3.0
-PREHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 POSTHOOK: query: -- make sure the partition predicate still works. 
 select count(*) from alter_coltype where dt = '100'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_coltype
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=3.0
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 50
 PREHOOK: query: explain extended select count(*) from alter_coltype where dt = '100'
@@ -90,162 +84,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain extended select count(*) from alter_coltype where dt = '100'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: alter_coltype
-            Statistics: Num rows: 50 Data size: 382 Basic stats: COMPLETE Column stats: NONE
-            GatherStats: false
-            Select Operator
-              Statistics: Num rows: 50 Data size: 382 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  null sort order: 
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  tag: -1
-                  value expressions: _col0 (type: bigint)
-                  auto parallelism: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: ts=3.0
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              dt 100
-              ts 3.0
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key,value
-              columns.comments 
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.alter_coltype
-              numFiles 1
-              numRows 25
-              partition_columns dt/ts
-              partition_columns.types int:string
-              rawDataSize 191
-              serialization.ddl struct alter_coltype { string key, string value}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 216
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,value
-                columns.comments 
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.alter_coltype
-                partition_columns dt/ts
-                partition_columns.types int:string
-                serialization.ddl struct alter_coltype { string key, string value}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.alter_coltype
-            name: default.alter_coltype
-#### A masked pattern was here ####
-          Partition
-            base file name: ts=6.30
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              dt 100
-              ts 6.30
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key,value
-              columns.comments 
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.alter_coltype
-              numFiles 1
-              numRows 25
-              partition_columns dt/ts
-              partition_columns.types int:string
-              rawDataSize 191
-              serialization.ddl struct alter_coltype { string key, string value}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 216
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,value
-                columns.comments 
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.alter_coltype
-                partition_columns dt/ts
-                partition_columns.types int:string
-                serialization.ddl struct alter_coltype { string key, string value}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.alter_coltype
-            name: default.alter_coltype
-      Truncated Path -> Alias:
-        /alter_coltype/dt=100/ts=3.0 [alter_coltype]
-        /alter_coltype/dt=100/ts=6.30 [alter_coltype]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0
-                  columns.types bigint
-                  escape.delim \
-                  hive.serialization.extend.additional.nesting.levels true
-                  serialization.escape.crlf true
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -281,13 +125,11 @@ PREHOOK: query: --  validate partition key column predicate can still work.
 select count(*) from alter_coltype where ts = '6.30'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_coltype
-PREHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 POSTHOOK: query: --  validate partition key column predicate can still work.
 select count(*) from alter_coltype where ts = '6.30'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_coltype
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 25
 PREHOOK: query: explain extended select count(*) from alter_coltype where ts = '6.30'
@@ -295,115 +137,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain extended select count(*) from alter_coltype where ts = '6.30'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: alter_coltype
-            Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE
-            GatherStats: false
-            Select Operator
-              Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  null sort order: 
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  tag: -1
-                  value expressions: _col0 (type: bigint)
-                  auto parallelism: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: ts=6.30
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              dt 100
-              ts 6.30
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key,value
-              columns.comments 
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.alter_coltype
-              numFiles 1
-              numRows 25
-              partition_columns dt/ts
-              partition_columns.types string:double
-              rawDataSize 191
-              serialization.ddl struct alter_coltype { string key, string value}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 216
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,value
-                columns.comments 
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.alter_coltype
-                partition_columns dt/ts
-                partition_columns.types string:double
-                serialization.ddl struct alter_coltype { string key, string value}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.alter_coltype
-            name: default.alter_coltype
-      Truncated Path -> Alias:
-        /alter_coltype/dt=100/ts=6.30 [alter_coltype]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0
-                  columns.types bigint
-                  escape.delim \
-                  hive.serialization.extend.additional.nesting.levels true
-                  serialization.escape.crlf true
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -412,14 +151,12 @@ PREHOOK: query: --  validate partition key column predicate on two different par
 select count(*) from alter_coltype where ts = 3.0 and dt=100
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_coltype
-PREHOOK: Input: default@alter_coltype@dt=100/ts=3.0
 #### A masked pattern was here ####
 POSTHOOK: query: --  validate partition key column predicate on two different partition column data type 
 --  can still work.
 select count(*) from alter_coltype where ts = 3.0 and dt=100
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_coltype
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=3.0
 #### A masked pattern was here ####
 25
 PREHOOK: query: explain extended select count(*) from alter_coltype where ts = 3.0 and dt=100
@@ -427,115 +164,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain extended select count(*) from alter_coltype where ts = 3.0 and dt=100
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: alter_coltype
-            Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE
-            GatherStats: false
-            Select Operator
-              Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  null sort order: 
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  tag: -1
-                  value expressions: _col0 (type: bigint)
-                  auto parallelism: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: ts=3.0
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              dt 100
-              ts 3.0
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key,value
-              columns.comments 
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.alter_coltype
-              numFiles 1
-              numRows 25
-              partition_columns dt/ts
-              partition_columns.types string:double
-              rawDataSize 191
-              serialization.ddl struct alter_coltype { string key, string value}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 216
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,value
-                columns.comments 
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.alter_coltype
-                partition_columns dt/ts
-                partition_columns.types string:double
-                serialization.ddl struct alter_coltype { string key, string value}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.alter_coltype
-            name: default.alter_coltype
-      Truncated Path -> Alias:
-        /alter_coltype/dt=100/ts=3.0 [alter_coltype]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0
-                  columns.types bigint
-                  escape.delim \
-                  hive.serialization.extend.additional.nesting.levels true
-                  serialization.escape.crlf true
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -717,27 +351,21 @@ STAGE PLANS:
 PREHOOK: query: select count(*) from alter_coltype where ts = 3.0
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_coltype
-PREHOOK: Input: default@alter_coltype@dt=100/ts=3.0
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from alter_coltype where ts = 3.0
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_coltype
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=3.0
 #### A masked pattern was here ####
 25
 PREHOOK: query: -- make sure the partition predicate still works. 
 select count(*) from alter_coltype where dt = '100'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_coltype
-PREHOOK: Input: default@alter_coltype@dt=100/ts=3.0
-PREHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 POSTHOOK: query: -- make sure the partition predicate still works. 
 select count(*) from alter_coltype where dt = '100'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_coltype
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=3.0
-POSTHOOK: Input: default@alter_coltype@dt=100/ts=6.30
 #### A masked pattern was here ####
 50
 PREHOOK: query: desc alter_coltype
diff --git a/ql/src/test/results/clientpositive/annotate_stats_select.q.out b/ql/src/test/results/clientpositive/annotate_stats_select.q.out
index c51b8954df..75401ae9ce 100644
--- a/ql/src/test/results/clientpositive/annotate_stats_select.q.out
+++ b/ql/src/test/results/clientpositive/annotate_stats_select.q.out
@@ -685,44 +685,12 @@ POSTHOOK: query: -- COUNT(*) is projected as new column. It is not projected as
 explain select count(*) from alltypes_orc
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: alltypes_orc
-            Statistics: Num rows: 2 Data size: 1686 Basic stats: COMPLETE Column stats: COMPLETE
-            Select Operator
-              Statistics: Num rows: 2 Data size: 1686 Basic stats: COMPLETE Column stats: COMPLETE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -735,44 +703,12 @@ POSTHOOK: query: -- COUNT(1) is projected as new column. It is not projected as
 explain select count(1) from alltypes_orc
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: alltypes_orc
-            Statistics: Num rows: 2 Data size: 1686 Basic stats: COMPLETE Column stats: COMPLETE
-            Select Operator
-              Statistics: Num rows: 2 Data size: 1686 Basic stats: COMPLETE Column stats: COMPLETE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
diff --git a/ql/src/test/results/clientpositive/avro_partitioned.q.out b/ql/src/test/results/clientpositive/avro_partitioned.q.out
index c642736e3a..bd45978a9b 100644
--- a/ql/src/test/results/clientpositive/avro_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/avro_partitioned.q.out
@@ -250,26 +250,10 @@ POSTHOOK: Output: default@episodes_partitioned@doctor_pt=7
 PREHOOK: query: SELECT COUNT(*) FROM episodes_partitioned
 PREHOOK: type: QUERY
 PREHOOK: Input: default@episodes_partitioned
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=1
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=11
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=2
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=4
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=5
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=6
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=7
-PREHOOK: Input: default@episodes_partitioned@doctor_pt=9
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT COUNT(*) FROM episodes_partitioned
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@episodes_partitioned
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=1
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=11
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=2
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=4
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=5
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=6
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=7
-POSTHOOK: Input: default@episodes_partitioned@doctor_pt=9
 #### A masked pattern was here ####
 8
 PREHOOK: query: -- Verify that reading from an Avro partition works
diff --git a/ql/src/test/results/clientpositive/bucketsortoptimize_insert_1.q.out b/ql/src/test/results/clientpositive/bucketsortoptimize_insert_1.q.out
index 9faa0d0273..48de4239b7 100644
--- a/ql/src/test/results/clientpositive/bucketsortoptimize_insert_1.q.out
+++ b/ql/src/test/results/clientpositive/bucketsortoptimize_insert_1.q.out
@@ -113,12 +113,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1'
@@ -220,12 +218,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1'
diff --git a/ql/src/test/results/clientpositive/bucketsortoptimize_insert_3.q.out b/ql/src/test/results/clientpositive/bucketsortoptimize_insert_3.q.out
index e778e35e94..88310806b2 100644
--- a/ql/src/test/results/clientpositive/bucketsortoptimize_insert_3.q.out
+++ b/ql/src/test/results/clientpositive/bucketsortoptimize_insert_3.q.out
@@ -107,12 +107,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1'
@@ -231,12 +229,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1'
diff --git a/ql/src/test/results/clientpositive/cbo_rp_udf_udaf.q.out b/ql/src/test/results/clientpositive/cbo_rp_udf_udaf.q.out
index 156d02f31e..b30d9da825 100644
--- a/ql/src/test/results/clientpositive/cbo_rp_udf_udaf.q.out
+++ b/ql/src/test/results/clientpositive/cbo_rp_udf_udaf.q.out
@@ -53,12 +53,10 @@ POSTHOOK: Input: default@cbo_t1@dt=2014
 PREHOOK: query: select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@cbo_t1
-PREHOOK: Input: default@cbo_t1@dt=2014
 #### A masked pattern was here ####
 POSTHOOK: query: select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@cbo_t1
-POSTHOOK: Input: default@cbo_t1@dt=2014
 #### A masked pattern was here ####
 1	20	1	18
 PREHOOK: query: select f,a,e,b from (select count(*) as a, count(distinct c_int) as b, sum(distinct c_int) as c, avg(distinct c_int) as d, max(distinct c_int) as e, min(distinct c_int) as f from cbo_t1) cbo_t1
diff --git a/ql/src/test/results/clientpositive/cbo_rp_udf_udaf_stats_opt.q.out b/ql/src/test/results/clientpositive/cbo_rp_udf_udaf_stats_opt.q.out
index a1e7fd80e8..3a589b4c24 100644
--- a/ql/src/test/results/clientpositive/cbo_rp_udf_udaf_stats_opt.q.out
+++ b/ql/src/test/results/clientpositive/cbo_rp_udf_udaf_stats_opt.q.out
@@ -54,12 +54,10 @@ POSTHOOK: Input: default@cbo_t1@dt=2014
 PREHOOK: query: select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@cbo_t1
-PREHOOK: Input: default@cbo_t1@dt=2014
 #### A masked pattern was here ####
 POSTHOOK: query: select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@cbo_t1
-POSTHOOK: Input: default@cbo_t1@dt=2014
 #### A masked pattern was here ####
 1	20	1	18
 PREHOOK: query: select f,a,e,b from (select count(*) as a, count(distinct c_int) as b, sum(distinct c_int) as c, avg(distinct c_int) as d, max(distinct c_int) as e, min(distinct c_int) as f from cbo_t1) cbo_t1
diff --git a/ql/src/test/results/clientpositive/cbo_udf_udaf.q.out b/ql/src/test/results/clientpositive/cbo_udf_udaf.q.out
index 156d02f31e..b30d9da825 100644
--- a/ql/src/test/results/clientpositive/cbo_udf_udaf.q.out
+++ b/ql/src/test/results/clientpositive/cbo_udf_udaf.q.out
@@ -53,12 +53,10 @@ POSTHOOK: Input: default@cbo_t1@dt=2014
 PREHOOK: query: select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@cbo_t1
-PREHOOK: Input: default@cbo_t1@dt=2014
 #### A masked pattern was here ####
 POSTHOOK: query: select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@cbo_t1
-POSTHOOK: Input: default@cbo_t1@dt=2014
 #### A masked pattern was here ####
 1	20	1	18
 PREHOOK: query: select f,a,e,b from (select count(*) as a, count(distinct c_int) as b, sum(distinct c_int) as c, avg(distinct c_int) as d, max(distinct c_int) as e, min(distinct c_int) as f from cbo_t1) cbo_t1
diff --git a/ql/src/test/results/clientpositive/combine2.q.out b/ql/src/test/results/clientpositive/combine2.q.out
index fb9ef84ce9..6616f66e81 100644
--- a/ql/src/test/results/clientpositive/combine2.q.out
+++ b/ql/src/test/results/clientpositive/combine2.q.out
@@ -153,462 +153,22 @@ POSTHOOK: query: explain extended
 select count(1) from combine2 where value is not null
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: combine2
-            Statistics: Num rows: 12 Data size: 14 Basic stats: COMPLETE Column stats: NONE
-            GatherStats: false
-            Select Operator
-              Statistics: Num rows: 12 Data size: 14 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  null sort order: 
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  tag: -1
-                  value expressions: _col0 (type: bigint)
-                  auto parallelism: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: value=2010-04-21 09%3A45%3A00
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value 2010-04-21 09:45:00
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 1
-              numRows 1
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 2
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 3
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=val_0
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value val_0
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 3
-              numRows 3
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 3
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 6
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=val_2
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value val_2
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 1
-              numRows 1
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 1
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 2
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=val_4
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value val_4
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 1
-              numRows 1
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 1
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 2
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=val_5
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value val_5
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 3
-              numRows 3
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 3
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 6
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=val_8
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value val_8
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 1
-              numRows 1
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 1
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 2
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=val_9
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value val_9
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 1
-              numRows 1
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 1
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 2
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-#### A masked pattern was here ####
-          Partition
-            base file name: value=|
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              value |
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key
-              columns.comments 
-              columns.types string
-#### A masked pattern was here ####
-              name default.combine2
-              numFiles 1
-              numRows 1
-              partition_columns value
-              partition_columns.types string
-              rawDataSize 2
-              serialization.ddl struct combine2 { string key}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 3
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key
-                columns.comments 
-                columns.types string
-#### A masked pattern was here ####
-                name default.combine2
-                partition_columns value
-                partition_columns.types string
-                serialization.ddl struct combine2 { string key}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.combine2
-            name: default.combine2
-      Truncated Path -> Alias:
-        /combine2/value=2010-04-21 09%3A45%3A00 [$hdt$_0:combine2]
-        /combine2/value=val_0 [$hdt$_0:combine2]
-        /combine2/value=val_2 [$hdt$_0:combine2]
-        /combine2/value=val_4 [$hdt$_0:combine2]
-        /combine2/value=val_5 [$hdt$_0:combine2]
-        /combine2/value=val_8 [$hdt$_0:combine2]
-        /combine2/value=val_9 [$hdt$_0:combine2]
-        /combine2/value=| [$hdt$_0:combine2]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0
-                  columns.types bigint
-                  escape.delim \
-                  hive.serialization.extend.additional.nesting.levels true
-                  serialization.escape.crlf true
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 PREHOOK: query: select count(1) from combine2 where value is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@combine2
-PREHOOK: Input: default@combine2@value=2010-04-21 09%3A45%3A00
-PREHOOK: Input: default@combine2@value=val_0
-PREHOOK: Input: default@combine2@value=val_2
-PREHOOK: Input: default@combine2@value=val_4
-PREHOOK: Input: default@combine2@value=val_5
-PREHOOK: Input: default@combine2@value=val_8
-PREHOOK: Input: default@combine2@value=val_9
-PREHOOK: Input: default@combine2@value=|
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from combine2 where value is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@combine2
-POSTHOOK: Input: default@combine2@value=2010-04-21 09%3A45%3A00
-POSTHOOK: Input: default@combine2@value=val_0
-POSTHOOK: Input: default@combine2@value=val_2
-POSTHOOK: Input: default@combine2@value=val_4
-POSTHOOK: Input: default@combine2@value=val_5
-POSTHOOK: Input: default@combine2@value=val_8
-POSTHOOK: Input: default@combine2@value=val_9
-POSTHOOK: Input: default@combine2@value=|
 #### A masked pattern was here ####
 12
 PREHOOK: query: explain
diff --git a/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out b/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out
index fc4f4830c8..cbfc7be7b1 100644
--- a/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out
+++ b/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out
@@ -1158,53 +1158,37 @@ Storage Desc Params:
 PREHOOK: query: select count(*) from over1k_part_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_orc
-PREHOOK: Input: default@over1k_part_orc@ds=foo/t=27
-PREHOOK: Input: default@over1k_part_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_orc
-POSTHOOK: Input: default@over1k_part_orc@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 38
 PREHOOK: query: select count(*) from over1k_part_limit_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_limit_orc
-PREHOOK: Input: default@over1k_part_limit_orc@ds=foo/t=27
-PREHOOK: Input: default@over1k_part_limit_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_limit_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_limit_orc
-POSTHOOK: Input: default@over1k_part_limit_orc@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part_limit_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 20
 PREHOOK: query: select count(*) from over1k_part_buck_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_orc
-PREHOOK: Input: default@over1k_part_buck_orc@t=27
-PREHOOK: Input: default@over1k_part_buck_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_orc
-POSTHOOK: Input: default@over1k_part_buck_orc@t=27
-POSTHOOK: Input: default@over1k_part_buck_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 38
 PREHOOK: query: select count(*) from over1k_part_buck_sort_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_sort_orc
-PREHOOK: Input: default@over1k_part_buck_sort_orc@t=27
-PREHOOK: Input: default@over1k_part_buck_sort_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_sort_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_sort_orc
-POSTHOOK: Input: default@over1k_part_buck_sort_orc@t=27
-POSTHOOK: Input: default@over1k_part_buck_sort_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 38
 PREHOOK: query: -- tests for HIVE-6883
@@ -1705,14 +1689,10 @@ POSTHOOK: Input: default@over1k_part2_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: select count(*) from over1k_part2_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part2_orc
-PREHOOK: Input: default@over1k_part2_orc@ds=foo/t=27
-PREHOOK: Input: default@over1k_part2_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part2_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part2_orc
-POSTHOOK: Input: default@over1k_part2_orc@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part2_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by i
@@ -1852,14 +1832,10 @@ POSTHOOK: Input: default@over1k_part2_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: select count(*) from over1k_part2_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part2_orc
-PREHOOK: Input: default@over1k_part2_orc@ds=foo/t=27
-PREHOOK: Input: default@over1k_part2_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part2_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part2_orc
-POSTHOOK: Input: default@over1k_part2_orc@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part2_orc@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: -- hadoop-1 does not honor number of reducers in local mode. There is always only 1 reducer irrespective of the number of buckets.
@@ -2164,58 +2140,22 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain select count(*) from over1k_part_buck_sort2_orc
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: over1k_part_buck_sort2_orc
-            Statistics: Num rows: 19 Data size: 493 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 19 Data size: 493 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 PREHOOK: query: select count(*) from over1k_part_buck_sort2_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_sort2_orc
-PREHOOK: Input: default@over1k_part_buck_sort2_orc@t=27
-PREHOOK: Input: default@over1k_part_buck_sort2_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_sort2_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_sort2_orc
-POSTHOOK: Input: default@over1k_part_buck_sort2_orc@t=27
-POSTHOOK: Input: default@over1k_part_buck_sort2_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: insert overwrite table over1k_part_buck_sort2_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27
@@ -2374,57 +2314,21 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain select count(*) from over1k_part_buck_sort2_orc
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: over1k_part_buck_sort2_orc
-            Statistics: Num rows: 19 Data size: 493 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 19 Data size: 493 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 PREHOOK: query: select count(*) from over1k_part_buck_sort2_orc
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_sort2_orc
-PREHOOK: Input: default@over1k_part_buck_sort2_orc@t=27
-PREHOOK: Input: default@over1k_part_buck_sort2_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_sort2_orc
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_sort2_orc
-POSTHOOK: Input: default@over1k_part_buck_sort2_orc@t=27
-POSTHOOK: Input: default@over1k_part_buck_sort2_orc@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
diff --git a/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out b/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out
index d24ee1648e..ce3fad24dc 100644
--- a/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out
+++ b/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out
@@ -1107,53 +1107,37 @@ Storage Desc Params:
 PREHOOK: query: select count(*) from over1k_part
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part
-PREHOOK: Input: default@over1k_part@ds=foo/t=27
-PREHOOK: Input: default@over1k_part@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part
-POSTHOOK: Input: default@over1k_part@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 38
 PREHOOK: query: select count(*) from over1k_part_limit
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_limit
-PREHOOK: Input: default@over1k_part_limit@ds=foo/t=27
-PREHOOK: Input: default@over1k_part_limit@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_limit
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_limit
-POSTHOOK: Input: default@over1k_part_limit@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part_limit@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 20
 PREHOOK: query: select count(*) from over1k_part_buck
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck
-PREHOOK: Input: default@over1k_part_buck@t=27
-PREHOOK: Input: default@over1k_part_buck@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck
-POSTHOOK: Input: default@over1k_part_buck@t=27
-POSTHOOK: Input: default@over1k_part_buck@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 38
 PREHOOK: query: select count(*) from over1k_part_buck_sort
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_sort
-PREHOOK: Input: default@over1k_part_buck_sort@t=27
-PREHOOK: Input: default@over1k_part_buck_sort@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_sort
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_sort
-POSTHOOK: Input: default@over1k_part_buck_sort@t=27
-POSTHOOK: Input: default@over1k_part_buck_sort@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 38
 PREHOOK: query: -- tests for HIVE-6883
@@ -1649,14 +1633,10 @@ POSTHOOK: Input: default@over1k_part2@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: select count(*) from over1k_part2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part2
-PREHOOK: Input: default@over1k_part2@ds=foo/t=27
-PREHOOK: Input: default@over1k_part2@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part2
-POSTHOOK: Input: default@over1k_part2@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part2@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 order by i
@@ -1796,14 +1776,10 @@ POSTHOOK: Input: default@over1k_part2@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: select count(*) from over1k_part2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part2
-PREHOOK: Input: default@over1k_part2@ds=foo/t=27
-PREHOOK: Input: default@over1k_part2@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part2
-POSTHOOK: Input: default@over1k_part2@ds=foo/t=27
-POSTHOOK: Input: default@over1k_part2@ds=foo/t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: -- hadoop-1 does not honor number of reducers in local mode. There is always only 1 reducer irrespective of the number of buckets.
@@ -2083,14 +2059,10 @@ POSTHOOK: Input: default@over1k_part_buck_sort2@t=__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: select count(*) from over1k_part_buck_sort2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_sort2
-PREHOOK: Input: default@over1k_part_buck_sort2@t=27
-PREHOOK: Input: default@over1k_part_buck_sort2@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_sort2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_sort2
-POSTHOOK: Input: default@over1k_part_buck_sort2@t=27
-POSTHOOK: Input: default@over1k_part_buck_sort2@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: insert overwrite table over1k_part_buck_sort2 partition(t) select si,i,b,f,t from over1k where t is null or t=27
@@ -2226,14 +2198,10 @@ POSTHOOK: Input: default@over1k_part_buck_sort2@t=__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: select count(*) from over1k_part_buck_sort2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@over1k_part_buck_sort2
-PREHOOK: Input: default@over1k_part_buck_sort2@t=27
-PREHOOK: Input: default@over1k_part_buck_sort2@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from over1k_part_buck_sort2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@over1k_part_buck_sort2
-POSTHOOK: Input: default@over1k_part_buck_sort2@t=27
-POSTHOOK: Input: default@over1k_part_buck_sort2@t=__HIVE_DEFAULT_PARTITION__
 #### A masked pattern was here ####
 19
 PREHOOK: query: create table over1k_part3(
diff --git a/ql/src/test/results/clientpositive/explain_dependency2.q.out b/ql/src/test/results/clientpositive/explain_dependency2.q.out
index 7973a6033b..20bed874ba 100644
--- a/ql/src/test/results/clientpositive/explain_dependency2.q.out
+++ b/ql/src/test/results/clientpositive/explain_dependency2.q.out
@@ -42,7 +42,7 @@ POSTHOOK: query: -- select from a partitioned table which involves a map-reduce
 -- and some partitions are being selected
 EXPLAIN DEPENDENCY SELECT count(*) FROM srcpart where ds is not null
 POSTHOOK: type: QUERY
-{"input_tables":[{"tablename":"default@srcpart","tabletype":"MANAGED_TABLE"}],"input_partitions":[{"partitionName":"default@srcpart@ds=2008-04-08/hr=11"},{"partitionName":"default@srcpart@ds=2008-04-08/hr=12"},{"partitionName":"default@srcpart@ds=2008-04-09/hr=11"},{"partitionName":"default@srcpart@ds=2008-04-09/hr=12"}]}
+{"input_tables":[{"tablename":"default@srcpart","tabletype":"MANAGED_TABLE"}],"input_partitions":[]}
 PREHOOK: query: -- select from a partitioned table which involves a map-reduce job
 -- and none of the partitions are being selected
 EXPLAIN DEPENDENCY SELECT count(*) FROM srcpart where ds = '1'
diff --git a/ql/src/test/results/clientpositive/fileformat_mix.q.out b/ql/src/test/results/clientpositive/fileformat_mix.q.out
index 7465914e81..1df6f51311 100644
--- a/ql/src/test/results/clientpositive/fileformat_mix.q.out
+++ b/ql/src/test/results/clientpositive/fileformat_mix.q.out
@@ -44,14 +44,10 @@ POSTHOOK: Output: default@fileformat_mix_test
 PREHOOK: query: select count(1) from fileformat_mix_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@fileformat_mix_test
-PREHOOK: Input: default@fileformat_mix_test@ds=1
-PREHOOK: Input: default@fileformat_mix_test@ds=2
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from fileformat_mix_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@fileformat_mix_test
-POSTHOOK: Input: default@fileformat_mix_test@ds=1
-POSTHOOK: Input: default@fileformat_mix_test@ds=2
 #### A masked pattern was here ####
 500
 PREHOOK: query: select src from fileformat_mix_test
diff --git a/ql/src/test/results/clientpositive/fold_case.q.out b/ql/src/test/results/clientpositive/fold_case.q.out
index acf1e4c0dc..304d37adbd 100644
--- a/ql/src/test/results/clientpositive/fold_case.q.out
+++ b/ql/src/test/results/clientpositive/fold_case.q.out
@@ -158,44 +158,12 @@ POSTHOOK: query: explain
 select count(1) from src where (case key when '238' then true else 1=1 end)
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
-            Select Operator
-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
diff --git a/ql/src/test/results/clientpositive/input24.q.out b/ql/src/test/results/clientpositive/input24.q.out
index 06df447ba7..935fff4047 100644
--- a/ql/src/test/results/clientpositive/input24.q.out
+++ b/ql/src/test/results/clientpositive/input24.q.out
@@ -20,55 +20,21 @@ POSTHOOK: query: explain
 select count(1) from tst x where x.d='2009-01-01'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: x
-            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 PREHOOK: query: select count(1) from tst x where x.d='2009-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tst
-PREHOOK: Input: default@tst@d=2009-01-01
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from tst x where x.d='2009-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tst
-POSTHOOK: Input: default@tst@d=2009-01-01
 #### A masked pattern was here ####
 0
diff --git a/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out b/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out
index 7f99f1763d..e0af0c619b 100644
--- a/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out
+++ b/ql/src/test/results/clientpositive/list_bucket_query_multiskew_1.q.out
@@ -100,12 +100,10 @@ Storage Desc Params:
 PREHOOK: query: SELECT count(1) FROM fact_daily WHERE ds='1' and hr='4'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@fact_daily
-PREHOOK: Input: default@fact_daily@ds=1/hr=4
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT count(1) FROM fact_daily WHERE ds='1' and hr='4'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@fact_daily
-POSTHOOK: Input: default@fact_daily@ds=1/hr=4
 #### A masked pattern was here ####
 500
 PREHOOK: query: -- pruner only pick up skewed-value directory
diff --git a/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out b/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
index 8c0810612a..dcc45d6eea 100644
--- a/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
+++ b/ql/src/test/results/clientpositive/list_bucket_query_multiskew_2.q.out
@@ -100,12 +100,10 @@ Storage Desc Params:
 PREHOOK: query: SELECT count(1) FROM fact_daily WHERE ds='1' and hr='4'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@fact_daily
-PREHOOK: Input: default@fact_daily@ds=1/hr=4
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT count(1) FROM fact_daily WHERE ds='1' and hr='4'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@fact_daily
-POSTHOOK: Input: default@fact_daily@ds=1/hr=4
 #### A masked pattern was here ####
 500
 PREHOOK: query: -- pruner only pick up default directory
diff --git a/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out b/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out
index 6a421f38a1..7da4e7ca2f 100644
--- a/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out
+++ b/ql/src/test/results/clientpositive/list_bucket_query_multiskew_3.q.out
@@ -319,127 +319,22 @@ POSTHOOK: query: explain extended
 select count(*) from fact_daily where ds = '1' and  hr='1'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: fact_daily
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-            GatherStats: false
-            Select Operator
-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  null sort order: 
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  tag: -1
-                  value expressions: _col0 (type: bigint)
-                  auto parallelism: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: hr=1
-            input format: org.apache.hadoop.mapred.TextInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-            partition values:
-              ds 1
-              hr 1
-            properties:
-              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-              bucket_count -1
-              columns key,value
-              columns.comments 
-              columns.types string:string
-#### A masked pattern was here ####
-              name default.fact_daily
-              numFiles 1
-              numRows 500
-              partition_columns ds/hr
-              partition_columns.types string:string
-              rawDataSize 5312
-              serialization.ddl struct fact_daily { string key, string value}
-              serialization.format 1
-              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              totalSize 5812
-#### A masked pattern was here ####
-            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-          
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              properties:
-                bucket_count -1
-                columns key,value
-                columns.comments 
-                columns.types string:string
-#### A masked pattern was here ####
-                name default.fact_daily
-                partition_columns ds/hr
-                partition_columns.types string:string
-                serialization.ddl struct fact_daily { string key, string value}
-                serialization.format 1
-                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.fact_daily
-            name: default.fact_daily
-      Truncated Path -> Alias:
-        /fact_daily/ds=1/hr=1 [fact_daily]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0
-                  columns.types bigint
-                  escape.delim \
-                  hive.serialization.extend.additional.nesting.levels true
-                  serialization.escape.crlf true
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 PREHOOK: query: select count(*) from fact_daily where ds = '1' and  hr='1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@fact_daily
-PREHOOK: Input: default@fact_daily@ds=1/hr=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from fact_daily where ds = '1' and  hr='1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@fact_daily
-POSTHOOK: Input: default@fact_daily@ds=1/hr=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: -- query skewed partition
diff --git a/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out b/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
index 86af66090f..ca168c81a8 100644
--- a/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
+++ b/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
@@ -242,13 +242,9 @@ ds=2008-04-08/hr=b1
 PREHOOK: query: select count(*) from merge_dynamic_part
 PREHOOK: type: QUERY
 PREHOOK: Input: default@merge_dynamic_part
-PREHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=a1
-PREHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=b1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from merge_dynamic_part
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@merge_dynamic_part
-POSTHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=a1
-POSTHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=b1
 #### A masked pattern was here ####
 1000
diff --git a/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out b/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
index c1468c17a2..38e1ad8e7a 100644
--- a/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
+++ b/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
@@ -216,13 +216,9 @@ ds=2008-04-08/hr=b1
 PREHOOK: query: select count(*) from merge_dynamic_part
 PREHOOK: type: QUERY
 PREHOOK: Input: default@merge_dynamic_part
-PREHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=a1
-PREHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=b1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from merge_dynamic_part
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@merge_dynamic_part
-POSTHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=a1
-POSTHOOK: Input: default@merge_dynamic_part@ds=2008-04-08/hr=b1
 #### A masked pattern was here ####
 618
diff --git a/ql/src/test/results/clientpositive/orc_merge1.q.out b/ql/src/test/results/clientpositive/orc_merge1.q.out
index 3f047dae20..63dd19cf08 100644
--- a/ql/src/test/results/clientpositive/orc_merge1.q.out
+++ b/ql/src/test/results/clientpositive/orc_merge1.q.out
@@ -400,40 +400,28 @@ POSTHOOK: Input: default@orcfile_merge1c@ds=1/part=1
 PREHOOK: query: select count(*) from orcfile_merge1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orcfile_merge1
-PREHOOK: Input: default@orcfile_merge1@ds=1/part=0
-PREHOOK: Input: default@orcfile_merge1@ds=1/part=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from orcfile_merge1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orcfile_merge1
-POSTHOOK: Input: default@orcfile_merge1@ds=1/part=0
-POSTHOOK: Input: default@orcfile_merge1@ds=1/part=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from orcfile_merge1b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orcfile_merge1b
-PREHOOK: Input: default@orcfile_merge1b@ds=1/part=0
-PREHOOK: Input: default@orcfile_merge1b@ds=1/part=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from orcfile_merge1b
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orcfile_merge1b
-POSTHOOK: Input: default@orcfile_merge1b@ds=1/part=0
-POSTHOOK: Input: default@orcfile_merge1b@ds=1/part=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from orcfile_merge1c
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orcfile_merge1c
-PREHOOK: Input: default@orcfile_merge1c@ds=1/part=0
-PREHOOK: Input: default@orcfile_merge1c@ds=1/part=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from orcfile_merge1c
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orcfile_merge1c
-POSTHOOK: Input: default@orcfile_merge1c@ds=1/part=0
-POSTHOOK: Input: default@orcfile_merge1c@ds=1/part=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: DROP TABLE orcfile_merge1
diff --git a/ql/src/test/results/clientpositive/orc_merge10.q.out b/ql/src/test/results/clientpositive/orc_merge10.q.out
index 1d64ae5f7a..efb5669a3f 100644
--- a/ql/src/test/results/clientpositive/orc_merge10.q.out
+++ b/ql/src/test/results/clientpositive/orc_merge10.q.out
@@ -381,27 +381,19 @@ POSTHOOK: Input: default@orcfile_merge1b@ds=1/part=1
 PREHOOK: query: select count(*) from orcfile_merge1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orcfile_merge1
-PREHOOK: Input: default@orcfile_merge1@ds=1/part=0
-PREHOOK: Input: default@orcfile_merge1@ds=1/part=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from orcfile_merge1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orcfile_merge1
-POSTHOOK: Input: default@orcfile_merge1@ds=1/part=0
-POSTHOOK: Input: default@orcfile_merge1@ds=1/part=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from orcfile_merge1b
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orcfile_merge1b
-PREHOOK: Input: default@orcfile_merge1b@ds=1/part=0
-PREHOOK: Input: default@orcfile_merge1b@ds=1/part=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from orcfile_merge1b
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orcfile_merge1b
-POSTHOOK: Input: default@orcfile_merge1b@ds=1/part=0
-POSTHOOK: Input: default@orcfile_merge1b@ds=1/part=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: -- concatenate
@@ -500,14 +492,10 @@ POSTHOOK: Input: default@orcfile_merge1@ds=1/part=1
 PREHOOK: query: select count(*) from orcfile_merge1c
 PREHOOK: type: QUERY
 PREHOOK: Input: default@orcfile_merge1c
-PREHOOK: Input: default@orcfile_merge1c@ds=1/part=0
-PREHOOK: Input: default@orcfile_merge1c@ds=1/part=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from orcfile_merge1c
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@orcfile_merge1c
-POSTHOOK: Input: default@orcfile_merge1c@ds=1/part=0
-POSTHOOK: Input: default@orcfile_merge1c@ds=1/part=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select * from orcfile_merge1 where ds='1' and part='0' limit 1
diff --git a/ql/src/test/results/clientpositive/partition_boolexpr.q.out b/ql/src/test/results/clientpositive/partition_boolexpr.q.out
index fe33e180e4..8e75910484 100644
--- a/ql/src/test/results/clientpositive/partition_boolexpr.q.out
+++ b/ql/src/test/results/clientpositive/partition_boolexpr.q.out
@@ -32,44 +32,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain select count(1) from srcpart where true
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: srcpart
-            Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -127,44 +95,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain select count(1) from srcpart where true and hr='11'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: srcpart
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -173,44 +109,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain select count(1) from srcpart where true or hr='11'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: srcpart
-            Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -219,44 +123,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: explain select count(1) from srcpart where false or hr='11'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: srcpart
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
diff --git a/ql/src/test/results/clientpositive/partition_date.q.out b/ql/src/test/results/clientpositive/partition_date.q.out
index 9720a48958..f6584ee8a7 100644
--- a/ql/src/test/results/clientpositive/partition_date.q.out
+++ b/ql/src/test/results/clientpositive/partition_date.q.out
@@ -110,71 +110,55 @@ PREHOOK: query: -- 15
 select count(*) from partition_date_1 where dt = date '2000-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 POSTHOOK: query: -- 15
 select count(*) from partition_date_1 where dt = date '2000-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 15
 PREHOOK: query: -- 15.  Also try with string value in predicate
 select count(*) from partition_date_1 where dt = '2000-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 POSTHOOK: query: -- 15.  Also try with string value in predicate
 select count(*) from partition_date_1 where dt = '2000-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 15
 PREHOOK: query: -- 5
 select count(*) from partition_date_1 where dt = date '2000-01-01' and region = '2'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 POSTHOOK: query: -- 5
 select count(*) from partition_date_1 where dt = date '2000-01-01' and region = '2'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 5
 PREHOOK: query: -- 11
 select count(*) from partition_date_1 where dt = date '2013-08-08' and region = '10'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2013-08-08/region=10
 #### A masked pattern was here ####
 POSTHOOK: query: -- 11
 select count(*) from partition_date_1 where dt = date '2013-08-08' and region = '10'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2013-08-08/region=10
 #### A masked pattern was here ####
 11
 PREHOOK: query: -- 30
 select count(*) from partition_date_1 where region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
-PREHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 30
 select count(*) from partition_date_1 where region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
-POSTHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 30
 PREHOOK: query: -- 0
@@ -205,7 +189,6 @@ PREHOOK: query: -- Try other comparison operations
 select count(*) from partition_date_1 where dt > date '2000-01-01' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- Try other comparison operations
 
@@ -213,72 +196,61 @@ POSTHOOK: query: -- Try other comparison operations
 select count(*) from partition_date_1 where dt > date '2000-01-01' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_date_1 where dt < date '2000-01-02' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_date_1 where dt < date '2000-01-02' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 20
 select count(*) from partition_date_1 where dt >= date '2000-01-02' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 20
 select count(*) from partition_date_1 where dt >= date '2000-01-02' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_date_1 where dt <= date '2000-01-01' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_date_1 where dt <= date '2000-01-01' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 20
 select count(*) from partition_date_1 where dt <> date '2000-01-01' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 20
 select count(*) from partition_date_1 where dt <> date '2000-01-01' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_date_1 where dt between date '1999-12-30' and date '2000-01-03' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_date_1 where dt between date '1999-12-30' and date '2000-01-03' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- Try a string key with date-like strings
@@ -287,7 +259,6 @@ PREHOOK: query: -- Try a string key with date-like strings
 select count(*) from partition_date_1 where region = '2020-20-20'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2013-12-10/region=2020-20-20
 #### A masked pattern was here ####
 POSTHOOK: query: -- Try a string key with date-like strings
 
@@ -295,20 +266,17 @@ POSTHOOK: query: -- Try a string key with date-like strings
 select count(*) from partition_date_1 where region = '2020-20-20'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2013-12-10/region=2020-20-20
 #### A masked pattern was here ####
 5
 PREHOOK: query: -- 5
 select count(*) from partition_date_1 where region > '2010-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_date_1
-PREHOOK: Input: default@partition_date_1@dt=2013-12-10/region=2020-20-20
 #### A masked pattern was here ####
 POSTHOOK: query: -- 5
 select count(*) from partition_date_1 where region > '2010-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_date_1
-POSTHOOK: Input: default@partition_date_1@dt=2013-12-10/region=2020-20-20
 #### A masked pattern was here ####
 5
 PREHOOK: query: drop table partition_date_1
diff --git a/ql/src/test/results/clientpositive/partition_decode_name.q.out b/ql/src/test/results/clientpositive/partition_decode_name.q.out
index 2c700dc6cd..ea60750075 100644
--- a/ql/src/test/results/clientpositive/partition_decode_name.q.out
+++ b/ql/src/test/results/clientpositive/partition_decode_name.q.out
@@ -53,16 +53,10 @@ ts=2011-01-11+16%3A18%3A26
 PREHOOK: query: select count(*) from sc_part where ts is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sc_part
-PREHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from sc_part where ts is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@sc_part
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 3
 PREHOOK: query: insert overwrite table sc_part partition(ts) select * from sc
@@ -90,15 +84,9 @@ ts=2011-01-11+16:18:26
 PREHOOK: query: select count(*) from sc_part where ts is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sc_part
-PREHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from sc_part where ts is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@sc_part
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 3
diff --git a/ql/src/test/results/clientpositive/partition_special_char.q.out b/ql/src/test/results/clientpositive/partition_special_char.q.out
index 18ca0c30e8..c7a845107c 100644
--- a/ql/src/test/results/clientpositive/partition_special_char.q.out
+++ b/ql/src/test/results/clientpositive/partition_special_char.q.out
@@ -53,16 +53,10 @@ ts=2011-01-11+16%3A18%3A26
 PREHOOK: query: select count(*) from sc_part where ts is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sc_part
-PREHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from sc_part where ts is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@sc_part
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 3
 PREHOOK: query: insert overwrite table sc_part partition(ts) select * from sc
@@ -90,15 +84,9 @@ ts=2011-01-11+16%3A18%3A26
 PREHOOK: query: select count(*) from sc_part where ts is not null
 PREHOOK: type: QUERY
 PREHOOK: Input: default@sc_part
-PREHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-PREHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from sc_part where ts is not null
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@sc_part
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+14%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+15%3A18%3A26
-POSTHOOK: Input: default@sc_part@ts=2011-01-11+16%3A18%3A26
 #### A masked pattern was here ####
 3
diff --git a/ql/src/test/results/clientpositive/partition_timestamp.q.out b/ql/src/test/results/clientpositive/partition_timestamp.q.out
index 566a9fcfbc..b32d98dea7 100644
--- a/ql/src/test/results/clientpositive/partition_timestamp.q.out
+++ b/ql/src/test/results/clientpositive/partition_timestamp.q.out
@@ -109,67 +109,55 @@ PREHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 01:00:00'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 01:00:00'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 10.  Also try with string value in predicate
 select count(*) from partition_timestamp_1 where dt = '2000-01-01 01:00:00'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10.  Also try with string value in predicate
 select count(*) from partition_timestamp_1 where dt = '2000-01-01 01:00:00'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 5
 select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 02:00:00' and region = '2'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 02%3A00%3A00.0/region=2
 #### A masked pattern was here ####
 POSTHOOK: query: -- 5
 select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 02:00:00' and region = '2'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 02%3A00%3A00.0/region=2
 #### A masked pattern was here ####
 5
 PREHOOK: query: -- 11
 select count(*) from partition_timestamp_1 where dt = timestamp '2001-01-01 03:00:00' and region = '10'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 03%3A00%3A00.0/region=10
 #### A masked pattern was here ####
 POSTHOOK: query: -- 11
 select count(*) from partition_timestamp_1 where dt = timestamp '2001-01-01 03:00:00' and region = '10'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 03%3A00%3A00.0/region=10
 #### A masked pattern was here ####
 11
 PREHOOK: query: -- 30
 select count(*) from partition_timestamp_1 where region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 30
 select count(*) from partition_timestamp_1 where region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 30
 PREHOOK: query: -- 0
@@ -200,7 +188,6 @@ PREHOOK: query: -- Try other comparison operations
 select count(*) from partition_timestamp_1 where dt > timestamp '2000-01-01 01:00:00' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- Try other comparison operations
 
@@ -208,72 +195,61 @@ POSTHOOK: query: -- Try other comparison operations
 select count(*) from partition_timestamp_1 where dt > timestamp '2000-01-01 01:00:00' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt < timestamp '2000-01-02 01:00:00' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt < timestamp '2000-01-02 01:00:00' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 20
 select count(*) from partition_timestamp_1 where dt >= timestamp '2000-01-02 01:00:00' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 20
 select count(*) from partition_timestamp_1 where dt >= timestamp '2000-01-02 01:00:00' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt <= timestamp '2000-01-01 01:00:00' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt <= timestamp '2000-01-01 01:00:00' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 20
 select count(*) from partition_timestamp_1 where dt <> timestamp '2000-01-01 01:00:00' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 20
 select count(*) from partition_timestamp_1 where dt <> timestamp '2000-01-01 01:00:00' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 02%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt between timestamp '1999-12-30 12:00:00' and timestamp '2000-01-03 12:00:00' and region = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_timestamp_1 where dt between timestamp '1999-12-30 12:00:00' and timestamp '2000-01-03 12:00:00' and region = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2000-01-01 01%3A00%3A00.0/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- Try a string key with timestamp-like strings
@@ -282,7 +258,6 @@ PREHOOK: query: -- Try a string key with timestamp-like strings
 select count(*) from partition_timestamp_1 where region = '2020-20-20'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 01%3A00%3A00.0/region=2020-20-20
 #### A masked pattern was here ####
 POSTHOOK: query: -- Try a string key with timestamp-like strings
 
@@ -290,20 +265,17 @@ POSTHOOK: query: -- Try a string key with timestamp-like strings
 select count(*) from partition_timestamp_1 where region = '2020-20-20'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 01%3A00%3A00.0/region=2020-20-20
 #### A masked pattern was here ####
 5
 PREHOOK: query: -- 5
 select count(*) from partition_timestamp_1 where region > '2010-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_timestamp_1
-PREHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 01%3A00%3A00.0/region=2020-20-20
 #### A masked pattern was here ####
 POSTHOOK: query: -- 5
 select count(*) from partition_timestamp_1 where region > '2010-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_timestamp_1
-POSTHOOK: Input: default@partition_timestamp_1@dt=2001-01-01 01%3A00%3A00.0/region=2020-20-20
 #### A masked pattern was here ####
 5
 PREHOOK: query: drop table partition_timestamp_1
diff --git a/ql/src/test/results/clientpositive/partition_varchar1.q.out b/ql/src/test/results/clientpositive/partition_varchar1.q.out
index 5b6649f30c..e6e770ac5b 100644
--- a/ql/src/test/results/clientpositive/partition_varchar1.q.out
+++ b/ql/src/test/results/clientpositive/partition_varchar1.q.out
@@ -95,56 +95,44 @@ PREHOOK: query: -- 15
 select count(*) from partition_varchar_1 where dt = '2000-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
-PREHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 POSTHOOK: query: -- 15
 select count(*) from partition_varchar_1 where dt = '2000-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
-POSTHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 15
 PREHOOK: query: -- 5
 select count(*) from partition_varchar_1 where dt = '2000-01-01' and region = 2
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 POSTHOOK: query: -- 5
 select count(*) from partition_varchar_1 where dt = '2000-01-01' and region = 2
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=2
 #### A masked pattern was here ####
 5
 PREHOOK: query: -- 11
 select count(*) from partition_varchar_1 where dt = '2013-08-08' and region = 10
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=10
 #### A masked pattern was here ####
 POSTHOOK: query: -- 11
 select count(*) from partition_varchar_1 where dt = '2013-08-08' and region = 10
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=10
 #### A masked pattern was here ####
 11
 PREHOOK: query: -- 30
 select count(*) from partition_varchar_1 where region = 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
-PREHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 30
 select count(*) from partition_varchar_1 where region = 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
-POSTHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 30
 PREHOOK: query: -- 0
@@ -175,7 +163,6 @@ PREHOOK: query: -- Try other comparison operations
 select count(*) from partition_varchar_1 where dt > '2000-01-01' and region = 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- Try other comparison operations
 
@@ -183,59 +170,50 @@ POSTHOOK: query: -- Try other comparison operations
 select count(*) from partition_varchar_1 where dt > '2000-01-01' and region = 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_varchar_1 where dt < '2000-01-02' and region = 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_varchar_1 where dt < '2000-01-02' and region = 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 20
 select count(*) from partition_varchar_1 where dt >= '2000-01-02' and region = 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 20
 select count(*) from partition_varchar_1 where dt >= '2000-01-02' and region = 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: -- 10
 select count(*) from partition_varchar_1 where dt <= '2000-01-01' and region = 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 10
 select count(*) from partition_varchar_1 where dt <= '2000-01-01' and region = 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2000-01-01/region=1
 #### A masked pattern was here ####
 10
 PREHOOK: query: -- 20
 select count(*) from partition_varchar_1 where dt <> '2000-01-01' and region = 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@partition_varchar_1
-PREHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 POSTHOOK: query: -- 20
 select count(*) from partition_varchar_1 where dt <> '2000-01-01' and region = 1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@partition_varchar_1
-POSTHOOK: Input: default@partition_varchar_1@dt=2013-08-08/region=1
 #### A masked pattern was here ####
 20
 PREHOOK: query: drop table partition_varchar_1
diff --git a/ql/src/test/results/clientpositive/plan_json.q.out b/ql/src/test/results/clientpositive/plan_json.q.out
index 98c6626f9e..e28c239811 100644
--- a/ql/src/test/results/clientpositive/plan_json.q.out
+++ b/ql/src/test/results/clientpositive/plan_json.q.out
@@ -8,4 +8,4 @@ POSTHOOK: query: -- explain plan json:  the query gets the formatted json output
 
 EXPLAIN FORMATTED SELECT count(1) FROM src
 POSTHOOK: type: QUERY
-{"STAGE DEPENDENCIES":{"Stage-1":{"ROOT STAGE":"TRUE"},"Stage-0":{"DEPENDENT STAGES":"Stage-1"}},"STAGE PLANS":{"Stage-1":{"Map Reduce":{"Map Operator Tree:":[{"TableScan":{"alias:":"src","Statistics:":"Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE","children":{"Select Operator":{"Statistics:":"Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE","children":{"Group By Operator":{"aggregations:":["count(1)"],"mode:":"hash","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE","children":{"Reduce Output Operator":{"sort order:":"","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE","value expressions:":"_col0 (type: bigint)"}}}}}}}}],"Reduce Operator Tree:":{"Group By Operator":{"aggregations:":["count(VALUE._col0)"],"mode:":"mergepartial","outputColumnNames:":["_col0"],"Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE","children":{"File Output Operator":{"compressed:":"false","Statistics:":"Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE","table:":{"input format:":"org.apache.hadoop.mapred.SequenceFileInputFormat","output format:":"org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat","serde:":"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"}}}}}}},"Stage-0":{"Fetch Operator":{"limit:":"-1","Processor Tree:":{"ListSink":{}}}}}}
+{"STAGE DEPENDENCIES":{"Stage-0":{"ROOT STAGE":"TRUE"}},"STAGE PLANS":{"Stage-0":{"Fetch Operator":{"limit:":"1","Processor Tree:":{"ListSink":{}}}}}}
diff --git a/ql/src/test/results/clientpositive/ppd_constant_where.q.out b/ql/src/test/results/clientpositive/ppd_constant_where.q.out
index 0cc8a29ef1..b268300048 100644
--- a/ql/src/test/results/clientpositive/ppd_constant_where.q.out
+++ b/ql/src/test/results/clientpositive/ppd_constant_where.q.out
@@ -7,57 +7,21 @@ POSTHOOK: query: -- Test that the partition pruner does not fail when there is a
 EXPLAIN SELECT COUNT(*) FROM srcpart WHERE ds = '2008-04-08' and 'a' = 'a'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: srcpart
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 PREHOOK: query: SELECT COUNT(*) FROM srcpart WHERE ds = '2008-04-08' and 'a' = 'a'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart
-PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
-PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT COUNT(*) FROM srcpart WHERE ds = '2008-04-08' and 'a' = 'a'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart
-POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
-POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 #### A masked pattern was here ####
 1000
diff --git a/ql/src/test/results/clientpositive/rename_partition_location.q.out b/ql/src/test/results/clientpositive/rename_partition_location.q.out
index b83bdd0e47..52a5fd0f6b 100644
--- a/ql/src/test/results/clientpositive/rename_partition_location.q.out
+++ b/ql/src/test/results/clientpositive/rename_partition_location.q.out
@@ -50,7 +50,6 @@ POSTHOOK: Output: default@rename_partition_table@part=2
 PREHOOK: query: SELECT count(*) FROM rename_partition_table where part = '2'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@rename_partition_table
-PREHOOK: Input: default@rename_partition_table@part=2
 #### A masked pattern was here ####
 500
 PREHOOK: query: DROP TABLE rename_partition_table
diff --git a/ql/src/test/results/clientpositive/select_unquote_and.q.out b/ql/src/test/results/clientpositive/select_unquote_and.q.out
index 18e280b368..b82b730927 100644
--- a/ql/src/test/results/clientpositive/select_unquote_and.q.out
+++ b/ql/src/test/results/clientpositive/select_unquote_and.q.out
@@ -33,14 +33,10 @@ POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.Fiel
 PREHOOK: query: SELECT count(*) FROM npe_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@npe_test
-PREHOOK: Input: default@npe_test@ds=2012-12-11
-PREHOOK: Input: default@npe_test@ds=2012-12-12
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT count(*) FROM npe_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@npe_test
-POSTHOOK: Input: default@npe_test@ds=2012-12-11
-POSTHOOK: Input: default@npe_test@ds=2012-12-12
 #### A masked pattern was here ####
 498
 PREHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15
diff --git a/ql/src/test/results/clientpositive/select_unquote_not.q.out b/ql/src/test/results/clientpositive/select_unquote_not.q.out
index 0e5958637c..1d2bc5fa88 100644
--- a/ql/src/test/results/clientpositive/select_unquote_not.q.out
+++ b/ql/src/test/results/clientpositive/select_unquote_not.q.out
@@ -33,14 +33,10 @@ POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.Fiel
 PREHOOK: query: SELECT count(*) FROM npe_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@npe_test
-PREHOOK: Input: default@npe_test@ds=2012-12-11
-PREHOOK: Input: default@npe_test@ds=2012-12-12
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT count(*) FROM npe_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@npe_test
-POSTHOOK: Input: default@npe_test@ds=2012-12-11
-POSTHOOK: Input: default@npe_test@ds=2012-12-12
 #### A masked pattern was here ####
 498
 PREHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE NOT ds < 2012-11-31
diff --git a/ql/src/test/results/clientpositive/select_unquote_or.q.out b/ql/src/test/results/clientpositive/select_unquote_or.q.out
index e1a6b170f4..6bf8430a55 100644
--- a/ql/src/test/results/clientpositive/select_unquote_or.q.out
+++ b/ql/src/test/results/clientpositive/select_unquote_or.q.out
@@ -33,14 +33,10 @@ POSTHOOK: Lineage: npe_test PARTITION(ds=2012-12-12).value SIMPLE [(src)src.Fiel
 PREHOOK: query: SELECT count(*) FROM npe_test
 PREHOOK: type: QUERY
 PREHOOK: Input: default@npe_test
-PREHOOK: Input: default@npe_test@ds=2012-12-11
-PREHOOK: Input: default@npe_test@ds=2012-12-12
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT count(*) FROM npe_test
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@npe_test
-POSTHOOK: Input: default@npe_test@ds=2012-12-11
-POSTHOOK: Input: default@npe_test@ds=2012-12-12
 #### A masked pattern was here ####
 498
 PREHOOK: query: EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out
index 4b29056379..60c329b8bd 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out
@@ -101,12 +101,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table1 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table1
-PREHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table1 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
-POSTHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table1 where ds = '1' and hash(key) % 2 = 0
@@ -156,12 +154,10 @@ POSTHOOK: Input: default@test_table1@ds=1
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 0
@@ -293,12 +289,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=2).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table2 where ds = '2'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=2
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '2'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=2
 #### A masked pattern was here ####
 2
 PREHOOK: query: select count(*) from test_table2 where ds = '2' and hash(key) % 2 = 0
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_19.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_19.q.out
index 95137cd95d..b7d6cdffaa 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_19.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_19.q.out
@@ -101,12 +101,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table1 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table1
-PREHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table1 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
-POSTHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table1 where ds = '1' and hash(key) % 16 = 0
@@ -178,12 +176,10 @@ POSTHOOK: Input: default@test_table1@ds=1
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 where ds = '1' and hash(key) % 16 = 0
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_20.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_20.q.out
index 0324aa47b7..0ad9a12e51 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_20.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_20.q.out
@@ -113,12 +113,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value2 SIMPLE [(test_table1)a.Fie
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 0
@@ -223,12 +221,10 @@ POSTHOOK: Lineage: test_table3 PARTITION(ds=1).value2 SIMPLE [(test_table1)a.Fie
 PREHOOK: query: select count(*) from test_table3 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table3
-PREHOOK: Input: default@test_table3@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table3 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table3
-POSTHOOK: Input: default@test_table3@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table3 where ds = '1' and hash(value1) % 2 = 0
diff --git a/ql/src/test/results/clientpositive/spark/list_bucket_dml_2.q.out b/ql/src/test/results/clientpositive/spark/list_bucket_dml_2.q.out
index fd46200938..dabe1ca732 100644
--- a/ql/src/test/results/clientpositive/spark/list_bucket_dml_2.q.out
+++ b/ql/src/test/results/clientpositive/spark/list_bucket_dml_2.q.out
@@ -306,25 +306,19 @@ Storage Desc Params:
 PREHOOK: query: select count(1) from srcpart where ds = '2008-04-08'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@srcpart
-PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
-PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from srcpart where ds = '2008-04-08'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@srcpart
-POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
-POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 #### A masked pattern was here ####
 1000
 PREHOOK: query: select count(*) from list_bucketing_static_part
 PREHOOK: type: QUERY
 PREHOOK: Input: default@list_bucketing_static_part
-PREHOOK: Input: default@list_bucketing_static_part@ds=2008-04-08/hr=11
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from list_bucketing_static_part
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@list_bucketing_static_part
-POSTHOOK: Input: default@list_bucketing_static_part@ds=2008-04-08/hr=11
 #### A masked pattern was here ####
 1000
 PREHOOK: query: explain extended
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out
index bfdd529fa0..d3494de312 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out
@@ -104,12 +104,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table1 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table1
-PREHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table1 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
-POSTHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table1 where ds = '1' and hash(key) % 2 = 0
@@ -159,12 +157,10 @@ POSTHOOK: Input: default@test_table1@ds=1
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 0
@@ -302,12 +298,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=2).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table2 where ds = '2'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=2
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '2'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=2
 #### A masked pattern was here ####
 2
 PREHOOK: query: select count(*) from test_table2 where ds = '2' and hash(key) % 2 = 0
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_19.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_19.q.out
index 5a47c45478..d09daae4ee 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_19.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_19.q.out
@@ -104,12 +104,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(test_table1)a.Fiel
 PREHOOK: query: select count(*) from test_table1 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table1
-PREHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table1 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
-POSTHOOK: Input: default@test_table1@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table1 where ds = '1' and hash(key) % 16 = 0
@@ -181,12 +179,10 @@ POSTHOOK: Input: default@test_table1@ds=1
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 where ds = '1' and hash(key) % 16 = 0
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_20.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_20.q.out
index e4a9ba107e..8974d140e3 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_20.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_20.q.out
@@ -119,12 +119,10 @@ POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value2 SIMPLE [(test_table1)a.Fie
 PREHOOK: query: select count(*) from test_table2 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table2
-PREHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table2 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table2
-POSTHOOK: Input: default@test_table2@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 0
@@ -232,12 +230,10 @@ POSTHOOK: Lineage: test_table3 PARTITION(ds=1).value2 SIMPLE [(test_table1)a.Fie
 PREHOOK: query: select count(*) from test_table3 where ds = '1'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@test_table3
-PREHOOK: Input: default@test_table3@ds=1
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from test_table3 where ds = '1'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table3
-POSTHOOK: Input: default@test_table3@ds=1
 #### A masked pattern was here ####
 500
 PREHOOK: query: select count(*) from test_table3 where ds = '1' and hash(value1) % 2 = 0
diff --git a/ql/src/test/results/clientpositive/spark/stats3.q.out b/ql/src/test/results/clientpositive/spark/stats3.q.out
index 7db4fa0c58..9200eca9a0 100644
--- a/ql/src/test/results/clientpositive/spark/stats3.q.out
+++ b/ql/src/test/results/clientpositive/spark/stats3.q.out
@@ -132,12 +132,10 @@ POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 PREHOOK: query: select count(1) from hive_test_dst
 PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_test_dst
-PREHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from hive_test_dst
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_test_dst
-POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 6
 PREHOOK: query: insert overwrite table hive_test_dst partition ( pCol1='test_part', pcol2='test_Part') select col1 from hive_test_src
@@ -160,12 +158,10 @@ POSTHOOK: Input: default@hive_test_dst
 PREHOOK: query: select count(1) from hive_test_dst
 PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_test_dst
-PREHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from hive_test_dst
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_test_dst
-POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 6
 PREHOOK: query: select * from hive_test_dst where pcol1='test_part'
diff --git a/ql/src/test/results/clientpositive/spark/stats_noscan_2.q.out b/ql/src/test/results/clientpositive/spark/stats_noscan_2.q.out
index 887b3fb5c2..40b2a66527 100644
--- a/ql/src/test/results/clientpositive/spark/stats_noscan_2.q.out
+++ b/ql/src/test/results/clientpositive/spark/stats_noscan_2.q.out
@@ -152,12 +152,10 @@ POSTHOOK: Lineage: texternal PARTITION(insertdate=2008-01-01).val SIMPLE [(src)s
 PREHOOK: query: select count(*) from texternal where insertdate='2008-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@texternal
-PREHOOK: Input: default@texternal@insertdate=2008-01-01
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from texternal where insertdate='2008-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@texternal
-POSTHOOK: Input: default@texternal@insertdate=2008-01-01
 #### A masked pattern was here ####
 500
 PREHOOK: query: -- create external table
diff --git a/ql/src/test/results/clientpositive/spark/union_view.q.out b/ql/src/test/results/clientpositive/spark/union_view.q.out
index f06b080c5a..892cc6f3e9 100644
--- a/ql/src/test/results/clientpositive/spark/union_view.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_view.q.out
@@ -103,149 +103,32 @@ STAGE PLANS:
 86	val_86	3
 86	val_86	3
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Spark
-      Edges:
-        Reducer 2 <- Map 1 (GROUP, 1)
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: src_union_1
-                  filterExpr: (ds = '1') (type: boolean)
-                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                  Select Operator
-                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                    Group By Operator
-                      aggregations: count(1)
-                      mode: hash
-                      outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                      Reduce Output Operator
-                        sort order: 
-                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: bigint)
-        Reducer 2 
-            Reduce Operator Tree:
-              Group By Operator
-                aggregations: count(VALUE._col0)
-                mode: mergepartial
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Spark
-      Edges:
-        Reducer 2 <- Map 1 (GROUP, 1)
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: src_union_2
-                  filterExpr: (ds = '2') (type: boolean)
-                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                  Select Operator
-                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                    Group By Operator
-                      aggregations: count(1)
-                      mode: hash
-                      outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                      Reduce Output Operator
-                        sort order: 
-                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: bigint)
-        Reducer 2 
-            Reduce Operator Tree:
-              Group By Operator
-                aggregations: count(VALUE._col0)
-                mode: mergepartial
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Spark
-      Edges:
-        Reducer 2 <- Map 1 (GROUP, 1)
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: src_union_3
-                  filterExpr: (ds = '3') (type: boolean)
-                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                  Select Operator
-                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                    Group By Operator
-                      aggregations: count(1)
-                      mode: hash
-                      outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                      Reduce Output Operator
-                        sort order: 
-                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: bigint)
-        Reducer 2 
-            Reduce Operator Tree:
-              Group By Operator
-                aggregations: count(VALUE._col0)
-                mode: mergepartial
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
diff --git a/ql/src/test/results/clientpositive/stats3.q.out b/ql/src/test/results/clientpositive/stats3.q.out
index 7db4fa0c58..9200eca9a0 100644
--- a/ql/src/test/results/clientpositive/stats3.q.out
+++ b/ql/src/test/results/clientpositive/stats3.q.out
@@ -132,12 +132,10 @@ POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 PREHOOK: query: select count(1) from hive_test_dst
 PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_test_dst
-PREHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from hive_test_dst
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_test_dst
-POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 6
 PREHOOK: query: insert overwrite table hive_test_dst partition ( pCol1='test_part', pcol2='test_Part') select col1 from hive_test_src
@@ -160,12 +158,10 @@ POSTHOOK: Input: default@hive_test_dst
 PREHOOK: query: select count(1) from hive_test_dst
 PREHOOK: type: QUERY
 PREHOOK: Input: default@hive_test_dst
-PREHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from hive_test_dst
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@hive_test_dst
-POSTHOOK: Input: default@hive_test_dst@pcol1=test_part/pcol2=test_Part
 #### A masked pattern was here ####
 6
 PREHOOK: query: select * from hive_test_dst where pcol1='test_part'
diff --git a/ql/src/test/results/clientpositive/stats_noscan_2.q.out b/ql/src/test/results/clientpositive/stats_noscan_2.q.out
index 887b3fb5c2..40b2a66527 100644
--- a/ql/src/test/results/clientpositive/stats_noscan_2.q.out
+++ b/ql/src/test/results/clientpositive/stats_noscan_2.q.out
@@ -152,12 +152,10 @@ POSTHOOK: Lineage: texternal PARTITION(insertdate=2008-01-01).val SIMPLE [(src)s
 PREHOOK: query: select count(*) from texternal where insertdate='2008-01-01'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@texternal
-PREHOOK: Input: default@texternal@insertdate=2008-01-01
 #### A masked pattern was here ####
 POSTHOOK: query: select count(*) from texternal where insertdate='2008-01-01'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@texternal
-POSTHOOK: Input: default@texternal@insertdate=2008-01-01
 #### A masked pattern was here ####
 500
 PREHOOK: query: -- create external table
diff --git a/ql/src/test/results/clientpositive/udf_count.q.out b/ql/src/test/results/clientpositive/udf_count.q.out
index 1b37a125fe..4c5c4ec784 100644
--- a/ql/src/test/results/clientpositive/udf_count.q.out
+++ b/ql/src/test/results/clientpositive/udf_count.q.out
@@ -17,46 +17,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT count(key) FROM src
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              expressions: key (type: string)
-              outputColumnNames: key
-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(key)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -190,44 +156,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT count(*) FROM src
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
-            Select Operator
-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
-              Group By Operator
-                aggregations: count()
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
@@ -245,44 +179,12 @@ PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT count(1) FROM src
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
-            Select Operator
-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
diff --git a/ql/src/test/results/clientpositive/union_view.q.out b/ql/src/test/results/clientpositive/union_view.q.out
index 6409d25d8a..2c30f0d0a5 100644
--- a/ql/src/test/results/clientpositive/union_view.q.out
+++ b/ql/src/test/results/clientpositive/union_view.q.out
@@ -112,203 +112,32 @@ STAGE PLANS:
 86	val_86	3
 86	val_86	3
 STAGE DEPENDENCIES:
-  Stage-3 is a root stage
-  Stage-2 depends on stages: Stage-3
-  Stage-1 depends on stages: Stage-2
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-3
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: default__src_union_1_src_union_1_key_idx__
-            filterExpr: (ds = '1') (type: boolean)
-            Select Operator
-              expressions: _bucketname (type: string), _offsets (type: array<bigint>)
-              outputColumnNames: _col0, _col1
-              File Output Operator
-                compressed: false
-                table:
-                    input format: org.apache.hadoop.mapred.TextInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-2
-    Move Operator
-      files:
-          hdfs directory: true
-#### A masked pattern was here ####
-
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src_union_1
-            filterExpr: (ds = '1') (type: boolean)
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 STAGE DEPENDENCIES:
-  Stage-3 is a root stage
-  Stage-2 depends on stages: Stage-3
-  Stage-1 depends on stages: Stage-2
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-3
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: default__src_union_2_src_union_2_key_idx__
-            filterExpr: (ds = '2') (type: boolean)
-            Select Operator
-              expressions: _bucketname (type: string), _offsets (type: array<bigint>)
-              outputColumnNames: _col0, _col1
-              File Output Operator
-                compressed: false
-                table:
-                    input format: org.apache.hadoop.mapred.TextInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-2
-    Move Operator
-      files:
-          hdfs directory: true
-#### A masked pattern was here ####
-
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src_union_2
-            filterExpr: (ds = '2') (type: boolean)
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
 STAGE DEPENDENCIES:
-  Stage-3 is a root stage
-  Stage-2 depends on stages: Stage-3
-  Stage-1 depends on stages: Stage-2
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-3
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: default__src_union_3_src_union_3_key_idx__
-            filterExpr: (ds = '3') (type: boolean)
-            Select Operator
-              expressions: _bucketname (type: string), _offsets (type: array<bigint>)
-              outputColumnNames: _col0, _col1
-              File Output Operator
-                compressed: false
-                table:
-                    input format: org.apache.hadoop.mapred.TextInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
-  Stage: Stage-2
-    Move Operator
-      files:
-          hdfs directory: true
-#### A masked pattern was here ####
-
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: src_union_3
-            filterExpr: (ds = '3') (type: boolean)
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Group By Operator
-                aggregations: count(1)
-                mode: hash
-                outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  sort order: 
-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: bigint)
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations: count(VALUE._col0)
-          mode: mergepartial
-          outputColumnNames: _col0
-          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
-      limit: -1
+      limit: 1
       Processor Tree:
         ListSink
 
diff --git a/ql/src/test/results/clientpositive/updateAccessTime.q.out b/ql/src/test/results/clientpositive/updateAccessTime.q.out
index 05f4d07777..d7e6651724 100644
--- a/ql/src/test/results/clientpositive/updateAccessTime.q.out
+++ b/ql/src/test/results/clientpositive/updateAccessTime.q.out
@@ -147,12 +147,10 @@ hr                  	string
 PREHOOK: query: select count(1) from tstsrcpart where ds = '2008-04-08' and hr = '11'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@tstsrcpart
-PREHOOK: Input: default@tstsrcpart@ds=2008-04-08/hr=11
 #### A masked pattern was here ####
 POSTHOOK: query: select count(1) from tstsrcpart where ds = '2008-04-08' and hr = '11'
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tstsrcpart
-POSTHOOK: Input: default@tstsrcpart@ds=2008-04-08/hr=11
 #### A masked pattern was here ####
 500
 PREHOOK: query: desc extended tstsrcpart
