diff --git a/CHANGES.txt b/CHANGES.txt
index e93113a01b..5be6f73b7b 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -501,6 +501,9 @@ Release 0.5.0 -  Unreleased
     HIVE-1072. Keep CreateTime when a partition is overwritten.
     (Paul Yang via zshao)
 
+    HIVE-1059. Date/DateTime/TimeStamp types should throw an error.
+    (Paul Yang via zshao)
+
 Release 0.4.0 -  Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index b5c8666278..ca954017c1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -76,7 +76,12 @@ public class DDLSemanticAnalyzer extends BaseSemanticAnalyzer {
         .put(HiveParser.TOK_TIMESTAMP, Constants.TIMESTAMP_TYPE_NAME);
   }
 
-  public static String getTypeName(int token) {
+  public static String getTypeName(int token) throws SemanticException {
+    // date, datetime, and timestamp types aren't currently supported
+    if (token == HiveParser.TOK_DATE || token == HiveParser.TOK_DATETIME || 
+        token == HiveParser.TOK_TIMESTAMP ) {
+      throw new SemanticException(ErrorMsg.UNSUPPORTED_TYPE.getMsg());
+    }
     return TokenToTypeName.get(token);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
index df67aa3cdb..194a37c0a8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
@@ -118,7 +118,8 @@ public enum ErrorMsg {
       "The same output cannot be present multiple times: "), INVALID_AS(
       "AS clause has an invalid number of aliases"), VIEW_COL_MISMATCH(
       "The number of columns produced by the SELECT clause does not match the number of column names specified by CREATE VIEW"), DML_AGAINST_VIEW(
-      "A view cannot be used as target table for LOAD or INSERT");
+      "A view cannot be used as target table for LOAD or INSERT"), UNSUPPORTED_TYPE(
+      "DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.");
   private String mesg;
   private String SQLState;
 
diff --git a/ql/src/test/queries/clientnegative/invalid_t_alter1.q b/ql/src/test/queries/clientnegative/invalid_t_alter1.q
new file mode 100644
index 0000000000..181526c7af
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/invalid_t_alter1.q
@@ -0,0 +1,2 @@
+CREATE TABLE alter_test (d STRING);
+ALTER TABLE alter_test CHANGE d d DATE;
diff --git a/ql/src/test/queries/clientnegative/invalid_t_alter2.q b/ql/src/test/queries/clientnegative/invalid_t_alter2.q
new file mode 100644
index 0000000000..a946086422
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/invalid_t_alter2.q
@@ -0,0 +1,2 @@
+CREATE TABLE alter_test (d STRING);
+ALTER TABLE alter_test ADD COLUMNS (ds DATE);
diff --git a/ql/src/test/queries/clientnegative/invalid_t_create1.q b/ql/src/test/queries/clientnegative/invalid_t_create1.q
new file mode 100644
index 0000000000..777e6cc102
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/invalid_t_create1.q
@@ -0,0 +1 @@
+CREATE TABLE date_test (d DATE);
diff --git a/ql/src/test/queries/clientnegative/invalid_t_create2.q b/ql/src/test/queries/clientnegative/invalid_t_create2.q
new file mode 100644
index 0000000000..978f4244a6
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/invalid_t_create2.q
@@ -0,0 +1 @@
+CREATE TABLE datetime_test (d DATETIME);
diff --git a/ql/src/test/queries/clientnegative/invalid_t_create3.q b/ql/src/test/queries/clientnegative/invalid_t_create3.q
new file mode 100644
index 0000000000..ce6b099591
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/invalid_t_create3.q
@@ -0,0 +1 @@
+CREATE TABLE timestamp_test (d TIMESTAMP);
diff --git a/ql/src/test/queries/clientnegative/invalid_t_transform.q b/ql/src/test/queries/clientnegative/invalid_t_transform.q
new file mode 100644
index 0000000000..435ea71c41
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/invalid_t_transform.q
@@ -0,0 +1 @@
+SELECT TRANSFORM(*) USING 'cat' AS (key DATE) FROM src;
diff --git a/ql/src/test/queries/clientpositive/inputddl2.q b/ql/src/test/queries/clientpositive/inputddl2.q
index d56ee0dbbd..8f2dbd5695 100644
--- a/ql/src/test/queries/clientpositive/inputddl2.q
+++ b/ql/src/test/queries/clientpositive/inputddl2.q
@@ -1,6 +1,6 @@
 EXPLAIN
-CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds DATETIME, country STRING) STORED AS TEXTFILE;
-CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds DATETIME, country STRING) STORED AS TEXTFILE;
+CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE;
+CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE;
 DESCRIBE INPUTDDL2;
 DROP TABLE INPUTDDL2;
 
diff --git a/ql/src/test/queries/clientpositive/inputddl4.q b/ql/src/test/queries/clientpositive/inputddl4.q
index 2e4311cf86..bee29bba82 100644
--- a/ql/src/test/queries/clientpositive/inputddl4.q
+++ b/ql/src/test/queries/clientpositive/inputddl4.q
@@ -5,7 +5,7 @@ CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                        friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
                        ip STRING COMMENT 'IP Address of the User') 
     COMMENT 'This is the page view table' 
-    PARTITIONED BY(ds DATETIME, country STRING) 
+    PARTITIONED BY(ds STRING, country STRING) 
     CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS;
 DESCRIBE INPUTDDL4;
 DESCRIBE EXTENDED INPUTDDL4;
diff --git a/ql/src/test/queries/clientpositive/inputddl6.q b/ql/src/test/queries/clientpositive/inputddl6.q
index b9edfcc550..ca605d719d 100644
--- a/ql/src/test/queries/clientpositive/inputddl6.q
+++ b/ql/src/test/queries/clientpositive/inputddl6.q
@@ -2,7 +2,7 @@
 -- test for describe extended table partition
 -- test for alter table drop partition
 DROP TABLE INPUTDDL6;
-CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE;
+CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-09');
 LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-08');
 DESCRIBE EXTENDED INPUTDDL6;
diff --git a/ql/src/test/queries/clientpositive/inputddl8.q b/ql/src/test/queries/clientpositive/inputddl8.q
index ed8bad5229..1ab3d33d58 100644
--- a/ql/src/test/queries/clientpositive/inputddl8.q
+++ b/ql/src/test/queries/clientpositive/inputddl8.q
@@ -1,6 +1,6 @@
 DROP TABLE INPUTDDL8;
 CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
-    PARTITIONED BY(ds DATETIME, country STRING)
+    PARTITIONED BY(ds STRING, country STRING)
     CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'
     WITH SERDEPROPERTIES ('serialization.class' = 'org.apache.hadoop.hive.serde2.thrift.test.Complex',
diff --git a/ql/src/test/queries/clientpositive/show_tables.q b/ql/src/test/queries/clientpositive/show_tables.q
index f432d73eae..9ef67f5092 100644
--- a/ql/src/test/queries/clientpositive/show_tables.q
+++ b/ql/src/test/queries/clientpositive/show_tables.q
@@ -1,5 +1,5 @@
-CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE;
-CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE;
+CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
+CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
 
 EXPLAIN
 SHOW TABLES 'shtb_*';
diff --git a/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out b/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
index 6abef443e2..a4b728f5ca 100644
--- a/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
+++ b/ql/src/test/results/clientnegative/invalid_create_tbl1.q.out
@@ -2,13 +2,4 @@ PREHOOK: query: DROP TABLE inv_valid_tbl1
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE inv_valid_tbl1
 POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE inv_valid_tbl1 COMMENT 'This is a thrift based table'
-    PARTITIONED BY(aint DATETIME, country STRING)
-    CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
-    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'
-    WITH SERDEPROPERTIES ('serialization.class' = 'org.apache.hadoop.hive.serde2.thrift.test.Complex',
-                          'serialization.format' = 'org.apache.thrift.protocol.TBinaryProtocol')
-    STORED AS SEQUENCEFILE
-PREHOOK: type: CREATETABLE
-FAILED: Error in metadata: org.apache.hadoop.hive.ql.metadata.HiveException: Partition column name aint conflicts with table columns.
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientnegative/invalid_t_alter1.q.out b/ql/src/test/results/clientnegative/invalid_t_alter1.q.out
new file mode 100644
index 0000000000..f7ae626a4b
--- /dev/null
+++ b/ql/src/test/results/clientnegative/invalid_t_alter1.q.out
@@ -0,0 +1,6 @@
+PREHOOK: query: CREATE TABLE alter_test (d STRING)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE alter_test (d STRING)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@alter_test
+FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientnegative/invalid_t_alter2.q.out b/ql/src/test/results/clientnegative/invalid_t_alter2.q.out
new file mode 100644
index 0000000000..38568c73d2
--- /dev/null
+++ b/ql/src/test/results/clientnegative/invalid_t_alter2.q.out
@@ -0,0 +1,4 @@
+PREHOOK: query: CREATE TABLE alter_test (d STRING)
+PREHOOK: type: CREATETABLE
+FAILED: Error in metadata: AlreadyExistsException(message:Table alter_test already exists)
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
diff --git a/ql/src/test/results/clientnegative/invalid_t_create1.q.out b/ql/src/test/results/clientnegative/invalid_t_create1.q.out
new file mode 100644
index 0000000000..d091d8ca43
--- /dev/null
+++ b/ql/src/test/results/clientnegative/invalid_t_create1.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientnegative/invalid_t_create2.q.out b/ql/src/test/results/clientnegative/invalid_t_create2.q.out
new file mode 100644
index 0000000000..d091d8ca43
--- /dev/null
+++ b/ql/src/test/results/clientnegative/invalid_t_create2.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientnegative/invalid_t_create3.q.out b/ql/src/test/results/clientnegative/invalid_t_create3.q.out
new file mode 100644
index 0000000000..d091d8ca43
--- /dev/null
+++ b/ql/src/test/results/clientnegative/invalid_t_create3.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientnegative/invalid_t_transform.q.out b/ql/src/test/results/clientnegative/invalid_t_transform.q.out
new file mode 100644
index 0000000000..d091d8ca43
--- /dev/null
+++ b/ql/src/test/results/clientnegative/invalid_t_transform.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: DATE, DATETIME, and TIMESTAMP types aren't supported yet. Please use STRING instead.
diff --git a/ql/src/test/results/clientpositive/inputddl2.q.out b/ql/src/test/results/clientpositive/inputddl2.q.out
index 85a694175c..64273a7fd2 100644
--- a/ql/src/test/results/clientpositive/inputddl2.q.out
+++ b/ql/src/test/results/clientpositive/inputddl2.q.out
@@ -1,11 +1,11 @@
 PREHOOK: query: EXPLAIN
-CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds DATETIME, country STRING) STORED AS TEXTFILE
+CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: EXPLAIN
-CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds DATETIME, country STRING) STORED AS TEXTFILE
+CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 ABSTRACT SYNTAX TREE:
-  (TOK_CREATETABLE INPUTDDL2 TOK_LIKETABLE (TOK_TABCOLLIST (TOK_TABCOL key TOK_INT) (TOK_TABCOL value TOK_STRING)) (TOK_TABLEPARTCOLS (TOK_TABCOLLIST (TOK_TABCOL ds TOK_DATETIME) (TOK_TABCOL country TOK_STRING))) TOK_TBLTEXTFILE)
+  (TOK_CREATETABLE INPUTDDL2 TOK_LIKETABLE (TOK_TABCOLLIST (TOK_TABCOL key TOK_INT) (TOK_TABCOL value TOK_STRING)) (TOK_TABLEPARTCOLS (TOK_TABCOLLIST (TOK_TABCOL ds TOK_STRING) (TOK_TABCOL country TOK_STRING))) TOK_TBLTEXTFILE)
 
 STAGE DEPENDENCIES:
   Stage-0 is a root stage
@@ -19,14 +19,14 @@ STAGE PLANS:
           input format: org.apache.hadoop.mapred.TextInputFormat
           # buckets: -1
           output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-          partition columns: ds datetime, country string
+          partition columns: ds string, country string
           name: INPUTDDL2
           isExternal: false
 
 
-PREHOOK: query: CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds DATETIME, country STRING) STORED AS TEXTFILE
+PREHOOK: query: CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds DATETIME, country STRING) STORED AS TEXTFILE
+POSTHOOK: query: CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@INPUTDDL2
 PREHOOK: query: DESCRIBE INPUTDDL2
@@ -35,7 +35,7 @@ POSTHOOK: query: DESCRIBE INPUTDDL2
 POSTHOOK: type: DESCTABLE
 key	int	
 value	string	
-ds	datetime	
+ds	string	
 country	string	
 PREHOOK: query: DROP TABLE INPUTDDL2
 PREHOOK: type: DROPTABLE
diff --git a/ql/src/test/results/clientpositive/inputddl4.q.out b/ql/src/test/results/clientpositive/inputddl4.q.out
index 8236eea17c..a35f655599 100644
--- a/ql/src/test/results/clientpositive/inputddl4.q.out
+++ b/ql/src/test/results/clientpositive/inputddl4.q.out
@@ -9,7 +9,7 @@ PREHOOK: query: CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                        friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
                        ip STRING COMMENT 'IP Address of the User') 
     COMMENT 'This is the page view table' 
-    PARTITIONED BY(ds DATETIME, country STRING) 
+    PARTITIONED BY(ds STRING, country STRING) 
     CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
@@ -17,7 +17,7 @@ POSTHOOK: query: CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                        friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
                        ip STRING COMMENT 'IP Address of the User') 
     COMMENT 'This is the page view table' 
-    PARTITIONED BY(ds DATETIME, country STRING) 
+    PARTITIONED BY(ds STRING, country STRING) 
     CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@INPUTDDL4
@@ -32,7 +32,7 @@ referrer_url	string
 friends	array<bigint>	
 properties	map<string,string>	
 ip	string	IP Address of the User
-ds	datetime	
+ds	string	
 country	string	
 PREHOOK: query: DESCRIBE EXTENDED INPUTDDL4
 PREHOOK: type: DESCTABLE
@@ -45,10 +45,10 @@ referrer_url	string
 friends	array<bigint>	
 properties	map<string,string>	
 ip	string	IP Address of the User
-ds	datetime	
+ds	string	
 country	string	
 	 	 
-Detailed Table Information	Table(tableName:inputddl4, dbName:default, owner:njain, createTime:1253780744, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:viewtime, type:string, comment:null), FieldSchema(name:userid, type:int, comment:null), FieldSchema(name:page_url, type:string, comment:null), FieldSchema(name:referrer_url, type:string, comment:null), FieldSchema(name:friends, type:array<bigint>, comment:null), FieldSchema(name:properties, type:map<string,string>, comment:null), FieldSchema(name:ip, type:string, comment:IP Address of the User)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/inputddl4, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[userid], sortCols:[Order(col:viewtime, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:datetime, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{comment=This is the page view table})	
+Detailed Table Information	Table(tableName:inputddl4, dbName:default, owner:pyang, createTime:1264208851, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:viewtime, type:string, comment:null), FieldSchema(name:userid, type:int, comment:null), FieldSchema(name:page_url, type:string, comment:null), FieldSchema(name:referrer_url, type:string, comment:null), FieldSchema(name:friends, type:array<bigint>, comment:null), FieldSchema(name:properties, type:map<string,string>, comment:null), FieldSchema(name:ip, type:string, comment:IP Address of the User)], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl4, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[userid], sortCols:[Order(col:viewtime, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{transient_lastDdlTime=1264208851,comment=This is the page view table}, viewOriginalText:null, viewExpandedText:null)	
 PREHOOK: query: DROP TABLE INPUTDDL4
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE INPUTDDL4
diff --git a/ql/src/test/results/clientpositive/inputddl6.q.out b/ql/src/test/results/clientpositive/inputddl6.q.out
index e05f9357ce..5a89a3d73d 100644
--- a/ql/src/test/results/clientpositive/inputddl6.q.out
+++ b/ql/src/test/results/clientpositive/inputddl6.q.out
@@ -8,9 +8,9 @@ POSTHOOK: query: -- test for describe extended table
 -- test for alter table drop partition
 DROP TABLE INPUTDDL6
 POSTHOOK: type: DROPTABLE
-PREHOOK: query: CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE
+PREHOOK: query: CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE
+POSTHOOK: query: CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@INPUTDDL6
 PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-09')
@@ -29,18 +29,18 @@ POSTHOOK: query: DESCRIBE EXTENDED INPUTDDL6
 POSTHOOK: type: DESCTABLE
 key	string	
 value	string	
-ds	datetime	
+ds	string	
 	 	 
-Detailed Table Information	Table(tableName:inputddl6, dbName:default, owner:njain, createTime:1253780755, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/inputddl6, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:datetime, comment:null)], parameters:{})	
+Detailed Table Information	Table(tableName:inputddl6, dbName:default, owner:pyang, createTime:1264209075, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl6, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null)], parameters:{transient_lastDdlTime=1264209075}, viewOriginalText:null, viewExpandedText:null)	
 PREHOOK: query: DESCRIBE EXTENDED INPUTDDL6 PARTITION (ds='2008-04-08')
 PREHOOK: type: DESCTABLE
 POSTHOOK: query: DESCRIBE EXTENDED INPUTDDL6 PARTITION (ds='2008-04-08')
 POSTHOOK: type: DESCTABLE
 key	string	
 value	string	
-ds	datetime	
+ds	string	
 	 	 
-Detailed Partition Information	Partition(values:[2008-04-08], dbName:default, tableName:inputddl6, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/inputddl6/ds=2008-04-08, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{})	
+Detailed Partition Information	Partition(values:[2008-04-08], dbName:default, tableName:inputddl6, createTime:1264209075, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:key, type:string, comment:null), FieldSchema(name:value, type:string, comment:null)], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl6/ds=2008-04-08, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}), parameters:{transient_lastDdlTime=1264209075})	
 PREHOOK: query: SHOW PARTITIONS INPUTDDL6
 PREHOOK: type: SHOWPARTITIONS
 POSTHOOK: query: SHOW PARTITIONS INPUTDDL6
diff --git a/ql/src/test/results/clientpositive/inputddl8.q.out b/ql/src/test/results/clientpositive/inputddl8.q.out
index 5c5abf34a0..f8ad16cb8a 100644
--- a/ql/src/test/results/clientpositive/inputddl8.q.out
+++ b/ql/src/test/results/clientpositive/inputddl8.q.out
@@ -3,7 +3,7 @@ PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE INPUTDDL8
 POSTHOOK: type: DROPTABLE
 PREHOOK: query: CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
-    PARTITIONED BY(ds DATETIME, country STRING)
+    PARTITIONED BY(ds STRING, country STRING)
     CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'
     WITH SERDEPROPERTIES ('serialization.class' = 'org.apache.hadoop.hive.serde2.thrift.test.Complex',
@@ -11,7 +11,7 @@ PREHOOK: query: CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
     STORED AS SEQUENCEFILE
 PREHOOK: type: CREATETABLE
 POSTHOOK: query: CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
-    PARTITIONED BY(ds DATETIME, country STRING)
+    PARTITIONED BY(ds STRING, country STRING)
     CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
     ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'
     WITH SERDEPROPERTIES ('serialization.class' = 'org.apache.hadoop.hive.serde2.thrift.test.Complex',
@@ -29,10 +29,10 @@ lint	array<int>	from deserializer
 lstring	array<string>	from deserializer
 lintstring	array<org.apache.hadoop.hive.serde2.thrift.test.IntString>	from deserializer
 mstringstring	map<string,string>	from deserializer
-ds	datetime	
+ds	string	
 country	string	
 	 	 
-Detailed Table Information	Table(tableName:inputddl8, dbName:default, owner:njain, createTime:1253780774, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[], location:file:/data/users/njain/hive5/hive5/build/ql/test/data/warehouse/inputddl8, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex,serialization.format=com.facebook.thrift.protocol.TBinaryProtocol}), bucketCols:[aint], sortCols:[Order(col:lint, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:datetime, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{comment=This is a thrift based table})	
+Detailed Table Information	Table(tableName:inputddl8, dbName:default, owner:pyang, createTime:1264209638, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[], location:file:/data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/test/data/warehouse/inputddl8, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:32, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer, parameters:{serialization.class=org.apache.hadoop.hive.serde2.thrift.test.Complex,serialization.format=com.facebook.thrift.protocol.TBinaryProtocol}), bucketCols:[aint], sortCols:[Order(col:lint, order:1)], parameters:{}), partitionKeys:[FieldSchema(name:ds, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], parameters:{transient_lastDdlTime=1264209638,comment=This is a thrift based table}, viewOriginalText:null, viewExpandedText:null)	
 PREHOOK: query: DROP TABLE INPUTDDL8
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE INPUTDDL8
diff --git a/ql/src/test/results/clientpositive/show_tables.q.out b/ql/src/test/results/clientpositive/show_tables.q.out
index bd3162b44d..77f98b5660 100644
--- a/ql/src/test/results/clientpositive/show_tables.q.out
+++ b/ql/src/test/results/clientpositive/show_tables.q.out
@@ -1,11 +1,11 @@
-PREHOOK: query: CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE
+PREHOOK: query: CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE
+POSTHOOK: query: CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@shtb_test1
-PREHOOK: query: CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE
+PREHOOK: query: CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 PREHOOK: type: CREATETABLE
-POSTHOOK: query: CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds DATETIME) STORED AS TEXTFILE
+POSTHOOK: query: CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@shtb_test2
 PREHOOK: query: EXPLAIN
