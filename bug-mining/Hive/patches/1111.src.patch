diff --git a/contrib/src/test/queries/clientpositive/udaf_example_avg.q b/contrib/src/test/queries/clientpositive/udaf_example_avg.q
index cdfd7aae01..06a7d789c4 100644
--- a/contrib/src/test/queries/clientpositive/udaf_example_avg.q
+++ b/contrib/src/test/queries/clientpositive/udaf_example_avg.q
@@ -1,14 +1,14 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_avg AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg';
-
-EXPLAIN
-SELECT example_avg(substr(value,5)),
-       example_avg(IF(substr(value,5) > 250, NULL, substr(value,5)))
-FROM src;
-
-SELECT example_avg(substr(value,5)),
-       example_avg(IF(substr(value,5) > 250, NULL, substr(value,5)))
-FROM src;
-
-DROP TEMPORARY FUNCTION example_avg;
+
+CREATE TEMPORARY FUNCTION example_avg AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg';
+
+EXPLAIN
+SELECT example_avg(substr(value,5)),
+       example_avg(IF(substr(value,5) > 250, NULL, substr(value,5)))
+FROM src;
+
+SELECT example_avg(substr(value,5)),
+       example_avg(IF(substr(value,5) > 250, NULL, substr(value,5)))
+FROM src;
+
+DROP TEMPORARY FUNCTION example_avg;
diff --git a/contrib/src/test/queries/clientpositive/udaf_example_group_concat.q b/contrib/src/test/queries/clientpositive/udaf_example_group_concat.q
index 0317f99c39..1993314461 100644
--- a/contrib/src/test/queries/clientpositive/udaf_example_group_concat.q
+++ b/contrib/src/test/queries/clientpositive/udaf_example_group_concat.q
@@ -1,15 +1,15 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_group_concat AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat';
-
-EXPLAIN
-SELECT substr(value,5,1), example_group_concat("(", key, ":", value, ")")
-FROM src
-GROUP BY substr(value,5,1);
-
-SELECT substr(value,5,1), example_group_concat("(", key, ":", value, ")")
-FROM src
-GROUP BY substr(value,5,1);
-
-
-DROP TEMPORARY FUNCTION example_group_concat;
+
+CREATE TEMPORARY FUNCTION example_group_concat AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat';
+
+EXPLAIN
+SELECT substr(value,5,1), example_group_concat("(", key, ":", value, ")")
+FROM src
+GROUP BY substr(value,5,1);
+
+SELECT substr(value,5,1), example_group_concat("(", key, ":", value, ")")
+FROM src
+GROUP BY substr(value,5,1);
+
+
+DROP TEMPORARY FUNCTION example_group_concat;
diff --git a/contrib/src/test/queries/clientpositive/udaf_example_max.q b/contrib/src/test/queries/clientpositive/udaf_example_max.q
index 118b81136c..9cd8514d50 100644
--- a/contrib/src/test/queries/clientpositive/udaf_example_max.q
+++ b/contrib/src/test/queries/clientpositive/udaf_example_max.q
@@ -1,16 +1,16 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_max AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax';
-
-DESCRIBE FUNCTION EXTENDED example_max;
-
-EXPLAIN
-SELECT example_max(substr(value,5)),
-       example_max(IF(substr(value,5) > 250, NULL, substr(value,5)))
-FROM src;
-
-SELECT example_max(substr(value,5)),
-       example_max(IF(substr(value,5) > 250, NULL, substr(value,5)))
-FROM src;
-
-DROP TEMPORARY FUNCTION example_max;
+
+CREATE TEMPORARY FUNCTION example_max AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax';
+
+DESCRIBE FUNCTION EXTENDED example_max;
+
+EXPLAIN
+SELECT example_max(substr(value,5)),
+       example_max(IF(substr(value,5) > 250, NULL, substr(value,5)))
+FROM src;
+
+SELECT example_max(substr(value,5)),
+       example_max(IF(substr(value,5) > 250, NULL, substr(value,5)))
+FROM src;
+
+DROP TEMPORARY FUNCTION example_max;
diff --git a/contrib/src/test/queries/clientpositive/udaf_example_max_n.q b/contrib/src/test/queries/clientpositive/udaf_example_max_n.q
index fb715450d4..2bab538719 100644
--- a/contrib/src/test/queries/clientpositive/udaf_example_max_n.q
+++ b/contrib/src/test/queries/clientpositive/udaf_example_max_n.q
@@ -1,14 +1,14 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_max_n AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxN';
-
-EXPLAIN
-SELECT example_max_n(substr(value,5),10),
-       example_max_n(IF(substr(value,5) > 250, NULL, substr(value,5)),10)
-FROM src;
-
-SELECT example_max_n(substr(value,5),10),
-       example_max_n(IF(substr(value,5) > 250, NULL, substr(value,5)),10)
-FROM src;
-
-DROP TEMPORARY FUNCTION example_max_n;
+
+CREATE TEMPORARY FUNCTION example_max_n AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxN';
+
+EXPLAIN
+SELECT example_max_n(substr(value,5),10),
+       example_max_n(IF(substr(value,5) > 250, NULL, substr(value,5)),10)
+FROM src;
+
+SELECT example_max_n(substr(value,5),10),
+       example_max_n(IF(substr(value,5) > 250, NULL, substr(value,5)),10)
+FROM src;
+
+DROP TEMPORARY FUNCTION example_max_n;
diff --git a/contrib/src/test/queries/clientpositive/udaf_example_min.q b/contrib/src/test/queries/clientpositive/udaf_example_min.q
index e9236de9a5..96568a8dcf 100644
--- a/contrib/src/test/queries/clientpositive/udaf_example_min.q
+++ b/contrib/src/test/queries/clientpositive/udaf_example_min.q
@@ -1,16 +1,16 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_min AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin';
-
-DESCRIBE FUNCTION EXTENDED example_min;
-
-EXPLAIN
-SELECT example_min(substr(value,5)),
-       example_min(IF(substr(value,5) > 250, NULL, substr(value,5)))
-FROM src;
-
-SELECT example_min(substr(value,5)),
-       example_min(IF(substr(value,5) > 250, NULL, substr(value,5)))
-FROM src;
-
-DROP TEMPORARY FUNCTION example_min;
+
+CREATE TEMPORARY FUNCTION example_min AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin';
+
+DESCRIBE FUNCTION EXTENDED example_min;
+
+EXPLAIN
+SELECT example_min(substr(value,5)),
+       example_min(IF(substr(value,5) > 250, NULL, substr(value,5)))
+FROM src;
+
+SELECT example_min(substr(value,5)),
+       example_min(IF(substr(value,5) > 250, NULL, substr(value,5)))
+FROM src;
+
+DROP TEMPORARY FUNCTION example_min;
diff --git a/contrib/src/test/queries/clientpositive/udaf_example_min_n.q b/contrib/src/test/queries/clientpositive/udaf_example_min_n.q
index 63e5267a79..32ed76ef87 100644
--- a/contrib/src/test/queries/clientpositive/udaf_example_min_n.q
+++ b/contrib/src/test/queries/clientpositive/udaf_example_min_n.q
@@ -1,13 +1,13 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-CREATE TEMPORARY FUNCTION example_min_n AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMinN';
-
-EXPLAIN
-SELECT example_min_n(substr(value,5),10),
-       example_min_n(IF(substr(value,5) < 250, NULL, substr(value,5)),10)
-FROM src;
-
-SELECT example_min_n(substr(value,5),10),
-       example_min_n(IF(substr(value,5) < 250, NULL, substr(value,5)),10)
-FROM src;
-
-DROP TEMPORARY FUNCTION example_min_n;
+CREATE TEMPORARY FUNCTION example_min_n AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMinN';
+
+EXPLAIN
+SELECT example_min_n(substr(value,5),10),
+       example_min_n(IF(substr(value,5) < 250, NULL, substr(value,5)),10)
+FROM src;
+
+SELECT example_min_n(substr(value,5),10),
+       example_min_n(IF(substr(value,5) < 250, NULL, substr(value,5)),10)
+FROM src;
+
+DROP TEMPORARY FUNCTION example_min_n;
diff --git a/contrib/src/test/queries/clientpositive/udf_example_add.q b/contrib/src/test/queries/clientpositive/udf_example_add.q
index 8f10ce2c80..284e8f03e4 100644
--- a/contrib/src/test/queries/clientpositive/udf_example_add.q
+++ b/contrib/src/test/queries/clientpositive/udf_example_add.q
@@ -1,24 +1,24 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_add AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd';
-
-EXPLAIN
-SELECT example_add(1, 2),
-       example_add(1, 2, 3),
-       example_add(1, 2, 3, 4),
-       example_add(1.1, 2.2),
-       example_add(1.1, 2.2, 3.3),
-       example_add(1.1, 2.2, 3.3, 4.4),
-       example_add(1, 2, 3, 4.4)
-FROM src LIMIT 1;
-
-SELECT example_add(1, 2),
-       example_add(1, 2, 3),
-       example_add(1, 2, 3, 4),
-       example_add(1.1, 2.2),
-       example_add(1.1, 2.2, 3.3),
-       example_add(1.1, 2.2, 3.3, 4.4),
-       example_add(1, 2, 3, 4.4)
-FROM src LIMIT 1;
-
-DROP TEMPORARY FUNCTION example_add;
+
+CREATE TEMPORARY FUNCTION example_add AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd';
+
+EXPLAIN
+SELECT example_add(1, 2),
+       example_add(1, 2, 3),
+       example_add(1, 2, 3, 4),
+       example_add(1.1, 2.2),
+       example_add(1.1, 2.2, 3.3),
+       example_add(1.1, 2.2, 3.3, 4.4),
+       example_add(1, 2, 3, 4.4)
+FROM src LIMIT 1;
+
+SELECT example_add(1, 2),
+       example_add(1, 2, 3),
+       example_add(1, 2, 3, 4),
+       example_add(1.1, 2.2),
+       example_add(1.1, 2.2, 3.3),
+       example_add(1.1, 2.2, 3.3, 4.4),
+       example_add(1, 2, 3, 4.4)
+FROM src LIMIT 1;
+
+DROP TEMPORARY FUNCTION example_add;
diff --git a/contrib/src/test/queries/clientpositive/udf_example_arraymapstruct.q b/contrib/src/test/queries/clientpositive/udf_example_arraymapstruct.q
index 5d84533ddf..5565be30ca 100644
--- a/contrib/src/test/queries/clientpositive/udf_example_arraymapstruct.q
+++ b/contrib/src/test/queries/clientpositive/udf_example_arraymapstruct.q
@@ -1,16 +1,16 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_arraysum    AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleArraySum';
-CREATE TEMPORARY FUNCTION example_mapconcat   AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleMapConcat';
-CREATE TEMPORARY FUNCTION example_structprint AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleStructPrint';
-
-EXPLAIN
-SELECT example_arraysum(lint), example_mapconcat(mstringstring), example_structprint(lintstring[0])
-FROM src_thrift;
-
-SELECT example_arraysum(lint), example_mapconcat(mstringstring), example_structprint(lintstring[0])
-FROM src_thrift;
-
-DROP TEMPORARY FUNCTION example_arraysum;
-DROP TEMPORARY FUNCTION example_mapconcat;
-DROP TEMPORARY FUNCTION example_structprint;
+
+CREATE TEMPORARY FUNCTION example_arraysum    AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleArraySum';
+CREATE TEMPORARY FUNCTION example_mapconcat   AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleMapConcat';
+CREATE TEMPORARY FUNCTION example_structprint AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleStructPrint';
+
+EXPLAIN
+SELECT example_arraysum(lint), example_mapconcat(mstringstring), example_structprint(lintstring[0])
+FROM src_thrift;
+
+SELECT example_arraysum(lint), example_mapconcat(mstringstring), example_structprint(lintstring[0])
+FROM src_thrift;
+
+DROP TEMPORARY FUNCTION example_arraysum;
+DROP TEMPORARY FUNCTION example_mapconcat;
+DROP TEMPORARY FUNCTION example_structprint;
diff --git a/contrib/src/test/queries/clientpositive/udf_example_format.q b/contrib/src/test/queries/clientpositive/udf_example_format.q
index 09c4e69d4c..589b1c8a76 100644
--- a/contrib/src/test/queries/clientpositive/udf_example_format.q
+++ b/contrib/src/test/queries/clientpositive/udf_example_format.q
@@ -1,18 +1,18 @@
 add jar ${system:build.dir}/hive-contrib-${system:hive.version}.jar;
-
-CREATE TEMPORARY FUNCTION example_format AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleFormat';
-
-EXPLAIN
-SELECT example_format("abc"),
-       example_format("%1$s", 1.1),
-       example_format("%1$s %2$e", 1.1, 1.2),
-       example_format("%1$x %2$o %3$d", 10, 10, 10)
-FROM src LIMIT 1;
-
-SELECT example_format("abc"),
-       example_format("%1$s", 1.1),
-       example_format("%1$s %2$e", 1.1, 1.2),
-       example_format("%1$x %2$o %3$d", 10, 10, 10)
-FROM src LIMIT 1;
-
-DROP TEMPORARY FUNCTION example_format;
+
+CREATE TEMPORARY FUNCTION example_format AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleFormat';
+
+EXPLAIN
+SELECT example_format("abc"),
+       example_format("%1$s", 1.1),
+       example_format("%1$s %2$e", 1.1, 1.2),
+       example_format("%1$x %2$o %3$d", 10, 10, 10)
+FROM src LIMIT 1;
+
+SELECT example_format("abc"),
+       example_format("%1$s", 1.1),
+       example_format("%1$s %2$e", 1.1, 1.2),
+       example_format("%1$x %2$o %3$d", 10, 10, 10)
+FROM src LIMIT 1;
+
+DROP TEMPORARY FUNCTION example_format;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveKey.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveKey.java
index 496bb63239..24896fc798 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveKey.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveKey.java
@@ -1,73 +1,73 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.io;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.WritableComparator;
-
-/**
- * HiveKey is a simple wrapper on Text which allows us to set the hashCode
- * easily. hashCode is used for hadoop partitioner.
- */
-public class HiveKey extends BytesWritable {
-
-  private static final int LENGTH_BYTES = 4;
-
-  boolean hashCodeValid;
-
-  public HiveKey() {
-    hashCodeValid = false;
-  }
-
-  protected int myHashCode;
-
-  public void setHashCode(int myHashCode) {
-    hashCodeValid = true;
-    this.myHashCode = myHashCode;
-  }
-
-  @Override
-  public int hashCode() {
-    if (!hashCodeValid) {
-      throw new RuntimeException("Cannot get hashCode() from deserialized "
-          + HiveKey.class);
-    }
-    return myHashCode;
-  }
-
-  /** A Comparator optimized for HiveKey. */
-  public static class Comparator extends WritableComparator {
-    public Comparator() {
-      super(HiveKey.class);
-    }
-
-    /**
-     * Compare the buffers in serialized form.
-     */
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      return compareBytes(b1, s1 + LENGTH_BYTES, l1 - LENGTH_BYTES, b2, s2
-          + LENGTH_BYTES, l2 - LENGTH_BYTES);
-    }
-  }
-
-  static {
-    WritableComparator.define(HiveKey.class, new Comparator());
-  }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.io;
+
+import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.WritableComparator;
+
+/**
+ * HiveKey is a simple wrapper on Text which allows us to set the hashCode
+ * easily. hashCode is used for hadoop partitioner.
+ */
+public class HiveKey extends BytesWritable {
+
+  private static final int LENGTH_BYTES = 4;
+
+  boolean hashCodeValid;
+
+  public HiveKey() {
+    hashCodeValid = false;
+  }
+
+  protected int myHashCode;
+
+  public void setHashCode(int myHashCode) {
+    hashCodeValid = true;
+    this.myHashCode = myHashCode;
+  }
+
+  @Override
+  public int hashCode() {
+    if (!hashCodeValid) {
+      throw new RuntimeException("Cannot get hashCode() from deserialized "
+          + HiveKey.class);
+    }
+    return myHashCode;
+  }
+
+  /** A Comparator optimized for HiveKey. */
+  public static class Comparator extends WritableComparator {
+    public Comparator() {
+      super(HiveKey.class);
+    }
+
+    /**
+     * Compare the buffers in serialized form.
+     */
+    @Override
+    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
+      return compareBytes(b1, s1 + LENGTH_BYTES, l1 - LENGTH_BYTES, b2, s2
+          + LENGTH_BYTES, l2 - LENGTH_BYTES);
+    }
+  }
+
+  static {
+    WritableComparator.define(HiveKey.class, new Comparator());
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java
index 030ea1cefd..1b5fb9e467 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/RenamePartitionDesc.java
@@ -1,136 +1,136 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.ql.plan;
-
-import java.io.Serializable;
-import java.util.LinkedHashMap;
-import java.util.Map;
-
-/**
- * Contains the information needed to rename a partition.
- */
-public class RenamePartitionDesc extends DDLDesc implements Serializable {
-
-  private static final long serialVersionUID = 1L;
-
-  String tableName;
-  String dbName;
-  String location;
-  LinkedHashMap<String, String> oldPartSpec;
-  LinkedHashMap<String, String> newPartSpec;
-
-  /**
-   * For serialization only.
-   */
-  public RenamePartitionDesc() {
-  }
-
-  /**
-   * @param dbName
-   *          database to add to.
-   * @param tableName
-   *          table to add to.
-   * @param oldPartSpec
-   *          old partition specification.
-   * @param newPartSpec
-   *          new partition specification.
-   */
-  public RenamePartitionDesc(String dbName, String tableName,
-      Map<String, String> oldPartSpec, Map<String, String> newPartSpec) {
-    super();
-    this.dbName = dbName;
-    this.tableName = tableName;
-    this.oldPartSpec = new LinkedHashMap<String,String>(oldPartSpec);
-    this.newPartSpec = new LinkedHashMap<String,String>(newPartSpec);
-  }
-
-  /**
-   * @return database name
-   */
-  public String getDbName() {
-    return dbName;
-  }
-
-  /**
-   * @param dbName
-   *          database name
-   */
-  public void setDbName(String dbName) {
-    this.dbName = dbName;
-  }
-
-  /**
-   * @return the table we're going to add the partitions to.
-   */
-  public String getTableName() {
-    return tableName;
-  }
-
-  /**
-   * @param tableName
-   *          the table we're going to add the partitions to.
-   */
-  public void setTableName(String tableName) {
-    this.tableName = tableName;
-  }
-
-  /**
-   * @return location of partition in relation to table
-   */
-  public String getLocation() {
-    return location;
-  }
-
-  /**
-   * @param location
-   *          location of partition in relation to table
-   */
-  public void setLocation(String location) {
-    this.location = location;
-  }
-
-  /**
-   * @return old partition specification.
-   */
-  public LinkedHashMap<String, String> getOldPartSpec() {
-    return oldPartSpec;
-  }
-
-  /**
-   * @param partSpec
-   *          partition specification
-   */
-  public void setOldPartSpec(LinkedHashMap<String, String> partSpec) {
-    this.oldPartSpec = partSpec;
-  }
-
-  /**
-   * @return new partition specification.
-   */
-  public LinkedHashMap<String, String> getNewPartSpec() {
-    return newPartSpec;
-  }
-
-  /**
-   * @param partSpec
-   *          partition specification
-   */
-  public void setNewPartSpec(LinkedHashMap<String, String> partSpec) {
-    this.newPartSpec = partSpec;
-  }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.plan;
+
+import java.io.Serializable;
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+/**
+ * Contains the information needed to rename a partition.
+ */
+public class RenamePartitionDesc extends DDLDesc implements Serializable {
+
+  private static final long serialVersionUID = 1L;
+
+  String tableName;
+  String dbName;
+  String location;
+  LinkedHashMap<String, String> oldPartSpec;
+  LinkedHashMap<String, String> newPartSpec;
+
+  /**
+   * For serialization only.
+   */
+  public RenamePartitionDesc() {
+  }
+
+  /**
+   * @param dbName
+   *          database to add to.
+   * @param tableName
+   *          table to add to.
+   * @param oldPartSpec
+   *          old partition specification.
+   * @param newPartSpec
+   *          new partition specification.
+   */
+  public RenamePartitionDesc(String dbName, String tableName,
+      Map<String, String> oldPartSpec, Map<String, String> newPartSpec) {
+    super();
+    this.dbName = dbName;
+    this.tableName = tableName;
+    this.oldPartSpec = new LinkedHashMap<String,String>(oldPartSpec);
+    this.newPartSpec = new LinkedHashMap<String,String>(newPartSpec);
+  }
+
+  /**
+   * @return database name
+   */
+  public String getDbName() {
+    return dbName;
+  }
+
+  /**
+   * @param dbName
+   *          database name
+   */
+  public void setDbName(String dbName) {
+    this.dbName = dbName;
+  }
+
+  /**
+   * @return the table we're going to add the partitions to.
+   */
+  public String getTableName() {
+    return tableName;
+  }
+
+  /**
+   * @param tableName
+   *          the table we're going to add the partitions to.
+   */
+  public void setTableName(String tableName) {
+    this.tableName = tableName;
+  }
+
+  /**
+   * @return location of partition in relation to table
+   */
+  public String getLocation() {
+    return location;
+  }
+
+  /**
+   * @param location
+   *          location of partition in relation to table
+   */
+  public void setLocation(String location) {
+    this.location = location;
+  }
+
+  /**
+   * @return old partition specification.
+   */
+  public LinkedHashMap<String, String> getOldPartSpec() {
+    return oldPartSpec;
+  }
+
+  /**
+   * @param partSpec
+   *          partition specification
+   */
+  public void setOldPartSpec(LinkedHashMap<String, String> partSpec) {
+    this.oldPartSpec = partSpec;
+  }
+
+  /**
+   * @return new partition specification.
+   */
+  public LinkedHashMap<String, String> getNewPartSpec() {
+    return newPartSpec;
+  }
+
+  /**
+   * @param partSpec
+   *          partition specification
+   */
+  public void setNewPartSpec(LinkedHashMap<String, String> partSpec) {
+    this.newPartSpec = partSpec;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIf.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIf.java
index 9b9a815639..855dab917e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIf.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIf.java
@@ -1,98 +1,98 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
-
-/**
- * IF(expr1,expr2,expr3) <br>
- * If expr1 is TRUE (expr1 <> 0 and expr1 <> NULL) then IF() returns expr2;
- * otherwise it returns expr3. IF() returns a numeric or string value, depending
- * on the context in which it is used.
- */
-public class GenericUDFIf extends GenericUDF {
-  private ObjectInspector[] argumentOIs;
-  private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    argumentOIs = arguments;
-    returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
-
-    if (arguments.length != 3) {
-      throw new UDFArgumentLengthException(
-          "The function IF(expr1,expr2,expr3) accepts exactly 3 arguments.");
-    }
-
-    boolean conditionTypeIsOk = (arguments[0].getCategory() == ObjectInspector.Category.PRIMITIVE);
-    if (conditionTypeIsOk) {
-      PrimitiveObjectInspector poi = ((PrimitiveObjectInspector) arguments[0]);
-      conditionTypeIsOk = (poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.BOOLEAN
-          || poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.VOID);
-    }
-    if (!conditionTypeIsOk) {
-      throw new UDFArgumentTypeException(0,
-          "The first argument of function IF should be \""
-          + serdeConstants.BOOLEAN_TYPE_NAME + "\", but \""
-          + arguments[0].getTypeName() + "\" is found");
-    }
-
-    if (!(returnOIResolver.update(arguments[1]) && returnOIResolver
-        .update(arguments[2]))) {
-      throw new UDFArgumentTypeException(2,
-          "The second and the third arguments of function IF should have the same type, "
-          + "but they are different: \"" + arguments[1].getTypeName()
-          + "\" and \"" + arguments[2].getTypeName() + "\"");
-    }
-
-    return returnOIResolver.get();
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    Object condition = arguments[0].get();
-    if (condition != null
-        && ((BooleanObjectInspector) argumentOIs[0]).get(condition)) {
-      return returnOIResolver.convertIfNecessary(arguments[1].get(),
-          argumentOIs[1]);
-    } else {
-      return returnOIResolver.convertIfNecessary(arguments[2].get(),
-          argumentOIs[2]);
-    }
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 3);
-    StringBuilder sb = new StringBuilder();
-    sb.append("if(");
-    sb.append(children[0]).append(", ");
-    sb.append(children[1]).append(", ");
-    sb.append(children[2]).append(")");
-    return sb.toString();
-  }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
+
+/**
+ * IF(expr1,expr2,expr3) <br>
+ * If expr1 is TRUE (expr1 <> 0 and expr1 <> NULL) then IF() returns expr2;
+ * otherwise it returns expr3. IF() returns a numeric or string value, depending
+ * on the context in which it is used.
+ */
+public class GenericUDFIf extends GenericUDF {
+  private ObjectInspector[] argumentOIs;
+  private GenericUDFUtils.ReturnObjectInspectorResolver returnOIResolver;
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    argumentOIs = arguments;
+    returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);
+
+    if (arguments.length != 3) {
+      throw new UDFArgumentLengthException(
+          "The function IF(expr1,expr2,expr3) accepts exactly 3 arguments.");
+    }
+
+    boolean conditionTypeIsOk = (arguments[0].getCategory() == ObjectInspector.Category.PRIMITIVE);
+    if (conditionTypeIsOk) {
+      PrimitiveObjectInspector poi = ((PrimitiveObjectInspector) arguments[0]);
+      conditionTypeIsOk = (poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.BOOLEAN
+          || poi.getPrimitiveCategory() == PrimitiveObjectInspector.PrimitiveCategory.VOID);
+    }
+    if (!conditionTypeIsOk) {
+      throw new UDFArgumentTypeException(0,
+          "The first argument of function IF should be \""
+          + serdeConstants.BOOLEAN_TYPE_NAME + "\", but \""
+          + arguments[0].getTypeName() + "\" is found");
+    }
+
+    if (!(returnOIResolver.update(arguments[1]) && returnOIResolver
+        .update(arguments[2]))) {
+      throw new UDFArgumentTypeException(2,
+          "The second and the third arguments of function IF should have the same type, "
+          + "but they are different: \"" + arguments[1].getTypeName()
+          + "\" and \"" + arguments[2].getTypeName() + "\"");
+    }
+
+    return returnOIResolver.get();
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    Object condition = arguments[0].get();
+    if (condition != null
+        && ((BooleanObjectInspector) argumentOIs[0]).get(condition)) {
+      return returnOIResolver.convertIfNecessary(arguments[1].get(),
+          argumentOIs[1]);
+    } else {
+      return returnOIResolver.convertIfNecessary(arguments[2].get(),
+          argumentOIs[2]);
+    }
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 3);
+    StringBuilder sb = new StringBuilder();
+    sb.append("if(");
+    sb.append(children[0]).append(", ");
+    sb.append(children[1]).append(", ");
+    sb.append(children[2]).append(")");
+    return sb.toString();
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTranslate.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTranslate.java
index d3ca9c624b..1c5de2ab48 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTranslate.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTranslate.java
@@ -1,291 +1,291 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import java.nio.ByteBuffer;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.udf.UDFType;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.io.Text;
-
-/**
- * TRANSLATE(string input, string from, string to) is an equivalent function to translate in
- * PostGresSQL. See explain extended annotation below to read more about how this UDF works
- * 
- */
-@UDFType(deterministic = true)
-//@formatter:off
-@Description(
-  name = "translate",
-  value = "_FUNC_(input, from, to) - translates the input string by" +
-          " replacing the characters present in the from string with the" +
-          " corresponding characters in the to string",
-  extended = "_FUNC_(string input, string from, string to) is an" +
-             " equivalent function to translate in PostGreSQL. It works" +
-             " on a character by character basis on the input string (first" +
-             " parameter). A character in the input is checked for" +
-             " presence in the from string (second parameter). If a" +
-             " match happens, the character from to string (third " +
-             "parameter) which appears at the same index as the character" +
-             " in from string is obtained. This character is emitted in" +
-             " the output string  instead of the original character from" +
-             " the input string. If the to string is shorter than the" +
-             " from string, there may not be a character present at" +
-             " the same index in the to string. In such a case, nothing is" +
-             " emitted for the original character and it's deleted from" +
-             " the output string." +
-             "\n" +
-             "For example," +
-             "\n" +
-             "\n" +
-             "_FUNC_('abcdef', 'adc', '19') returns '1b9ef' replacing" +
-             " 'a' with '1', 'd' with '9' and removing 'c' from the input" +
-             " string" +
-             "\n" +
-             "\n" +
-             "_FUNC_('a b c d', ' ', '') return 'abcd'" +
-             " removing all spaces from the input string" +
-             "\n" +
-             "\n" +
-             "If the same character is present multiple times in the" +
-             " input string, the first occurence of the character is the" +
-             " one that's considered for matching. However, it is not recommended" +
-             " to have the same character more than once in the from" +
-             " string since it's not required and adds to confusion." +
-             "\n" +
-             "\n" +
-             "For example," +
-             "\n" +
-             "\n" +
-             "_FUNC_('abcdef', 'ada', '192') returns '1bc9ef' replaces" +
-             " 'a' with '1' and 'd' with '9' ignoring the second" +
-             " occurence of 'a' in the from string mapping it to '2'"
-)
-//@formatter:on
-public class GenericUDFTranslate extends GenericUDF {
-
-  // For all practical purposes a code point is a fancy name for character. A java char data type
-  // can store characters that require 16 bits or less. However, the unicode specification has
-  // changed to allow for characters whose representation requires more than 16 bits. Therefore we
-  // need to represent each character (called a code point from hereon) as int. More details at
-  // http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html
-
-  /**
-   * If a code point needs to be replaced with another code point, this map with store the mapping.
-   */
-  private Map<Integer, Integer> replacementMap = new HashMap<Integer, Integer>();
-
-  /**
-   * This set stores all the code points which needed to be deleted from the input string. The
-   * objects in deletionSet and keys in replacementMap are mutually exclusive
-   */
-  private Set<Integer> deletionSet = new HashSet<Integer>();
-  /**
-   * A placeholder for result.
-   */
-  private Text result = new Text();
-
-  /**
-   * The values of from parameter from the previous evaluate() call.
-   */
-  private Text lastFrom = null;
-  /**
-   * The values of to parameter from the previous evaluate() call.
-   */
-  private Text lastTo = null;
-  /**
-   * Converters for retrieving the arguments to the UDF.
-   */
-  private ObjectInspectorConverters.Converter[] converters;
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    if (arguments.length != 3) {
-      throw new UDFArgumentLengthException("_FUNC_ expects exactly 3 arguments");
-    }
-
-    for (int i = 0; i < arguments.length; i++) {
-      if (arguments[i].getCategory() != Category.PRIMITIVE) {
-        throw new UDFArgumentTypeException(i,
-            "A string argument was expected but an argument of type " + arguments[i].getTypeName()
-                + " was given.");
-
-      }
-
-      // Now that we have made sure that the argument is of primitive type, we can get the primitive
-      // category
-      PrimitiveCategory primitiveCategory = ((PrimitiveObjectInspector) arguments[i])
-          .getPrimitiveCategory();
-
-      if (primitiveCategory != PrimitiveCategory.STRING
-          && primitiveCategory != PrimitiveCategory.VOID) {
-        throw new UDFArgumentTypeException(i,
-            "A string argument was expected but an argument of type " + arguments[i].getTypeName()
-                + " was given.");
-
-      }
-    }
-
-    converters = new ObjectInspectorConverters.Converter[arguments.length];
-    for (int i = 0; i < arguments.length; i++) {
-      converters[i] = ObjectInspectorConverters.getConverter(arguments[i],
-          PrimitiveObjectInspectorFactory.writableStringObjectInspector);
-    }
-
-    // We will be returning a Text object
-    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    assert (arguments.length == 3);
-    if (arguments[0].get() == null || arguments[1].get() == null || arguments[2].get() == null) {
-      return null;
-    }
-
-    Text input = (Text) converters[0].convert(arguments[0].get());
-    Text from = (Text) converters[1].convert(arguments[1].get());
-    Text to = (Text) converters[2].convert(arguments[2].get());
-
-    populateMappingsIfNecessary(from, to);
-    String resultString = processInput(input);
-    result.set(resultString);
-    return result;
-  }
-
-  /**
-   * Pre-processes the from and to strings by calling {@link #populateMappings(Text, Text)} if
-   * necessary.
-   * 
-   * @param from
-   *          from string to be used for translation
-   * @param to
-   *          to string to be used for translation
-   */
-  private void populateMappingsIfNecessary(Text from, Text to) {
-    // If the from and to strings haven't changed, we don't need to preprocess again to regenerate
-    // the mappings of code points that need to replaced or deleted
-    if ((lastFrom == null) || (lastTo == null) || !from.equals(lastFrom) || !to.equals(lastTo)) {
-      populateMappings(from, to);
-      // These are null when evaluate() is called for the first time
-      if (lastFrom == null) {
-        lastFrom = new Text();
-      }
-      if (lastTo == null) {
-        lastTo = new Text();
-      }
-      // Need to deep copy here since doing something like lastFrom = from instead, will make
-      // lastFrom point to the same Text object which would make from.equals(lastFrom) always true
-      lastFrom.set(from);
-      lastTo.set(to);
-    }
-  }
-
-  /**
-   * Pre-process the from and to strings populate {@link #replacementMap} and {@link #deletionSet}.
-   * 
-   * @param from
-   *          from string to be used for translation
-   * @param to
-   *          to string to be used for translation
-   */
-  private void populateMappings(Text from, Text to) {
-    replacementMap.clear();
-    deletionSet.clear();
-
-    ByteBuffer fromBytes = ByteBuffer.wrap(from.getBytes(), 0, from.getLength());
-    ByteBuffer toBytes = ByteBuffer.wrap(to.getBytes(), 0, to.getLength());
-
-    // Traverse through the from string, one code point at a time
-    while (fromBytes.hasRemaining()) {
-      // This will also move the iterator ahead by one code point
-      int fromCodePoint = Text.bytesToCodePoint(fromBytes);
-      // If the to string has more code points, make sure to traverse it too
-      if (toBytes.hasRemaining()) {
-        int toCodePoint = Text.bytesToCodePoint(toBytes);
-        // If the code point from from string already has a replacement or is to be deleted, we
-        // don't need to do anything, just move on to the next code point
-        if (replacementMap.containsKey(fromCodePoint) || deletionSet.contains(fromCodePoint)) {
-          continue;
-        }
-        replacementMap.put(fromCodePoint, toCodePoint);
-      } else {
-        // If the code point from from string already has a replacement or is to be deleted, we
-        // don't need to do anything, just move on to the next code point
-        if (replacementMap.containsKey(fromCodePoint) || deletionSet.contains(fromCodePoint)) {
-          continue;
-        }
-        deletionSet.add(fromCodePoint);
-      }
-    }
-  }
-
-  /**
-   * Translates the input string based on {@link #replacementMap} and {@link #deletionSet} and
-   * returns the translated string.
-   * 
-   * @param input
-   *          input string to perform the translation on
-   * @return translated string
-   */
-  private String processInput(Text input) {
-    StringBuilder resultBuilder = new StringBuilder();
-    // Obtain the byte buffer from the input string so we can traverse it code point by code point
-    ByteBuffer inputBytes = ByteBuffer.wrap(input.getBytes(), 0, input.getLength());
-    // Traverse the byte buffer containing the input string one code point at a time
-    while (inputBytes.hasRemaining()) {
-      int inputCodePoint = Text.bytesToCodePoint(inputBytes);
-      // If the code point exists in deletion set, no need to emit out anything for this code point.
-      // Continue on to the next code point
-      if (deletionSet.contains(inputCodePoint)) {
-        continue;
-      }
-
-      Integer replacementCodePoint = replacementMap.get(inputCodePoint);
-      // If a replacement exists for this code point, emit out the replacement and append it to the
-      // output string. If no such replacement exists, emit out the original input code point
-      char[] charArray = Character.toChars((replacementCodePoint != null) ? replacementCodePoint
-          : inputCodePoint);
-      resultBuilder.append(charArray);
-    }
-    String resultString = resultBuilder.toString();
-    return resultString;
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 3);
-    return "translate(" + children[0] + ", " + children[1] + ", " + children[2] + ")";
-  }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.udf.UDFType;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.io.Text;
+
+/**
+ * TRANSLATE(string input, string from, string to) is an equivalent function to translate in
+ * PostGresSQL. See explain extended annotation below to read more about how this UDF works
+ * 
+ */
+@UDFType(deterministic = true)
+//@formatter:off
+@Description(
+  name = "translate",
+  value = "_FUNC_(input, from, to) - translates the input string by" +
+          " replacing the characters present in the from string with the" +
+          " corresponding characters in the to string",
+  extended = "_FUNC_(string input, string from, string to) is an" +
+             " equivalent function to translate in PostGreSQL. It works" +
+             " on a character by character basis on the input string (first" +
+             " parameter). A character in the input is checked for" +
+             " presence in the from string (second parameter). If a" +
+             " match happens, the character from to string (third " +
+             "parameter) which appears at the same index as the character" +
+             " in from string is obtained. This character is emitted in" +
+             " the output string  instead of the original character from" +
+             " the input string. If the to string is shorter than the" +
+             " from string, there may not be a character present at" +
+             " the same index in the to string. In such a case, nothing is" +
+             " emitted for the original character and it's deleted from" +
+             " the output string." +
+             "\n" +
+             "For example," +
+             "\n" +
+             "\n" +
+             "_FUNC_('abcdef', 'adc', '19') returns '1b9ef' replacing" +
+             " 'a' with '1', 'd' with '9' and removing 'c' from the input" +
+             " string" +
+             "\n" +
+             "\n" +
+             "_FUNC_('a b c d', ' ', '') return 'abcd'" +
+             " removing all spaces from the input string" +
+             "\n" +
+             "\n" +
+             "If the same character is present multiple times in the" +
+             " input string, the first occurence of the character is the" +
+             " one that's considered for matching. However, it is not recommended" +
+             " to have the same character more than once in the from" +
+             " string since it's not required and adds to confusion." +
+             "\n" +
+             "\n" +
+             "For example," +
+             "\n" +
+             "\n" +
+             "_FUNC_('abcdef', 'ada', '192') returns '1bc9ef' replaces" +
+             " 'a' with '1' and 'd' with '9' ignoring the second" +
+             " occurence of 'a' in the from string mapping it to '2'"
+)
+//@formatter:on
+public class GenericUDFTranslate extends GenericUDF {
+
+  // For all practical purposes a code point is a fancy name for character. A java char data type
+  // can store characters that require 16 bits or less. However, the unicode specification has
+  // changed to allow for characters whose representation requires more than 16 bits. Therefore we
+  // need to represent each character (called a code point from hereon) as int. More details at
+  // http://docs.oracle.com/javase/7/docs/api/java/lang/Character.html
+
+  /**
+   * If a code point needs to be replaced with another code point, this map with store the mapping.
+   */
+  private Map<Integer, Integer> replacementMap = new HashMap<Integer, Integer>();
+
+  /**
+   * This set stores all the code points which needed to be deleted from the input string. The
+   * objects in deletionSet and keys in replacementMap are mutually exclusive
+   */
+  private Set<Integer> deletionSet = new HashSet<Integer>();
+  /**
+   * A placeholder for result.
+   */
+  private Text result = new Text();
+
+  /**
+   * The values of from parameter from the previous evaluate() call.
+   */
+  private Text lastFrom = null;
+  /**
+   * The values of to parameter from the previous evaluate() call.
+   */
+  private Text lastTo = null;
+  /**
+   * Converters for retrieving the arguments to the UDF.
+   */
+  private ObjectInspectorConverters.Converter[] converters;
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    if (arguments.length != 3) {
+      throw new UDFArgumentLengthException("_FUNC_ expects exactly 3 arguments");
+    }
+
+    for (int i = 0; i < arguments.length; i++) {
+      if (arguments[i].getCategory() != Category.PRIMITIVE) {
+        throw new UDFArgumentTypeException(i,
+            "A string argument was expected but an argument of type " + arguments[i].getTypeName()
+                + " was given.");
+
+      }
+
+      // Now that we have made sure that the argument is of primitive type, we can get the primitive
+      // category
+      PrimitiveCategory primitiveCategory = ((PrimitiveObjectInspector) arguments[i])
+          .getPrimitiveCategory();
+
+      if (primitiveCategory != PrimitiveCategory.STRING
+          && primitiveCategory != PrimitiveCategory.VOID) {
+        throw new UDFArgumentTypeException(i,
+            "A string argument was expected but an argument of type " + arguments[i].getTypeName()
+                + " was given.");
+
+      }
+    }
+
+    converters = new ObjectInspectorConverters.Converter[arguments.length];
+    for (int i = 0; i < arguments.length; i++) {
+      converters[i] = ObjectInspectorConverters.getConverter(arguments[i],
+          PrimitiveObjectInspectorFactory.writableStringObjectInspector);
+    }
+
+    // We will be returning a Text object
+    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    assert (arguments.length == 3);
+    if (arguments[0].get() == null || arguments[1].get() == null || arguments[2].get() == null) {
+      return null;
+    }
+
+    Text input = (Text) converters[0].convert(arguments[0].get());
+    Text from = (Text) converters[1].convert(arguments[1].get());
+    Text to = (Text) converters[2].convert(arguments[2].get());
+
+    populateMappingsIfNecessary(from, to);
+    String resultString = processInput(input);
+    result.set(resultString);
+    return result;
+  }
+
+  /**
+   * Pre-processes the from and to strings by calling {@link #populateMappings(Text, Text)} if
+   * necessary.
+   * 
+   * @param from
+   *          from string to be used for translation
+   * @param to
+   *          to string to be used for translation
+   */
+  private void populateMappingsIfNecessary(Text from, Text to) {
+    // If the from and to strings haven't changed, we don't need to preprocess again to regenerate
+    // the mappings of code points that need to replaced or deleted
+    if ((lastFrom == null) || (lastTo == null) || !from.equals(lastFrom) || !to.equals(lastTo)) {
+      populateMappings(from, to);
+      // These are null when evaluate() is called for the first time
+      if (lastFrom == null) {
+        lastFrom = new Text();
+      }
+      if (lastTo == null) {
+        lastTo = new Text();
+      }
+      // Need to deep copy here since doing something like lastFrom = from instead, will make
+      // lastFrom point to the same Text object which would make from.equals(lastFrom) always true
+      lastFrom.set(from);
+      lastTo.set(to);
+    }
+  }
+
+  /**
+   * Pre-process the from and to strings populate {@link #replacementMap} and {@link #deletionSet}.
+   * 
+   * @param from
+   *          from string to be used for translation
+   * @param to
+   *          to string to be used for translation
+   */
+  private void populateMappings(Text from, Text to) {
+    replacementMap.clear();
+    deletionSet.clear();
+
+    ByteBuffer fromBytes = ByteBuffer.wrap(from.getBytes(), 0, from.getLength());
+    ByteBuffer toBytes = ByteBuffer.wrap(to.getBytes(), 0, to.getLength());
+
+    // Traverse through the from string, one code point at a time
+    while (fromBytes.hasRemaining()) {
+      // This will also move the iterator ahead by one code point
+      int fromCodePoint = Text.bytesToCodePoint(fromBytes);
+      // If the to string has more code points, make sure to traverse it too
+      if (toBytes.hasRemaining()) {
+        int toCodePoint = Text.bytesToCodePoint(toBytes);
+        // If the code point from from string already has a replacement or is to be deleted, we
+        // don't need to do anything, just move on to the next code point
+        if (replacementMap.containsKey(fromCodePoint) || deletionSet.contains(fromCodePoint)) {
+          continue;
+        }
+        replacementMap.put(fromCodePoint, toCodePoint);
+      } else {
+        // If the code point from from string already has a replacement or is to be deleted, we
+        // don't need to do anything, just move on to the next code point
+        if (replacementMap.containsKey(fromCodePoint) || deletionSet.contains(fromCodePoint)) {
+          continue;
+        }
+        deletionSet.add(fromCodePoint);
+      }
+    }
+  }
+
+  /**
+   * Translates the input string based on {@link #replacementMap} and {@link #deletionSet} and
+   * returns the translated string.
+   * 
+   * @param input
+   *          input string to perform the translation on
+   * @return translated string
+   */
+  private String processInput(Text input) {
+    StringBuilder resultBuilder = new StringBuilder();
+    // Obtain the byte buffer from the input string so we can traverse it code point by code point
+    ByteBuffer inputBytes = ByteBuffer.wrap(input.getBytes(), 0, input.getLength());
+    // Traverse the byte buffer containing the input string one code point at a time
+    while (inputBytes.hasRemaining()) {
+      int inputCodePoint = Text.bytesToCodePoint(inputBytes);
+      // If the code point exists in deletion set, no need to emit out anything for this code point.
+      // Continue on to the next code point
+      if (deletionSet.contains(inputCodePoint)) {
+        continue;
+      }
+
+      Integer replacementCodePoint = replacementMap.get(inputCodePoint);
+      // If a replacement exists for this code point, emit out the replacement and append it to the
+      // output string. If no such replacement exists, emit out the original input code point
+      char[] charArray = Character.toChars((replacementCodePoint != null) ? replacementCodePoint
+          : inputCodePoint);
+      resultBuilder.append(charArray);
+    }
+    String resultString = resultBuilder.toString();
+    return resultString;
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 3);
+    return "translate(" + children[0] + ", " + children[1] + ", " + children[2] + ")";
+  }
+
+}
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java b/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
index c0b12e742c..4ec74313c4 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaBoolean.java
@@ -1,56 +1,56 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-
-/**
- * A test GenericUDF to return native Java's boolean type
- */
-public class GenericUDFTestGetJavaBoolean extends GenericUDF {
-  ObjectInspector[] argumentOIs;
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.javaBooleanObjectInspector;
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    String input = ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
-    if (input.equalsIgnoreCase("true")) {
-      return Boolean.TRUE;
-    } else if (input.equalsIgnoreCase("false")) {
-      return false;
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 1);
-    return "TestGetJavaBoolean(" + children[0] + ")";
-  }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+
+/**
+ * A test GenericUDF to return native Java's boolean type
+ */
+public class GenericUDFTestGetJavaBoolean extends GenericUDF {
+  ObjectInspector[] argumentOIs;
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.javaBooleanObjectInspector;
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    String input = ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
+    if (input.equalsIgnoreCase("true")) {
+      return Boolean.TRUE;
+    } else if (input.equalsIgnoreCase("false")) {
+      return false;
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 1);
+    return "TestGetJavaBoolean(" + children[0] + ")";
+  }
+}
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java b/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
index f94139c752..ead45ae48a 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestGetJavaString.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-
-/**
- * A test GenericUDF to return native Java's string type
- */
-public class GenericUDFTestGetJavaString extends GenericUDF {
-  ObjectInspector[] argumentOIs;
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.javaStringObjectInspector;
-  }
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    if (arguments[0].get() == null) {
-      return null;
-    }
-    return ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 1);
-    return "GenericUDFTestGetJavaString(" + children[0] + ")";
-  }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+
+/**
+ * A test GenericUDF to return native Java's string type
+ */
+public class GenericUDFTestGetJavaString extends GenericUDF {
+  ObjectInspector[] argumentOIs;
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.javaStringObjectInspector;
+  }
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    if (arguments[0].get() == null) {
+      return null;
+    }
+    return ((StringObjectInspector) argumentOIs[0]).getPrimitiveJavaObject(arguments[0].get());
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 1);
+    return "GenericUDFTestGetJavaString(" + children[0] + ")";
+  }
+}
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java b/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
index 28c8960320..d3bc7554f5 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/GenericUDFTestTranslate.java
@@ -1,122 +1,122 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.udf.generic;
-
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
-import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
-import org.apache.hadoop.io.Text;
-
-/**
- * Mimics oracle's function translate(str1, str2, str3).
- */
-public class GenericUDFTestTranslate extends GenericUDF {
-  ObjectInspector[] argumentOIs;
-
-  /**
-   * Return a corresponding ordinal from an integer.
-   */
-  static String getOrdinal(int i) {
-    int unit = i % 10;
-    return (i <= 0) ? "" : (i != 11 && unit == 1) ? i + "st"
-        : (i != 12 && unit == 2) ? i + "nd" : (i != 13 && unit == 3) ? i + "rd"
-        : i + "th";
-  }
-
-  @Override
-  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
-    if (arguments.length != 3) {
-      throw new UDFArgumentLengthException(
-          "The function TRANSLATE(expr,from_string,to_string) accepts exactly 3 arguments, but "
-          + arguments.length + " arguments is found.");
-    }
-
-    for (int i = 0; i < 3; i++) {
-      if (arguments[i].getTypeName() != serdeConstants.STRING_TYPE_NAME
-          && arguments[i].getTypeName() != serdeConstants.VOID_TYPE_NAME) {
-        throw new UDFArgumentTypeException(i, "The " + getOrdinal(i + 1)
-            + " argument of function TRANSLATE is expected to \""
-            + serdeConstants.STRING_TYPE_NAME + "\", but \""
-            + arguments[i].getTypeName() + "\" is found");
-      }
-    }
-
-    argumentOIs = arguments;
-    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
-  }
-
-  private final Text resultText = new Text();
-
-  @Override
-  public Object evaluate(DeferredObject[] arguments) throws HiveException {
-    if (arguments[0].get() == null || arguments[1].get() == null
-        || arguments[2].get() == null) {
-      return null;
-    }
-    String exprString = ((StringObjectInspector) argumentOIs[0])
-        .getPrimitiveJavaObject(arguments[0].get());
-    String fromString = ((StringObjectInspector) argumentOIs[1])
-        .getPrimitiveJavaObject(arguments[1].get());
-    String toString = ((StringObjectInspector) argumentOIs[2])
-        .getPrimitiveJavaObject(arguments[2].get());
-
-    char[] expr = exprString.toCharArray();
-    char[] from = fromString.toCharArray();
-    char[] to = toString.toCharArray();
-    char[] result = new char[expr.length];
-    System.arraycopy(expr, 0, result, 0, expr.length);
-    Set<Character> seen = new HashSet<Character>();
-
-    for (int i = 0; i < from.length; i++) {
-      if (seen.contains(from[i])) {
-        continue;
-      }
-      seen.add(from[i]);
-      for (int j = 0; j < expr.length; j++) {
-        if (expr[j] == from[i]) {
-          result[j] = (i < to.length) ? to[i] : 0;
-        }
-      }
-    }
-
-    int pos = 0;
-    for (int i = 0; i < result.length; i++) {
-      if (result[i] != 0) {
-        result[pos++] = result[i];
-      }
-    }
-    resultText.set(new String(result, 0, pos));
-    return resultText;
-  }
-
-  @Override
-  public String getDisplayString(String[] children) {
-    assert (children.length == 3);
-    return "translate(" + children[0] + "," + children[1] + "," + children[2]
-        + ")";
-  }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;
+import org.apache.hadoop.io.Text;
+
+/**
+ * Mimics oracle's function translate(str1, str2, str3).
+ */
+public class GenericUDFTestTranslate extends GenericUDF {
+  ObjectInspector[] argumentOIs;
+
+  /**
+   * Return a corresponding ordinal from an integer.
+   */
+  static String getOrdinal(int i) {
+    int unit = i % 10;
+    return (i <= 0) ? "" : (i != 11 && unit == 1) ? i + "st"
+        : (i != 12 && unit == 2) ? i + "nd" : (i != 13 && unit == 3) ? i + "rd"
+        : i + "th";
+  }
+
+  @Override
+  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
+    if (arguments.length != 3) {
+      throw new UDFArgumentLengthException(
+          "The function TRANSLATE(expr,from_string,to_string) accepts exactly 3 arguments, but "
+          + arguments.length + " arguments is found.");
+    }
+
+    for (int i = 0; i < 3; i++) {
+      if (arguments[i].getTypeName() != serdeConstants.STRING_TYPE_NAME
+          && arguments[i].getTypeName() != serdeConstants.VOID_TYPE_NAME) {
+        throw new UDFArgumentTypeException(i, "The " + getOrdinal(i + 1)
+            + " argument of function TRANSLATE is expected to \""
+            + serdeConstants.STRING_TYPE_NAME + "\", but \""
+            + arguments[i].getTypeName() + "\" is found");
+      }
+    }
+
+    argumentOIs = arguments;
+    return PrimitiveObjectInspectorFactory.writableStringObjectInspector;
+  }
+
+  private final Text resultText = new Text();
+
+  @Override
+  public Object evaluate(DeferredObject[] arguments) throws HiveException {
+    if (arguments[0].get() == null || arguments[1].get() == null
+        || arguments[2].get() == null) {
+      return null;
+    }
+    String exprString = ((StringObjectInspector) argumentOIs[0])
+        .getPrimitiveJavaObject(arguments[0].get());
+    String fromString = ((StringObjectInspector) argumentOIs[1])
+        .getPrimitiveJavaObject(arguments[1].get());
+    String toString = ((StringObjectInspector) argumentOIs[2])
+        .getPrimitiveJavaObject(arguments[2].get());
+
+    char[] expr = exprString.toCharArray();
+    char[] from = fromString.toCharArray();
+    char[] to = toString.toCharArray();
+    char[] result = new char[expr.length];
+    System.arraycopy(expr, 0, result, 0, expr.length);
+    Set<Character> seen = new HashSet<Character>();
+
+    for (int i = 0; i < from.length; i++) {
+      if (seen.contains(from[i])) {
+        continue;
+      }
+      seen.add(from[i]);
+      for (int j = 0; j < expr.length; j++) {
+        if (expr[j] == from[i]) {
+          result[j] = (i < to.length) ? to[i] : 0;
+        }
+      }
+    }
+
+    int pos = 0;
+    for (int i = 0; i < result.length; i++) {
+      if (result[i] != 0) {
+        result[pos++] = result[i];
+      }
+    }
+    resultText.set(new String(result, 0, pos));
+    return resultText;
+  }
+
+  @Override
+  public String getDisplayString(String[] children) {
+    assert (children.length == 3);
+    return "translate(" + children[0] + "," + children[1] + "," + children[2]
+        + ")";
+  }
+}
diff --git a/ql/src/test/queries/clientnegative/alter_rename_partition_failure.q b/ql/src/test/queries/clientnegative/alter_rename_partition_failure.q
index f7530585d0..26ba287890 100644
--- a/ql/src/test/queries/clientnegative/alter_rename_partition_failure.q
+++ b/ql/src/test/queries/clientnegative/alter_rename_partition_failure.q
@@ -1,6 +1,6 @@
-create table alter_rename_partition_src ( col1 string ) stored as textfile ;
-load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
-create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
-insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
-
-alter table alter_rename_partition partition (pCol1='nonexist_part1:', pcol2='nonexist_part2:') rename to partition (pCol1='new_part1:', pcol2='new_part2:');
+create table alter_rename_partition_src ( col1 string ) stored as textfile ;
+load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
+create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
+insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
+
+alter table alter_rename_partition partition (pCol1='nonexist_part1:', pcol2='nonexist_part2:') rename to partition (pCol1='new_part1:', pcol2='new_part2:');
diff --git a/ql/src/test/queries/clientnegative/alter_rename_partition_failure2.q b/ql/src/test/queries/clientnegative/alter_rename_partition_failure2.q
index bb9ca0bdfd..6e51c2f762 100644
--- a/ql/src/test/queries/clientnegative/alter_rename_partition_failure2.q
+++ b/ql/src/test/queries/clientnegative/alter_rename_partition_failure2.q
@@ -1,6 +1,6 @@
-create table alter_rename_partition_src ( col1 string ) stored as textfile ;
-load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
-create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
-insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
-
-alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='old_part1:', pcol2='old_part2:');
+create table alter_rename_partition_src ( col1 string ) stored as textfile ;
+load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
+create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
+insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
+
+alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='old_part1:', pcol2='old_part2:');
diff --git a/ql/src/test/queries/clientnegative/alter_rename_partition_failure3.q b/ql/src/test/queries/clientnegative/alter_rename_partition_failure3.q
index ef3bf939db..2d4ce0b9f6 100644
--- a/ql/src/test/queries/clientnegative/alter_rename_partition_failure3.q
+++ b/ql/src/test/queries/clientnegative/alter_rename_partition_failure3.q
@@ -1,6 +1,6 @@
-create table alter_rename_partition_src ( col1 string ) stored as textfile ;
-load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
-create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
-insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
-
+create table alter_rename_partition_src ( col1 string ) stored as textfile ;
+load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
+create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
+insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
+
 alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='old_part1:', pcol2='old_part2:', pcol3='old_part3:');
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/create_udaf_failure.q b/ql/src/test/queries/clientnegative/create_udaf_failure.q
index 4a494c6327..e0bb408a64 100644
--- a/ql/src/test/queries/clientnegative/create_udaf_failure.q
+++ b/ql/src/test/queries/clientnegative/create_udaf_failure.q
@@ -1,6 +1,6 @@
-CREATE TEMPORARY FUNCTION test_udaf AS 'org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase';
-
-EXPLAIN
-SELECT test_udaf(length(src.value)) FROM src;
-
-SELECT test_udaf(length(src.value)) FROM src;
+CREATE TEMPORARY FUNCTION test_udaf AS 'org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase';
+
+EXPLAIN
+SELECT test_udaf(length(src.value)) FROM src;
+
+SELECT test_udaf(length(src.value)) FROM src;
diff --git a/ql/src/test/queries/clientpositive/alter_rename_partition.q b/ql/src/test/queries/clientpositive/alter_rename_partition.q
index 3aad60ad97..d498cd52a5 100644
--- a/ql/src/test/queries/clientpositive/alter_rename_partition.q
+++ b/ql/src/test/queries/clientpositive/alter_rename_partition.q
@@ -1,41 +1,41 @@
--- Cleanup
-DROP TABLE alter_rename_partition_src;
-DROP TABLE alter_rename_partition;
-SHOW TABLES;
-
-create table alter_rename_partition_src ( col1 string ) stored as textfile ;
-load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
-
-create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
-
-insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
-select * from alter_rename_partition where pcol1='old_part1:' and pcol2='old_part2:';
-
-alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='new_part1:', pcol2='new_part2:');
-SHOW PARTITIONS alter_rename_partition;
-select * from alter_rename_partition where pcol1='old_part1:' and pcol2='old_part2:';
-select * from alter_rename_partition where pcol1='new_part1:' and pcol2='new_part2:';
-
--- Cleanup
-DROP TABLE alter_rename_partition_src;
-DROP TABLE alter_rename_partition;
-SHOW TABLES;
-
--- With non-default Database
-
-CREATE DATABASE alter_rename_partition_db;
-USE alter_rename_partition_db;
-SHOW TABLES;
-
-CREATE TABLE alter_rename_partition_src (col1 STRING) STORED AS TEXTFILE ;
-LOAD DATA LOCAL INPATH '../data/files/test.dat' OVERWRITE INTO TABLE alter_rename_partition_src ;
-
-CREATE TABLE alter_rename_partition (col1 STRING) PARTITIONED BY (pcol1 STRING, pcol2 STRING) STORED AS SEQUENCEFILE;
-
-INSERT OVERWRITE TABLE alter_rename_partition PARTITION (pCol1='old_part1:', pcol2='old_part2:') SELECT col1 FROM alter_rename_partition_src ;
-SELECT * FROM alter_rename_partition WHERE pcol1='old_part1:' AND pcol2='old_part2:';
-
-ALTER TABLE alter_rename_partition PARTITION (pCol1='old_part1:', pcol2='old_part2:') RENAME TO PARTITION (pCol1='new_part1:', pcol2='new_part2:');
-SHOW PARTITIONS alter_rename_partition;
-SELECT * FROM alter_rename_partition WHERE pcol1='old_part1:' and pcol2='old_part2:';
-SELECT * FROM alter_rename_partition WHERE pcol1='new_part1:' and pcol2='new_part2:';
+-- Cleanup
+DROP TABLE alter_rename_partition_src;
+DROP TABLE alter_rename_partition;
+SHOW TABLES;
+
+create table alter_rename_partition_src ( col1 string ) stored as textfile ;
+load data local inpath '../data/files/test.dat' overwrite into table alter_rename_partition_src ;
+
+create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
+
+insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
+select * from alter_rename_partition where pcol1='old_part1:' and pcol2='old_part2:';
+
+alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='new_part1:', pcol2='new_part2:');
+SHOW PARTITIONS alter_rename_partition;
+select * from alter_rename_partition where pcol1='old_part1:' and pcol2='old_part2:';
+select * from alter_rename_partition where pcol1='new_part1:' and pcol2='new_part2:';
+
+-- Cleanup
+DROP TABLE alter_rename_partition_src;
+DROP TABLE alter_rename_partition;
+SHOW TABLES;
+
+-- With non-default Database
+
+CREATE DATABASE alter_rename_partition_db;
+USE alter_rename_partition_db;
+SHOW TABLES;
+
+CREATE TABLE alter_rename_partition_src (col1 STRING) STORED AS TEXTFILE ;
+LOAD DATA LOCAL INPATH '../data/files/test.dat' OVERWRITE INTO TABLE alter_rename_partition_src ;
+
+CREATE TABLE alter_rename_partition (col1 STRING) PARTITIONED BY (pcol1 STRING, pcol2 STRING) STORED AS SEQUENCEFILE;
+
+INSERT OVERWRITE TABLE alter_rename_partition PARTITION (pCol1='old_part1:', pcol2='old_part2:') SELECT col1 FROM alter_rename_partition_src ;
+SELECT * FROM alter_rename_partition WHERE pcol1='old_part1:' AND pcol2='old_part2:';
+
+ALTER TABLE alter_rename_partition PARTITION (pCol1='old_part1:', pcol2='old_part2:') RENAME TO PARTITION (pCol1='new_part1:', pcol2='new_part2:');
+SHOW PARTITIONS alter_rename_partition;
+SELECT * FROM alter_rename_partition WHERE pcol1='old_part1:' and pcol2='old_part2:';
+SELECT * FROM alter_rename_partition WHERE pcol1='new_part1:' and pcol2='new_part2:';
diff --git a/ql/src/test/queries/clientpositive/alter_rename_partition_authorization.q b/ql/src/test/queries/clientpositive/alter_rename_partition_authorization.q
index 1547d80182..7c4495a971 100644
--- a/ql/src/test/queries/clientpositive/alter_rename_partition_authorization.q
+++ b/ql/src/test/queries/clientpositive/alter_rename_partition_authorization.q
@@ -1,20 +1,20 @@
-create table src_auth_tmp as select * from src;
-
-create table authorization_part (key int, value string) partitioned by (ds string);
-ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
-set hive.security.authorization.enabled=true;
-grant select on table src_auth_tmp to user hive_test_user;
-
--- column grant to user
-grant Create on table authorization_part to user hive_test_user;
-grant Update on table authorization_part to user hive_test_user;
-grant Drop on table authorization_part to user hive_test_user;
-
-show grant user hive_test_user on table authorization_part;
-grant select(key) on table authorization_part to user hive_test_user;
-insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp; 
-show grant user hive_test_user on table authorization_part(key) partition (ds='2010');
-alter table authorization_part partition (ds='2010') rename to partition (ds='2010_tmp');
-show grant user hive_test_user on table authorization_part(key) partition (ds='2010_tmp');
-
-drop table authorization_part;
+create table src_auth_tmp as select * from src;
+
+create table authorization_part (key int, value string) partitioned by (ds string);
+ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
+set hive.security.authorization.enabled=true;
+grant select on table src_auth_tmp to user hive_test_user;
+
+-- column grant to user
+grant Create on table authorization_part to user hive_test_user;
+grant Update on table authorization_part to user hive_test_user;
+grant Drop on table authorization_part to user hive_test_user;
+
+show grant user hive_test_user on table authorization_part;
+grant select(key) on table authorization_part to user hive_test_user;
+insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp; 
+show grant user hive_test_user on table authorization_part(key) partition (ds='2010');
+alter table authorization_part partition (ds='2010') rename to partition (ds='2010_tmp');
+show grant user hive_test_user on table authorization_part(key) partition (ds='2010_tmp');
+
+drop table authorization_part;
diff --git a/ql/src/test/queries/clientpositive/auto_join25.q b/ql/src/test/queries/clientpositive/auto_join25.q
index 6d08a197cb..6dd3a6274b 100644
--- a/ql/src/test/queries/clientpositive/auto_join25.q
+++ b/ql/src/test/queries/clientpositive/auto_join25.q
@@ -1,28 +1,28 @@
-set hive.auto.convert.join = true;
-set hive.mapjoin.localtask.max.memory.usage = 0.0001;
-set hive.mapjoin.check.memory.rows = 2;
-
-CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
-
-FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value 
-where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');
-
-SELECT sum(hash(dest1.key,dest1.value)) FROM dest1;
-
-
-
-CREATE TABLE dest_j2(key INT, value STRING) STORED AS TEXTFILE;
-
-FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
-INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;
-
-SELECT sum(hash(dest_j2.key,dest_j2.value)) FROM dest_j2;
-
-CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;
-
-FROM src src1 JOIN src src2 ON (src1.key = src2.key)
-INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;
-
-SELECT sum(hash(dest_j1.key,dest_j1.value)) FROM dest_j1;
-
+set hive.auto.convert.join = true;
+set hive.mapjoin.localtask.max.memory.usage = 0.0001;
+set hive.mapjoin.check.memory.rows = 2;
+
+CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
+
+FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
+INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value 
+where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');
+
+SELECT sum(hash(dest1.key,dest1.value)) FROM dest1;
+
+
+
+CREATE TABLE dest_j2(key INT, value STRING) STORED AS TEXTFILE;
+
+FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
+INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;
+
+SELECT sum(hash(dest_j2.key,dest_j2.value)) FROM dest_j2;
+
+CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;
+
+FROM src src1 JOIN src src2 ON (src1.key = src2.key)
+INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;
+
+SELECT sum(hash(dest_j1.key,dest_j1.value)) FROM dest_j1;
+
diff --git a/ql/src/test/queries/clientpositive/auto_join26.q b/ql/src/test/queries/clientpositive/auto_join26.q
index 1a0ae9d194..16fbfe6cfb 100644
--- a/ql/src/test/queries/clientpositive/auto_join26.q
+++ b/ql/src/test/queries/clientpositive/auto_join26.q
@@ -1,10 +1,10 @@
-CREATE TABLE dest_j1(key INT, cnt INT);
-set hive.auto.convert.join = true;
-EXPLAIN
-INSERT OVERWRITE TABLE dest_j1 
-SELECT x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
-
-INSERT OVERWRITE TABLE dest_j1 
-SELECT  x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
-
-select * from dest_j1 x order by x.key;
+CREATE TABLE dest_j1(key INT, cnt INT);
+set hive.auto.convert.join = true;
+EXPLAIN
+INSERT OVERWRITE TABLE dest_j1 
+SELECT x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
+
+INSERT OVERWRITE TABLE dest_j1 
+SELECT  x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
+
+select * from dest_j1 x order by x.key;
diff --git a/ql/src/test/queries/clientpositive/create_genericudaf.q b/ql/src/test/queries/clientpositive/create_genericudaf.q
index e4f2e4a20a..b2bc0423e8 100644
--- a/ql/src/test/queries/clientpositive/create_genericudaf.q
+++ b/ql/src/test/queries/clientpositive/create_genericudaf.q
@@ -1,17 +1,17 @@
-EXPLAIN
-CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';
-
-CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';
-
-EXPLAIN
-SELECT
-    test_avg(1),
-    test_avg(substr(value,5))
-FROM src;
-
-SELECT
-    test_avg(1),
-    test_avg(substr(value,5))
-FROM src;
-
-DROP TEMPORARY FUNCTIOn test_avg;
+EXPLAIN
+CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';
+
+CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';
+
+EXPLAIN
+SELECT
+    test_avg(1),
+    test_avg(substr(value,5))
+FROM src;
+
+SELECT
+    test_avg(1),
+    test_avg(substr(value,5))
+FROM src;
+
+DROP TEMPORARY FUNCTIOn test_avg;
diff --git a/ql/src/test/queries/clientpositive/create_genericudf.q b/ql/src/test/queries/clientpositive/create_genericudf.q
index 4fedaab3df..1d43f30e8e 100644
--- a/ql/src/test/queries/clientpositive/create_genericudf.q
+++ b/ql/src/test/queries/clientpositive/create_genericudf.q
@@ -1,21 +1,21 @@
-EXPLAIN
-CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';
-
-CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';
-
-CREATE TABLE dest1(c1 STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 STRING);
-
-FROM src 
-INSERT OVERWRITE TABLE dest1 
-SELECT 
-    test_translate('abc', 'a', 'b'),
-    test_translate('abc', 'ab', 'bc'),
-    test_translate(NULL, 'a', 'b'),
-    test_translate('a', NULL, 'b'),
-    test_translate('a', 'a', NULL),
-    test_translate('abc', 'ab', 'b'),
-    test_translate('abc', 'a', 'ab');
-
-SELECT dest1.* FROM dest1 LIMIT 1;
-
-DROP TEMPORARY FUNCTION test_translate;
+EXPLAIN
+CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';
+
+CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';
+
+CREATE TABLE dest1(c1 STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 STRING);
+
+FROM src 
+INSERT OVERWRITE TABLE dest1 
+SELECT 
+    test_translate('abc', 'a', 'b'),
+    test_translate('abc', 'ab', 'bc'),
+    test_translate(NULL, 'a', 'b'),
+    test_translate('a', NULL, 'b'),
+    test_translate('a', 'a', NULL),
+    test_translate('abc', 'ab', 'b'),
+    test_translate('abc', 'a', 'ab');
+
+SELECT dest1.* FROM dest1 LIMIT 1;
+
+DROP TEMPORARY FUNCTION test_translate;
diff --git a/ql/src/test/queries/clientpositive/create_udaf.q b/ql/src/test/queries/clientpositive/create_udaf.q
index 52e71de4dd..c88a2607fc 100644
--- a/ql/src/test/queries/clientpositive/create_udaf.q
+++ b/ql/src/test/queries/clientpositive/create_udaf.q
@@ -1,12 +1,12 @@
-EXPLAIN
-CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';
-
-CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';
-
-CREATE TABLE dest1(col INT);
-
-FROM src INSERT OVERWRITE TABLE dest1 SELECT test_max(length(src.value));
-
-SELECT dest1.* FROM dest1;
-
-DROP TEMPORARY FUNCTION test_max;
+EXPLAIN
+CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';
+
+CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';
+
+CREATE TABLE dest1(col INT);
+
+FROM src INSERT OVERWRITE TABLE dest1 SELECT test_max(length(src.value));
+
+SELECT dest1.* FROM dest1;
+
+DROP TEMPORARY FUNCTION test_max;
diff --git a/ql/src/test/queries/clientpositive/hook_context_cs.q b/ql/src/test/queries/clientpositive/hook_context_cs.q
index 7989aff4ae..94ba14802f 100644
--- a/ql/src/test/queries/clientpositive/hook_context_cs.q
+++ b/ql/src/test/queries/clientpositive/hook_context_cs.q
@@ -1,14 +1,14 @@
-drop table vcsc;
-CREATE TABLE vcsc (c STRING) PARTITIONED BY (ds STRING);
-ALTER TABLE vcsc ADD partition (ds='dummy') location '${system:test.tmp.dir}/VerifyContentSummaryCacheHook';
-
-set hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook;
-SELECT a.c, b.c FROM vcsc a JOIN vcsc b ON a.ds = 'dummy' AND b.ds = 'dummy' AND a.c = b.c;
-
-set mapred.job.tracker=local;
-set hive.exec.pre.hooks = ;
-set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook;
-SELECT a.c, b.c FROM vcsc a JOIN vcsc b ON a.ds = 'dummy' AND b.ds = 'dummy' AND a.c = b.c;
-
-set hive.exec.post.hooks=;
-drop table vcsc;
+drop table vcsc;
+CREATE TABLE vcsc (c STRING) PARTITIONED BY (ds STRING);
+ALTER TABLE vcsc ADD partition (ds='dummy') location '${system:test.tmp.dir}/VerifyContentSummaryCacheHook';
+
+set hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook;
+SELECT a.c, b.c FROM vcsc a JOIN vcsc b ON a.ds = 'dummy' AND b.ds = 'dummy' AND a.c = b.c;
+
+set mapred.job.tracker=local;
+set hive.exec.pre.hooks = ;
+set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook;
+SELECT a.c, b.c FROM vcsc a JOIN vcsc b ON a.ds = 'dummy' AND b.ds = 'dummy' AND a.c = b.c;
+
+set hive.exec.post.hooks=;
+drop table vcsc;
diff --git a/ql/src/test/queries/clientpositive/join_empty.q b/ql/src/test/queries/clientpositive/join_empty.q
index 5cb18297f4..1982487c84 100644
--- a/ql/src/test/queries/clientpositive/join_empty.q
+++ b/ql/src/test/queries/clientpositive/join_empty.q
@@ -1,10 +1,10 @@
-create table srcpart_empty(key int, value string) partitioned by (ds string);
-create table src2_empty (key int, value string);
-
-select /*+mapjoin(a)*/ a.key, b.value from srcpart_empty a join src b on a.key=b.key;
-select /*+mapjoin(a)*/ a.key, b.value from src2_empty a join src b on a.key=b.key;
-
-set hive.mapred.mode=nonstrict;
-set hive.auto.convert.join = true;
-select a.key, b.value from srcpart_empty a join src b on a.key=b.key;
+create table srcpart_empty(key int, value string) partitioned by (ds string);
+create table src2_empty (key int, value string);
+
+select /*+mapjoin(a)*/ a.key, b.value from srcpart_empty a join src b on a.key=b.key;
+select /*+mapjoin(a)*/ a.key, b.value from src2_empty a join src b on a.key=b.key;
+
+set hive.mapred.mode=nonstrict;
+set hive.auto.convert.join = true;
+select a.key, b.value from srcpart_empty a join src b on a.key=b.key;
 select a.key, b.value from src2_empty a join src b on a.key=b.key;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/mapjoin_hook.q b/ql/src/test/queries/clientpositive/mapjoin_hook.q
index ef4b203e8e..e3e1baab51 100644
--- a/ql/src/test/queries/clientpositive/mapjoin_hook.q
+++ b/ql/src/test/queries/clientpositive/mapjoin_hook.q
@@ -1,30 +1,30 @@
-set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.MapJoinCounterHook ;
-drop table dest1;
-CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
-
-set hive.auto.convert.join = true;
-
-INSERT OVERWRITE TABLE dest1 
-SELECT /*+ MAPJOIN(x) */ x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
-
-
-FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;
-
-
-
-set hive.mapjoin.localtask.max.memory.usage = 0.0001;
-set hive.mapjoin.check.memory.rows = 2;
-
-
-FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value 
-where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');
-
-
-FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
-INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;
-
-
-
-
+set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.MapJoinCounterHook ;
+drop table dest1;
+CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
+
+set hive.auto.convert.join = true;
+
+INSERT OVERWRITE TABLE dest1 
+SELECT /*+ MAPJOIN(x) */ x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
+
+
+FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
+INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;
+
+
+
+set hive.mapjoin.localtask.max.memory.usage = 0.0001;
+set hive.mapjoin.check.memory.rows = 2;
+
+
+FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
+INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value 
+where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');
+
+
+FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
+INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;
+
+
+
+
diff --git a/ql/src/test/queries/clientpositive/split_sample.q b/ql/src/test/queries/clientpositive/split_sample.q
index 0270449831..10dfd352e9 100644
--- a/ql/src/test/queries/clientpositive/split_sample.q
+++ b/ql/src/test/queries/clientpositive/split_sample.q
@@ -1,112 +1,112 @@
-USE default;
-
-set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
-set mapred.max.split.size=300;
-set mapred.min.split.size=300;
-set mapred.min.split.size.per.node=300;
-set mapred.min.split.size.per.rack=300;
-set hive.merge.smallfiles.avgsize=1;
-
--- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
--- This test sets mapred.max.split.size=300 and hive.merge.smallfiles.avgsize=1
--- in an attempt to force the generation of multiple splits and multiple output files.
--- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
--- when using CombineFileInputFormat, so only one split is generated. This has a
--- significant impact on the results of the TABLESAMPLE(x PERCENT). This issue was
--- fixed in MAPREDUCE-2046 which is included in 0.22.
-
--- create multiple file inputs (two enable multiple splits)
-create table ss_i_part (key int, value string) partitioned by (p string);
-insert overwrite table ss_i_part partition (p='1') select key, value from src;
-insert overwrite table ss_i_part partition (p='2') select key, value from src;
-insert overwrite table ss_i_part partition (p='3') select key, value from src;
-create table ss_src2 as select key, value from ss_i_part;
-
-select count(1) from ss_src2 tablesample(1 percent);
-
--- sample first split
-desc ss_src2;
-set hive.sample.seednumber=0;
-explain select key, value from ss_src2 tablesample(1 percent) limit 10;
-select key, value from ss_src2 tablesample(1 percent) limit 10;
-
--- verify seed number of sampling
-insert overwrite table ss_i_part partition (p='1') select key+10000, value from src;
-insert overwrite table ss_i_part partition (p='2') select key+20000, value from src;
-insert overwrite table ss_i_part partition (p='3') select key+30000, value from src;
-create table ss_src3 as select key, value from ss_i_part;
-set hive.sample.seednumber=3;
-create table ss_t3 as select sum(key) % 397 as s from ss_src3 tablesample(1 percent) limit 10;
-set hive.sample.seednumber=4;
-create table ss_t4 as select sum(key) % 397 as s from ss_src3 tablesample(1 percent) limit 10;
-set hive.sample.seednumber=5;
-create table ss_t5 as select sum(key) % 397 as s from ss_src3 tablesample(1 percent) limit 10;
-select sum(s) from (select s from ss_t3 union all select s from ss_t4 union all select s from ss_t5) t;
-
--- sample more than one split
-explain select count(distinct key) from ss_src2 tablesample(70 percent) limit 10;
-select count(distinct key) from ss_src2 tablesample(70 percent) limit 10;
-
--- sample all splits
-select count(1) from ss_src2 tablesample(100 percent);
-
--- subquery
-explain select key from (select key from ss_src2 tablesample(1 percent) limit 10) subq;
-select key from (select key from ss_src2 tablesample(1 percent) limit 10) subq;
-
--- groupby
-select key, count(1) from ss_src2 tablesample(1 percent) group by key order by key;
-
--- sample one of two tables:
-create table ss_src1 as select * from ss_src2;
-select t2.key as k from ss_src1 join ss_src2 tablesample(1 percent) t2 on ss_src1.key=t2.key order by k;
-
--- sample two tables
-explain select * from (
-select t1.key as k1, t2.key as k from ss_src1 tablesample(80 percent) t1 full outer join ss_src2 tablesample(2 percent) t2 on t1.key=t2.key
-) subq where k in (199, 10199, 20199) or k1 in (199, 10199, 20199);
-
-select * from (
-select t1.key as k1, t2.key as k from ss_src1 tablesample(80 percent) t1 full outer join ss_src2 tablesample(2 percent) t2 on t1.key=t2.key
-) subq where k in (199, 10199, 20199) or k1 in (199, 10199, 20199);
-
--- shrink last split
-explain select count(1) from ss_src2 tablesample(1 percent);
-set mapred.max.split.size=300000;
-set mapred.min.split.size=300000;
-set mapred.min.split.size.per.node=300000;
-set mapred.min.split.size.per.rack=300000;
-select count(1) from ss_src2 tablesample(1 percent);
-select count(1) from ss_src2 tablesample(50 percent);
-
---HIVE-3401 more split samplings
-
--- total length
-explain
-select count(1) from ss_src2 tablesample(100B);
-select count(1) from ss_src2 tablesample(100B);
-
-explain
-select count(1) from ss_src2 tablesample(1K);
-select count(1) from ss_src2 tablesample(1K);
-
--- row per split
-explain
-select key, value from ss_src2 tablesample(0 ROWS);
-select key, value from ss_src2 tablesample(0 ROWS);
-
-explain
-select count(1) from ss_src2 tablesample(10 ROWS);
-select count(1) from ss_src2 tablesample(10 ROWS);
-
-explain
-select count(1) from ss_src2 tablesample(100 ROWS);
-select count(1) from ss_src2 tablesample(100 ROWS);
-
-set hive.fetch.task.conversion=more;
-select key from ss_src2 tablesample(200B);
-select key from ss_src2 tablesample(10 ROWS);
-
-set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
--- ROW type works with other input formats (others, don't)
-select count(1) from ss_src2 tablesample(10 ROWS);
+USE default;
+
+set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
+set mapred.max.split.size=300;
+set mapred.min.split.size=300;
+set mapred.min.split.size.per.node=300;
+set mapred.min.split.size.per.rack=300;
+set hive.merge.smallfiles.avgsize=1;
+
+-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
+-- This test sets mapred.max.split.size=300 and hive.merge.smallfiles.avgsize=1
+-- in an attempt to force the generation of multiple splits and multiple output files.
+-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
+-- when using CombineFileInputFormat, so only one split is generated. This has a
+-- significant impact on the results of the TABLESAMPLE(x PERCENT). This issue was
+-- fixed in MAPREDUCE-2046 which is included in 0.22.
+
+-- create multiple file inputs (two enable multiple splits)
+create table ss_i_part (key int, value string) partitioned by (p string);
+insert overwrite table ss_i_part partition (p='1') select key, value from src;
+insert overwrite table ss_i_part partition (p='2') select key, value from src;
+insert overwrite table ss_i_part partition (p='3') select key, value from src;
+create table ss_src2 as select key, value from ss_i_part;
+
+select count(1) from ss_src2 tablesample(1 percent);
+
+-- sample first split
+desc ss_src2;
+set hive.sample.seednumber=0;
+explain select key, value from ss_src2 tablesample(1 percent) limit 10;
+select key, value from ss_src2 tablesample(1 percent) limit 10;
+
+-- verify seed number of sampling
+insert overwrite table ss_i_part partition (p='1') select key+10000, value from src;
+insert overwrite table ss_i_part partition (p='2') select key+20000, value from src;
+insert overwrite table ss_i_part partition (p='3') select key+30000, value from src;
+create table ss_src3 as select key, value from ss_i_part;
+set hive.sample.seednumber=3;
+create table ss_t3 as select sum(key) % 397 as s from ss_src3 tablesample(1 percent) limit 10;
+set hive.sample.seednumber=4;
+create table ss_t4 as select sum(key) % 397 as s from ss_src3 tablesample(1 percent) limit 10;
+set hive.sample.seednumber=5;
+create table ss_t5 as select sum(key) % 397 as s from ss_src3 tablesample(1 percent) limit 10;
+select sum(s) from (select s from ss_t3 union all select s from ss_t4 union all select s from ss_t5) t;
+
+-- sample more than one split
+explain select count(distinct key) from ss_src2 tablesample(70 percent) limit 10;
+select count(distinct key) from ss_src2 tablesample(70 percent) limit 10;
+
+-- sample all splits
+select count(1) from ss_src2 tablesample(100 percent);
+
+-- subquery
+explain select key from (select key from ss_src2 tablesample(1 percent) limit 10) subq;
+select key from (select key from ss_src2 tablesample(1 percent) limit 10) subq;
+
+-- groupby
+select key, count(1) from ss_src2 tablesample(1 percent) group by key order by key;
+
+-- sample one of two tables:
+create table ss_src1 as select * from ss_src2;
+select t2.key as k from ss_src1 join ss_src2 tablesample(1 percent) t2 on ss_src1.key=t2.key order by k;
+
+-- sample two tables
+explain select * from (
+select t1.key as k1, t2.key as k from ss_src1 tablesample(80 percent) t1 full outer join ss_src2 tablesample(2 percent) t2 on t1.key=t2.key
+) subq where k in (199, 10199, 20199) or k1 in (199, 10199, 20199);
+
+select * from (
+select t1.key as k1, t2.key as k from ss_src1 tablesample(80 percent) t1 full outer join ss_src2 tablesample(2 percent) t2 on t1.key=t2.key
+) subq where k in (199, 10199, 20199) or k1 in (199, 10199, 20199);
+
+-- shrink last split
+explain select count(1) from ss_src2 tablesample(1 percent);
+set mapred.max.split.size=300000;
+set mapred.min.split.size=300000;
+set mapred.min.split.size.per.node=300000;
+set mapred.min.split.size.per.rack=300000;
+select count(1) from ss_src2 tablesample(1 percent);
+select count(1) from ss_src2 tablesample(50 percent);
+
+--HIVE-3401 more split samplings
+
+-- total length
+explain
+select count(1) from ss_src2 tablesample(100B);
+select count(1) from ss_src2 tablesample(100B);
+
+explain
+select count(1) from ss_src2 tablesample(1K);
+select count(1) from ss_src2 tablesample(1K);
+
+-- row per split
+explain
+select key, value from ss_src2 tablesample(0 ROWS);
+select key, value from ss_src2 tablesample(0 ROWS);
+
+explain
+select count(1) from ss_src2 tablesample(10 ROWS);
+select count(1) from ss_src2 tablesample(10 ROWS);
+
+explain
+select count(1) from ss_src2 tablesample(100 ROWS);
+select count(1) from ss_src2 tablesample(100 ROWS);
+
+set hive.fetch.task.conversion=more;
+select key from ss_src2 tablesample(200B);
+select key from ss_src2 tablesample(10 ROWS);
+
+set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
+-- ROW type works with other input formats (others, don't)
+select count(1) from ss_src2 tablesample(10 ROWS);
diff --git a/ql/src/test/queries/clientpositive/udf_compare_java_string.q b/ql/src/test/queries/clientpositive/udf_compare_java_string.q
index 3c9d5e1189..6c12f81304 100644
--- a/ql/src/test/queries/clientpositive/udf_compare_java_string.q
+++ b/ql/src/test/queries/clientpositive/udf_compare_java_string.q
@@ -1,10 +1,10 @@
-EXPLAIN
-CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';
-
-CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';
-
-select * from src where value = test_udf_get_java_string("val_66"); 
-select * from (select * from src where value = 'val_66' or value = 'val_8') t where value <> test_udf_get_java_string("val_8"); 
-
-
-DROP TEMPORARY FUNCTION test_udf_get_java_boolean;
+EXPLAIN
+CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';
+
+CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';
+
+select * from src where value = test_udf_get_java_string("val_66"); 
+select * from (select * from src where value = 'val_66' or value = 'val_8') t where value <> test_udf_get_java_string("val_8"); 
+
+
+DROP TEMPORARY FUNCTION test_udf_get_java_boolean;
diff --git a/ql/src/test/queries/clientpositive/udf_logic_java_boolean.q b/ql/src/test/queries/clientpositive/udf_logic_java_boolean.q
index ef48907a5b..508f9fd558 100644
--- a/ql/src/test/queries/clientpositive/udf_logic_java_boolean.q
+++ b/ql/src/test/queries/clientpositive/udf_logic_java_boolean.q
@@ -1,28 +1,28 @@
-EXPLAIN
-CREATE TEMPORARY FUNCTION test_udf_get_java_boolean AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean';
-
-CREATE TEMPORARY FUNCTION test_udf_get_java_boolean AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean';
-
-select 1 from src where test_udf_get_java_boolean("false") and True limit 1; 
-select 1 from src where test_udf_get_java_boolean("true") and True limit 1; 
-select 1 from src where True and test_udf_get_java_boolean("false") limit 1; 
-select 1 from src where False and test_udf_get_java_boolean("false") limit 1; 
-select 1 from src where test_udf_get_java_boolean("true") and test_udf_get_java_boolean("true") limit 1; 
-select 1 from src where test_udf_get_java_boolean("true") and test_udf_get_java_boolean("false") limit 1;
-select 1 from src where test_udf_get_java_boolean("false") and test_udf_get_java_boolean("true") limit 1; 
-select 1 from src where test_udf_get_java_boolean("false") and test_udf_get_java_boolean("false") limit 1; 
-
-select 1 from src where test_udf_get_java_boolean("false") or True limit 1; 
-select 1 from src where test_udf_get_java_boolean("true") or True limit 1; 
-select 1 from src where True or test_udf_get_java_boolean("false") limit 1; 
-select 1 from src where False or test_udf_get_java_boolean("false") limit 1; 
-select 1 from src where test_udf_get_java_boolean("true") or test_udf_get_java_boolean("true") limit 1; 
-select 1 from src where test_udf_get_java_boolean("true") or test_udf_get_java_boolean("false") limit 1;
-select 1 from src where test_udf_get_java_boolean("false") or test_udf_get_java_boolean("true") limit 1; 
-select 1 from src where test_udf_get_java_boolean("false") or test_udf_get_java_boolean("false") limit 1; 
-
-select 1 from src where not(test_udf_get_java_boolean("false")) limit 1; 
-select 1 from src where not(test_udf_get_java_boolean("true")) limit 1; 
-
-
-DROP TEMPORARY FUNCTION test_udf_get_java_boolean;
+EXPLAIN
+CREATE TEMPORARY FUNCTION test_udf_get_java_boolean AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean';
+
+CREATE TEMPORARY FUNCTION test_udf_get_java_boolean AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean';
+
+select 1 from src where test_udf_get_java_boolean("false") and True limit 1; 
+select 1 from src where test_udf_get_java_boolean("true") and True limit 1; 
+select 1 from src where True and test_udf_get_java_boolean("false") limit 1; 
+select 1 from src where False and test_udf_get_java_boolean("false") limit 1; 
+select 1 from src where test_udf_get_java_boolean("true") and test_udf_get_java_boolean("true") limit 1; 
+select 1 from src where test_udf_get_java_boolean("true") and test_udf_get_java_boolean("false") limit 1;
+select 1 from src where test_udf_get_java_boolean("false") and test_udf_get_java_boolean("true") limit 1; 
+select 1 from src where test_udf_get_java_boolean("false") and test_udf_get_java_boolean("false") limit 1; 
+
+select 1 from src where test_udf_get_java_boolean("false") or True limit 1; 
+select 1 from src where test_udf_get_java_boolean("true") or True limit 1; 
+select 1 from src where True or test_udf_get_java_boolean("false") limit 1; 
+select 1 from src where False or test_udf_get_java_boolean("false") limit 1; 
+select 1 from src where test_udf_get_java_boolean("true") or test_udf_get_java_boolean("true") limit 1; 
+select 1 from src where test_udf_get_java_boolean("true") or test_udf_get_java_boolean("false") limit 1;
+select 1 from src where test_udf_get_java_boolean("false") or test_udf_get_java_boolean("true") limit 1; 
+select 1 from src where test_udf_get_java_boolean("false") or test_udf_get_java_boolean("false") limit 1; 
+
+select 1 from src where not(test_udf_get_java_boolean("false")) limit 1; 
+select 1 from src where not(test_udf_get_java_boolean("true")) limit 1; 
+
+
+DROP TEMPORARY FUNCTION test_udf_get_java_boolean;
