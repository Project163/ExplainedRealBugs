diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java b/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java
index d04895e95a..6fd89713f3 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/TUGIBasedProcessor.java
@@ -131,6 +131,8 @@ public Void run() {
         throw new RuntimeException(ie); // unexpected!
       } catch (IOException ioe) {
         throw new RuntimeException(ioe); // unexpected!
+      } finally {
+          shim.closeAllForUGI(clientUgi);
       }
     }
   }
diff --git a/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java b/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
index dd5be7482d..afaa6fd841 100644
--- a/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
+++ b/shims/src/0.20/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
@@ -588,4 +588,10 @@ public void progress() {
   public org.apache.hadoop.mapreduce.JobContext newJobContext(Job job) {
     return new org.apache.hadoop.mapreduce.JobContext(job.getConfiguration(), job.getJobID());
   }
+
+  @Override
+  public void closeAllForUGI(UserGroupInformation ugi) {
+    // No such functionality in ancient hadoop
+    return;
+  }
 }
diff --git a/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java b/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
index 5832f67f82..8dd6432bef 100644
--- a/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
+++ b/shims/src/common-secure/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
@@ -27,6 +27,8 @@
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -66,6 +68,9 @@
  * Base implemention for shims against secure Hadoop 0.20.3/0.23.
  */
 public abstract class HadoopShimsSecure implements HadoopShims {
+
+  static final Log LOG = LogFactory.getLog(HadoopShimsSecure.class);
+
   public boolean usesJobShell() {
     return false;
   }
@@ -532,6 +537,15 @@ public UserGroupInformation createRemoteUser(String userName, List<String> group
     return UserGroupInformation.createRemoteUser(userName);
   }
 
+  @Override
+  public void closeAllForUGI(UserGroupInformation ugi) {
+    try {
+      FileSystem.closeAllForUGI(ugi);
+    } catch (IOException e) {
+      LOG.error("Could not clean up file-system handles for UGI: " + ugi, e);
+    }
+  }
+
   @Override
   abstract public JobTrackerState getJobTrackerState(ClusterStatus clusterStatus) throws Exception;
 
diff --git a/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java b/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
index da36e9d345..ff45021b1a 100644
--- a/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
+++ b/shims/src/common-secure/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java
@@ -39,6 +39,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport;
 import org.apache.hadoop.security.SaslRpcServer;
 import org.apache.hadoop.security.SaslRpcServer.AuthMethod;
@@ -510,8 +511,9 @@ public boolean process(final TProtocol inProt, final TProtocol outProt) throws T
          }
          Socket socket = ((TSocket)(saslTrans.getUnderlyingTransport())).getSocket();
          remoteAddress.set(socket.getInetAddress());
+         UserGroupInformation clientUgi = null;
          try {
-           UserGroupInformation clientUgi = UserGroupInformation.createProxyUser(
+           clientUgi = UserGroupInformation.createProxyUser(
               endUser, UserGroupInformation.getLoginUser());
            return clientUgi.doAs(new PrivilegedExceptionAction<Boolean>() {
                public Boolean run() {
@@ -532,6 +534,14 @@ public Boolean run() {
          } catch (IOException ioe) {
            throw new RuntimeException(ioe); // unexpected!
          }
+         finally {
+           if (clientUgi != null) {
+            try { FileSystem.closeAllForUGI(clientUgi); }
+              catch(IOException exception) {
+                LOG.error("Could not clean up file-system handles for UGI: " + clientUgi, exception);
+              }
+          }
+         }
        }
      }
 
diff --git a/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java b/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
index 0721736bac..7d74329c45 100644
--- a/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
+++ b/shims/src/common/java/org/apache/hadoop/hive/shims/HadoopShims.java
@@ -188,6 +188,9 @@ public URI getHarUri(URI original, URI base, URI originalBase)
    * In secure versions of Hadoop, this simply returns the current
    * access control context's user, ignoring the configuration.
    */
+
+  public void closeAllForUGI(UserGroupInformation ugi);
+
   public UserGroupInformation getUGIForConf(Configuration conf) throws LoginException, IOException;
 
   /**
