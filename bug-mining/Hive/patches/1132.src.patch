diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index d0b14625e1..a721da7cc4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -3542,21 +3542,30 @@ private Operator genCommonGroupByPlanReduceSinkOperator(QB qb, List<String> dest
       // Need to pass all of the columns used in the where clauses as reduce values
       ASTNode whereClause = parseInfo.getWhrForClause(destination);
       if (whereClause != null) {
-        List<ASTNode> columnExprs =
-            getColumnExprsFromASTNode(whereClause, reduceSinkInputRowResolver);
-        for (int i = 0; i < columnExprs.size(); i++) {
-          ASTNode parameter = columnExprs.get(i);
-          if (reduceSinkOutputRowResolver.getExpression(parameter) == null) {
-            reduceValues.add(genExprNodeDesc(parameter,
-                reduceSinkInputRowResolver));
-            outputValueColumnNames
-                .add(getColumnInternalName(reduceValues.size() - 1));
-            String field = Utilities.ReduceField.VALUE.toString() + "."
-                + getColumnInternalName(reduceValues.size() - 1);
-            reduceSinkOutputRowResolver.putExpression(parameter, new ColumnInfo(field,
-                reduceValues.get(reduceValues.size() - 1).getTypeInfo(), null,
-                false));
+        assert whereClause.getChildCount() == 1;
+        ASTNode predicates = (ASTNode) whereClause.getChild(0);
+
+        Map<ASTNode, ExprNodeDesc> nodeOutputs =
+            genAllExprNodeDesc(predicates, reduceSinkInputRowResolver);
+        removeMappingForKeys(predicates, nodeOutputs, reduceKeys);
+
+        // extract columns missing in current RS key/value
+        for (Map.Entry<ASTNode, ExprNodeDesc> entry : nodeOutputs.entrySet()) {
+          ASTNode parameter = (ASTNode) entry.getKey();
+          ExprNodeDesc expression = (ExprNodeDesc) entry.getValue();
+          if (!(expression instanceof ExprNodeColumnDesc)) {
+            continue;
           }
+          if (ExprNodeDescUtils.indexOf(expression, reduceValues) >= 0) {
+            continue;
+          }
+          String internalName = getColumnInternalName(reduceValues.size());
+          String field = Utilities.ReduceField.VALUE.toString() + "." + internalName;
+
+          reduceValues.add(expression);
+          outputValueColumnNames.add(internalName);
+          reduceSinkOutputRowResolver.putExpression(parameter,
+              new ColumnInfo(field, expression.getTypeInfo(), null, false));
         }
       }
     }
@@ -3571,32 +3580,27 @@ private Operator genCommonGroupByPlanReduceSinkOperator(QB qb, List<String> dest
     return rsOp;
   }
 
-  /**
-   * Given an ASTNode, it returns all of the descendant ASTNodes which represent column expressions
-   *
-   * @param node
-   * @param inputRR
-   * @return
-   * @throws SemanticException
-   */
-  private List<ASTNode> getColumnExprsFromASTNode(ASTNode node, RowResolver inputRR)
-      throws SemanticException {
-
-    List<ASTNode> nodes = new ArrayList<ASTNode>();
-    if (node.getChildCount() == 0) {
-      return nodes;
-    }
-    for (int i = 0; i < node.getChildCount(); i++) {
-      ASTNode child = (ASTNode) node.getChild(i);
-      if (child.getType() == HiveParser.TOK_TABLE_OR_COL && child.getChild(0) != null &&
-          inputRR.get(null,
-              BaseSemanticAnalyzer.unescapeIdentifier(child.getChild(0).getText())) != null) {
-        nodes.add(child);
-      } else {
-        nodes.addAll(getColumnExprsFromASTNode(child, inputRR));
+  // Remove expression node descriptor and children of it for a given predicate
+  // from mapping if it's already on RS keys.
+  // Remaining column expressions would be a candidate for an RS value
+  private void removeMappingForKeys(ASTNode predicate, Map<ASTNode, ExprNodeDesc> mapping,
+      List<ExprNodeDesc> keys) {
+    ExprNodeDesc expr = (ExprNodeDesc) mapping.get(predicate);
+    if (expr != null && ExprNodeDescUtils.indexOf(expr, keys) >= 0) {
+      removeRecursively(predicate, mapping);
+    } else {
+      for (int i = 0; i < predicate.getChildCount(); i++) {
+        removeMappingForKeys((ASTNode) predicate.getChild(i), mapping, keys);
       }
     }
-    return nodes;
+  }
+
+  // Remove expression node desc and all children of it from mapping
+  private void removeRecursively(ASTNode current, Map<ASTNode, ExprNodeDesc> mapping) {
+    mapping.remove(current);
+    for (int i = 0; i < current.getChildCount(); i++) {
+      removeRecursively((ASTNode) current.getChild(i), mapping);
+    }
   }
 
   /**
@@ -5906,7 +5910,7 @@ private Operator genJoinOperator(QB qb, QBJoinTree joinTree,
     genJoinOperatorTypeCheck(joinSrcOp, srcOps);
 
     JoinOperator joinOp = (JoinOperator) genJoinOperatorChildren(joinTree,
-        joinSrcOp, srcOps, omitOpts);
+      joinSrcOp, srcOps, omitOpts);
     joinContext.put(joinOp, joinTree);
     return joinOp;
   }
@@ -8633,16 +8637,7 @@ private List<FieldSchema> convertRowSchemaToViewSchema(RowResolver rr) {
   }
 
   /**
-   * Generates an expression node descriptor for the expression passed in the
-   * arguments. This function uses the row resolver and the metadata information
-   * that are passed as arguments to resolve the column names to internal names.
-   *
-   * @param expr
-   *          The expression
-   * @param input
-   *          The row resolver
-   * @return exprNodeDesc
-   * @throws SemanticException
+   * Generates an expression node descriptor for the expression with TypeCheckCtx.
    */
   public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input)
       throws SemanticException {
@@ -8653,20 +8648,19 @@ public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input)
   }
 
   /**
-   * Generates an expression node descriptor for the expression passed in the
-   * arguments. This function uses the row resolver and the metadata information
-   * that are passed as arguments to resolve the column names to internal names.
-   *
-   * @param expr
-   *          The expression
-   * @param input
-   *          The row resolver
-   * @param tcCtx
-   *          Customized type-checking context
-   * @return exprNodeDesc
-   * @throws SemanticException
+   * Generates an expression node descriptors for the expression and children of it
+   * with default TypeCheckCtx.
+   */
+  public Map<ASTNode, ExprNodeDesc> genAllExprNodeDesc(ASTNode expr, RowResolver input)
+      throws SemanticException {
+    TypeCheckCtx tcCtx = new TypeCheckCtx(input);
+    return genAllExprNodeDesc(expr, input, tcCtx);
+  }
+
+  /**
+   * Returns expression node descriptor for the expression.
+   * If it's evaluated already in previous operator, it can be retrieved from cache.
    */
-  @SuppressWarnings("nls")
   public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input,
       TypeCheckCtx tcCtx) throws SemanticException {
     // We recursively create the exprNodeDesc. Base cases: when we encounter
@@ -8677,6 +8671,19 @@ public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input,
     // build the exprNodeFuncDesc with recursively built children.
 
     // If the current subExpression is pre-calculated, as in Group-By etc.
+    ExprNodeDesc cached = getExprNodeDescCached(expr, input);
+    if (cached == null) {
+      Map<ASTNode, ExprNodeDesc> allExprs = genAllExprNodeDesc(expr, input, tcCtx);
+      return allExprs.get(expr);
+    }
+    return cached;
+  }
+
+  /**
+   * Find ExprNodeDesc for the expression cached in the RowResolver. Returns null if not exists.
+   */
+  private ExprNodeDesc getExprNodeDescCached(ASTNode expr, RowResolver input)
+      throws SemanticException {
     ColumnInfo colInfo = input.getExpression(expr);
     if (colInfo != null) {
       ASTNode source = input.getExpressionSource(expr);
@@ -8687,11 +8694,30 @@ public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input,
           .getInternalName(), colInfo.getTabAlias(), colInfo
           .getIsVirtualCol(), colInfo.isSkewedCol());
     }
+    return null;
+  }
 
-    // Create the walker and the rules dispatcher.
+  /**
+   * Generates all of the expression node descriptors for the expression and children of it
+   * passed in the arguments. This function uses the row resolver and the metadata information
+   * that are passed as arguments to resolve the column names to internal names.
+   *
+   * @param expr
+   *          The expression
+   * @param input
+   *          The row resolver
+   * @param tcCtx
+   *          Customized type-checking context
+   * @return expression to exprNodeDesc mapping
+   * @throws SemanticException Failed to evaluate expression
+   */
+  @SuppressWarnings("nls")
+  public Map<ASTNode, ExprNodeDesc> genAllExprNodeDesc(ASTNode expr, RowResolver input,
+    TypeCheckCtx tcCtx) throws SemanticException {
+    // Create the walker and  the rules dispatcher.
     tcCtx.setUnparseTranslator(unparseTranslator);
 
-    HashMap<Node, Object> nodeOutputs =
+    Map<ASTNode, ExprNodeDesc> nodeOutputs =
         TypeCheckProcFactory.genExprNode(expr, tcCtx);
     ExprNodeDesc desc = (ExprNodeDesc) nodeOutputs.get(expr);
     if (desc == null) {
@@ -8704,17 +8730,14 @@ public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input,
 
     if (!unparseTranslator.isEnabled()) {
       // Not creating a view, so no need to track view expansions.
-      return desc;
+      return nodeOutputs;
     }
 
-    for (Map.Entry<Node, Object> entry : nodeOutputs.entrySet()) {
-      if (!(entry.getKey() instanceof ASTNode)) {
-        continue;
-      }
+    for (Map.Entry<ASTNode, ExprNodeDesc> entry : nodeOutputs.entrySet()) {
       if (!(entry.getValue() instanceof ExprNodeColumnDesc)) {
         continue;
       }
-      ASTNode node = (ASTNode) entry.getKey();
+      ASTNode node = entry.getKey();
       ExprNodeColumnDesc columnDesc = (ExprNodeColumnDesc) entry.getValue();
       if ((columnDesc.getTabAlias() == null)
           || (columnDesc.getTabAlias().length() == 0)) {
@@ -8730,7 +8753,7 @@ public ExprNodeDesc genExprNodeDesc(ASTNode expr, RowResolver input,
       unparseTranslator.addTranslation(node, replacementText.toString());
     }
 
-    return desc;
+    return nodeOutputs;
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
index c267ad56e7..6cedf287cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java
@@ -124,7 +124,7 @@ public static ExprNodeDesc processGByExpr(Node nd, Object procCtx)
     return desc;
   }
 
-  public static HashMap<Node, Object> genExprNode(ASTNode expr,
+  public static Map<ASTNode, ExprNodeDesc> genExprNode(ASTNode expr,
       TypeCheckCtx tcCtx) throws SemanticException {
     // Create the walker, the rules dispatcher and the context.
     // create a walker which walks the tree in a DFS manner while maintaining
@@ -162,10 +162,23 @@ public static HashMap<Node, Object> genExprNode(ASTNode expr,
     // Create a list of topop nodes
     ArrayList<Node> topNodes = new ArrayList<Node>();
     topNodes.add(expr);
-    HashMap<Node, Object> nodeOutputs = new HashMap<Node, Object>();
+    HashMap<Node, Object> nodeOutputs = new LinkedHashMap<Node, Object>();
     ogw.startWalking(topNodes, nodeOutputs);
 
-    return nodeOutputs;
+    return convert(nodeOutputs);
+  }
+
+  // temporary type-safe casting
+  private static Map<ASTNode, ExprNodeDesc> convert(Map<Node, Object> outputs) {
+    Map<ASTNode, ExprNodeDesc> converted = new LinkedHashMap<ASTNode, ExprNodeDesc>();
+    for (Map.Entry<Node, Object> entry : outputs.entrySet()) {
+      if (entry.getKey() instanceof ASTNode && entry.getValue() instanceof ExprNodeDesc) {
+        converted.put((ASTNode)entry.getKey(), (ExprNodeDesc)entry.getValue());
+      } else {
+        LOG.warn("Invalid type entry " + entry);
+      }
+    }
+    return converted;
   }
 
   /**
diff --git a/ql/src/test/queries/clientpositive/groupby_multi_insert_common_distinct.q b/ql/src/test/queries/clientpositive/groupby_multi_insert_common_distinct.q
new file mode 100644
index 0000000000..aa1c48939a
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/groupby_multi_insert_common_distinct.q
@@ -0,0 +1,32 @@
+set hive.map.aggr=true;
+
+create table dest1(key int, cnt int);
+create table dest2(key int, cnt int);
+
+explain
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
+
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
+
+
+select * from dest1 where key < 10 order by key;
+select * from dest2 where key < 20 order by key limit 10;
+
+set hive.optimize.multigroupby.common.distincts=false;
+
+-- no need to spray by distinct key first
+explain
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
+
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
+
+select * from dest1 where key < 10 order by key;
+select * from dest2 where key < 20 order by key limit 10;
diff --git a/ql/src/test/queries/clientpositive/groupby_multi_single_reducer3.q b/ql/src/test/queries/clientpositive/groupby_multi_single_reducer3.q
new file mode 100644
index 0000000000..1253ddfed5
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/groupby_multi_single_reducer3.q
@@ -0,0 +1,101 @@
+-- HIVE-3849 Aliased column in where clause for multi-groupby single reducer cannot be resolved
+create table e1 (key string, count int);
+create table e2 (key string, count int);
+
+explain
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key;
+
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key;
+
+select * from e1;
+select * from e2;
+
+explain
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value;
+
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value;
+
+select * from e1;
+select * from e2;
+
+set hive.optimize.ppd=false;
+
+explain
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key;
+
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key;
+
+select * from e1;
+select * from e2;
+
+explain
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value;
+
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value;
+
+select * from e1;
+select * from e2;
diff --git a/ql/src/test/queries/clientpositive/groupby_mutli_insert_common_distinct.q b/ql/src/test/queries/clientpositive/groupby_mutli_insert_common_distinct.q
index aa1c48939a..e69de29bb2 100644
--- a/ql/src/test/queries/clientpositive/groupby_mutli_insert_common_distinct.q
+++ b/ql/src/test/queries/clientpositive/groupby_mutli_insert_common_distinct.q
@@ -1,32 +0,0 @@
-set hive.map.aggr=true;
-
-create table dest1(key int, cnt int);
-create table dest2(key int, cnt int);
-
-explain
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
-
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
-
-
-select * from dest1 where key < 10 order by key;
-select * from dest2 where key < 20 order by key limit 10;
-
-set hive.optimize.multigroupby.common.distincts=false;
-
--- no need to spray by distinct key first
-explain
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
-
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key;
-
-select * from dest1 where key < 10 order by key;
-select * from dest2 where key < 20 order by key limit 10;
diff --git a/ql/src/test/results/clientpositive/groupby_multi_insert_common_distinct.q.out b/ql/src/test/results/clientpositive/groupby_multi_insert_common_distinct.q.out
new file mode 100644
index 0000000000..fdf577dd08
--- /dev/null
+++ b/ql/src/test/results/clientpositive/groupby_multi_insert_common_distinct.q.out
@@ -0,0 +1,504 @@
+PREHOOK: query: create table dest1(key int, cnt int)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table dest1(key int, cnt int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@dest1
+PREHOOK: query: create table dest2(key int, cnt int)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table dest2(key int, cnt int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@dest2
+PREHOOK: query: explain
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest2))) (TOK_SELECT (TOK_SELEXPR (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key))) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key)))))
+
+STAGE DEPENDENCIES:
+  Stage-2 is a root stage
+  Stage-3 depends on stages: Stage-2
+  Stage-0 depends on stages: Stage-3
+  Stage-4 depends on stages: Stage-0
+  Stage-5 depends on stages: Stage-2
+  Stage-1 depends on stages: Stage-5
+  Stage-6 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Reduce Output Operator
+              key expressions:
+                    expr: value
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: value
+                    type: string
+              tag: -1
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: (key + key)
+                    type: double
+      Reduce Operator Tree:
+        Forward
+          Group By Operator
+            aggregations:
+                  expr: count(DISTINCT KEY._col0)
+            bucketGroup: false
+            keys:
+                  expr: VALUE._col0
+                  type: string
+            mode: hash
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+          Group By Operator
+            aggregations:
+                  expr: count(DISTINCT KEY._col0)
+            bucketGroup: false
+            keys:
+                  expr: VALUE._col1
+                  type: double
+            mode: hash
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+  Stage: Stage-3
+    Map Reduce
+      Alias -> Map Operator Tree:
+#### A masked pattern was here ####
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: _col0
+                    type: string
+              tag: -1
+              value expressions:
+                    expr: _col1
+                    type: bigint
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: final
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: UDFToInteger(_col0)
+                  type: int
+                  expr: UDFToInteger(_col1)
+                  type: int
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 1
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.dest1
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.dest1
+
+  Stage: Stage-4
+    Stats-Aggr Operator
+
+  Stage: Stage-5
+    Map Reduce
+      Alias -> Map Operator Tree:
+#### A masked pattern was here ####
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: double
+              sort order: +
+              Map-reduce partition columns:
+                    expr: _col0
+                    type: double
+              tag: -1
+              value expressions:
+                    expr: _col1
+                    type: bigint
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: double
+          mode: final
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: UDFToInteger(_col0)
+                  type: int
+                  expr: UDFToInteger(_col1)
+                  type: int
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 2
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.dest2
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.dest2
+
+  Stage: Stage-6
+    Stats-Aggr Operator
+
+
+PREHOOK: query: from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@dest1
+PREHOOK: Output: default@dest2
+POSTHOOK: query: from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@dest1
+POSTHOOK: Output: default@dest2
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: select * from dest1 where key < 10 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from dest1 where key < 10 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+0	1
+2	1
+4	1
+5	1
+8	1
+9	1
+PREHOOK: query: select * from dest2 where key < 20 order by key limit 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from dest2 where key < 20 order by key limit 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+0	1
+4	1
+8	1
+10	1
+16	1
+18	1
+PREHOOK: query: -- no need to spray by distinct key first
+explain
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- no need to spray by distinct key first
+explain
+from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest2))) (TOK_SELECT (TOK_SELEXPR (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key))) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key)))))
+
+STAGE DEPENDENCIES:
+  Stage-2 is a root stage
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+  Stage-4 depends on stages: Stage-2
+  Stage-1 depends on stages: Stage-4
+  Stage-5 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+              outputColumnNames: key, value
+              Group By Operator
+                aggregations:
+                      expr: count(DISTINCT value)
+                bucketGroup: false
+                keys:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2
+                Reduce Output Operator
+                  key expressions:
+                        expr: _col0
+                        type: string
+                        expr: _col1
+                        type: string
+                  sort order: ++
+                  Map-reduce partition columns:
+                        expr: _col0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: _col2
+                        type: bigint
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+              outputColumnNames: key, value
+              Group By Operator
+                aggregations:
+                      expr: count(DISTINCT value)
+                bucketGroup: false
+                keys:
+                      expr: (key + key)
+                      type: double
+                      expr: value
+                      type: string
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(DISTINCT KEY._col1:0._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: UDFToInteger(_col0)
+                  type: int
+                  expr: UDFToInteger(_col1)
+                  type: int
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 1
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.dest1
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.dest1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-4
+    Map Reduce
+      Alias -> Map Operator Tree:
+#### A masked pattern was here ####
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: double
+                    expr: _col1
+                    type: string
+              sort order: ++
+              Map-reduce partition columns:
+                    expr: _col0
+                    type: double
+              tag: -1
+              value expressions:
+                    expr: _col2
+                    type: bigint
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(DISTINCT KEY._col1:0._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: double
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: UDFToInteger(_col0)
+                  type: int
+                  expr: UDFToInteger(_col1)
+                  type: int
+            outputColumnNames: _col0, _col1
+            File Output Operator
+              compressed: false
+              GlobalTableId: 2
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.dest2
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.dest2
+
+  Stage: Stage-5
+    Stats-Aggr Operator
+
+
+PREHOOK: query: from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@dest1
+PREHOOK: Output: default@dest2
+POSTHOOK: query: from src
+insert overwrite table dest1 select key, count(distinct value) group by key
+insert overwrite table dest2 select key+key, count(distinct value) group by key+key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@dest1
+POSTHOOK: Output: default@dest2
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: select * from dest1 where key < 10 order by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from dest1 where key < 10 order by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+0	1
+2	1
+4	1
+5	1
+8	1
+9	1
+PREHOOK: query: select * from dest2 where key < 20 order by key limit 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from dest2 where key < 20 order by key limit 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+0	1
+4	1
+8	1
+10	1
+16	1
+18	1
diff --git a/ql/src/test/results/clientpositive/groupby_multi_single_reducer3.q.out b/ql/src/test/results/clientpositive/groupby_multi_single_reducer3.q.out
new file mode 100644
index 0000000000..f5b1b194cf
--- /dev/null
+++ b/ql/src/test/results/clientpositive/groupby_multi_single_reducer3.q.out
@@ -0,0 +1,928 @@
+PREHOOK: query: -- HIVE-3849 Aliased column in where clause for multi-groupby single reducer cannot be resolved
+create table e1 (key string, count int)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: -- HIVE-3849 Aliased column in where clause for multi-groupby single reducer cannot be resolved
+create table e1 (key string, count int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@e1
+PREHOOK: query: create table e2 (key string, count int)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: create table e2 (key string, count int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@e2
+PREHOOK: query: explain
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (AND (TOK_FUNCTION in (. (TOK_TABLE_OR_COL src) value) 'val_100' 'val_200' 'val_300') (TOK_FUNCTION in (TOK_TABLE_OR_COL key) 100 150 200))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (AND (TOK_FUNCTION in (. (TOK_TABLE_OR_COL src) value) 'val_400' 'val_500') (TOK_FUNCTION in (TOK_TABLE_OR_COL key) 400 450))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))))
+
+STAGE DEPENDENCIES:
+  Stage-2 is a root stage
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+  Stage-1 depends on stages: Stage-2
+  Stage-4 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Filter Operator
+              predicate:
+                  expr: (((value) IN ('val_100', 'val_200', 'val_300') and (key) IN (100, 150, 200)) or ((value) IN ('val_400', 'val_500') and (key) IN (400, 450)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: key, value
+                Reduce Output Operator
+                  key expressions:
+                        expr: key
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: key
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: value
+                        type: string
+      Reduce Operator Tree:
+        Forward
+          Filter Operator
+            predicate:
+                expr: ((VALUE._col0) IN ('val_100', 'val_200', 'val_300') and (KEY._col0) IN (100, 150, 200))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 1
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e1
+          Filter Operator
+            predicate:
+                expr: ((VALUE._col0) IN ('val_400', 'val_500') and (KEY._col0) IN (400, 450))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 2
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e2
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e2
+
+  Stage: Stage-4
+    Stats-Aggr Operator
+
+
+PREHOOK: query: from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@e1
+PREHOOK: Output: default@e2
+POSTHOOK: query: from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@e1
+POSTHOOK: Output: default@e2
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from e1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+100	2
+200	2
+PREHOOK: query: select * from e2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+400	1
+PREHOOK: query: explain
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (or (or (= (+ (. (TOK_TABLE_OR_COL src) key) (. (TOK_TABLE_OR_COL src) key)) 200) (= (- (. (TOK_TABLE_OR_COL src) key) 100) 100)) (AND (= (. (TOK_TABLE_OR_COL src) key) 300) (TOK_FUNCTION TOK_ISNOTNULL (TOK_TABLE_OR_COL VALUE))))) (TOK_GROUPBY (TOK_TABLE_OR_COL value))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (or (= (+ (. (TOK_TABLE_OR_COL src) key) (. (TOK_TABLE_OR_COL src) key)) 400) (AND (= (- (. (TOK_TABLE_OR_COL src) key) 100) 500) (TOK_FUNCTION TOK_ISNOTNULL (TOK_TABLE_OR_COL VALUE))))) (TOK_GROUPBY (TOK_TABLE_OR_COL value))))
+
+STAGE DEPENDENCIES:
+  Stage-2 is a root stage
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+  Stage-1 depends on stages: Stage-2
+  Stage-4 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Filter Operator
+              predicate:
+                  expr: (((((key + key) = 200) or ((key - 100) = 100)) or ((key = 300.0) and value is not null)) or (((key + key) = 400) or (((key - 100) = 500) and value is not null)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: value
+                      type: string
+                      expr: key
+                      type: string
+                outputColumnNames: value, key
+                Reduce Output Operator
+                  key expressions:
+                        expr: value
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: value
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: key
+                        type: string
+      Reduce Operator Tree:
+        Forward
+          Filter Operator
+            predicate:
+                expr: ((((VALUE._col0 + VALUE._col0) = 200) or ((VALUE._col0 - 100) = 100)) or ((VALUE._col0 = 300.0) and KEY._col0 is not null))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 1
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e1
+          Filter Operator
+            predicate:
+                expr: (((VALUE._col0 + VALUE._col0) = 400) or (((VALUE._col0 - 100) = 500) and KEY._col0 is not null))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 2
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e2
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e2
+
+  Stage: Stage-4
+    Stats-Aggr Operator
+
+
+PREHOOK: query: from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@e1
+PREHOOK: Output: default@e2
+POSTHOOK: query: from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@e1
+POSTHOOK: Output: default@e2
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: select * from e1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+val_100	2
+val_200	2
+PREHOOK: query: select * from e2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+val_200	2
+PREHOOK: query: explain
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (AND (TOK_FUNCTION in (. (TOK_TABLE_OR_COL src) value) 'val_100' 'val_200' 'val_300') (TOK_FUNCTION in (TOK_TABLE_OR_COL key) 100 150 200))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (AND (TOK_FUNCTION in (. (TOK_TABLE_OR_COL src) value) 'val_400' 'val_500') (TOK_FUNCTION in (TOK_TABLE_OR_COL key) 400 450))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))))
+
+STAGE DEPENDENCIES:
+  Stage-2 is a root stage
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+  Stage-1 depends on stages: Stage-2
+  Stage-4 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Filter Operator
+              predicate:
+                  expr: (((value) IN ('val_100', 'val_200', 'val_300') and (key) IN (100, 150, 200)) or ((value) IN ('val_400', 'val_500') and (key) IN (400, 450)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                outputColumnNames: key, value
+                Reduce Output Operator
+                  key expressions:
+                        expr: key
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: key
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: value
+                        type: string
+      Reduce Operator Tree:
+        Forward
+          Filter Operator
+            predicate:
+                expr: ((VALUE._col0) IN ('val_100', 'val_200', 'val_300') and (KEY._col0) IN (100, 150, 200))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 1
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e1
+          Filter Operator
+            predicate:
+                expr: ((VALUE._col0) IN ('val_400', 'val_500') and (KEY._col0) IN (400, 450))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 2
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e2
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e2
+
+  Stage: Stage-4
+    Stats-Aggr Operator
+
+
+PREHOOK: query: from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@e1
+PREHOOK: Output: default@e2
+POSTHOOK: query: from src
+insert overwrite table e1
+select key, count(*)
+where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
+group by key
+insert overwrite table e2
+select key, count(*)
+where src.value in ('val_400', 'val_500') AND key in (400, 450)
+group by key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@e1
+POSTHOOK: Output: default@e2
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from e1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+100	2
+200	2
+PREHOOK: query: select * from e2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+400	1
+PREHOOK: query: explain
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (or (or (= (+ (. (TOK_TABLE_OR_COL src) key) (. (TOK_TABLE_OR_COL src) key)) 200) (= (- (. (TOK_TABLE_OR_COL src) key) 100) 100)) (AND (= (. (TOK_TABLE_OR_COL src) key) 300) (TOK_FUNCTION TOK_ISNOTNULL (TOK_TABLE_OR_COL VALUE))))) (TOK_GROUPBY (TOK_TABLE_OR_COL value))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME e2))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL value)) (TOK_SELEXPR (TOK_FUNCTIONSTAR count))) (TOK_WHERE (or (= (+ (. (TOK_TABLE_OR_COL src) key) (. (TOK_TABLE_OR_COL src) key)) 400) (AND (= (- (. (TOK_TABLE_OR_COL src) key) 100) 500) (TOK_FUNCTION TOK_ISNOTNULL (TOK_TABLE_OR_COL VALUE))))) (TOK_GROUPBY (TOK_TABLE_OR_COL value))))
+
+STAGE DEPENDENCIES:
+  Stage-2 is a root stage
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+  Stage-1 depends on stages: Stage-2
+  Stage-4 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        src 
+          TableScan
+            alias: src
+            Filter Operator
+              predicate:
+                  expr: (((((key + key) = 200) or ((key - 100) = 100)) or ((key = 300.0) and value is not null)) or (((key + key) = 400) or (((key - 100) = 500) and value is not null)))
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: value
+                      type: string
+                      expr: key
+                      type: string
+                outputColumnNames: value, key
+                Reduce Output Operator
+                  key expressions:
+                        expr: value
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: value
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: key
+                        type: string
+      Reduce Operator Tree:
+        Forward
+          Filter Operator
+            predicate:
+                expr: ((((VALUE._col0 + VALUE._col0) = 200) or ((VALUE._col0 - 100) = 100)) or ((VALUE._col0 = 300.0) and KEY._col0 is not null))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 1
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e1
+          Filter Operator
+            predicate:
+                expr: (((VALUE._col0 + VALUE._col0) = 400) or (((VALUE._col0 - 100) = 500) and KEY._col0 is not null))
+                type: boolean
+            Group By Operator
+              aggregations:
+                    expr: count()
+              bucketGroup: false
+              keys:
+                    expr: KEY._col0
+                    type: string
+              mode: complete
+              outputColumnNames: _col0, _col1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                      expr: UDFToInteger(_col1)
+                      type: int
+                outputColumnNames: _col0, _col1
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 2
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.e2
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-1
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.e2
+
+  Stage: Stage-4
+    Stats-Aggr Operator
+
+
+PREHOOK: query: from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@e1
+PREHOOK: Output: default@e2
+POSTHOOK: query: from src
+insert overwrite table e1
+select value, count(*)
+where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
+group by value
+insert overwrite table e2
+select value, count(*)
+where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
+group by value
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@e1
+POSTHOOK: Output: default@e2
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: select * from e1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e1
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+val_100	2
+val_200	2
+PREHOOK: query: select * from e2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from e2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@e2
+#### A masked pattern was here ####
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.count EXPRESSION [(src)src.null, ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: e2.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+val_200	2
diff --git a/ql/src/test/results/clientpositive/groupby_mutli_insert_common_distinct.q.out b/ql/src/test/results/clientpositive/groupby_mutli_insert_common_distinct.q.out
index fdf577dd08..e69de29bb2 100644
--- a/ql/src/test/results/clientpositive/groupby_mutli_insert_common_distinct.q.out
+++ b/ql/src/test/results/clientpositive/groupby_mutli_insert_common_distinct.q.out
@@ -1,504 +0,0 @@
-PREHOOK: query: create table dest1(key int, cnt int)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table dest1(key int, cnt int)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@dest1
-PREHOOK: query: create table dest2(key int, cnt int)
-PREHOOK: type: CREATETABLE
-POSTHOOK: query: create table dest2(key int, cnt int)
-POSTHOOK: type: CREATETABLE
-POSTHOOK: Output: default@dest2
-PREHOOK: query: explain
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-PREHOOK: type: QUERY
-POSTHOOK: query: explain
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-POSTHOOK: type: QUERY
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest2))) (TOK_SELECT (TOK_SELEXPR (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key))) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key)))))
-
-STAGE DEPENDENCIES:
-  Stage-2 is a root stage
-  Stage-3 depends on stages: Stage-2
-  Stage-0 depends on stages: Stage-3
-  Stage-4 depends on stages: Stage-0
-  Stage-5 depends on stages: Stage-2
-  Stage-1 depends on stages: Stage-5
-  Stage-6 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-2
-    Map Reduce
-      Alias -> Map Operator Tree:
-        src 
-          TableScan
-            alias: src
-            Reduce Output Operator
-              key expressions:
-                    expr: value
-                    type: string
-              sort order: +
-              Map-reduce partition columns:
-                    expr: value
-                    type: string
-              tag: -1
-              value expressions:
-                    expr: key
-                    type: string
-                    expr: (key + key)
-                    type: double
-      Reduce Operator Tree:
-        Forward
-          Group By Operator
-            aggregations:
-                  expr: count(DISTINCT KEY._col0)
-            bucketGroup: false
-            keys:
-                  expr: VALUE._col0
-                  type: string
-            mode: hash
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-          Group By Operator
-            aggregations:
-                  expr: count(DISTINCT KEY._col0)
-            bucketGroup: false
-            keys:
-                  expr: VALUE._col1
-                  type: double
-            mode: hash
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 0
-              table:
-                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-
-  Stage: Stage-3
-    Map Reduce
-      Alias -> Map Operator Tree:
-#### A masked pattern was here ####
-            Reduce Output Operator
-              key expressions:
-                    expr: _col0
-                    type: string
-              sort order: +
-              Map-reduce partition columns:
-                    expr: _col0
-                    type: string
-              tag: -1
-              value expressions:
-                    expr: _col1
-                    type: bigint
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations:
-                expr: count(VALUE._col0)
-          bucketGroup: false
-          keys:
-                expr: KEY._col0
-                type: string
-          mode: final
-          outputColumnNames: _col0, _col1
-          Select Operator
-            expressions:
-                  expr: UDFToInteger(_col0)
-                  type: int
-                  expr: UDFToInteger(_col1)
-                  type: int
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 1
-              table:
-                  input format: org.apache.hadoop.mapred.TextInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  name: default.dest1
-
-  Stage: Stage-0
-    Move Operator
-      tables:
-          replace: true
-          table:
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.dest1
-
-  Stage: Stage-4
-    Stats-Aggr Operator
-
-  Stage: Stage-5
-    Map Reduce
-      Alias -> Map Operator Tree:
-#### A masked pattern was here ####
-            Reduce Output Operator
-              key expressions:
-                    expr: _col0
-                    type: double
-              sort order: +
-              Map-reduce partition columns:
-                    expr: _col0
-                    type: double
-              tag: -1
-              value expressions:
-                    expr: _col1
-                    type: bigint
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations:
-                expr: count(VALUE._col0)
-          bucketGroup: false
-          keys:
-                expr: KEY._col0
-                type: double
-          mode: final
-          outputColumnNames: _col0, _col1
-          Select Operator
-            expressions:
-                  expr: UDFToInteger(_col0)
-                  type: int
-                  expr: UDFToInteger(_col1)
-                  type: int
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 2
-              table:
-                  input format: org.apache.hadoop.mapred.TextInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  name: default.dest2
-
-  Stage: Stage-1
-    Move Operator
-      tables:
-          replace: true
-          table:
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.dest2
-
-  Stage: Stage-6
-    Stats-Aggr Operator
-
-
-PREHOOK: query: from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@dest1
-PREHOOK: Output: default@dest2
-POSTHOOK: query: from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@dest1
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: select * from dest1 where key < 10 order by key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@dest1
-#### A masked pattern was here ####
-POSTHOOK: query: select * from dest1 where key < 10 order by key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@dest1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-0	1
-2	1
-4	1
-5	1
-8	1
-9	1
-PREHOOK: query: select * from dest2 where key < 20 order by key limit 10
-PREHOOK: type: QUERY
-PREHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: query: select * from dest2 where key < 20 order by key limit 10
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-0	1
-4	1
-8	1
-10	1
-16	1
-18	1
-PREHOOK: query: -- no need to spray by distinct key first
-explain
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-PREHOOK: type: QUERY
-POSTHOOK: query: -- no need to spray by distinct key first
-explain
-from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-POSTHOOK: type: QUERY
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF (TOK_TABNAME src))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest1))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL key)) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (TOK_TABLE_OR_COL key))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME dest2))) (TOK_SELECT (TOK_SELEXPR (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key))) (TOK_SELEXPR (TOK_FUNCTIONDI count (TOK_TABLE_OR_COL value)))) (TOK_GROUPBY (+ (TOK_TABLE_OR_COL key) (TOK_TABLE_OR_COL key)))))
-
-STAGE DEPENDENCIES:
-  Stage-2 is a root stage
-  Stage-0 depends on stages: Stage-2
-  Stage-3 depends on stages: Stage-0
-  Stage-4 depends on stages: Stage-2
-  Stage-1 depends on stages: Stage-4
-  Stage-5 depends on stages: Stage-1
-
-STAGE PLANS:
-  Stage: Stage-2
-    Map Reduce
-      Alias -> Map Operator Tree:
-        src 
-          TableScan
-            alias: src
-            Select Operator
-              expressions:
-                    expr: key
-                    type: string
-                    expr: value
-                    type: string
-              outputColumnNames: key, value
-              Group By Operator
-                aggregations:
-                      expr: count(DISTINCT value)
-                bucketGroup: false
-                keys:
-                      expr: key
-                      type: string
-                      expr: value
-                      type: string
-                mode: hash
-                outputColumnNames: _col0, _col1, _col2
-                Reduce Output Operator
-                  key expressions:
-                        expr: _col0
-                        type: string
-                        expr: _col1
-                        type: string
-                  sort order: ++
-                  Map-reduce partition columns:
-                        expr: _col0
-                        type: string
-                  tag: -1
-                  value expressions:
-                        expr: _col2
-                        type: bigint
-            Select Operator
-              expressions:
-                    expr: key
-                    type: string
-                    expr: value
-                    type: string
-              outputColumnNames: key, value
-              Group By Operator
-                aggregations:
-                      expr: count(DISTINCT value)
-                bucketGroup: false
-                keys:
-                      expr: (key + key)
-                      type: double
-                      expr: value
-                      type: string
-                mode: hash
-                outputColumnNames: _col0, _col1, _col2
-                File Output Operator
-                  compressed: false
-                  GlobalTableId: 0
-                  table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations:
-                expr: count(DISTINCT KEY._col1:0._col0)
-          bucketGroup: false
-          keys:
-                expr: KEY._col0
-                type: string
-          mode: mergepartial
-          outputColumnNames: _col0, _col1
-          Select Operator
-            expressions:
-                  expr: UDFToInteger(_col0)
-                  type: int
-                  expr: UDFToInteger(_col1)
-                  type: int
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 1
-              table:
-                  input format: org.apache.hadoop.mapred.TextInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  name: default.dest1
-
-  Stage: Stage-0
-    Move Operator
-      tables:
-          replace: true
-          table:
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.dest1
-
-  Stage: Stage-3
-    Stats-Aggr Operator
-
-  Stage: Stage-4
-    Map Reduce
-      Alias -> Map Operator Tree:
-#### A masked pattern was here ####
-            Reduce Output Operator
-              key expressions:
-                    expr: _col0
-                    type: double
-                    expr: _col1
-                    type: string
-              sort order: ++
-              Map-reduce partition columns:
-                    expr: _col0
-                    type: double
-              tag: -1
-              value expressions:
-                    expr: _col2
-                    type: bigint
-      Reduce Operator Tree:
-        Group By Operator
-          aggregations:
-                expr: count(DISTINCT KEY._col1:0._col0)
-          bucketGroup: false
-          keys:
-                expr: KEY._col0
-                type: double
-          mode: mergepartial
-          outputColumnNames: _col0, _col1
-          Select Operator
-            expressions:
-                  expr: UDFToInteger(_col0)
-                  type: int
-                  expr: UDFToInteger(_col1)
-                  type: int
-            outputColumnNames: _col0, _col1
-            File Output Operator
-              compressed: false
-              GlobalTableId: 2
-              table:
-                  input format: org.apache.hadoop.mapred.TextInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                  name: default.dest2
-
-  Stage: Stage-1
-    Move Operator
-      tables:
-          replace: true
-          table:
-              input format: org.apache.hadoop.mapred.TextInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-              name: default.dest2
-
-  Stage: Stage-5
-    Stats-Aggr Operator
-
-
-PREHOOK: query: from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-PREHOOK: Output: default@dest1
-PREHOOK: Output: default@dest2
-POSTHOOK: query: from src
-insert overwrite table dest1 select key, count(distinct value) group by key
-insert overwrite table dest2 select key+key, count(distinct value) group by key+key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-POSTHOOK: Output: default@dest1
-POSTHOOK: Output: default@dest2
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-PREHOOK: query: select * from dest1 where key < 10 order by key
-PREHOOK: type: QUERY
-PREHOOK: Input: default@dest1
-#### A masked pattern was here ####
-POSTHOOK: query: select * from dest1 where key < 10 order by key
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@dest1
-#### A masked pattern was here ####
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-0	1
-2	1
-4	1
-5	1
-8	1
-9	1
-PREHOOK: query: select * from dest2 where key < 20 order by key limit 10
-PREHOOK: type: QUERY
-PREHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: query: select * from dest2 where key < 20 order by key limit 10
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@dest2
-#### A masked pattern was here ####
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.cnt EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-POSTHOOK: Lineage: dest2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-0	1
-4	1
-8	1
-10	1
-16	1
-18	1
