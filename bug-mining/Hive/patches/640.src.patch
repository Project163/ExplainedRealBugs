diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
index bd75cc4626..7887ea122a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java
@@ -752,7 +752,6 @@ private void genAllOneUniqueJoinObject()
 
   protected void checkAndGenObject() throws HiveException {
     if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {
-      new IntermediateObject(new ArrayList[numAliases], 0);
 
       // Check if results need to be emitted.
       // Results only need to be emitted if there is a non-null entry in a table
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index c9e61a1843..fc210a13ea 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -740,7 +740,7 @@ public void closeOp(boolean abort) throws HiveException {
    */
   @Override
   public String getName() {
-    return new String("FS");
+    return "FS";
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
index 44e0e310cc..62fc63ed38 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java
@@ -104,7 +104,7 @@ public void processOp(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return new String("FIL");
+    return "FIL";
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 75fc077a46..f8bc71f0f3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -451,11 +451,11 @@ private int getSize(int pos, PrimitiveCategory category) {
    **/
   private int getSize(int pos, Class<?> c, Field f) {
     if (c.isPrimitive()
-        || c.isInstance(new Boolean(true))
-        || c.isInstance(new Byte((byte) 0))
-        || c.isInstance(new Short((short) 0))
-        || c.isInstance(new Integer(0))
-        || c.isInstance(new Long(0))
+        || c.isInstance(Boolean.valueOf(true))
+        || c.isInstance(Byte.valueOf((byte) 0))
+        || c.isInstance(Short.valueOf((short) 0))
+        || c.isInstance(Integer.valueOf(0))
+        || c.isInstance(Long.valueOf(0))
         || c.isInstance(new Float(0))
         || c.isInstance(new Double(0))) {
       return javaSizePrimitiveType;
@@ -1051,7 +1051,7 @@ public List<String> genColLists(
    */
   @Override
   public String getName() {
-    return new String("GBY");
+    return "GBY";
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index a8276318f8..097d33b0b6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -814,7 +814,7 @@ public void logStats() {
    * @return the name of the operator
    */
   public String getName() {
-    return new String("OP");
+    return "OP";
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index 268582083f..732a5aaa1f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -310,7 +310,7 @@ public void processOp(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return new String("RS");
+    return "RS";
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
index 3582e78900..88c3a8bd8e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java
@@ -89,7 +89,7 @@ public void processOp(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return new String("SEL");
+    return "SEL";
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index 141df13c58..32249633c1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -36,9 +36,9 @@
 import org.apache.hadoop.hive.ql.stats.StatsSetupConst;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.JobConf;
 
@@ -208,7 +208,7 @@ public void closeOp(boolean abort) throws HiveException {
    **/
   @Override
   public String getName() {
-    return new String("TS");
+    return "TS";
   }
 
   // this 'neededColumnIDs' field is included in this operator class instead of
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
index 56669cda3e..870196cc07 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TaskFactory.java
@@ -85,7 +85,7 @@ public taskTuple(Class<T> workClass, Class<? extends Task<T>> taskClass) {
   private static ThreadLocal<Integer> tid = new ThreadLocal<Integer>() {
     @Override
     protected synchronized Integer initialValue() {
-      return new Integer(0);
+      return Integer.valueOf(0);
     }
   };
 
@@ -96,7 +96,7 @@ public static int getAndIncrementId() {
   }
 
   public static void resetId() {
-    tid.set(new Integer(0));
+    tid.set(Integer.valueOf(0));
   }
 
   @SuppressWarnings("unchecked")
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
index b36f4814d4..f1ad15e61f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/UnionOperator.java
@@ -137,7 +137,7 @@ public synchronized void processOp(Object row, int tag) throws HiveException {
    */
   @Override
   public String getName() {
-    return new String("UNION");
+    return "UNION";
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
index d19e9df71d..2a83e0545e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java
@@ -71,7 +71,7 @@ public ArrayList<Node> getChildren() {
    * @see org.apache.hadoop.hive.ql.lib.Node#getName()
    */
   public String getName() {
-    return (new Integer(super.getToken().getType())).toString();
+    return (Integer.valueOf(super.getToken().getType())).toString();
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
index 533fa98809..3059bf19ee 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java
@@ -297,7 +297,7 @@ public static String charSetString(String charSetName, String charSetString)
           if (val > 127) {
             val = val - 256;
           }
-          bArray[j++] = new Integer(val).byteValue();
+          bArray[j++] = (byte)val;
         }
 
         String res = new String(bArray, charSetName);
@@ -557,17 +557,18 @@ private static String getStructTypeStringFromAST(ASTNode typeNode)
     if (children <= 0) {
       throw new SemanticException("empty struct not allowed.");
     }
+    StringBuilder buffer = new StringBuilder(typeStr);
     for (int i = 0; i < children; i++) {
       ASTNode child = (ASTNode) typeNode.getChild(i);
-      typeStr += unescapeIdentifier(child.getChild(0).getText()) + ":";
-      typeStr += getTypeStringFromAST((ASTNode) child.getChild(1));
+      buffer.append(unescapeIdentifier(child.getChild(0).getText())).append(":");
+      buffer.append(getTypeStringFromAST((ASTNode) child.getChild(1)));
       if (i < children - 1) {
-        typeStr += ",";
+        buffer.append(",");
       }
     }
 
-    typeStr += ">";
-    return typeStr;
+    buffer.append(">");
+    return buffer.toString();
   }
 
   private static String getUnionTypeStringFromAST(ASTNode typeNode)
@@ -578,13 +579,15 @@ private static String getUnionTypeStringFromAST(ASTNode typeNode)
     if (children <= 0) {
       throw new SemanticException("empty union not allowed.");
     }
+    StringBuilder buffer = new StringBuilder(typeStr);
     for (int i = 0; i < children; i++) {
-      typeStr += getTypeStringFromAST((ASTNode) typeNode.getChild(i));
+      buffer.append(getTypeStringFromAST((ASTNode) typeNode.getChild(i)));
       if (i < children - 1) {
-        typeStr += ",";
+        buffer.append(",");
       }
     }
-    typeStr += ">";
+    buffer.append(">");
+    typeStr = buffer.toString();
     return typeStr;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
index 793157854b..4c5a5f3d3c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseContext.java
@@ -99,7 +99,7 @@ public class ParseContext {
 
   private HashSet<ReadEntity> semanticInputs;
   private List<Task<? extends Serializable>> rootTasks;
-  
+
   public ParseContext() {
   }
 
@@ -176,7 +176,6 @@ public ParseContext(
     this.uCtx = uCtx;
     this.listMapJoinOpsNoReducer = listMapJoinOpsNoReducer;
     hasNonPartCols = false;
-    this.groupOpToInputTables = new HashMap<GroupByOperator, Set<String>>();
     this.groupOpToInputTables = groupOpToInputTables;
     this.prunedPartitions = prunedPartitions;
     this.opToSamplePruner = opToSamplePruner;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index e1febc36c8..cec0d46a9b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -3858,8 +3858,8 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)
               + dest_path, e);
         }
       }
-      String cols = new String();
-      String colTypes = new String();
+      String cols = "";
+      String colTypes = "";
       ArrayList<ColumnInfo> colInfos = inputRR.getColumnInfos();
 
       // CTAS case: the file output format and serde are defined by the create
@@ -4277,7 +4277,6 @@ private Operator genLimitMapRedPlan(String dest, QB qb, Operator input,
   private ArrayList<ExprNodeDesc> getParitionColsFromBucketCols(String dest, QB qb, Table tab,
                                                                 TableDesc table_desc, Operator input, boolean convert)
     throws SemanticException {
-    RowResolver inputRR = opParseCtx.get(input).getRowResolver();
     List<String> tabBucketCols = tab.getBucketCols();
     List<FieldSchema> tabCols  = tab.getCols();
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeField.java b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeField.java
index 708c4e01a0..8480a73b5c 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeField.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeField.java
@@ -28,8 +28,8 @@ public class DynamicSerDeField extends DynamicSerDeSimpleNode {
   // [this.fieldid :] Requiredness() FieldType() this.name FieldValue()
   // [CommaOrSemicolon()]
 
-  private final int FD_REQUIREDNESS = 0;
-  private final int FD_FIELD_TYPE = 1;
+  private static final int FD_REQUIREDNESS = 0;
+  private static final int FD_FIELD_TYPE = 1;
 
   public boolean isSkippable() {
     return ((DynamicSerDeFieldRequiredness) jjtGetChild(FD_REQUIREDNESS))
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFieldType.java b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFieldType.java
index a3f7c813ea..41264b0819 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFieldType.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFieldType.java
@@ -27,7 +27,7 @@ public class DynamicSerDeFieldType extends DynamicSerDeSimpleNode {
 
   // production: this.name | BaseType() | MapType() | SetType() | ListType()
 
-  private final int FD_FIELD_TYPE = 0;
+  private static final int FD_FIELD_TYPE = 0;
 
   public DynamicSerDeFieldType(int i) {
     super(i);
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFunction.java b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFunction.java
index b233fb1c24..b4259bb8e0 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFunction.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/dynamic_type/DynamicSerDeFunction.java
@@ -29,7 +29,7 @@ public class DynamicSerDeFunction extends DynamicSerDeStructBase {
   // production is: Async() FunctionType() NAME FieldList() Throws()
   // [CommaOrSemicolon]
 
-  private final int FD_FIELD_LIST = 2;
+  private static final int FD_FIELD_LIST = 2;
 
   public DynamicSerDeFunction(int i) {
     super(i);
