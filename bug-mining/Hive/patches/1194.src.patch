diff --git a/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java b/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java
new file mode 100644
index 0000000000..28f25e59c9
--- /dev/null
+++ b/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java
@@ -0,0 +1,221 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.common.type;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.math.RoundingMode;
+
+/**
+ *
+ * HiveDecimal. Simple wrapper for BigDecimal. Adds fixed max precision and non scientific string
+ * representation
+ *
+ */
+public class HiveDecimal implements Comparable<HiveDecimal> {
+
+  public static final HiveDecimal ZERO = new HiveDecimal(BigDecimal.ZERO);
+
+  public static final int MAX_PRECISION = 36; // multiple of 9 for compact representation
+
+  public static final HiveDecimal ONE = new HiveDecimal(BigDecimal.ONE);
+
+  public static final int ROUND_FLOOR = BigDecimal.ROUND_FLOOR;
+
+  public static final int ROUND_CEILING = BigDecimal.ROUND_CEILING;
+
+  public static final int ROUND_HALF_UP = BigDecimal.ROUND_HALF_UP;
+
+  private BigDecimal bd = BigDecimal.ZERO;
+
+  public HiveDecimal(BigDecimal b) {
+    this(b, false);
+  }
+
+  public HiveDecimal(BigDecimal b, boolean allowRounding) {
+    bd = this.normalize(b, MAX_PRECISION, allowRounding);
+    if (bd == null) {
+      throw new NumberFormatException("Assignment would result in truncation");
+    }
+  }
+
+  public HiveDecimal(BigInteger unscaled, int scale) {
+    bd = this.normalize(new BigDecimal(unscaled, scale), MAX_PRECISION, false);
+    if (bd == null) {
+      throw new NumberFormatException("Assignment would result in truncation");
+    }
+  }
+
+  public HiveDecimal(String dec) {
+    bd = this.normalize(new BigDecimal(dec), MAX_PRECISION, false);
+    if (bd == null) {
+      throw new NumberFormatException("Assignment would result in truncation");
+    }
+  }
+
+  public HiveDecimal(BigInteger bi) {
+    bd = this.normalize(new BigDecimal(bi), MAX_PRECISION, false);
+    if (bd == null) {
+      throw new NumberFormatException("Assignment would result in truncation");
+    }
+  }
+
+  public HiveDecimal(int i) {
+    bd = new BigDecimal(i);
+  }
+
+  public HiveDecimal(long l) {
+    bd = new BigDecimal(l);
+  }
+
+  @Override
+  public String toString() {
+     return bd.toPlainString();
+  }
+
+  public HiveDecimal setScale(int i) {
+    return new HiveDecimal(bd.setScale(i));
+  }
+
+  @Override
+  public int compareTo(HiveDecimal dec) {
+    return bd.compareTo(dec.bd);
+  }
+
+  public int scale() {
+    return bd.scale();
+  }
+
+  public int precision() {
+    return bd.precision();
+  }
+
+  public int intValue() {
+    return bd.intValue();
+  }
+
+  public double doubleValue() {
+    return bd.doubleValue();
+  }
+
+  public long longValue() {
+    return bd.longValue();
+  }
+
+  public short shortValue() {
+    return bd.shortValue();
+  }
+
+  public float floatValue() {
+    return bd.floatValue();
+  }
+
+  public BigDecimal bigDecimalValue() {
+    return bd;
+  }
+
+  public byte byteValue() {
+    return bd.byteValue();
+  }
+
+  public HiveDecimal setScale(int adjustedScale, int rm) {
+    return new HiveDecimal(bd.setScale(adjustedScale, rm));
+  }
+
+  public HiveDecimal subtract(HiveDecimal dec) {
+    return new HiveDecimal(bd.subtract(dec.bd));
+  }
+
+  public HiveDecimal multiply(HiveDecimal dec) {
+    return new HiveDecimal(bd.multiply(dec.bd));
+  }
+
+  public BigInteger unscaledValue() {
+    return bd.unscaledValue();
+  }
+
+  public HiveDecimal scaleByPowerOfTen(int n) {
+    return new HiveDecimal(bd.scaleByPowerOfTen(n));
+  }
+
+  public HiveDecimal abs() {
+    return new HiveDecimal(bd.abs());
+  }
+
+  public HiveDecimal negate() {
+    return new HiveDecimal(bd.negate());
+  }
+
+  public HiveDecimal add(HiveDecimal dec) {
+    return new HiveDecimal(bd.add(dec.bd));
+  }
+
+  public HiveDecimal pow(int n) {
+    return new HiveDecimal(bd.pow(n));
+  }
+
+  public HiveDecimal remainder(HiveDecimal dec) {
+    return new HiveDecimal(bd.remainder(dec.bd));
+  }
+
+  public HiveDecimal divide(HiveDecimal dec) {
+    return new HiveDecimal(bd.divide(dec.bd, MAX_PRECISION, RoundingMode.HALF_UP), true);
+  }
+
+  private BigDecimal trim(BigDecimal d) {
+    if (d.compareTo(BigDecimal.ZERO) == 0) {
+      // Special case for 0, because java doesn't strip zeros correctly on that number.
+      d = BigDecimal.ZERO;
+    } else {
+      d = d.stripTrailingZeros();
+      if (d.scale() < 0) {
+        // no negative scale decimals
+        d = d.setScale(0);
+      }
+    }
+    return d;
+  }
+
+  private BigDecimal normalize(BigDecimal d, int precision, boolean allowRounding) {
+    if (d == null) {
+      return null;
+    }
+
+    d = trim(d);
+
+    // compute the number of digits of the decimal
+    int valuePrecision = d.precision()
+        + Math.max(0, 1 + d.scale() - d.precision());
+
+    if (valuePrecision > precision) {
+      if (allowRounding) {
+        // round "half up" until we hit the decimal point
+        int adjustedScale = d.scale() - (valuePrecision-precision);
+        if (adjustedScale >= 0) {
+          d = d.setScale(adjustedScale, RoundingMode.HALF_UP);
+          d = trim(d);
+        } else {
+          d = null;
+        }
+      } else {
+        d = null;
+      }
+    }
+    return d;
+  }
+}
diff --git a/data/files/kv8.txt b/data/files/kv8.txt
new file mode 100644
index 0000000000..4e4dc74fc2
--- /dev/null
+++ b/data/files/kv8.txt
@@ -0,0 +1,73 @@
+1234567890123456789012345678901234567
+1.234567890123456789012345678901234567
+12.34567890123456789012345678901234567
+123.4567890123456789012345678901234567
+1234.567890123456789012345678901234567
+123456789012345678901234567890123456
+0.12345678901234567890123456789012345
+1.23456789012345678901234567890123456
+12.3456789012345678901234567890123456
+123.456789012345678901234567890123456
+1234.56789012345678901234567890123456
+12345.6789012345678901234567890123456
+123456.789012345678901234567890123456
+1234567.89012345678901234567890123456
+12345678.9012345678901234567890123456
+123456789.012345678901234567890123456
+1234567890.12345678901234567890123456
+12345678901.2345678901234567890123456
+123456789012.345678901234567890123456
+1234567890123.45678901234567890123456
+12345678901234.5678901234567890123456
+123456789012345.678901234567890123456
+1234567890123456.78901234567890123456
+12345678901234567.8901234567890123456
+123456789012345678.901234567890123456
+1234567890123456789.01234567890123456
+12345678901234567890.1234567890123456
+123456789012345678901.234567890123456
+1234567890123456789012.34567890123456
+12345678901234567890123.4567890123456
+123456789012345678901234.567890123456
+1234567890123456789012345.67890123456
+12345678901234567890123456.7890123456
+123456789012345678901234567.890123456
+1234567890123456789012345678.90123456
+12345678901234567890123456789.0123456
+123456789012345678901234567890.123456
+1234567890123456789012345678901.23456
+12345678901234567890123456789012.3456
+123456789012345678901234567890123.456
+1234567890123456789012345678901234.56
+12345678901234567890123456789012345.6
+123456789012345678901234567890123456.0
+123456789012345678901234567890123456.00
+123456789012345678901234567890123456.000
+000123456789012345678901234567890123456.000
+999999999999999999999999999999999999
+9999999999999999999999999999999999
+999999999999999999999999999999999
+-999999999999999999999999999999999999
+-9999999999999999999999999999999999
+-999999999999999999999999999999999
+0000000000000000000000000000000000000000000
+0.00000000000000000000000000000000001
+-0.00000000000000000000000000000000001
+0.000000000000000000000000000000000001
+-0.000000000000000000000000000000000001
+0.123456789012345
+1.234567890123456
+12.34567890123456
+123.4567890123456
+1234.567890123456
+12345.67890123456
+123456.7890123456
+1234567.890123456
+12345678.90123456
+123456789.0123456
+1234567890.123456
+12345678901.23456
+123456789012.3456
+1234567890123.456
+12345678901234.56
+123456789012345.6
diff --git a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveBaseResultSet.java b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveBaseResultSet.java
index ab9aa919f3..092fa28148 100644
--- a/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveBaseResultSet.java
+++ b/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveBaseResultSet.java
@@ -42,6 +42,8 @@
 import java.util.List;
 import java.util.Map;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+
 /**
  * Data independed base class which implements the common part of
  * all hive resultsets.
@@ -110,8 +112,11 @@ public BigDecimal getBigDecimal(int columnIndex) throws SQLException {
     if (obj instanceof BigDecimal) {
       return ((BigDecimal) obj);
     }
-    throw new SQLException("Cannot convert column " + columnIndex 
-                           + " to BigDecimal. Found data of type: " 
+    if (obj instanceof HiveDecimal) {
+      return ((HiveDecimal) obj).bigDecimalValue();
+    }
+    throw new SQLException("Cannot convert column " + columnIndex
+                           + " to BigDecimal. Found data of type: "
                            + obj.getClass()+", value: " + obj.toString());
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAbs.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAbs.java
index bfce482b07..acaaa5b3f5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAbs.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFAbs.java
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
@@ -39,7 +39,7 @@ public class UDFAbs extends UDF {
   private final DoubleWritable resultDouble = new DoubleWritable();
   private final LongWritable resultLong = new LongWritable();
   private final IntWritable resultInt = new IntWritable();
-  private final BigDecimalWritable resultBigDecimal = new BigDecimalWritable();
+  private final HiveDecimalWritable resultHiveDecimal = new HiveDecimalWritable();
 
   public DoubleWritable evaluate(DoubleWritable n) {
     if (n == null) {
@@ -71,12 +71,12 @@ public IntWritable evaluate(IntWritable n) {
     return resultInt;
   }
 
-  public BigDecimalWritable evaluate(BigDecimalWritable n) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable n) {
     if (n == null) {
       return null;
     }
 
-    resultBigDecimal.set(n.getBigDecimal().abs());
-    return resultBigDecimal;
+    resultHiveDecimal.set(n.getHiveDecimal().abs());
+    return resultHiveDecimal;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java
index 14c16ec504..1e74fce5da 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericOp.java
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -49,7 +49,7 @@ public UDFBaseNumericOp() {
   protected LongWritable longWritable = new LongWritable();
   protected FloatWritable floatWritable = new FloatWritable();
   protected DoubleWritable doubleWritable = new DoubleWritable();
-  protected BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
+  protected HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
 
   public abstract ByteWritable evaluate(ByteWritable a, ByteWritable b);
 
@@ -63,5 +63,5 @@ public UDFBaseNumericOp() {
 
   public abstract DoubleWritable evaluate(DoubleWritable a, DoubleWritable b);
 
-  public abstract BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b);
+  public abstract HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b);
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericUnaryOp.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericUnaryOp.java
index cb7dca48b7..00075ebff4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericUnaryOp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFBaseNumericUnaryOp.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -46,7 +46,7 @@ public UDFBaseNumericUnaryOp() {
   protected LongWritable longWritable = new LongWritable();
   protected FloatWritable floatWritable = new FloatWritable();
   protected DoubleWritable doubleWritable = new DoubleWritable();
-  protected BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
+  protected HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
 
   public abstract ByteWritable evaluate(ByteWritable a);
 
@@ -60,5 +60,5 @@ public UDFBaseNumericUnaryOp() {
 
   public abstract DoubleWritable evaluate(DoubleWritable a);
 
-  public abstract BigDecimalWritable evaluate(BigDecimalWritable a);
+  public abstract HiveDecimalWritable evaluate(HiveDecimalWritable a);
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFCeil.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFCeil.java
index 927c8b79fc..92c813d962 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFCeil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFCeil.java
@@ -18,14 +18,11 @@
 
 package org.apache.hadoop.hive.ql.udf;
 
-import java.math.BigDecimal;
-import java.math.MathContext;
-import java.math.RoundingMode;
-
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.io.LongWritable;
 
 /**
@@ -40,7 +37,7 @@
     + "  > SELECT _FUNC_(5) FROM src LIMIT 1;\n" + "  5")
 public class UDFCeil extends UDF {
   private final LongWritable longWritable = new LongWritable();
-  private final BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
+  private final HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
 
   public UDFCeil() {
   }
@@ -54,14 +51,14 @@ public LongWritable evaluate(DoubleWritable i) {
     }
   }
 
-  public BigDecimalWritable evaluate(BigDecimalWritable i) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      BigDecimal bd = i.getBigDecimal();
+      HiveDecimal bd = i.getHiveDecimal();
       int origScale = bd.scale();
-      bigDecimalWritable.set(bd.setScale(0, BigDecimal.ROUND_CEILING).setScale(origScale));
-      return bigDecimalWritable;
+      decimalWritable.set(bd.setScale(0, HiveDecimal.ROUND_CEILING).setScale(origScale));
+      return decimalWritable;
     }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFFloor.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFFloor.java
index fc071374f3..66a0478f0f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFFloor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFFloor.java
@@ -18,14 +18,11 @@
 
 package org.apache.hadoop.hive.ql.udf;
 
-import java.math.BigDecimal;
-import java.math.MathContext;
-import java.math.RoundingMode;
-
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.io.LongWritable;
 
 /**
@@ -40,7 +37,7 @@
     + "  > SELECT _FUNC_(5) FROM src LIMIT 1;\n" + "  5")
 public class UDFFloor extends UDF {
   private final LongWritable result = new LongWritable();
-  private final BigDecimalWritable bdResult = new BigDecimalWritable();
+  private final HiveDecimalWritable bdResult = new HiveDecimalWritable();
 
   public UDFFloor() {
   }
@@ -54,13 +51,13 @@ public LongWritable evaluate(DoubleWritable i) {
     }
   }
 
-  public BigDecimalWritable evaluate(BigDecimalWritable i) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      BigDecimal bd = i.getBigDecimal();
+      HiveDecimal bd = i.getHiveDecimal();
       int origScale = bd.scale();
-      bdResult.set(bd.setScale(0, BigDecimal.ROUND_FLOOR).setScale(origScale));
+      bdResult.set(bd.setScale(0, HiveDecimal.ROUND_FLOOR).setScale(origScale));
       return bdResult;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java
index 1844d61dd7..f6167d4141 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPDivide.java
@@ -18,13 +18,11 @@
 
 package org.apache.hadoop.hive.ql.udf;
 
-import java.math.BigDecimal;
-import java.math.RoundingMode;
-
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 
 /**
  * UDFOPDivide.
@@ -38,10 +36,9 @@
  * return a double for divide.
  */
 public class UDFOPDivide extends UDF {
-  private final DoubleWritable doubleWritable = new DoubleWritable();
-  private final BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
 
-  public static final int MAX_SCALE = 65; // max compatible with MySQL
+  private final DoubleWritable doubleWritable = new DoubleWritable();
+  private final HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
 
   public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
     // LOG.info("Get input " + a.getClass() + ":" + a + " " + b.getClass() + ":"
@@ -54,17 +51,17 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
     return doubleWritable;
   }
 
-  public BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b) {
     if ((a == null) || (b == null)) {
       return null;
     }
-    if (b.getBigDecimal().compareTo(BigDecimal.ZERO) == 0) {
+    if (b.getHiveDecimal().compareTo(HiveDecimal.ZERO) == 0) {
       return null;
     } else {
-        bigDecimalWritable.set(a.getBigDecimal().divide(
-          b.getBigDecimal(), MAX_SCALE, RoundingMode.HALF_UP));
+        decimalWritable.set(a.getHiveDecimal().divide(
+          b.getHiveDecimal()));
     }
 
-    return bigDecimalWritable;
+    return decimalWritable;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java
index f884b9a30f..a61d10c762 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMinus.java
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
@@ -110,13 +110,18 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b) {
 
     if ((a == null) || (b == null)) {
       return null;
     }
 
-    bigDecimalWritable.set(a.getBigDecimal().subtract(b.getBigDecimal()));
-    return bigDecimalWritable;
+    try {
+      decimalWritable.set(a.getHiveDecimal().subtract(b.getHiveDecimal()));
+    } catch (NumberFormatException e) {
+      return null;
+    }
+
+    return decimalWritable;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java
index 9948c1f84d..0ed69e9188 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMod.java
@@ -18,12 +18,11 @@
 
 package org.apache.hadoop.hive.ql.udf;
 
-import java.math.BigDecimal;
-
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
@@ -112,19 +111,24 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b) {
     if ((a == null) || (b == null)) {
       return null;
     }
 
-    BigDecimal av = a.getBigDecimal();
-    BigDecimal bv = b.getBigDecimal();
+    HiveDecimal av = a.getHiveDecimal();
+    HiveDecimal bv = b.getHiveDecimal();
+
+    if (bv.compareTo(HiveDecimal.ZERO) == 0) {
+      return null;
+    }
 
-    if (bv.compareTo(BigDecimal.ZERO) == 0) {
+    try {
+      decimalWritable.set(av.remainder(bv));
+    } catch(NumberFormatException e) {
       return null;
     }
 
-    bigDecimalWritable.set(av.remainder(bv));
-    return bigDecimalWritable;
+    return decimalWritable;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java
index 905865129e..4a0393e1a7 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPMultiply.java
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
@@ -110,12 +110,17 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b) {
     if ((a == null) || (b == null)) {
       return null;
     }
 
-    bigDecimalWritable.set(a.getBigDecimal().multiply(b.getBigDecimal()));
-    return bigDecimalWritable;
+    try {
+      decimalWritable.set(a.getHiveDecimal().multiply(b.getHiveDecimal()));
+    } catch (NumberFormatException e) {
+      return null;
+    }
+
+    return decimalWritable;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNegative.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNegative.java
index 3c14fefc82..5560cbfd83 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNegative.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPNegative.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -92,12 +92,12 @@ public DoubleWritable evaluate(DoubleWritable a) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a) {
     if (a == null) {
       return null;
     }
-    bigDecimalWritable.set(a.getBigDecimal().negate());
-    return bigDecimalWritable;
+    decimalWritable.set(a.getHiveDecimal().negate());
+    return decimalWritable;
   }
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java
index 5722d8b361..1e9e1aa7ce 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPlus.java
@@ -19,9 +19,9 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
@@ -115,13 +115,17 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b) {
     if ((a == null) || (b == null)) {
       return null;
     }
 
-    bigDecimalWritable.set(a.getBigDecimal().add(b.getBigDecimal()));
-    return bigDecimalWritable;
+    try {
+      decimalWritable.set(a.getHiveDecimal().add(b.getHiveDecimal()));
+    } catch(NumberFormatException e) {
+      return null;
+    }
+    return decimalWritable;
   }
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPositive.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPositive.java
index 071189035b..ae11d74407 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPositive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFOPPositive.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -68,7 +68,7 @@ public DoubleWritable evaluate(DoubleWritable a) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a) {
     return a;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java
index 85531a65be..614a5bc84e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPosMod.java
@@ -18,12 +18,11 @@
 
 package org.apache.hadoop.hive.ql.udf;
 
-import java.math.BigDecimal;
-
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
@@ -113,19 +112,24 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
   }
 
   @Override
-  public BigDecimalWritable evaluate(BigDecimalWritable a, BigDecimalWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, HiveDecimalWritable b) {
     if ((a == null) || (b == null)) {
       return null;
     }
 
-    BigDecimal av = a.getBigDecimal();
-    BigDecimal bv = b.getBigDecimal();
+    HiveDecimal av = a.getHiveDecimal();
+    HiveDecimal bv = b.getHiveDecimal();
+
+    if (bv.compareTo(HiveDecimal.ZERO) == 0) {
+      return null;
+    }
 
-    if (bv.compareTo(BigDecimal.ZERO) == 0) {
+    try {
+      decimalWritable.set(av.remainder(bv).add(bv).remainder(bv));
+    } catch (NumberFormatException e) {
       return null;
     }
 
-    bigDecimalWritable.set(av.remainder(bv).add(bv).remainder(bv));
-    return bigDecimalWritable;
+    return decimalWritable;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPower.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPower.java
index 67956686a4..a462116ebb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPower.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFPower.java
@@ -20,8 +20,8 @@
 
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.io.IntWritable;
 
 /**
@@ -34,7 +34,7 @@
     + "  > SELECT _FUNC_(2, 3) FROM src LIMIT 1;\n" + "  8")
 public class UDFPower extends UDF {
   private final DoubleWritable resultDouble = new DoubleWritable();
-  private final BigDecimalWritable resultBigDecimal = new BigDecimalWritable();
+  private final HiveDecimalWritable resultHiveDecimal = new HiveDecimalWritable();
 
   public UDFPower() {
   }
@@ -50,7 +50,7 @@ public DoubleWritable evaluate(DoubleWritable a, DoubleWritable b) {
       return resultDouble;
     }
   }
-  
+
   /**
    * Raise a to the power of b.
    */
@@ -62,16 +62,20 @@ public DoubleWritable evaluate(DoubleWritable a, IntWritable b) {
       return resultDouble;
     }
   }
-  
+
   /**
    * Raise a to the power of b
    */
-  public BigDecimalWritable evaluate(BigDecimalWritable a, IntWritable b) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable a, IntWritable b) {
     if (a == null || b == null) {
       return null;
     } else {
-      resultBigDecimal.set(a.getBigDecimal().pow(b.get()));
-      return resultBigDecimal;
+      try {
+        resultHiveDecimal.set(a.getHiveDecimal().pow(b.get()));
+      } catch (NumberFormatException e) {
+        return null;
+      }
+      return resultHiveDecimal;
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRound.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRound.java
index f19fb91eba..57e3f2dec3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRound.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRound.java
@@ -21,11 +21,12 @@
 import java.math.BigDecimal;
 import java.math.RoundingMode;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
@@ -39,7 +40,7 @@
     extended = "Example:\n"
     + "  > SELECT _FUNC_(12.3456, 1) FROM src LIMIT 1;\n" + "  12.3'")
 public class UDFRound extends UDF {
-  private final BigDecimalWritable bigDecimalWritable = new BigDecimalWritable();
+  private final HiveDecimalWritable decimalWritable = new HiveDecimalWritable();
   private final DoubleWritable doubleWritable = new DoubleWritable();
   private final LongWritable longWritable = new LongWritable();
   private final IntWritable intWritable = new IntWritable();
@@ -74,21 +75,25 @@ public DoubleWritable evaluate(DoubleWritable n, IntWritable i) {
     return evaluate(n, i.get());
   }
 
-  private BigDecimalWritable evaluate(BigDecimalWritable n, int i) {
+  private HiveDecimalWritable evaluate(HiveDecimalWritable n, int i) {
     if (n == null) {
       return null;
     }
-    BigDecimal bd = n.getBigDecimal();
-    bd = n.getBigDecimal().setScale(i, RoundingMode.HALF_UP);
-    bigDecimalWritable.set(bd);
-    return bigDecimalWritable;
+    HiveDecimal bd = n.getHiveDecimal();
+    try {
+      bd = n.getHiveDecimal().setScale(i, HiveDecimal.ROUND_HALF_UP);
+    } catch (NumberFormatException e) {
+      return null;
+    }
+    decimalWritable.set(bd);
+    return decimalWritable;
   }
 
-  public BigDecimalWritable evaluate(BigDecimalWritable n) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable n) {
     return evaluate(n, 0);
   }
 
-  public BigDecimalWritable evaluate(BigDecimalWritable n, IntWritable i) {
+  public HiveDecimalWritable evaluate(HiveDecimalWritable n, IntWritable i) {
     if (i == null) {
       return null;
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java
index 0d985515d4..e96791c7c4 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java
@@ -18,10 +18,10 @@
 
 package org.apache.hadoop.hive.ql.udf;
 
-import java.math.BigDecimal;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -175,11 +175,11 @@ public BooleanWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public BooleanWritable evaluate(BigDecimalWritable i) {
+  public BooleanWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      booleanWritable.set(BigDecimal.ZERO.compareTo(i.getBigDecimal()) != 0);
+      booleanWritable.set(HiveDecimal.ZERO.compareTo(i.getHiveDecimal()) != 0);
       return booleanWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
index c5830eaf21..d0bb6672af 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -182,11 +182,11 @@ public ByteWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public ByteWritable evaluate(BigDecimalWritable i) {
+  public ByteWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      byteWritable.set(i.getBigDecimal().byteValue());
+      byteWritable.set(i.getHiveDecimal().byteValue());
       return byteWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java
index c57e31e0d3..8a9e662595 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -184,11 +184,11 @@ public DoubleWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public DoubleWritable evaluate(BigDecimalWritable i) {
+  public DoubleWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      doubleWritable.set(i.getBigDecimal().doubleValue());
+      doubleWritable.set(i.getHiveDecimal().doubleValue());
       return doubleWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java
index 61591e9cbe..aea11fc985 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -185,11 +185,11 @@ public FloatWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public FloatWritable evaluate(BigDecimalWritable i) {
+  public FloatWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      floatWritable.set(i.getBigDecimal().floatValue());
+      floatWritable.set(i.getHiveDecimal().floatValue());
       return floatWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
index 018b3def52..c34e6b6e6a 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -189,11 +189,11 @@ public IntWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public IntWritable evaluate(BigDecimalWritable i) {
+  public IntWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      intWritable.set(i.getBigDecimal().intValue());
+      intWritable.set(i.getHiveDecimal().intValue());
       return intWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
index 426bc6465b..b5925fe148 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -193,11 +193,11 @@ public LongWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public LongWritable evaluate(BigDecimalWritable i) {
+  public LongWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      longWritable.set(i.getBigDecimal().longValue());
+      longWritable.set(i.getHiveDecimal().longValue());
       return longWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
index 5f42865952..1f63ce964b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java
@@ -19,7 +19,7 @@
 package org.apache.hadoop.hive.ql.udf;
 
 import org.apache.hadoop.hive.ql.exec.UDF;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -182,11 +182,11 @@ public ShortWritable evaluate(TimestampWritable i) {
     }
   }
 
-  public ShortWritable evaluate(BigDecimalWritable i) {
+  public ShortWritable evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
-      shortWritable.set(i.getBigDecimal().shortValue());
+      shortWritable.set(i.getHiveDecimal().shortValue());
       return shortWritable;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java
index 1d06eb34b7..d6c7a30cfc 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToString.java
@@ -20,7 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.UDF;
 import org.apache.hadoop.hive.serde2.ByteStream;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -142,7 +142,7 @@ public Text evaluate(TimestampWritable i) {
     }
   }
 
-  public Text evaluate(BigDecimalWritable i) {
+  public Text evaluate(HiveDecimalWritable i) {
     if (i == null) {
       return null;
     } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
index f060818203..41b7a3d92d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java
@@ -17,27 +17,25 @@
  */
 package org.apache.hadoop.hive.ql.udf.generic;
 
-import java.math.BigDecimal;
-import java.math.RoundingMode;
 import java.util.ArrayList;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.ql.udf.UDFOPDivide;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AggregationBuffer;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
@@ -150,63 +148,85 @@ public AggregationBuffer getNewAggregationBuffer() throws HiveException {
     }
   }
 
-  public static class GenericUDAFAverageEvaluatorDecimal extends AbstractGenericUDAFAverageEvaluator<BigDecimal> {
+  public static class GenericUDAFAverageEvaluatorDecimal extends AbstractGenericUDAFAverageEvaluator<HiveDecimal> {
 
     @Override
-    public void doReset(AverageAggregationBuffer<BigDecimal> aggregation) throws HiveException {
+    public void doReset(AverageAggregationBuffer<HiveDecimal> aggregation) throws HiveException {
       aggregation.count = 0;
-      aggregation.sum = BigDecimal.ZERO;
+      aggregation.sum = HiveDecimal.ZERO;
     }
 
     @Override
     protected ObjectInspector getSumFieldJavaObjectInspector() {
-      return PrimitiveObjectInspectorFactory.javaBigDecimalObjectInspector;
+      return PrimitiveObjectInspectorFactory.javaHiveDecimalObjectInspector;
     }
     @Override
     protected ObjectInspector getSumFieldWritableObjectInspector() {
-      return PrimitiveObjectInspectorFactory.writableBigDecimalObjectInspector;
+      return PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector;
     }
 
     @Override
-    protected void doIterate(AverageAggregationBuffer<BigDecimal> aggregation,
+    protected void doIterate(AverageAggregationBuffer<HiveDecimal> aggregation,
         PrimitiveObjectInspector oi, Object parameter) {
-      BigDecimal value = PrimitiveObjectInspectorUtils.getBigDecimal(parameter, oi);
+      HiveDecimal value = PrimitiveObjectInspectorUtils.getHiveDecimal(parameter, oi);
       aggregation.count++;
-      aggregation.sum = aggregation.sum.add(value);
-
+      if (aggregation.sum != null) {
+        try {
+          aggregation.sum = aggregation.sum.add(value);
+        } catch (NumberFormatException e) {
+          aggregation.sum = null;
+        }
+      }
     }
 
     @Override
-    protected void doMerge(AverageAggregationBuffer<BigDecimal> aggregation, Long partialCount,
+    protected void doMerge(AverageAggregationBuffer<HiveDecimal> aggregation, Long partialCount,
         ObjectInspector sumFieldOI, Object partialSum) {
-      BigDecimal value = ((BigDecimalObjectInspector)sumFieldOI).getPrimitiveJavaObject(partialSum);
+      HiveDecimal value = ((HiveDecimalObjectInspector)sumFieldOI).getPrimitiveJavaObject(partialSum);
+      if (value == null) {
+        aggregation.sum = null;
+      }
       aggregation.count += partialCount;
-      aggregation.sum = aggregation.sum.add(value);
+      if (aggregation.sum != null) {
+        try {
+          aggregation.sum = aggregation.sum.add(value);
+        } catch (NumberFormatException e) {
+          aggregation.sum = null;
+        }
+      }
     }
 
     @Override
-    protected void doTerminatePartial(AverageAggregationBuffer<BigDecimal> aggregation) {
-      if(partialResult[1] == null) {
-        partialResult[1] = new BigDecimalWritable(BigDecimal.ZERO);
+    protected void doTerminatePartial(AverageAggregationBuffer<HiveDecimal> aggregation) {
+      if(partialResult[1] == null && aggregation.sum != null) {
+        partialResult[1] = new HiveDecimalWritable(HiveDecimal.ZERO);
       }
       ((LongWritable) partialResult[0]).set(aggregation.count);
-      ((BigDecimalWritable) partialResult[1]).set(aggregation.sum);
+      if (aggregation.sum != null) {
+        ((HiveDecimalWritable) partialResult[1]).set(aggregation.sum);
+      } else {
+        partialResult[1] = null;
+      }
     }
 
     @Override
-    protected Object doTerminate(AverageAggregationBuffer<BigDecimal> aggregation) {
-      if(aggregation.count == 0) {
+    protected Object doTerminate(AverageAggregationBuffer<HiveDecimal> aggregation) {
+      if(aggregation.count == 0 || aggregation.sum == null) {
         return null;
       } else {
-        BigDecimalWritable result = new BigDecimalWritable(BigDecimal.ZERO);
-        result.set(aggregation.sum.divide(new BigDecimal(aggregation.count),
-            UDFOPDivide.MAX_SCALE, RoundingMode.HALF_UP));
+        HiveDecimalWritable result = new HiveDecimalWritable(HiveDecimal.ZERO);
+        try {
+          result.set(aggregation.sum.divide(new HiveDecimal(aggregation.count)));
+        } catch (NumberFormatException e) {
+          result = null;
+        }
         return result;
       }
     }
+
     @Override
     public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      AverageAggregationBuffer<BigDecimal> result = new AverageAggregationBuffer<BigDecimal>();
+      AverageAggregationBuffer<HiveDecimal> result = new AverageAggregationBuffer<HiveDecimal>();
       reset(result);
       return result;
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
index 8ef27c164c..4092530a32 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java
@@ -17,16 +17,15 @@
  */
 package org.apache.hadoop.hive.ql.udf.generic;
 
-import java.math.BigDecimal;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
@@ -70,7 +69,7 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
     case STRING:
       return new GenericUDAFSumDouble();
     case DECIMAL:
-      return new GenericUDAFSumBigDecimal();
+      return new GenericUDAFSumHiveDecimal();
     case BOOLEAN:
     default:
       throw new UDFArgumentTypeException(0,
@@ -80,40 +79,40 @@ public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
   }
 
   /**
-   * GenericUDAFSumBigDecimal.
+   * GenericUDAFSumHiveDecimal.
    *
    */
-  public static class GenericUDAFSumBigDecimal extends GenericUDAFEvaluator {
+  public static class GenericUDAFSumHiveDecimal extends GenericUDAFEvaluator {
     private PrimitiveObjectInspector inputOI;
-    private BigDecimalWritable result;
+    private HiveDecimalWritable result;
 
     @Override
     public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException {
       assert (parameters.length == 1);
       super.init(m, parameters);
-      result = new BigDecimalWritable(BigDecimal.ZERO);
+      result = new HiveDecimalWritable(HiveDecimal.ZERO);
       inputOI = (PrimitiveObjectInspector) parameters[0];
-      return PrimitiveObjectInspectorFactory.writableBigDecimalObjectInspector;
+      return PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector;
     }
 
     /** class for storing decimal sum value. */
-    static class SumBigDecimalAgg implements AggregationBuffer {
+    static class SumHiveDecimalAgg implements AggregationBuffer {
       boolean empty;
-      BigDecimal sum;
+      HiveDecimal sum;
     }
 
     @Override
     public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      SumBigDecimalAgg agg = new SumBigDecimalAgg();
+      SumHiveDecimalAgg agg = new SumHiveDecimalAgg();
       reset(agg);
       return agg;
     }
 
     @Override
     public void reset(AggregationBuffer agg) throws HiveException {
-      SumBigDecimalAgg bdAgg = (SumBigDecimalAgg) agg;
+      SumHiveDecimalAgg bdAgg = (SumHiveDecimalAgg) agg;
       bdAgg.empty = true;
-      bdAgg.sum = BigDecimal.ZERO;
+      bdAgg.sum = HiveDecimal.ZERO;
     }
 
     boolean warned = false;
@@ -143,17 +142,26 @@ public Object terminatePartial(AggregationBuffer agg) throws HiveException {
     @Override
     public void merge(AggregationBuffer agg, Object partial) throws HiveException {
       if (partial != null) {
-        SumBigDecimalAgg myagg = (SumBigDecimalAgg) agg;
+        SumHiveDecimalAgg myagg = (SumHiveDecimalAgg) agg;
+        if (myagg.sum == null) {
+          return;
+        }
+
         myagg.empty = false;
-        myagg.sum = myagg.sum.add(
-            PrimitiveObjectInspectorUtils.getBigDecimal(partial, inputOI));
+
+        try {
+          myagg.sum = myagg.sum.add(
+            PrimitiveObjectInspectorUtils.getHiveDecimal(partial, inputOI));
+        } catch (NumberFormatException e) {
+          myagg.sum = null;
+        }
       }
     }
 
     @Override
     public Object terminate(AggregationBuffer agg) throws HiveException {
-      SumBigDecimalAgg myagg = (SumBigDecimalAgg) agg;
-      if (myagg.empty) {
+      SumHiveDecimalAgg myagg = (SumHiveDecimalAgg) agg;
+      if (myagg.empty || myagg.sum == null) {
         return null;
       }
       result.set(myagg.sum);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFReflect2.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFReflect2.java
index ce468ec7b7..c2ab37d14d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFReflect2.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFReflect2.java
@@ -20,16 +20,16 @@
 
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
-import java.math.BigDecimal;
 import java.sql.Timestamp;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.UDFType;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -166,7 +166,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {
         ((BytesWritable)returnObj).set((byte[])result, 0, ((byte[]) result).length);
         return returnObj;
       case DECIMAL:
-        ((BigDecimalWritable)returnObj).set((BigDecimal)result);
+        ((HiveDecimalWritable)returnObj).set((HiveDecimal)result);
         return returnObj;
     }
     throw new HiveException("Invalid type " + returnOI.getPrimitiveCategory());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToDecimal.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToDecimal.java
index d6776d129a..6c2f5cf060 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToDecimal.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToDecimal.java
@@ -23,14 +23,14 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.BigDecimalConverter;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.HiveDecimalConverter;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
 @Description(name = "decimal", value = "_FUNC_(a) - cast a to decimal")
 public class GenericUDFToDecimal extends GenericUDF {
 
   private PrimitiveObjectInspector argumentOI;
-  private BigDecimalConverter bdConverter;
+  private HiveDecimalConverter bdConverter;
 
   @Override
   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
@@ -46,9 +46,9 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
           "The function DECIMAL takes only primitive types");
     }
 
-    bdConverter = new BigDecimalConverter(argumentOI,
-        PrimitiveObjectInspectorFactory.writableBigDecimalObjectInspector);
-    return PrimitiveObjectInspectorFactory.writableBigDecimalObjectInspector;
+    bdConverter = new HiveDecimalConverter(argumentOI,
+        PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector);
+    return PrimitiveObjectInspectorFactory.writableHiveDecimalObjectInspector;
   }
 
   @Override
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
index 0db19ac326..4eff65a631 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
@@ -28,7 +28,7 @@
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
@@ -37,11 +37,11 @@ public class TestFunctionRegistry extends TestCase {
 
   public class TestUDF {
     public void same(DoubleWritable x, DoubleWritable y) {}
-    public void same(BigDecimalWritable x, BigDecimalWritable y) {}
-    public void one(IntWritable x, BigDecimalWritable y) {}
+    public void same(HiveDecimalWritable x, HiveDecimalWritable y) {}
+    public void one(IntWritable x, HiveDecimalWritable y) {}
     public void one(IntWritable x, DoubleWritable y) {}
     public void one(IntWritable x, IntWritable y) {}
-    public void mismatch(TimestampWritable x, BigDecimalWritable y) {}
+    public void mismatch(TimestampWritable x, HiveDecimalWritable y) {}
     public void mismatch(BytesWritable x, DoubleWritable y) {}
   }
   
@@ -90,16 +90,16 @@ public void testGetMethodInternal() {
            DoubleWritable.class, DoubleWritable.class, false);
 
     verify(TestUDF.class, "same", TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
-           BigDecimalWritable.class, BigDecimalWritable.class, false);
+           HiveDecimalWritable.class, HiveDecimalWritable.class, false);
 
     verify(TestUDF.class, "same", TypeInfoFactory.decimalTypeInfo, TypeInfoFactory.doubleTypeInfo,
-           BigDecimalWritable.class, BigDecimalWritable.class, false);
+           HiveDecimalWritable.class, HiveDecimalWritable.class, false);
 
     verify(TestUDF.class, "same", TypeInfoFactory.decimalTypeInfo, TypeInfoFactory.decimalTypeInfo,
-           BigDecimalWritable.class, BigDecimalWritable.class, false);
+           HiveDecimalWritable.class, HiveDecimalWritable.class, false);
 
     verify(TestUDF.class, "one", TypeInfoFactory.intTypeInfo, TypeInfoFactory.decimalTypeInfo,
-           IntWritable.class, BigDecimalWritable.class, false);
+           IntWritable.class, HiveDecimalWritable.class, false);
 
     verify(TestUDF.class, "one", TypeInfoFactory.intTypeInfo, TypeInfoFactory.floatTypeInfo,
            IntWritable.class, DoubleWritable.class, false);
diff --git a/ql/src/test/queries/clientpositive/decimal_precision.q b/ql/src/test/queries/clientpositive/decimal_precision.q
new file mode 100644
index 0000000000..7744f40a7d
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/decimal_precision.q
@@ -0,0 +1,20 @@
+DROP TABLE IF EXISTS DECIMAL_PRECISION;
+
+CREATE TABLE DECIMAL_PRECISION(dec decimal) 
+ROW FORMAT DELIMITED
+   FIELDS TERMINATED BY ' '
+STORED AS TEXTFILE;
+
+LOAD DATA LOCAL INPATH '../data/files/kv8.txt' INTO TABLE DECIMAL_PRECISION;
+
+SELECT * FROM DECIMAL_PRECISION ORDER BY dec;
+
+SELECT dec, dec + 1, dec - 1 FROM DECIMAL_PRECISION ORDER BY dec;
+SELECT dec, dec * 2, dec / 3  FROM DECIMAL_PRECISION ORDER BY dec;
+SELECT dec, dec / 9 FROM DECIMAL_PRECISION ORDER BY dec;
+SELECT dec, dec / 27 FROM DECIMAL_PRECISION ORDER BY dec;
+SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec;
+
+SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;
+
+DROP TABLE DECIMAL_PRECISION;
diff --git a/ql/src/test/results/clientpositive/decimal_3.q.out b/ql/src/test/results/clientpositive/decimal_3.q.out
index 440ce9e57d..219c91acc3 100644
--- a/ql/src/test/results/clientpositive/decimal_3.q.out
+++ b/ql/src/test/results/clientpositive/decimal_3.q.out
@@ -27,8 +27,10 @@ POSTHOOK: query: SELECT * FROM DECIMAL_3 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_3
 #### A masked pattern was here ####
+NULL	0
+NULL	0
 -1234567890.123456789	-1234567890
--4.4E+3	4400
+-4400	4400
 -1255.49	-1255
 -1.122	-11
 -1.12	-1
@@ -38,7 +40,6 @@ POSTHOOK: Input: default@decimal_3
 -0.3	0
 0	0
 0	0
-1E-99	0
 0.01	0
 0.02	0
 0.1	0
@@ -57,14 +58,13 @@ POSTHOOK: Input: default@decimal_3
 3.14	3
 3.14	3
 3.14	4
-1E+1	10
-2E+1	20
-1E+2	100
+10	10
+20	20
+100	100
 124	124
 125.2	125
-2E+2	200
+200	200
 1234567890.12345678	1234567890
-1E+99	0
 PREHOOK: query: SELECT * FROM DECIMAL_3 ORDER BY key DESC, value DESC
 PREHOOK: type: QUERY
 PREHOOK: Input: default@decimal_3
@@ -73,14 +73,13 @@ POSTHOOK: query: SELECT * FROM DECIMAL_3 ORDER BY key DESC, value DESC
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_3
 #### A masked pattern was here ####
-1E+99	0
 1234567890.12345678	1234567890
-2E+2	200
+200	200
 125.2	125
 124	124
-1E+2	100
-2E+1	20
-1E+1	10
+100	100
+20	20
+10	10
 3.14	4
 3.14	3
 3.14	3
@@ -99,7 +98,6 @@ POSTHOOK: Input: default@decimal_3
 0.1	0
 0.02	0
 0.01	0
-1E-99	0
 0	0
 0	0
 -0.3	0
@@ -109,8 +107,10 @@ POSTHOOK: Input: default@decimal_3
 -1.12	-1
 -1.122	-11
 -1255.49	-1255
--4.4E+3	4400
+-4400	4400
 -1234567890.123456789	-1234567890
+NULL	0
+NULL	0
 PREHOOK: query: SELECT * FROM DECIMAL_3 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@decimal_3
@@ -119,8 +119,10 @@ POSTHOOK: query: SELECT * FROM DECIMAL_3 ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_3
 #### A masked pattern was here ####
+NULL	0
+NULL	0
 -1234567890.123456789	-1234567890
--4.4E+3	4400
+-4400	4400
 -1255.49	-1255
 -1.122	-11
 -1.12	-1
@@ -130,7 +132,6 @@ POSTHOOK: Input: default@decimal_3
 -0.3	0
 0	0
 0	0
-1E-99	0
 0.01	0
 0.02	0
 0.1	0
@@ -149,14 +150,13 @@ POSTHOOK: Input: default@decimal_3
 3.14	3
 3.14	3
 3.14	4
-1E+1	10
-2E+1	20
-1E+2	100
+10	10
+20	20
+100	100
 124	124
 125.2	125
-2E+2	200
+200	200
 1234567890.12345678	1234567890
-1E+99	0
 PREHOOK: query: SELECT DISTINCT key FROM DECIMAL_3 ORDER BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@decimal_3
@@ -165,8 +165,9 @@ POSTHOOK: query: SELECT DISTINCT key FROM DECIMAL_3 ORDER BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_3
 #### A masked pattern was here ####
+NULL
 -1234567890.123456789
--4.4E+3
+-4400
 -1255.49
 -1.122
 -1.12
@@ -174,7 +175,6 @@ POSTHOOK: Input: default@decimal_3
 -0.33
 -0.3
 0
-1E-99
 0.01
 0.02
 0.1
@@ -188,14 +188,13 @@ POSTHOOK: Input: default@decimal_3
 1.122
 2
 3.14
-1E+1
-2E+1
-1E+2
+10
+20
+100
 124
 125.2
-2E+2
+200
 1234567890.12345678
-1E+99
 PREHOOK: query: SELECT key, sum(value) FROM DECIMAL_3 GROUP BY key ORDER BY key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@decimal_3
@@ -204,8 +203,9 @@ POSTHOOK: query: SELECT key, sum(value) FROM DECIMAL_3 GROUP BY key ORDER BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_3
 #### A masked pattern was here ####
+NULL	0
 -1234567890.123456789	-1234567890
--4.4E+3	4400
+-4400	4400
 -1255.49	-1255
 -1.122	-11
 -1.12	-2
@@ -213,7 +213,6 @@ POSTHOOK: Input: default@decimal_3
 -0.33	0
 -0.3	0
 0	0
-1E-99	0
 0.01	0
 0.02	0
 0.1	0
@@ -227,14 +226,13 @@ POSTHOOK: Input: default@decimal_3
 1.122	1
 2	4
 3.14	13
-1E+1	10
-2E+1	20
-1E+2	100
+10	10
+20	20
+100	100
 124	124
 125.2	125
-2E+2	200
+200	200
 1234567890.12345678	1234567890
-1E+99	0
 PREHOOK: query: SELECT value, sum(key) FROM DECIMAL_3 GROUP BY value ORDER BY value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@decimal_3
@@ -247,18 +245,18 @@ POSTHOOK: Input: default@decimal_3
 -1255	-1255.49
 -11	-1.122
 -1	-2.24
-0	1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.330000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001
+0	0.33
 1	5.2419999999999999999999999
 2	4
 3	9.42
 4	3.14
-10	1E+1
-20	2E+1
-100	1E+2
+10	10
+20	20
+100	100
 124	124
 125	125.2
-200	2E+2
-4400	-4.4E+3
+200	200
+4400	-4400
 1234567890	1234567890.12345678
 PREHOOK: query: SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.value
 PREHOOK: type: QUERY
@@ -269,7 +267,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_3
 #### A masked pattern was here ####
 -1234567890.123456789	-1234567890	-1234567890.123456789	-1234567890
--4.4E+3	4400	-4.4E+3	4400
+-4400	4400	-4400	4400
 -1255.49	-1255	-1255.49	-1255
 -1.122	-11	-1.122	-11
 -1.12	-1	-1.12	-1
@@ -283,7 +281,6 @@ POSTHOOK: Input: default@decimal_3
 0	0	0	0
 0	0	0	0
 0	0	0	0
-1E-99	0	1E-99	0
 0.01	0	0.01	0
 0.02	0	0.02	0
 0.1	0	0.1	0
@@ -318,14 +315,13 @@ POSTHOOK: Input: default@decimal_3
 3.14	4	3.14	3
 3.14	4	3.14	3
 3.14	4	3.14	4
-1E+1	10	1E+1	10
-2E+1	20	2E+1	20
-1E+2	100	1E+2	100
+10	10	10	10
+20	20	20	20
+100	100	100	100
 124	124	124	124
 125.2	125	125.2	125
-2E+2	200	2E+2	200
+200	200	200	200
 1234567890.12345678	1234567890	1234567890.12345678	1234567890
-1E+99	0	1E+99	0
 PREHOOK: query: SELECT * FROM DECIMAL_3 WHERE key=3.14 ORDER BY key, value
 PREHOOK: type: QUERY
 PREHOOK: Input: default@decimal_3
diff --git a/ql/src/test/results/clientpositive/decimal_precision.q.out b/ql/src/test/results/clientpositive/decimal_precision.q.out
new file mode 100644
index 0000000000..23488697c2
--- /dev/null
+++ b/ql/src/test/results/clientpositive/decimal_precision.q.out
@@ -0,0 +1,524 @@
+PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_PRECISION
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) 
+ROW FORMAT DELIMITED
+   FIELDS TERMINATED BY ' '
+STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE DECIMAL_PRECISION(dec decimal) 
+ROW FORMAT DELIMITED
+   FIELDS TERMINATED BY ' '
+STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@DECIMAL_PRECISION
+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv8.txt' INTO TABLE DECIMAL_PRECISION
+PREHOOK: type: LOAD
+PREHOOK: Output: default@decimal_precision
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/kv8.txt' INTO TABLE DECIMAL_PRECISION
+POSTHOOK: type: LOAD
+POSTHOOK: Output: default@decimal_precision
+PREHOOK: query: SELECT * FROM DECIMAL_PRECISION ORDER BY dec
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM DECIMAL_PRECISION ORDER BY dec
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL
+NULL
+NULL
+NULL
+NULL
+NULL
+NULL
+-999999999999999999999999999999999999
+-9999999999999999999999999999999999
+-999999999999999999999999999999999
+-0.00000000000000000000000000000000001
+0
+0.00000000000000000000000000000000001
+0.123456789012345
+0.12345678901234567890123456789012345
+1.234567890123456
+1.23456789012345678901234567890123456
+12.34567890123456
+12.3456789012345678901234567890123456
+123.4567890123456
+123.456789012345678901234567890123456
+1234.567890123456
+1234.56789012345678901234567890123456
+12345.67890123456
+12345.6789012345678901234567890123456
+123456.7890123456
+123456.789012345678901234567890123456
+1234567.890123456
+1234567.89012345678901234567890123456
+12345678.90123456
+12345678.9012345678901234567890123456
+123456789.0123456
+123456789.012345678901234567890123456
+1234567890.123456
+1234567890.12345678901234567890123456
+12345678901.23456
+12345678901.2345678901234567890123456
+123456789012.3456
+123456789012.345678901234567890123456
+1234567890123.456
+1234567890123.45678901234567890123456
+12345678901234.56
+12345678901234.5678901234567890123456
+123456789012345.6
+123456789012345.678901234567890123456
+1234567890123456.78901234567890123456
+12345678901234567.8901234567890123456
+123456789012345678.901234567890123456
+1234567890123456789.01234567890123456
+12345678901234567890.1234567890123456
+123456789012345678901.234567890123456
+1234567890123456789012.34567890123456
+12345678901234567890123.4567890123456
+123456789012345678901234.567890123456
+1234567890123456789012345.67890123456
+12345678901234567890123456.7890123456
+123456789012345678901234567.890123456
+1234567890123456789012345678.90123456
+12345678901234567890123456789.0123456
+123456789012345678901234567890.123456
+1234567890123456789012345678901.23456
+12345678901234567890123456789012.3456
+123456789012345678901234567890123.456
+999999999999999999999999999999999
+1234567890123456789012345678901234.56
+9999999999999999999999999999999999
+12345678901234567890123456789012345.6
+123456789012345678901234567890123456
+123456789012345678901234567890123456
+123456789012345678901234567890123456
+123456789012345678901234567890123456
+123456789012345678901234567890123456
+999999999999999999999999999999999999
+PREHOOK: query: SELECT dec, dec + 1, dec - 1 FROM DECIMAL_PRECISION ORDER BY dec
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT dec, dec + 1, dec - 1 FROM DECIMAL_PRECISION ORDER BY dec
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+-999999999999999999999999999999999999	-999999999999999999999999999999999998	NULL
+-9999999999999999999999999999999999	-9999999999999999999999999999999998	-10000000000000000000000000000000000
+-999999999999999999999999999999999	-999999999999999999999999999999998	-1000000000000000000000000000000000
+-0.00000000000000000000000000000000001	0.99999999999999999999999999999999999	-1.00000000000000000000000000000000001
+0	1	-1
+0.00000000000000000000000000000000001	1.00000000000000000000000000000000001	-0.99999999999999999999999999999999999
+0.123456789012345	1.123456789012345	-0.876543210987655
+0.12345678901234567890123456789012345	1.12345678901234567890123456789012345	-0.87654321098765432109876543210987655
+1.234567890123456	2.234567890123456	0.234567890123456
+1.23456789012345678901234567890123456	2.23456789012345678901234567890123456	0.23456789012345678901234567890123456
+12.34567890123456	13.34567890123456	11.34567890123456
+12.3456789012345678901234567890123456	13.3456789012345678901234567890123456	11.3456789012345678901234567890123456
+123.4567890123456	124.4567890123456	122.4567890123456
+123.456789012345678901234567890123456	124.456789012345678901234567890123456	122.456789012345678901234567890123456
+1234.567890123456	1235.567890123456	1233.567890123456
+1234.56789012345678901234567890123456	1235.56789012345678901234567890123456	1233.56789012345678901234567890123456
+12345.67890123456	12346.67890123456	12344.67890123456
+12345.6789012345678901234567890123456	12346.6789012345678901234567890123456	12344.6789012345678901234567890123456
+123456.7890123456	123457.7890123456	123455.7890123456
+123456.789012345678901234567890123456	123457.789012345678901234567890123456	123455.789012345678901234567890123456
+1234567.890123456	1234568.890123456	1234566.890123456
+1234567.89012345678901234567890123456	1234568.89012345678901234567890123456	1234566.89012345678901234567890123456
+12345678.90123456	12345679.90123456	12345677.90123456
+12345678.9012345678901234567890123456	12345679.9012345678901234567890123456	12345677.9012345678901234567890123456
+123456789.0123456	123456790.0123456	123456788.0123456
+123456789.012345678901234567890123456	123456790.012345678901234567890123456	123456788.012345678901234567890123456
+1234567890.123456	1234567891.123456	1234567889.123456
+1234567890.12345678901234567890123456	1234567891.12345678901234567890123456	1234567889.12345678901234567890123456
+12345678901.23456	12345678902.23456	12345678900.23456
+12345678901.2345678901234567890123456	12345678902.2345678901234567890123456	12345678900.2345678901234567890123456
+123456789012.3456	123456789013.3456	123456789011.3456
+123456789012.345678901234567890123456	123456789013.345678901234567890123456	123456789011.345678901234567890123456
+1234567890123.456	1234567890124.456	1234567890122.456
+1234567890123.45678901234567890123456	1234567890124.45678901234567890123456	1234567890122.45678901234567890123456
+12345678901234.56	12345678901235.56	12345678901233.56
+12345678901234.5678901234567890123456	12345678901235.5678901234567890123456	12345678901233.5678901234567890123456
+123456789012345.6	123456789012346.6	123456789012344.6
+123456789012345.678901234567890123456	123456789012346.678901234567890123456	123456789012344.678901234567890123456
+1234567890123456.78901234567890123456	1234567890123457.78901234567890123456	1234567890123455.78901234567890123456
+12345678901234567.8901234567890123456	12345678901234568.8901234567890123456	12345678901234566.8901234567890123456
+123456789012345678.901234567890123456	123456789012345679.901234567890123456	123456789012345677.901234567890123456
+1234567890123456789.01234567890123456	1234567890123456790.01234567890123456	1234567890123456788.01234567890123456
+12345678901234567890.1234567890123456	12345678901234567891.1234567890123456	12345678901234567889.1234567890123456
+123456789012345678901.234567890123456	123456789012345678902.234567890123456	123456789012345678900.234567890123456
+1234567890123456789012.34567890123456	1234567890123456789013.34567890123456	1234567890123456789011.34567890123456
+12345678901234567890123.4567890123456	12345678901234567890124.4567890123456	12345678901234567890122.4567890123456
+123456789012345678901234.567890123456	123456789012345678901235.567890123456	123456789012345678901233.567890123456
+1234567890123456789012345.67890123456	1234567890123456789012346.67890123456	1234567890123456789012344.67890123456
+12345678901234567890123456.7890123456	12345678901234567890123457.7890123456	12345678901234567890123455.7890123456
+123456789012345678901234567.890123456	123456789012345678901234568.890123456	123456789012345678901234566.890123456
+1234567890123456789012345678.90123456	1234567890123456789012345679.90123456	1234567890123456789012345677.90123456
+12345678901234567890123456789.0123456	12345678901234567890123456790.0123456	12345678901234567890123456788.0123456
+123456789012345678901234567890.123456	123456789012345678901234567891.123456	123456789012345678901234567889.123456
+1234567890123456789012345678901.23456	1234567890123456789012345678902.23456	1234567890123456789012345678900.23456
+12345678901234567890123456789012.3456	12345678901234567890123456789013.3456	12345678901234567890123456789011.3456
+123456789012345678901234567890123.456	123456789012345678901234567890124.456	123456789012345678901234567890122.456
+999999999999999999999999999999999	1000000000000000000000000000000000	999999999999999999999999999999998
+1234567890123456789012345678901234.56	1234567890123456789012345678901235.56	1234567890123456789012345678901233.56
+9999999999999999999999999999999999	10000000000000000000000000000000000	9999999999999999999999999999999998
+12345678901234567890123456789012345.6	12345678901234567890123456789012346.6	12345678901234567890123456789012344.6
+123456789012345678901234567890123456	123456789012345678901234567890123457	123456789012345678901234567890123455
+123456789012345678901234567890123456	123456789012345678901234567890123457	123456789012345678901234567890123455
+123456789012345678901234567890123456	123456789012345678901234567890123457	123456789012345678901234567890123455
+123456789012345678901234567890123456	123456789012345678901234567890123457	123456789012345678901234567890123455
+123456789012345678901234567890123456	123456789012345678901234567890123457	123456789012345678901234567890123455
+999999999999999999999999999999999999	NULL	999999999999999999999999999999999998
+PREHOOK: query: SELECT dec, dec * 2, dec / 3  FROM DECIMAL_PRECISION ORDER BY dec
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT dec, dec * 2, dec / 3  FROM DECIMAL_PRECISION ORDER BY dec
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+NULL	NULL	NULL
+-999999999999999999999999999999999999	NULL	-333333333333333333333333333333333333
+-9999999999999999999999999999999999	-19999999999999999999999999999999998	-3333333333333333333333333333333333
+-999999999999999999999999999999999	-1999999999999999999999999999999998	-333333333333333333333333333333333
+-0.00000000000000000000000000000000001	-0.00000000000000000000000000000000002	0
+0	0	0
+0.00000000000000000000000000000000001	0.00000000000000000000000000000000002	0
+0.123456789012345	0.24691357802469	0.041152263004115
+0.12345678901234567890123456789012345	0.2469135780246913578024691357802469	0.04115226300411522630041152263004115
+1.234567890123456	2.469135780246912	0.411522630041152
+1.23456789012345678901234567890123456	2.46913578024691357802469135780246912	0.41152263004115226300411522630041152
+12.34567890123456	24.69135780246912	4.11522630041152
+12.3456789012345678901234567890123456	24.6913578024691357802469135780246912	4.1152263004115226300411522630041152
+123.4567890123456	246.9135780246912	41.1522630041152
+123.456789012345678901234567890123456	246.913578024691357802469135780246912	41.152263004115226300411522630041152
+1234.567890123456	2469.135780246912	411.522630041152
+1234.56789012345678901234567890123456	2469.13578024691357802469135780246912	411.52263004115226300411522630041152
+12345.67890123456	24691.35780246912	4115.22630041152
+12345.6789012345678901234567890123456	24691.3578024691357802469135780246912	4115.2263004115226300411522630041152
+123456.7890123456	246913.5780246912	41152.2630041152
+123456.789012345678901234567890123456	246913.578024691357802469135780246912	41152.263004115226300411522630041152
+1234567.890123456	2469135.780246912	411522.630041152
+1234567.89012345678901234567890123456	2469135.78024691357802469135780246912	411522.63004115226300411522630041152
+12345678.90123456	24691357.80246912	4115226.30041152
+12345678.9012345678901234567890123456	24691357.8024691357802469135780246912	4115226.3004115226300411522630041152
+123456789.0123456	246913578.0246912	41152263.0041152
+123456789.012345678901234567890123456	246913578.024691357802469135780246912	41152263.004115226300411522630041152
+1234567890.123456	2469135780.246912	411522630.041152
+1234567890.12345678901234567890123456	2469135780.24691357802469135780246912	411522630.04115226300411522630041152
+12345678901.23456	24691357802.46912	4115226300.41152
+12345678901.2345678901234567890123456	24691357802.4691357802469135780246912	4115226300.4115226300411522630041152
+123456789012.3456	246913578024.6912	41152263004.1152
+123456789012.345678901234567890123456	246913578024.691357802469135780246912	41152263004.115226300411522630041152
+1234567890123.456	2469135780246.912	411522630041.152
+1234567890123.45678901234567890123456	2469135780246.91357802469135780246912	411522630041.15226300411522630041152
+12345678901234.56	24691357802469.12	4115226300411.52
+12345678901234.5678901234567890123456	24691357802469.1357802469135780246912	4115226300411.5226300411522630041152
+123456789012345.6	246913578024691.2	41152263004115.2
+123456789012345.678901234567890123456	246913578024691.357802469135780246912	41152263004115.226300411522630041152
+1234567890123456.78901234567890123456	2469135780246913.57802469135780246912	411522630041152.26300411522630041152
+12345678901234567.8901234567890123456	24691357802469135.7802469135780246912	4115226300411522.6300411522630041152
+123456789012345678.901234567890123456	246913578024691357.802469135780246912	41152263004115226.300411522630041152
+1234567890123456789.01234567890123456	2469135780246913578.02469135780246912	411522630041152263.00411522630041152
+12345678901234567890.1234567890123456	24691357802469135780.2469135780246912	4115226300411522630.0411522630041152
+123456789012345678901.234567890123456	246913578024691357802.469135780246912	41152263004115226300.411522630041152
+1234567890123456789012.34567890123456	2469135780246913578024.69135780246912	411522630041152263004.11522630041152
+12345678901234567890123.4567890123456	24691357802469135780246.9135780246912	4115226300411522630041.1522630041152
+123456789012345678901234.567890123456	246913578024691357802469.135780246912	41152263004115226300411.522630041152
+1234567890123456789012345.67890123456	2469135780246913578024691.35780246912	411522630041152263004115.22630041152
+12345678901234567890123456.7890123456	24691357802469135780246913.5780246912	4115226300411522630041152.2630041152
+123456789012345678901234567.890123456	246913578024691357802469135.780246912	41152263004115226300411522.630041152
+1234567890123456789012345678.90123456	2469135780246913578024691357.80246912	411522630041152263004115226.30041152
+12345678901234567890123456789.0123456	24691357802469135780246913578.0246912	4115226300411522630041152263.0041152
+123456789012345678901234567890.123456	246913578024691357802469135780.246912	41152263004115226300411522630.041152
+1234567890123456789012345678901.23456	2469135780246913578024691357802.46912	411522630041152263004115226300.41152
+12345678901234567890123456789012.3456	24691357802469135780246913578024.6912	4115226300411522630041152263004.1152
+123456789012345678901234567890123.456	246913578024691357802469135780246.912	41152263004115226300411522630041.152
+999999999999999999999999999999999	1999999999999999999999999999999998	333333333333333333333333333333333
+1234567890123456789012345678901234.56	2469135780246913578024691357802469.12	411522630041152263004115226300411.52
+9999999999999999999999999999999999	19999999999999999999999999999999998	3333333333333333333333333333333333
+12345678901234567890123456789012345.6	24691357802469135780246913578024691.2	4115226300411522630041152263004115.2
+123456789012345678901234567890123456	246913578024691357802469135780246912	41152263004115226300411522630041152
+123456789012345678901234567890123456	246913578024691357802469135780246912	41152263004115226300411522630041152
+123456789012345678901234567890123456	246913578024691357802469135780246912	41152263004115226300411522630041152
+123456789012345678901234567890123456	246913578024691357802469135780246912	41152263004115226300411522630041152
+123456789012345678901234567890123456	246913578024691357802469135780246912	41152263004115226300411522630041152
+999999999999999999999999999999999999	NULL	333333333333333333333333333333333333
+PREHOOK: query: SELECT dec, dec / 9 FROM DECIMAL_PRECISION ORDER BY dec
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT dec, dec / 9 FROM DECIMAL_PRECISION ORDER BY dec
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+-999999999999999999999999999999999999	-111111111111111111111111111111111111
+-9999999999999999999999999999999999	-1111111111111111111111111111111111
+-999999999999999999999999999999999	-111111111111111111111111111111111
+-0.00000000000000000000000000000000001	0
+0	0
+0.00000000000000000000000000000000001	0
+0.123456789012345	0.01371742100137166666666666666666667
+0.12345678901234567890123456789012345	0.01371742100137174210013717421001372
+1.234567890123456	0.13717421001371733333333333333333333
+1.23456789012345678901234567890123456	0.13717421001371742100137174210013717
+12.34567890123456	1.37174210013717333333333333333333333
+12.3456789012345678901234567890123456	1.37174210013717421001371742100137173
+123.4567890123456	13.7174210013717333333333333333333333
+123.456789012345678901234567890123456	13.7174210013717421001371742100137173
+1234.567890123456	137.174210013717333333333333333333333
+1234.56789012345678901234567890123456	137.174210013717421001371742100137173
+12345.67890123456	1371.74210013717333333333333333333333
+12345.6789012345678901234567890123456	1371.74210013717421001371742100137173
+123456.7890123456	13717.4210013717333333333333333333333
+123456.789012345678901234567890123456	13717.4210013717421001371742100137173
+1234567.890123456	137174.210013717333333333333333333333
+1234567.89012345678901234567890123456	137174.210013717421001371742100137173
+12345678.90123456	1371742.10013717333333333333333333333
+12345678.9012345678901234567890123456	1371742.10013717421001371742100137173
+123456789.0123456	13717421.0013717333333333333333333333
+123456789.012345678901234567890123456	13717421.0013717421001371742100137173
+1234567890.123456	137174210.013717333333333333333333333
+1234567890.12345678901234567890123456	137174210.013717421001371742100137173
+12345678901.23456	1371742100.13717333333333333333333333
+12345678901.2345678901234567890123456	1371742100.13717421001371742100137173
+123456789012.3456	13717421001.3717333333333333333333333
+123456789012.345678901234567890123456	13717421001.3717421001371742100137173
+1234567890123.456	137174210013.717333333333333333333333
+1234567890123.45678901234567890123456	137174210013.717421001371742100137173
+12345678901234.56	1371742100137.17333333333333333333333
+12345678901234.5678901234567890123456	1371742100137.17421001371742100137173
+123456789012345.6	13717421001371.7333333333333333333333
+123456789012345.678901234567890123456	13717421001371.7421001371742100137173
+1234567890123456.78901234567890123456	137174210013717.421001371742100137173
+12345678901234567.8901234567890123456	1371742100137174.21001371742100137173
+123456789012345678.901234567890123456	13717421001371742.1001371742100137173
+1234567890123456789.01234567890123456	137174210013717421.001371742100137173
+12345678901234567890.1234567890123456	1371742100137174210.01371742100137173
+123456789012345678901.234567890123456	13717421001371742100.1371742100137173
+1234567890123456789012.34567890123456	137174210013717421001.371742100137173
+12345678901234567890123.4567890123456	1371742100137174210013.71742100137173
+123456789012345678901234.567890123456	13717421001371742100137.1742100137173
+1234567890123456789012345.67890123456	137174210013717421001371.742100137173
+12345678901234567890123456.7890123456	1371742100137174210013717.42100137173
+123456789012345678901234567.890123456	13717421001371742100137174.2100137173
+1234567890123456789012345678.90123456	137174210013717421001371742.100137173
+12345678901234567890123456789.0123456	1371742100137174210013717421.00137173
+123456789012345678901234567890.123456	13717421001371742100137174210.0137173
+1234567890123456789012345678901.23456	137174210013717421001371742100.137173
+12345678901234567890123456789012.3456	1371742100137174210013717421001.37173
+123456789012345678901234567890123.456	13717421001371742100137174210013.7173
+999999999999999999999999999999999	111111111111111111111111111111111
+1234567890123456789012345678901234.56	137174210013717421001371742100137.173
+9999999999999999999999999999999999	1111111111111111111111111111111111
+12345678901234567890123456789012345.6	1371742100137174210013717421001371.73
+123456789012345678901234567890123456	13717421001371742100137174210013717.3
+123456789012345678901234567890123456	13717421001371742100137174210013717.3
+123456789012345678901234567890123456	13717421001371742100137174210013717.3
+123456789012345678901234567890123456	13717421001371742100137174210013717.3
+123456789012345678901234567890123456	13717421001371742100137174210013717.3
+999999999999999999999999999999999999	111111111111111111111111111111111111
+PREHOOK: query: SELECT dec, dec / 27 FROM DECIMAL_PRECISION ORDER BY dec
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT dec, dec / 27 FROM DECIMAL_PRECISION ORDER BY dec
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+-999999999999999999999999999999999999	-37037037037037037037037037037037037
+-9999999999999999999999999999999999	-370370370370370370370370370370370.333
+-999999999999999999999999999999999	-37037037037037037037037037037037
+-0.00000000000000000000000000000000001	0
+0	0
+0.00000000000000000000000000000000001	0
+0.123456789012345	0.00457247366712388888888888888888889
+0.12345678901234567890123456789012345	0.00457247366712391403337905807000457
+1.234567890123456	0.04572473667123911111111111111111111
+1.23456789012345678901234567890123456	0.04572473667123914033379058070004572
+12.34567890123456	0.45724736671239111111111111111111111
+12.3456789012345678901234567890123456	0.45724736671239140333790580700045724
+123.4567890123456	4.57247366712391111111111111111111111
+123.456789012345678901234567890123456	4.57247366712391403337905807000457244
+1234.567890123456	45.7247366712391111111111111111111111
+1234.56789012345678901234567890123456	45.7247366712391403337905807000457244
+12345.67890123456	457.247366712391111111111111111111111
+12345.6789012345678901234567890123456	457.247366712391403337905807000457244
+123456.7890123456	4572.47366712391111111111111111111111
+123456.789012345678901234567890123456	4572.47366712391403337905807000457244
+1234567.890123456	45724.7366712391111111111111111111111
+1234567.89012345678901234567890123456	45724.7366712391403337905807000457244
+12345678.90123456	457247.366712391111111111111111111111
+12345678.9012345678901234567890123456	457247.366712391403337905807000457244
+123456789.0123456	4572473.66712391111111111111111111111
+123456789.012345678901234567890123456	4572473.66712391403337905807000457244
+1234567890.123456	45724736.6712391111111111111111111111
+1234567890.12345678901234567890123456	45724736.6712391403337905807000457244
+12345678901.23456	457247366.712391111111111111111111111
+12345678901.2345678901234567890123456	457247366.712391403337905807000457244
+123456789012.3456	4572473667.12391111111111111111111111
+123456789012.345678901234567890123456	4572473667.12391403337905807000457244
+1234567890123.456	45724736671.2391111111111111111111111
+1234567890123.45678901234567890123456	45724736671.2391403337905807000457244
+12345678901234.56	457247366712.391111111111111111111111
+12345678901234.5678901234567890123456	457247366712.391403337905807000457244
+123456789012345.6	4572473667123.91111111111111111111111
+123456789012345.678901234567890123456	4572473667123.91403337905807000457244
+1234567890123456.78901234567890123456	45724736671239.1403337905807000457244
+12345678901234567.8901234567890123456	457247366712391.403337905807000457244
+123456789012345678.901234567890123456	4572473667123914.03337905807000457244
+1234567890123456789.01234567890123456	45724736671239140.3337905807000457244
+12345678901234567890.1234567890123456	457247366712391403.337905807000457244
+123456789012345678901.234567890123456	4572473667123914033.37905807000457244
+1234567890123456789012.34567890123456	45724736671239140333.7905807000457244
+12345678901234567890123.4567890123456	457247366712391403337.905807000457244
+123456789012345678901234.567890123456	4572473667123914033379.05807000457244
+1234567890123456789012345.67890123456	45724736671239140333790.5807000457244
+12345678901234567890123456.7890123456	457247366712391403337905.807000457244
+123456789012345678901234567.890123456	4572473667123914033379058.07000457244
+1234567890123456789012345678.90123456	45724736671239140333790580.7000457244
+12345678901234567890123456789.0123456	457247366712391403337905807.000457244
+123456789012345678901234567890.123456	4572473667123914033379058070.00457244
+1234567890123456789012345678901.23456	45724736671239140333790580700.0457244
+12345678901234567890123456789012.3456	457247366712391403337905807000.457244
+123456789012345678901234567890123.456	4572473667123914033379058070004.57244
+999999999999999999999999999999999	37037037037037037037037037037037
+1234567890123456789012345678901234.56	45724736671239140333790580700045.7244
+9999999999999999999999999999999999	370370370370370370370370370370370.333
+12345678901234567890123456789012345.6	457247366712391403337905807000457.244
+123456789012345678901234567890123456	4572473667123914033379058070004572.44
+123456789012345678901234567890123456	4572473667123914033379058070004572.44
+123456789012345678901234567890123456	4572473667123914033379058070004572.44
+123456789012345678901234567890123456	4572473667123914033379058070004572.44
+123456789012345678901234567890123456	4572473667123914033379058070004572.44
+999999999999999999999999999999999999	37037037037037037037037037037037037
+PREHOOK: query: SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+NULL	NULL
+-999999999999999999999999999999999999	NULL
+-9999999999999999999999999999999999	NULL
+-999999999999999999999999999999999	NULL
+-0.00000000000000000000000000000000001	NULL
+0	0
+0.00000000000000000000000000000000001	NULL
+0.123456789012345	0.015241578753238669120562399025
+0.12345678901234567890123456789012345	NULL
+1.234567890123456	1.524157875323881726870921383936
+1.23456789012345678901234567890123456	NULL
+12.34567890123456	152.4157875323881726870921383936
+12.3456789012345678901234567890123456	NULL
+123.4567890123456	15241.57875323881726870921383936
+123.456789012345678901234567890123456	NULL
+1234.567890123456	1524157.875323881726870921383936
+1234.56789012345678901234567890123456	NULL
+12345.67890123456	152415787.5323881726870921383936
+12345.6789012345678901234567890123456	NULL
+123456.7890123456	15241578753.23881726870921383936
+123456.789012345678901234567890123456	NULL
+1234567.890123456	1524157875323.881726870921383936
+1234567.89012345678901234567890123456	NULL
+12345678.90123456	152415787532388.1726870921383936
+12345678.9012345678901234567890123456	NULL
+123456789.0123456	15241578753238817.26870921383936
+123456789.012345678901234567890123456	NULL
+1234567890.123456	1524157875323881726.870921383936
+1234567890.12345678901234567890123456	NULL
+12345678901.23456	152415787532388172687.0921383936
+12345678901.2345678901234567890123456	NULL
+123456789012.3456	15241578753238817268709.21383936
+123456789012.345678901234567890123456	NULL
+1234567890123.456	1524157875323881726870921.383936
+1234567890123.45678901234567890123456	NULL
+12345678901234.56	152415787532388172687092138.3936
+12345678901234.5678901234567890123456	NULL
+123456789012345.6	15241578753238817268709213839.36
+123456789012345.678901234567890123456	NULL
+1234567890123456.78901234567890123456	NULL
+12345678901234567.8901234567890123456	NULL
+123456789012345678.901234567890123456	NULL
+1234567890123456789.01234567890123456	NULL
+12345678901234567890.1234567890123456	NULL
+123456789012345678901.234567890123456	NULL
+1234567890123456789012.34567890123456	NULL
+12345678901234567890123.4567890123456	NULL
+123456789012345678901234.567890123456	NULL
+1234567890123456789012345.67890123456	NULL
+12345678901234567890123456.7890123456	NULL
+123456789012345678901234567.890123456	NULL
+1234567890123456789012345678.90123456	NULL
+12345678901234567890123456789.0123456	NULL
+123456789012345678901234567890.123456	NULL
+1234567890123456789012345678901.23456	NULL
+12345678901234567890123456789012.3456	NULL
+123456789012345678901234567890123.456	NULL
+999999999999999999999999999999999	NULL
+1234567890123456789012345678901234.56	NULL
+9999999999999999999999999999999999	NULL
+12345678901234567890123456789012345.6	NULL
+123456789012345678901234567890123456	NULL
+123456789012345678901234567890123456	NULL
+123456789012345678901234567890123456	NULL
+123456789012345678901234567890123456	NULL
+123456789012345678901234567890123456	NULL
+999999999999999999999999999999999999	NULL
+PREHOOK: query: SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION
+PREHOOK: type: QUERY
+PREHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@decimal_precision
+#### A masked pattern was here ####
+NULL	NULL
+PREHOOK: query: DROP TABLE DECIMAL_PRECISION
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@decimal_precision
+PREHOOK: Output: default@decimal_precision
+POSTHOOK: query: DROP TABLE DECIMAL_PRECISION
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@decimal_precision
+POSTHOOK: Output: default@decimal_precision
diff --git a/ql/src/test/results/clientpositive/decimal_serde.q.out b/ql/src/test/results/clientpositive/decimal_serde.q.out
index a90be8c8c5..138dbc0934 100644
--- a/ql/src/test/results/clientpositive/decimal_serde.q.out
+++ b/ql/src/test/results/clientpositive/decimal_serde.q.out
@@ -39,8 +39,10 @@ POSTHOOK: query: SELECT * FROM DECIMAL_TEXT ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_text
 #### A masked pattern was here ####
+NULL	0
+NULL	0
 -1234567890.123456789	-1234567890
--4.4E+3	4400
+-4400	4400
 -1255.49	-1255
 -1.122	-11
 -1.12	-1
@@ -50,7 +52,6 @@ POSTHOOK: Input: default@decimal_text
 -0.3	0
 0	0
 0	0
-1E-99	0
 0.01	0
 0.02	0
 0.1	0
@@ -69,14 +70,13 @@ POSTHOOK: Input: default@decimal_text
 3.14	3
 3.14	3
 3.14	4
-1E+1	10
-2E+1	20
-1E+2	100
+10	10
+20	20
+100	100
 124	124
 125.2	125
-2E+2	200
+200	200
 1234567890.12345678	1234567890
-1E+99	0
 PREHOOK: query: CREATE TABLE DECIMAL_RC
 STORED AS RCFile AS
 SELECT * FROM DECIMAL_TEXT
@@ -128,8 +128,10 @@ POSTHOOK: query: SELECT * FROM DECIMAL_SEQUENCE ORDER BY key, value
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_sequence
 #### A masked pattern was here ####
+NULL	0
+NULL	0
 -1234567890.123456789	-1234567890
--4.4E+3	4400
+-4400	4400
 -1255.49	-1255
 -1.122	-11
 -1.12	-1
@@ -139,7 +141,6 @@ POSTHOOK: Input: default@decimal_sequence
 -0.3	0
 0	0
 0	0
-1E-99	0
 0.01	0
 0.02	0
 0.1	0
@@ -158,14 +159,13 @@ POSTHOOK: Input: default@decimal_sequence
 3.14	3
 3.14	3
 3.14	4
-1E+1	10
-2E+1	20
-1E+2	100
+10	10
+20	20
+100	100
 124	124
 125.2	125
-2E+2	200
+200	200
 1234567890.12345678	1234567890
-1E+99	0
 PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_TEXT
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@decimal_text
diff --git a/ql/src/test/results/clientpositive/decimal_udf.q.out b/ql/src/test/results/clientpositive/decimal_udf.q.out
index 943fc34382..5bd52c9bbf 100644
--- a/ql/src/test/results/clientpositive/decimal_udf.q.out
+++ b/ql/src/test/results/clientpositive/decimal_udf.q.out
@@ -64,17 +64,17 @@ POSTHOOK: query: SELECT key + key FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--8.8E+3
-2E+99
-2E-99
+-8800
+NULL
+NULL
 0
-2E+2
-2E+1
+200
+20
 2
 0.2
 0.02
-4E+2
-4E+1
+400
+40
 4
 0
 0.4
@@ -146,16 +146,16 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
 0
-1E+99
-1E-99
+NULL
+NULL
 0
-2E+2
-2E+1
+200
+20
 2
 0.1
 0.01
-4E+2
-4E+1
+400
+40
 4
 0
 0.2
@@ -226,17 +226,17 @@ POSTHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--2.2E+3
-1E+99
-1E-99
+-2200
+NULL
+NULL
 0
-1.5E+2
+150
 15
 1.5
 0.1
 0.01
-3E+2
-3E+1
+300
+30
 3
 0
 0.2
@@ -308,8 +308,8 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
 -4399
-1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001
-1.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001
+NULL
+NULL
 1
 101
 11
@@ -391,8 +391,8 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
 0
-0
-0
+NULL
+NULL
 0
 0
 0
@@ -471,9 +471,9 @@ POSTHOOK: query: SELECT key - value FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--8.8E+3
-1E+99
-1E-99
+-8800
+NULL
+NULL
 0
 0
 0
@@ -506,7 +506,7 @@ POSTHOOK: Input: default@decimal_udf
 0.14
 0.14
 -0.86
--1E-25
+-0.0000000000000000000000001
 -0.123456789
 0.12345678
 PREHOOK: query: EXPLAIN SELECT key - (value/2) FROM DECIMAL_UDF
@@ -552,17 +552,17 @@ POSTHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--6.6E+3
-1E+99
-1E-99
+-6600
+NULL
+NULL
 0
-5E+1
+50
 5
 0.5
 0.1
 0.01
-1E+2
-1E+1
+100
+10
 1
 0
 0.2
@@ -634,8 +634,8 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
 -4401
-999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999
--0.999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999
+NULL
+NULL
 -1
 99
 9
@@ -668,7 +668,7 @@ POSTHOOK: Input: default@decimal_udf
 2.14
 2.14
 2.14
--1E-25
+-0.0000000000000000000000001
 -1234567891.123456789
 1234567889.12345678
 PREHOOK: query: -- multiplication
@@ -716,17 +716,17 @@ POSTHOOK: query: SELECT key * key FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
-1.936E+7
-1E+198
-1E-198
+19360000
+NULL
+NULL
 0
-1E+4
-1E+2
+10000
+100
 1
 0.01
 0.0001
-4E+4
-4E+2
+40000
+400
 4
 0
 0.04
@@ -751,8 +751,8 @@ POSTHOOK: Input: default@decimal_udf
 9.8596
 9.8596
 9.8596
-0.99999999999999999999999980000000000000000000000001
-1524157875323883675.019051998750190521
+NULL
+NULL
 1524157875323883652.7968299765279684
 PREHOOK: query: EXPLAIN SELECT key * value FROM DECIMAL_UDF
 PREHOOK: type: QUERY
@@ -797,17 +797,17 @@ POSTHOOK: query: SELECT key * value FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--1.936E+7
-0
-0
+-19360000
+NULL
+NULL
 0
-1E+4
-1E+2
+10000
+100
 1
 0
 0
-4E+4
-4E+2
+40000
+400
 4
 0
 0
@@ -827,7 +827,7 @@ POSTHOOK: Input: default@decimal_udf
 1.12
 1.122
 15376
-1.565E+4
+15650
 1575639.95
 9.42
 9.42
@@ -878,17 +878,17 @@ POSTHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--9.68E+6
-0
-0
+-9680000
+NULL
+NULL
 0
-5E+3
-5E+1
+5000
+50
 0.5
 0
 0
-2E+4
-2E+2
+20000
+200
 2
 0
 0
@@ -959,17 +959,17 @@ POSTHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--8.8E+3
-2E+99
-2E-99
+-8800
+NULL
+NULL
 0
-2E+2
-2E+1
+200
+20
 2
 0.2
 0.02
-4E+2
-4E+1
+400
+40
 4
 0
 0.4
@@ -1170,8 +1170,6 @@ POSTHOOK: Input: default@decimal_udf
 1
 1
 1
-1
-1
 PREHOOK: query: EXPLAIN SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0
@@ -1228,7 +1226,7 @@ POSTHOOK: Input: default@decimal_udf
 1
 1
 1
-1.04666666666666666666666666666666666666666666666666666666666666667
+1.04666666666666666666666666666666667
 1.12
 1.12
 0.102
@@ -1236,13 +1234,13 @@ POSTHOOK: Input: default@decimal_udf
 1.122
 1
 1.0016
-1.00039043824701195219123505976095617529880478087649402390438247012
-1.04666666666666666666666666666666666666666666666666666666666666667
-1.04666666666666666666666666666666666666666666666666666666666666667
+1.00039043824701195219123505976095618
+1.04666666666666666666666666666666667
+1.04666666666666666666666666666666667
 0.785
 0.9999999999999999999999999
 1.0000000001
-1.00000000009999999270999993366099939631509450646736000885297608056
+1.0000000000999999927099999336609994
 PREHOOK: query: EXPLAIN SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0
@@ -1299,7 +1297,7 @@ POSTHOOK: Input: default@decimal_udf
 2
 2
 2
-2.09333333333333333333333333333333333333333333333333333333333333333
+2.09333333333333333333333333333333333
 2.24
 2.24
 0.204
@@ -1307,13 +1305,13 @@ POSTHOOK: Input: default@decimal_udf
 2.244
 2
 2.0032
-2.00078087649402390438247011952191235059760956175298804780876494024
-2.09333333333333333333333333333333333333333333333333333333333333333
-2.09333333333333333333333333333333333333333333333333333333333333333
+2.00078087649402390438247011952191235
+2.09333333333333333333333333333333333
+2.09333333333333333333333333333333333
 1.57
 1.9999999999999999999999998
 2.0000000002
-2.00000000019999998541999986732199879263018901293472001770595216112
+2.00000000019999998541999986732199879
 PREHOOK: query: EXPLAIN SELECT key / '2.0' FROM DECIMAL_UDF
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN SELECT key / '2.0' FROM DECIMAL_UDF
@@ -1357,17 +1355,17 @@ POSTHOOK: query: SELECT key / '2.0' FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--2.2E+3
-5E+98
-0
+-2200
+NULL
+NULL
 0
-5E+1
+50
 5
 0.5
 0.05
 0.005
-1E+2
-1E+1
+100
+10
 1
 0
 0.1
@@ -1440,17 +1438,17 @@ POSTHOOK: query: SELECT abs(key) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
-4.4E+3
-1E+99
-1E-99
+4400
+NULL
+NULL
 0
-1E+2
-1E+1
+100
+10
 1
 0.1
 0.01
-2E+2
-2E+1
+200
+20
 2
 0
 0.2
@@ -1604,18 +1602,18 @@ POSTHOOK: Input: default@decimal_udf
 -1255	-1255.49	-1255.49
 -11	-1.122	-1.122
 -1	-1.12	-1.12
-0	71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571.45214285714285714285714285714285714285714285714285714285714285714	71428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571428571.45214285714285714285714285714285714285714285714285714285714285714
+0	0.0275	0.0275
 1	1.04839999999999999999999998	1.04839999999999999999999998
 2	2	2
 3	3.14	3.14
 4	3.14	3.14
-10	1E+1	1E+1
-20	2E+1	2E+1
-100	1E+2	1E+2
+10	10	10
+20	20	20
+100	100	100
 124	124	124
 125	125.2	125.2
-200	2E+2	2E+2
-4400	-4.4E+3	-4.4E+3
+200	200	200
+4400	-4400	-4400
 1234567890	1234567890.12345678	1234567890.12345678
 PREHOOK: query: -- negative
 EXPLAIN SELECT -key FROM DECIMAL_UDF
@@ -1662,17 +1660,17 @@ POSTHOOK: query: SELECT -key FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
-4.4E+3
--1E+99
--1E-99
+4400
+NULL
+NULL
 0
--1E+2
--1E+1
+-100
+-10
 -1
 -0.1
 -0.01
--2E+2
--2E+1
+-200
+-20
 -2
 0
 -0.2
@@ -1745,17 +1743,17 @@ POSTHOOK: query: SELECT +key FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--4.4E+3
-1E+99
-1E-99
+-4400
+NULL
+NULL
 0
-1E+2
-1E+1
+100
+10
 1
 0.1
 0.01
-2E+2
-2E+1
+200
+20
 2
 0
 0.2
@@ -1828,17 +1826,17 @@ POSTHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--4.4E+3
-1E+99
-1
+-4400
+NULL
+NULL
 0
-1E+2
-1E+1
+100
+10
 1
 1
 1
-2E+2
-2E+1
+200
+20
 2
 0
 1
@@ -1864,7 +1862,7 @@ POSTHOOK: Input: default@decimal_udf
 4
 4
 1
--1.23456789E+9
+-1234567890
 1234567891
 PREHOOK: query: -- floor
 EXPLAIN SELECT FLOOR(key) FROM DECIMAL_UDF
@@ -1911,17 +1909,17 @@ POSTHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--4.4E+3
-1E+99
-0
+-4400
+NULL
+NULL
 0
-1E+2
-1E+1
+100
+10
 1
 0
 0
-2E+2
-2E+1
+200
+20
 2
 0
 0
@@ -1948,7 +1946,7 @@ POSTHOOK: Input: default@decimal_udf
 3
 0
 -1234567891
-1.23456789E+9
+1234567890
 PREHOOK: query: -- round
 EXPLAIN SELECT ROUND(key, 2) FROM DECIMAL_UDF
 PREHOOK: type: QUERY
@@ -1994,17 +1992,17 @@ POSTHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
--4.4E+3
-1E+99
-0
+-4400
+NULL
+NULL
 0
-1E+2
-1E+1
+100
+10
 1
 0.1
 0.01
-2E+2
-2E+1
+200
+20
 2
 0
 0.2
@@ -2077,17 +2075,17 @@ POSTHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
-1.936E+7
-1E+198
-1E-198
+19360000
+NULL
+NULL
 0
-1E+4
-1E+2
+10000
+100
 1
 0.01
 0.0001
-4E+4
-4E+2
+40000
+400
 4
 0
 0.04
@@ -2112,8 +2110,8 @@ POSTHOOK: Input: default@decimal_udf
 9.8596
 9.8596
 9.8596
-0.99999999999999999999999980000000000000000000000001
-1524157875323883675.019051998750190521
+NULL
+NULL
 1524157875323883652.7968299765279684
 PREHOOK: query: -- modulo
 EXPLAIN SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF
@@ -2161,7 +2159,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
 -2199
-1
+NULL
 NULL
 NULL
 1
@@ -2195,7 +2193,7 @@ NULL
 1
 1
 1
-1E-25
+0.0000000000000000000000001
 -617283944.0617283945
 1
 PREHOOK: query: -- stddev, var
@@ -2293,7 +2291,7 @@ POSTHOOK: Input: default@decimal_udf
 -1255	0.0	0.0
 -11	0.0	0.0
 -1	0.0	0.0
-0	2.5753937681885636E98	6.632653061224489E196
+0	0.23469892060538614	0.055083583333333345
 1	0.05928102563215321	0.0035142400000000066
 2	0.0	0.0
 3	0.0	0.0
@@ -2401,7 +2399,7 @@ POSTHOOK: Input: default@decimal_udf
 -1255	0.0	0.0
 -11	0.0	0.0
 -1	0.0	0.0
-0	2.6726124191242437E98	7.142857142857142E196
+0	0.24513502772590828	0.06009118181818183
 1	0.06627820154470102	0.004392800000000008
 2	0.0	0.0
 3	0.0	0.0
@@ -2483,7 +2481,7 @@ POSTHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@decimal_udf
 #### A masked pattern was here ####
-[{"x":-3.429369299009602E7,"y":36.0},{"x":1.2345678901234567E9,"y":1.0},{"x":1.0E99,"y":1.0}]
+[{"x":-1.2345678901234567E9,"y":1.0},{"x":-148.75058823529412,"y":34.0},{"x":1.2345678901234567E9,"y":1.0}]
 PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@decimal_udf
diff --git a/ql/src/test/results/clientpositive/literal_decimal.q.out b/ql/src/test/results/clientpositive/literal_decimal.q.out
index dd139220e3..944037b28d 100644
--- a/ql/src/test/results/clientpositive/literal_decimal.q.out
+++ b/ql/src/test/results/clientpositive/literal_decimal.q.out
@@ -58,4 +58,4 @@ POSTHOOK: query: SELECT -1BD, 0BD, 1BD, 3.14BD, -3.14BD, 99999999999999999BD, 99
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
--1	0	1	3.14	-3.14	99999999999999999	99999999999999999.9999999999999	1E-99	1E+99
+-1	0	1	3.14	-3.14	99999999999999999	99999999999999999.9999999999999	NULL	NULL
diff --git a/ql/src/test/results/clientpositive/serde_regex.q.out b/ql/src/test/results/clientpositive/serde_regex.q.out
index a1b0c3fd2c..26d14d04f1 100644
--- a/ql/src/test/results/clientpositive/serde_regex.q.out
+++ b/ql/src/test/results/clientpositive/serde_regex.q.out
@@ -202,8 +202,10 @@ POSTHOOK: query: SELECT key, value FROM serde_regex1 ORDER BY key
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@serde_regex1
 #### A masked pattern was here ####
+NULL	0
+NULL	0
 -1234567890.123456789	-1234567890
--4.4E+3	4400
+-4400	4400
 -1255.49	-1255
 -1.122	-11
 -1.12	-1
@@ -213,7 +215,6 @@ POSTHOOK: Input: default@serde_regex1
 -0.3	0
 0	0
 0	0
-1E-99	0
 0.01	0
 0.02	0
 0.1	0
@@ -230,16 +231,15 @@ POSTHOOK: Input: default@serde_regex1
 2	2
 3.14	3
 3.14	3
-3.14	3
 3.14	4
-1E+1	10
-2E+1	20
-1E+2	100
+3.14	3
+10	10
+20	20
+100	100
 124	124
 125.2	125
-2E+2	200
+200	200
 1234567890.12345678	1234567890
-1E+99	0
 PREHOOK: query: DROP TABLE serde_regex1
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@serde_regex1
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java
index 5031b53c41..2736d4be92 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.hive.serde2;
 
-import java.math.BigDecimal;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
@@ -28,6 +27,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
@@ -135,7 +135,7 @@ public void initialize(Configuration conf, Properties tbl)
       } else if (typeName.equals(serdeConstants.BOOLEAN_TYPE_NAME)) {
         columnOIs.add(PrimitiveObjectInspectorFactory.javaBooleanObjectInspector);
       } else if (typeName.equals(serdeConstants.DECIMAL_TYPE_NAME)) {
-        columnOIs.add(PrimitiveObjectInspectorFactory.javaBigDecimalObjectInspector);
+        columnOIs.add(PrimitiveObjectInspectorFactory.javaHiveDecimalObjectInspector);
       } else {
          throw new SerDeException(getClass().getName()
          + " doesn't allow column [" + c + "] named "
@@ -230,8 +230,8 @@ public Object deserialize(Writable blob) throws SerDeException {
           b = Boolean.valueOf(t);
           row.set(c, b);
         } else if (typeName.equals(serdeConstants.DECIMAL_TYPE_NAME)) {
-          BigDecimal bd;
-          bd = new BigDecimal(t);
+          HiveDecimal bd;
+          bd = new HiveDecimal(t);
           row.set(c, bd);
         }
       } catch (RuntimeException e) {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
index f022ba3e18..f68388fb54 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
@@ -34,7 +34,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
@@ -298,7 +298,7 @@ static void buildJSONString(StringBuilder sb, Object o, ObjectInspector oi, Stri
           break;
         }
         case DECIMAL: {
-          sb.append(((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o));
+          sb.append(((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o));
           break;
         }
         default:
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
index c07814683e..2cb15cc90e 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.serde2.binarysortable;
 
 import java.io.IOException;
-import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.nio.charset.Charset;
 import java.util.ArrayList;
@@ -32,13 +31,14 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
@@ -49,12 +49,12 @@
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.UnionObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ShortObjectInspector;
@@ -382,8 +382,8 @@ static Object deserialize(InputByteBuffer buffer, TypeInfo type,
       case DECIMAL: {
         // See serialization of decimal for explanation (below)
 
-        BigDecimalWritable bdw = (reuse == null ? new BigDecimalWritable() :
-          (BigDecimalWritable) reuse);
+        HiveDecimalWritable bdw = (reuse == null ? new HiveDecimalWritable() :
+          (HiveDecimalWritable) reuse);
 
         int b = buffer.read(invert) - 1;
         assert (b == 1 || b == -1 || b == 0);
@@ -427,7 +427,7 @@ static Object deserialize(InputByteBuffer buffer, TypeInfo type,
 
         String digits = new String(decimalBuffer, 0, length, decimalCharSet);
         BigInteger bi = new BigInteger(digits);
-        BigDecimal bd = new BigDecimal(bi).scaleByPowerOfTen(factor-length);
+        HiveDecimal bd = new HiveDecimal(bi).scaleByPowerOfTen(factor-length);
 
         if (!positive) {
           bd = bd.negate();
@@ -688,11 +688,11 @@ static void serialize(OutputByteBuffer buffer, Object o, ObjectInspector oi,
         // Factor is -2 (move decimal point 2 positions right)
         // Digits are: 123
 
-        BigDecimalObjectInspector boi = (BigDecimalObjectInspector) poi;
-        BigDecimal dec = boi.getPrimitiveJavaObject(o).stripTrailingZeros();
+        HiveDecimalObjectInspector boi = (HiveDecimalObjectInspector) poi;
+        HiveDecimal dec = boi.getPrimitiveJavaObject(o);
 
         // get the sign of the big decimal
-        int sign = dec.compareTo(BigDecimal.ZERO);
+        int sign = dec.compareTo(HiveDecimal.ZERO);
 
         // we'll encode the absolute value (sign is separate)
         dec = dec.abs();
@@ -788,6 +788,7 @@ private static void serializeBytes(OutputByteBuffer buffer, byte[] data, int len
     }
     buffer.write((byte) 0, invert);
   }
+  @Override
   public SerDeStats getSerDeStats() {
     // no support for statistics
     return null;
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/io/BigDecimalWritable.java b/serde/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java
similarity index 70%
rename from serde/src/java/org/apache/hadoop/hive/serde2/io/BigDecimalWritable.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java
index db009c4bce..81500a8849 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/io/BigDecimalWritable.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java
@@ -20,52 +20,47 @@
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
-import java.math.BigDecimal;
 import java.math.BigInteger;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt;
 import org.apache.hadoop.io.WritableComparable;
 import org.apache.hadoop.io.WritableUtils;
 
-public class BigDecimalWritable implements WritableComparable<BigDecimalWritable> {
+public class HiveDecimalWritable implements WritableComparable<HiveDecimalWritable> {
 
-  static final private Log LOG = LogFactory.getLog(BigDecimalWritable.class);
+  static final private Log LOG = LogFactory.getLog(HiveDecimalWritable.class);
 
   private byte[] internalStorage = new byte[0];
   private int scale;
 
   private final VInt vInt = new VInt(); // reusable integer
 
-  public BigDecimalWritable() {
+  public HiveDecimalWritable() {
   }
 
-  public BigDecimalWritable(byte[] bytes, int scale) {
+  public HiveDecimalWritable(byte[] bytes, int scale) {
     set(bytes, scale);
   }
 
-  public BigDecimalWritable(BigDecimalWritable writable) {
-    set(writable.getBigDecimal());
+  public HiveDecimalWritable(HiveDecimalWritable writable) {
+    set(writable.getHiveDecimal());
   }
 
-  public BigDecimalWritable(BigDecimal value) {
+  public HiveDecimalWritable(HiveDecimal value) {
     set(value);
   }
 
-  public void set(BigDecimal value) {
-    value = value.stripTrailingZeros();
-    if (value.compareTo(BigDecimal.ZERO) == 0) {
-      // Special case for 0, because java doesn't strip zeros correctly on that number.
-      value = BigDecimal.ZERO;
-    }
+  public void set(HiveDecimal value) {
     set(value.unscaledValue().toByteArray(), value.scale());
   }
 
-  public void set(BigDecimalWritable writable) {
-    set(writable.getBigDecimal());
+  public void set(HiveDecimalWritable writable) {
+    set(writable.getHiveDecimal());
   }
 
   public void set(byte[] bytes, int scale) {
@@ -85,8 +80,8 @@ public void setFromBytes(byte[] bytes, int offset, int length) {
     System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);
   }
 
-  public BigDecimal getBigDecimal() {
-    return new BigDecimal(new BigInteger(internalStorage), scale);
+  public HiveDecimal getHiveDecimal() {
+    return new HiveDecimal(new BigInteger(internalStorage), scale);
   }
 
   @Override
@@ -107,8 +102,8 @@ public void write(DataOutput out) throws IOException {
   }
 
   @Override
-  public int compareTo(BigDecimalWritable that) {
-    return getBigDecimal().compareTo(that.getBigDecimal());
+  public int compareTo(HiveDecimalWritable that) {
+    return getHiveDecimal().compareTo(that.getHiveDecimal());
   }
 
   public void writeToByteStream(Output byteStream) {
@@ -119,25 +114,25 @@ public void writeToByteStream(Output byteStream) {
 
   @Override
   public String toString() {
-    return getBigDecimal().toString();
+    return getHiveDecimal().toString();
   }
 
   @Override
   public boolean equals(Object other) {
-    if (other == null || !(other instanceof BigDecimalWritable)) {
+    if (other == null || !(other instanceof HiveDecimalWritable)) {
       return false;
     }
-    BigDecimalWritable bdw = (BigDecimalWritable) other;
+    HiveDecimalWritable bdw = (HiveDecimalWritable) other;
 
-    // 'equals' and 'compareTo' are not compatible with BigDecimals. We want 
-    // compareTo which returns true iff the numbers are equal (e.g.: 3.14 is 
-    // the same as 3.140). 'Equals' returns true iff equal and the same scale 
+    // 'equals' and 'compareTo' are not compatible with HiveDecimals. We want
+    // compareTo which returns true iff the numbers are equal (e.g.: 3.14 is
+    // the same as 3.140). 'Equals' returns true iff equal and the same scale
     // is set in the decimals (e.g.: 3.14 is not the same as 3.140)
-    return getBigDecimal().compareTo(bdw.getBigDecimal()) == 0;
+    return getHiveDecimal().compareTo(bdw.getHiveDecimal()) == 0;
   }
 
   @Override
   public int hashCode() {
-    return getBigDecimal().hashCode();
+    return getHiveDecimal().hashCode();
   }
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java b/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
index b178f1ad35..097da9c099 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
@@ -29,6 +29,7 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt;
@@ -451,10 +452,10 @@ public static Timestamp floatToTimestamp(float f) {
     return doubleToTimestamp((double) f);
   }
 
-  public static Timestamp decimalToTimestamp(BigDecimal d) {
+  public static Timestamp decimalToTimestamp(HiveDecimal d) {
     BigDecimal seconds = new BigDecimal(d.longValue());
-    long millis = d.multiply(new BigDecimal(1000)).longValue();
-    int nanos = d.subtract(seconds).multiply(new BigDecimal(1000000000)).intValue();
+    long millis = d.bigDecimalValue().multiply(new BigDecimal(1000)).longValue();
+    int nanos = d.bigDecimalValue().subtract(seconds).multiply(new BigDecimal(1000000000)).intValue();
 
     Timestamp t = new Timestamp(millis);
     t.setNanos(nanos);
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
index 2c6251fe87..59b1406cbe 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
@@ -26,7 +26,7 @@
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyUnionObjectInspector;
-import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyByteObjectInspector;
@@ -114,7 +114,7 @@ public final class LazyFactory {
     case BINARY:
       return new LazyBinary((LazyBinaryObjectInspector) oi);
     case DECIMAL:
-      return new LazyBigDecimal((LazyBigDecimalObjectInspector) oi);
+      return new LazyHiveDecimal((LazyHiveDecimalObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyObject for " + p);
     }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyBigDecimal.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
similarity index 64%
rename from serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyBigDecimal.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
index 719e78c9b3..08f251c533 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyBigDecimal.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
@@ -17,30 +17,30 @@
  */
 package org.apache.hadoop.hive.serde2.lazy;
 
-import java.math.BigDecimal;
 import java.nio.charset.CharacterCodingException;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
-import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyBigDecimalObjectInspector;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveDecimalObjectInspector;
 import org.apache.hadoop.io.Text;
 
-public class LazyBigDecimal extends LazyPrimitive<LazyBigDecimalObjectInspector, BigDecimalWritable> {
-  static final private Log LOG = LogFactory.getLog(LazyBigDecimal.class);
+public class LazyHiveDecimal extends LazyPrimitive<LazyHiveDecimalObjectInspector, HiveDecimalWritable> {
+  static final private Log LOG = LogFactory.getLog(LazyHiveDecimal.class);
 
-  public LazyBigDecimal(LazyBigDecimalObjectInspector oi) {
+  public LazyHiveDecimal(LazyHiveDecimalObjectInspector oi) {
     super(oi);
-    data = new BigDecimalWritable();
+    data = new HiveDecimalWritable();
   }
 
-  public LazyBigDecimal(LazyBigDecimal copy) {
+  public LazyHiveDecimal(LazyHiveDecimal copy) {
     super(copy);
-    data = new BigDecimalWritable(copy.data);
+    data = new HiveDecimalWritable(copy.data);
   }
 
   /**
-   * Initilizes LazyBigDecimal object by interpreting the input bytes
+   * Initilizes LazyHiveDecimal object by interpreting the input bytes
    * as a numeric string
    *
    * @param bytes
@@ -52,20 +52,20 @@ public void init(ByteArrayRef bytes, int start, int length) {
     String byteData = null;
     try {
       byteData = Text.decode(bytes.getData(), start, length);
-      data.set(new BigDecimal(byteData));
+      data.set(new HiveDecimal(byteData));
       isNull = false;
     } catch (NumberFormatException e) {
       isNull = true;
-      LOG.debug("Data not in the BigDecimal data type range so converted to null. Given data is :"
+      LOG.debug("Data not in the HiveDecimal data type range so converted to null. Given data is :"
           + byteData, e);
     } catch (CharacterCodingException e) {
       isNull = true;
-      LOG.debug("Data not in the BigDecimal data type range so converted to null.", e);
+      LOG.debug("Data not in the HiveDecimal data type range so converted to null.", e);
     }
   }
 
   @Override
-  public BigDecimalWritable getWritableObject() {
+  public HiveDecimalWritable getWritableObject() {
     return data;
   }
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
index b93709eb36..27ed4ef6fd 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyUtils.java
@@ -21,7 +21,6 @@
 import java.io.DataOutputStream;
 import java.io.IOException;
 import java.io.OutputStream;
-import java.math.BigDecimal;
 import java.nio.ByteBuffer;
 import java.nio.charset.CharacterCodingException;
 import java.util.ArrayList;
@@ -29,11 +28,12 @@
 import java.util.Properties;
 
 import org.apache.commons.codec.binary.Base64;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.SerDeParameters;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
@@ -238,7 +238,7 @@ public static void writePrimitiveUTF8(OutputStream out, Object o,
       break;
     }
     case DECIMAL: {
-      BigDecimal bd = ((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o);
+      HiveDecimal bd = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);
       ByteBuffer b = Text.encode(bd.toString());
       out.write(b.array(), 0, b.limit());
       break;
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBigDecimalObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveDecimalObjectInspector.java
similarity index 61%
rename from serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBigDecimalObjectInspector.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveDecimalObjectInspector.java
index b1e8bc33f3..564a1aa420 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyBigDecimalObjectInspector.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveDecimalObjectInspector.java
@@ -17,29 +17,29 @@
  */
 package org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive;
 
-import java.math.BigDecimal;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
-import org.apache.hadoop.hive.serde2.lazy.LazyBigDecimal;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;
 
-public class LazyBigDecimalObjectInspector
-    extends AbstractPrimitiveLazyObjectInspector<BigDecimalWritable>
-    implements BigDecimalObjectInspector {
+public class LazyHiveDecimalObjectInspector
+    extends AbstractPrimitiveLazyObjectInspector<HiveDecimalWritable>
+    implements HiveDecimalObjectInspector {
 
-  protected LazyBigDecimalObjectInspector() {
+  protected LazyHiveDecimalObjectInspector() {
     super(PrimitiveObjectInspectorUtils.decimalTypeEntry);
   }
 
   @Override
   public Object copyObject(Object o) {
-    return o == null ? null : new LazyBigDecimal((LazyBigDecimal) o);
+    return o == null ? null : new LazyHiveDecimal((LazyHiveDecimal) o);
   }
 
   @Override
-  public BigDecimal getPrimitiveJavaObject(Object o) {
-    return o == null ? null : ((LazyBigDecimal) o).getWritableObject().getBigDecimal();
+  public HiveDecimal getPrimitiveJavaObject(Object o) {
+    return o == null ? null : ((LazyHiveDecimal) o).getWritableObject().getHiveDecimal();
   }
 
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java
index 1e0ad00029..8652a468c3 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyPrimitiveObjectInspectorFactory.java
@@ -57,8 +57,8 @@ public final class LazyPrimitiveObjectInspectorFactory {
       new LazyTimestampObjectInspector();
   public static final LazyBinaryObjectInspector LAZY_BINARY_OBJECT_INSPECTOR =
       new LazyBinaryObjectInspector();
-  public static final LazyBigDecimalObjectInspector LAZY_BIG_DECIMAL_OBJECT_INSPECTOR =
-      new LazyBigDecimalObjectInspector();
+  public static final LazyHiveDecimalObjectInspector LAZY_BIG_DECIMAL_OBJECT_INSPECTOR =
+      new LazyHiveDecimalObjectInspector();
 
   static HashMap<ArrayList<Object>, LazyStringObjectInspector> cachedLazyStringObjectInspector =
       new HashMap<ArrayList<Object>, LazyStringObjectInspector>();
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
index 3111cbc362..3219aa1651 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryFactory.java
@@ -27,7 +27,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableByteObjectInspector;
@@ -77,7 +77,7 @@ public final class LazyBinaryFactory {
     case BINARY:
       return new LazyBinaryBinary((WritableBinaryObjectInspector) oi);
     case DECIMAL:
-      return new LazyBinaryBigDecimal((WritableBigDecimalObjectInspector) oi);
+      return new LazyBinaryHiveDecimal((WritableHiveDecimalObjectInspector) oi);
     default:
       throw new RuntimeException("Internal error: no LazyBinaryObject for " + p);
     }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryBigDecimal.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryHiveDecimal.java
similarity index 73%
rename from serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryBigDecimal.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryHiveDecimal.java
index 5d8a6a1203..a1d0e4c2dc 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryBigDecimal.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryHiveDecimal.java
@@ -17,21 +17,21 @@
  */
 package org.apache.hadoop.hive.serde2.lazybinary;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveDecimalObjectInspector;
 
-public class LazyBinaryBigDecimal extends
-    LazyBinaryPrimitive<WritableBigDecimalObjectInspector, BigDecimalWritable> {
+public class LazyBinaryHiveDecimal extends
+    LazyBinaryPrimitive<WritableHiveDecimalObjectInspector, HiveDecimalWritable> {
 
-  LazyBinaryBigDecimal(WritableBigDecimalObjectInspector oi) {
+  LazyBinaryHiveDecimal(WritableHiveDecimalObjectInspector oi) {
     super(oi);
-    data = new BigDecimalWritable();
+    data = new HiveDecimalWritable();
   }
 
-  LazyBinaryBigDecimal(LazyBinaryBigDecimal copy) {
+  LazyBinaryHiveDecimal(LazyBinaryHiveDecimal copy) {
     super(copy);
-    data = new BigDecimalWritable(copy.data);
+    data = new HiveDecimalWritable(copy.data);
   }
 
   @Override
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
index f34bbe1d58..e39c709ccb 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java
@@ -33,7 +33,7 @@
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.ByteArrayRef;
 import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;
@@ -43,7 +43,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.StructField;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
@@ -386,8 +386,8 @@ public static boolean serialize(Output byteStream, Object obj,
       }
 
       case DECIMAL: {
-        BigDecimalObjectInspector bdoi = (BigDecimalObjectInspector) poi;
-        BigDecimalWritable t = bdoi.getPrimitiveWritableObject(obj);
+        HiveDecimalObjectInspector bdoi = (HiveDecimalObjectInspector) poi;
+        HiveDecimalWritable t = bdoi.getPrimitiveWritableObject(obj);
         t.writeToByteStream(byteStream);
         return warnedOnceNullMapKey;
       }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
index acfcff5580..0b1ccd89ad 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
@@ -25,7 +25,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaStringObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableHiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableBooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.SettableByteObjectInspector;
@@ -109,9 +109,9 @@ private static Converter getConverter(PrimitiveObjectInspector inputOI,
           inputOI,
           (SettableBinaryObjectInspector)outputOI);
     case DECIMAL:
-      return new PrimitiveObjectInspectorConverter.BigDecimalConverter(
+      return new PrimitiveObjectInspectorConverter.HiveDecimalConverter(
           (PrimitiveObjectInspector) inputOI,
-          (SettableBigDecimalObjectInspector) outputOI);
+          (SettableHiveDecimalObjectInspector) outputOI);
     default:
       throw new RuntimeException("Hive internal error: conversion of "
           + inputOI.getTypeName() + " to " + outputOI.getTypeName()
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
index 67132fca49..2570eb8ec1 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
@@ -30,12 +30,12 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.ObjectInspectorOptions;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveWritableObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.primitive.BigDecimalObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.primitive.HiveDecimalObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.ByteObjectInspector;
@@ -493,7 +493,7 @@ public static int hashCode(Object o, ObjectInspector objIns) {
             .getPrimitiveWritableObject(o);
         return t.hashCode();
       case DECIMAL:
-        return ((BigDecimalObjectInspector) poi).getPrimitiveWritableObject(o).hashCode();
+        return ((HiveDecimalObjectInspector) poi).getPrimitiveWritableObject(o).hashCode();
 
       default: {
         throw new RuntimeException("Unknown type: "
@@ -680,9 +680,9 @@ public static int compare(Object o1, ObjectInspector oi1, Object o2,
         return t1.compareTo(t2);
       }
       case DECIMAL: {
-        BigDecimalWritable t1 = ((BigDecimalObjectInspector) poi1)
+        HiveDecimalWritable t1 = ((HiveDecimalObjectInspector) poi1)
             .getPrimitiveWritableObject(o1);
-        BigDecimalWritable t2 = ((BigDecimalObjectInspector) poi2)
+        HiveDecimalWritable t2 = ((HiveDecimalObjectInspector) poi2)
             .getPrimitiveWritableObject(o2);
         return t1.compareTo(t2);
       }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/BigDecimalObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveDecimalObjectInspector.java
similarity index 71%
rename from serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/BigDecimalObjectInspector.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveDecimalObjectInspector.java
index 44db243ae0..51d33fa6c2 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/BigDecimalObjectInspector.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/HiveDecimalObjectInspector.java
@@ -17,17 +17,17 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import java.math.BigDecimal;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
 
 /**
- * A DecimalObjectInspector inspects an Object representing a BigDecimal.
+ * A DecimalObjectInspector inspects an Object representing a HiveDecimal.
  */
-public interface BigDecimalObjectInspector extends PrimitiveObjectInspector {
+public interface HiveDecimalObjectInspector extends PrimitiveObjectInspector {
 
-  BigDecimalWritable getPrimitiveWritableObject(Object o);
+  HiveDecimalWritable getPrimitiveWritableObject(Object o);
 
-  BigDecimal getPrimitiveJavaObject(Object o);
+  HiveDecimal getPrimitiveJavaObject(Object o);
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBigDecimalObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveDecimalObjectInspector.java
similarity index 54%
rename from serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBigDecimalObjectInspector.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveDecimalObjectInspector.java
index 5070a0a880..d330c5eee0 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaBigDecimalObjectInspector.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/JavaHiveDecimalObjectInspector.java
@@ -17,59 +17,71 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import java.math.BigDecimal;
 import java.math.BigInteger;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 
-public class JavaBigDecimalObjectInspector
+public class JavaHiveDecimalObjectInspector
     extends AbstractPrimitiveJavaObjectInspector
-    implements SettableBigDecimalObjectInspector {
+    implements SettableHiveDecimalObjectInspector {
 
-  protected JavaBigDecimalObjectInspector() {
+  protected JavaHiveDecimalObjectInspector() {
     super(PrimitiveObjectInspectorUtils.decimalTypeEntry);
   }
 
   @Override
-  public BigDecimalWritable getPrimitiveWritableObject(Object o) {
+  public HiveDecimalWritable getPrimitiveWritableObject(Object o) {
     if (o == null) {
       return null;
     }
 
     if (o instanceof String) {
-      o = new BigDecimal((String)o);
+      try {
+        o = new HiveDecimal((String)o);
+      } catch(NumberFormatException e) {
+        return null;
+      }
     }
-    return new BigDecimalWritable((BigDecimal) o);
+    return new HiveDecimalWritable((HiveDecimal) o);
   }
 
   @Override
-  public BigDecimal getPrimitiveJavaObject(Object o) {
-    return o == null ? null : (BigDecimal) o;
+  public HiveDecimal getPrimitiveJavaObject(Object o) {
+    return o == null ? null : (HiveDecimal) o;
   }
 
   @Override
   public Object set(Object o, byte[] bytes, int scale) {
-    return new BigDecimal(new BigInteger(bytes), scale);
+    return new HiveDecimal(new BigInteger(bytes), scale);
   }
 
   @Override
-  public Object set(Object o, BigDecimal t) {
+  public Object set(Object o, HiveDecimal t) {
     return t;
   }
 
   @Override
-  public Object set(Object o, BigDecimalWritable t) {
-    return t == null ? null : t.getBigDecimal();
+  public Object set(Object o, HiveDecimalWritable t) {
+    return t == null ? null : t.getHiveDecimal();
   }
 
   @Override
   public Object create(byte[] bytes, int scale) {
-    return new BigDecimal(new BigInteger(bytes), scale);
+    try {
+      return new HiveDecimal(new BigInteger(bytes), scale);
+    } catch (NumberFormatException e) {
+      return null;
+    }
   }
 
   @Override
-  public Object create(BigDecimal t) {
-    return t == null ? null : new BigDecimal(t.unscaledValue(), t.scale());
+  public Object create(HiveDecimal t) {
+    try {
+      return t == null ? null : new HiveDecimal(t.unscaledValue(), t.scale());
+    } catch(NumberFormatException e) {
+      return null;
+    }
   }
 
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java
index 1c4c8bd30b..16afd720ca 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorConverter.java
@@ -18,9 +18,9 @@
 
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import java.math.BigDecimal;
 import java.sql.Timestamp;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.ByteStream;
 import org.apache.hadoop.hive.serde2.lazy.LazyInteger;
 import org.apache.hadoop.hive.serde2.lazy.LazyLong;
@@ -258,17 +258,17 @@ public Object convert(Object input) {
     }
   }
 
-  public static class BigDecimalConverter implements Converter {
+  public static class HiveDecimalConverter implements Converter {
 
     PrimitiveObjectInspector inputOI;
-    SettableBigDecimalObjectInspector outputOI;
+    SettableHiveDecimalObjectInspector outputOI;
     Object r;
 
-    public BigDecimalConverter(PrimitiveObjectInspector inputOI,
-        SettableBigDecimalObjectInspector outputOI) {
+    public HiveDecimalConverter(PrimitiveObjectInspector inputOI,
+        SettableHiveDecimalObjectInspector outputOI) {
       this.inputOI = inputOI;
       this.outputOI = outputOI;
-      this.r = outputOI.create(BigDecimal.ZERO);
+      this.r = outputOI.create(HiveDecimal.ZERO);
     }
 
     @Override
@@ -276,7 +276,7 @@ public Object convert(Object input) {
       if (input == null) {
         return null;
       }
-      return outputOI.set(r, PrimitiveObjectInspectorUtils.getBigDecimal(input,
+      return outputOI.set(r, PrimitiveObjectInspectorUtils.getHiveDecimal(input,
           inputOI));
     }
 
@@ -372,7 +372,7 @@ public Text convert(Object input) {
         t.set(((BinaryObjectInspector) inputOI).getPrimitiveWritableObject(input).getBytes());
         return t;
       case DECIMAL:
-        t.set(((BigDecimalObjectInspector) inputOI).getPrimitiveWritableObject(input).toString());
+        t.set(((HiveDecimalObjectInspector) inputOI).getPrimitiveWritableObject(input).toString());
         return t;
       default:
         throw new RuntimeException("Hive 2 Internal error: type = " + inputOI.getTypeName());
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorFactory.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorFactory.java
index a39934c917..5234a88705 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorFactory.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorFactory.java
@@ -20,7 +20,7 @@
 
 import java.util.HashMap;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -70,8 +70,8 @@ public final class PrimitiveObjectInspectorFactory {
       new JavaTimestampObjectInspector();
   public static final JavaBinaryObjectInspector javaByteArrayObjectInspector =
       new JavaBinaryObjectInspector();
-  public static final JavaBigDecimalObjectInspector javaBigDecimalObjectInspector =
-      new JavaBigDecimalObjectInspector();
+  public static final JavaHiveDecimalObjectInspector javaHiveDecimalObjectInspector =
+      new JavaHiveDecimalObjectInspector();
 
   public static final WritableBooleanObjectInspector writableBooleanObjectInspector =
       new WritableBooleanObjectInspector();
@@ -95,8 +95,8 @@ public final class PrimitiveObjectInspectorFactory {
       new WritableTimestampObjectInspector();
   public static final WritableBinaryObjectInspector writableBinaryObjectInspector =
       new WritableBinaryObjectInspector();
-  public static final WritableBigDecimalObjectInspector writableBigDecimalObjectInspector =
-      new WritableBigDecimalObjectInspector();
+  public static final WritableHiveDecimalObjectInspector writableHiveDecimalObjectInspector =
+      new WritableHiveDecimalObjectInspector();
 
   private static HashMap<PrimitiveCategory, AbstractPrimitiveWritableObjectInspector> cachedPrimitiveWritableInspectorCache =
       new HashMap<PrimitiveCategory, AbstractPrimitiveWritableObjectInspector>();
@@ -124,7 +124,7 @@ public final class PrimitiveObjectInspectorFactory {
     cachedPrimitiveWritableInspectorCache.put(PrimitiveCategory.BINARY,
         writableBinaryObjectInspector);
     cachedPrimitiveWritableInspectorCache.put(PrimitiveCategory.DECIMAL,
-        writableBigDecimalObjectInspector);
+        writableHiveDecimalObjectInspector);
   }
 
   private static HashMap<PrimitiveCategory, AbstractPrimitiveJavaObjectInspector> cachedPrimitiveJavaInspectorCache =
@@ -153,7 +153,7 @@ public final class PrimitiveObjectInspectorFactory {
     cachedPrimitiveJavaInspectorCache.put(PrimitiveCategory.BINARY,
         javaByteArrayObjectInspector);
     cachedPrimitiveJavaInspectorCache.put(PrimitiveCategory.DECIMAL,
-        javaBigDecimalObjectInspector);
+        javaHiveDecimalObjectInspector);
   }
 
   /**
@@ -201,7 +201,7 @@ public static ConstantObjectInspector getPrimitiveWritableConstantObjectInspecto
     case TIMESTAMP:
       return new WritableConstantTimestampObjectInspector((TimestampWritable)value);
     case DECIMAL:
-      return new WritableConstantBigDecimalObjectInspector((BigDecimalWritable)value);
+      return new WritableConstantHiveDecimalObjectInspector((HiveDecimalWritable)value);
     case BINARY:
       return new WritableConstantBinaryObjectInspector((BytesWritable)value);
     case VOID:
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
index df6479d6c3..e7d890f262 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
@@ -21,15 +21,15 @@
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
-import java.math.BigDecimal;
 import java.sql.Timestamp;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.lazy.LazyInteger;
@@ -183,7 +183,7 @@ static void registerType(PrimitiveTypeEntry t) {
       Timestamp.class, TimestampWritable.class);
   public static final PrimitiveTypeEntry decimalTypeEntry = new PrimitiveTypeEntry(
       PrimitiveCategory.DECIMAL, serdeConstants.DECIMAL_TYPE_NAME, null,
-      BigDecimal.class, BigDecimalWritable.class);
+      HiveDecimal.class, HiveDecimalWritable.class);
 
   // The following is a complex type for special handling
   public static final PrimitiveTypeEntry unknownTypeEntry = new PrimitiveTypeEntry(
@@ -370,8 +370,8 @@ public static boolean comparePrimitiveObjects(Object o1,
           equals(((BinaryObjectInspector) oi2).getPrimitiveWritableObject(o2));
     }
     case DECIMAL: {
-      return ((BigDecimalObjectInspector) oi1).getPrimitiveJavaObject(o1)
-          .compareTo(((BigDecimalObjectInspector) oi2).getPrimitiveJavaObject(o2)) == 0;
+      return ((HiveDecimalObjectInspector) oi1).getPrimitiveJavaObject(o1)
+          .compareTo(((HiveDecimalObjectInspector) oi2).getPrimitiveJavaObject(o2)) == 0;
     }
     default:
       return false;
@@ -403,7 +403,7 @@ public static double convertPrimitiveToDouble(Object o, PrimitiveObjectInspector
       return ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)
           .getDouble();
     case DECIMAL:
-      return ((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o).doubleValue();
+      return ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o).doubleValue();
     default:
       throw new NumberFormatException();
     }
@@ -479,8 +479,8 @@ public static boolean getBoolean(Object o, PrimitiveObjectInspector oi) {
           .getPrimitiveWritableObject(o).getSeconds() != 0);
       break;
     case DECIMAL:
-      result = BigDecimal.ZERO.compareTo(
-          ((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o)) != 0;
+      result = HiveDecimal.ZERO.compareTo(
+          ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o)) != 0;
       break;
     default:
       throw new RuntimeException("Hive 2 Internal error: unknown type: "
@@ -563,7 +563,7 @@ public static int getInt(Object o, PrimitiveObjectInspector oi) {
           .getPrimitiveWritableObject(o).getSeconds());
       break;
     case DECIMAL:
-      result = ((BigDecimalObjectInspector) oi)
+      result = ((HiveDecimalObjectInspector) oi)
           .getPrimitiveJavaObject(o).intValue();
       break;
     default: {
@@ -621,7 +621,7 @@ public static long getLong(Object o, PrimitiveObjectInspector oi) {
           .getSeconds();
       break;
     case DECIMAL:
-      result = ((BigDecimalObjectInspector) oi)
+      result = ((HiveDecimalObjectInspector) oi)
           .getPrimitiveJavaObject(o).longValue();
       break;
     default:
@@ -672,7 +672,7 @@ public static double getDouble(Object o, PrimitiveObjectInspector oi) {
       result = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o).getDouble();
       break;
     case DECIMAL:
-      result = ((BigDecimalObjectInspector) oi)
+      result = ((HiveDecimalObjectInspector) oi)
           .getPrimitiveJavaObject(o).doubleValue();
       break;
     default:
@@ -736,7 +736,7 @@ public static String getString(Object o, PrimitiveObjectInspector oi) {
       result = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o).toString();
       break;
     case DECIMAL:
-      result = ((BigDecimalObjectInspector) oi)
+      result = ((HiveDecimalObjectInspector) oi)
           .getPrimitiveJavaObject(o).toString();
       break;
     default:
@@ -772,54 +772,58 @@ public static BytesWritable getBinary(Object o, PrimitiveObjectInspector oi) {
     }
   }
 
-  public static BigDecimal getBigDecimal(Object o, PrimitiveObjectInspector oi) {
+  public static HiveDecimal getHiveDecimal(Object o, PrimitiveObjectInspector oi) {
     if (o == null) {
       return null;
     }
 
-    BigDecimal result = null;
-    switch (oi.getPrimitiveCategory()) {
-    case VOID:
-      result = null;
-      break;
-    case BOOLEAN:
-      result = ((BooleanObjectInspector) oi).get(o) ?
-          BigDecimal.ONE : BigDecimal.ZERO;
-      break;
-    case BYTE:
-      result = new BigDecimal(((ByteObjectInspector) oi).get(o));
-      break;
-    case SHORT:
-      result = new BigDecimal(((ShortObjectInspector) oi).get(o));
-      break;
-    case INT:
-      result = new BigDecimal(((IntObjectInspector) oi).get(o));
-      break;
-    case LONG:
-      result = new BigDecimal(((LongObjectInspector) oi).get(o));
-      break;
-    case FLOAT:
-      Float f = ((FloatObjectInspector) oi).get(o);
-      result = new BigDecimal(f.toString());
-      break;
-    case DOUBLE:
-      Double d = ((DoubleObjectInspector) oi).get(o);
-      result = new BigDecimal(d.toString());
-      break;
-    case STRING:
-      result = new BigDecimal(((StringObjectInspector) oi).getPrimitiveJavaObject(o));
-      break;
-    case TIMESTAMP:
-      Double ts = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)
-          .getDouble();
-      result = new BigDecimal(ts.toString());
-      break;
-    case DECIMAL:
-      result = ((BigDecimalObjectInspector) oi).getPrimitiveJavaObject(o);
-      break;
-    default:
-      throw new RuntimeException("Hive 2 Internal error: unknown type: "
-          + oi.getTypeName());
+    HiveDecimal result = null;
+    try {
+      switch (oi.getPrimitiveCategory()) {
+      case VOID:
+        result = null;
+        break;
+      case BOOLEAN:
+        result = ((BooleanObjectInspector) oi).get(o) ?
+            HiveDecimal.ONE : HiveDecimal.ZERO;
+        break;
+      case BYTE:
+        result = new HiveDecimal(((ByteObjectInspector) oi).get(o));
+        break;
+      case SHORT:
+        result = new HiveDecimal(((ShortObjectInspector) oi).get(o));
+        break;
+      case INT:
+        result = new HiveDecimal(((IntObjectInspector) oi).get(o));
+        break;
+      case LONG:
+        result = new HiveDecimal(((LongObjectInspector) oi).get(o));
+        break;
+      case FLOAT:
+        Float f = ((FloatObjectInspector) oi).get(o);
+        result = new HiveDecimal(f.toString());
+        break;
+      case DOUBLE:
+        Double d = ((DoubleObjectInspector) oi).get(o);
+        result = new HiveDecimal(d.toString());
+        break;
+      case STRING:
+        result = new HiveDecimal(((StringObjectInspector) oi).getPrimitiveJavaObject(o));
+        break;
+      case TIMESTAMP:
+        Double ts = ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o)
+        .getDouble();
+        result = new HiveDecimal(ts.toString());
+        break;
+      case DECIMAL:
+        result = ((HiveDecimalObjectInspector) oi).getPrimitiveJavaObject(o);
+        break;
+      default:
+        throw new RuntimeException("Hive 2 Internal error: unknown type: "
+            + oi.getTypeName());
+      }
+    } catch(NumberFormatException e) {
+      // return null
     }
     return result;
   }
@@ -856,7 +860,7 @@ public static Timestamp getTimestamp(Object o, PrimitiveObjectInspector oi) {
       result = TimestampWritable.doubleToTimestamp(((DoubleObjectInspector) oi).get(o));
       break;
     case DECIMAL:
-      result = TimestampWritable.decimalToTimestamp(((BigDecimalObjectInspector) oi)
+      result = TimestampWritable.decimalToTimestamp(((HiveDecimalObjectInspector) oi)
                                                     .getPrimitiveJavaObject(o));
       break;
     case STRING:
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableBigDecimalObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveDecimalObjectInspector.java
similarity index 70%
rename from serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableBigDecimalObjectInspector.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveDecimalObjectInspector.java
index ff262b29b8..1e4b3b4d3e 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableBigDecimalObjectInspector.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/SettableHiveDecimalObjectInspector.java
@@ -17,23 +17,23 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import java.math.BigDecimal;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 
 /**
- * A SettableDecimalObjectInspector can set a BigDecimal value to an object.
+ * A SettableDecimalObjectInspector can set a HiveDecimal value to an object.
  */
-public interface SettableBigDecimalObjectInspector extends BigDecimalObjectInspector {
+public interface SettableHiveDecimalObjectInspector extends HiveDecimalObjectInspector {
 
   Object set(Object o, byte[] bytes, int scale);
 
-  Object set(Object o, BigDecimal t);
+  Object set(Object o, HiveDecimal t);
 
-  Object set(Object o, BigDecimalWritable t);
+  Object set(Object o, HiveDecimalWritable t);
 
   Object create(byte[] bytes, int scale);
 
-  Object create (BigDecimal t);
+  Object create (HiveDecimal t);
 
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantBigDecimalObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java
similarity index 72%
rename from serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantBigDecimalObjectInspector.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java
index 672b1061fb..c0413e2d21 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantBigDecimalObjectInspector.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java
@@ -17,24 +17,24 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;
 
 /**
- * A WritableConstantBigDecimalObjectInspector is a WritableBigDecimalObjectInspector
+ * A WritableConstantHiveDecimalObjectInspector is a WritableHiveDecimalObjectInspector
  * that implements ConstantObjectInspector.
  */
-public class WritableConstantBigDecimalObjectInspector extends WritableBigDecimalObjectInspector
+public class WritableConstantHiveDecimalObjectInspector extends WritableHiveDecimalObjectInspector
     implements ConstantObjectInspector {
 
-  private final BigDecimalWritable value;
+  private final HiveDecimalWritable value;
 
-  WritableConstantBigDecimalObjectInspector(BigDecimalWritable value) {
+  WritableConstantHiveDecimalObjectInspector(HiveDecimalWritable value) {
     this.value = value;
   }
 
   @Override
-  public BigDecimalWritable getWritableConstantValue() {
+  public HiveDecimalWritable getWritableConstantValue() {
     return value;
   }
 }
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBigDecimalObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveDecimalObjectInspector.java
similarity index 57%
rename from serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBigDecimalObjectInspector.java
rename to serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveDecimalObjectInspector.java
index 88184cf1d2..8c9aedd6ed 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableBigDecimalObjectInspector.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableHiveDecimalObjectInspector.java
@@ -17,59 +17,59 @@
  */
 package org.apache.hadoop.hive.serde2.objectinspector.primitive;
 
-import java.math.BigDecimal;
 
-import org.apache.hadoop.hive.serde2.io.BigDecimalWritable;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 
-public class WritableBigDecimalObjectInspector
+public class WritableHiveDecimalObjectInspector
     extends AbstractPrimitiveWritableObjectInspector
-    implements SettableBigDecimalObjectInspector {
+    implements SettableHiveDecimalObjectInspector {
 
-  protected WritableBigDecimalObjectInspector() {
+  protected WritableHiveDecimalObjectInspector() {
     super(PrimitiveObjectInspectorUtils.decimalTypeEntry);
   }
 
   @Override
-  public BigDecimalWritable getPrimitiveWritableObject(Object o) {
-    return o == null ? null : (BigDecimalWritable) o;
+  public HiveDecimalWritable getPrimitiveWritableObject(Object o) {
+    return o == null ? null : (HiveDecimalWritable) o;
   }
 
   @Override
-  public BigDecimal getPrimitiveJavaObject(Object o) {
-    return o == null ? null : ((BigDecimalWritable) o).getBigDecimal();
+  public HiveDecimal getPrimitiveJavaObject(Object o) {
+    return o == null ? null : ((HiveDecimalWritable) o).getHiveDecimal();
   }
 
   @Override
   public Object copyObject(Object o) {
-    return o == null ? null : new BigDecimalWritable((BigDecimalWritable) o);
+    return o == null ? null : new HiveDecimalWritable((HiveDecimalWritable) o);
   }
 
   @Override
   public Object set(Object o, byte[] bytes, int scale) {
-    ((BigDecimalWritable) o).set(bytes, scale);
+    ((HiveDecimalWritable) o).set(bytes, scale);
     return o;
   }
 
   @Override
-  public Object set(Object o, BigDecimal t) {
-    ((BigDecimalWritable) o).set(t);
+  public Object set(Object o, HiveDecimal t) {
+    ((HiveDecimalWritable) o).set(t);
     return o;
   }
 
   @Override
-  public Object set(Object o, BigDecimalWritable t) {
-    ((BigDecimalWritable) o).set(t);
+  public Object set(Object o, HiveDecimalWritable t) {
+    ((HiveDecimalWritable) o).set(t);
     return o;
   }
 
   @Override
   public Object create(byte[] bytes, int scale) {
-    return new BigDecimalWritable(bytes, scale);
+    return new HiveDecimalWritable(bytes, scale);
   }
 
   @Override
-  public Object create(BigDecimal t) {
-    return new BigDecimalWritable(t);
+  public Object create(HiveDecimal t) {
+    return new HiveDecimalWritable(t);
   }
 
 }
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java b/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
index 095387fb78..3ba26997fc 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/TestStatsSerde.java
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.serde2;
 
-import java.math.BigDecimal;
 import java.math.BigInteger;
 import java.util.List;
 import java.util.Properties;
@@ -27,6 +26,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestClass;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
@@ -114,7 +114,7 @@ public void testLazyBinarySerDe() throws Throwable {
         Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
         String st = randField > 6 ? null : TestBinarySortableSerDe
             .getRandString(r);
-	BigDecimal bd = randField > 8 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);
+	HiveDecimal bd = randField > 8 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);
         MyTestInnerStruct is = randField > 9 ? null : new MyTestInnerStruct(r
             .nextInt(5) - 2, r.nextInt(5) - 2);
         List<Integer> li = randField > 10 ? null : TestBinarySortableSerDe
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/MyTestClass.java b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/MyTestClass.java
index bf7b265eda..0f364fdb9d 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/MyTestClass.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/MyTestClass.java
@@ -17,7 +17,8 @@
  */
 package org.apache.hadoop.hive.serde2.binarysortable;
 
-import java.math.BigDecimal;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+
 import java.util.List;
 
 public class MyTestClass {
@@ -28,7 +29,7 @@ public class MyTestClass {
     Float myFloat;
     Double myDouble;
     String myString;
-    BigDecimal myDecimal;
+    HiveDecimal myDecimal;
     MyTestInnerStruct myStruct;
     List<Integer> myList;
     byte[] myBA;
@@ -37,7 +38,7 @@ public MyTestClass() {
     }
 
     public MyTestClass(Byte b, Short s, Integer i, Long l, Float f, Double d,
-		       String st, BigDecimal bd, MyTestInnerStruct is, List<Integer> li, byte[] ba) {
+		       String st, HiveDecimal bd, MyTestInnerStruct is, List<Integer> li, byte[] ba) {
 	myByte = b;
 	myShort = s;
 	myInt = i;
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
index a6af537d70..229cdff526 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableSerDe.java
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.hive.serde2.binarysortable;
 
-import java.math.BigDecimal;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
@@ -27,6 +26,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
@@ -135,9 +135,9 @@ private void sort(Object[] structs, ObjectInspector oi) {
     }
   }
 
-  public static BigDecimal getRandBigDecimal(Random r) {
+  public static HiveDecimal getRandHiveDecimal(Random r) {
     StringBuilder sb = new StringBuilder();
-    int l1 = 1+r.nextInt(500), l2 = r.nextInt(500);
+    int l1 = 1+r.nextInt(18), l2 = r.nextInt(19);
 
     if (r.nextBoolean()) {
       sb.append("-");
@@ -149,7 +149,7 @@ public static BigDecimal getRandBigDecimal(Random r) {
       sb.append(getRandString(r, DECIMAL_CHARS, l2));
     }
 
-    BigDecimal bd = new BigDecimal(sb.toString());
+    HiveDecimal bd = new HiveDecimal(sb.toString());
     return bd;
   }
 
@@ -207,7 +207,7 @@ public void testBinarySortableSerDe() throws Throwable {
         t.myDouble = randField > 5 ? null : Double
             .valueOf(r.nextDouble() * 10 - 5);
         t.myString = randField > 6 ? null : getRandString(r);
-        t.myDecimal = randField > 7 ? null : getRandBigDecimal(r);
+        t.myDecimal = randField > 7 ? null : getRandHiveDecimal(r);
         t.myStruct = randField > 8 ? null : new MyTestInnerStruct(
             r.nextInt(5) - 2, r.nextInt(5) - 2);
         t.myList = randField > 9 ? null : getRandIntegerArray(r);
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassBigger.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassBigger.java
index 75a2b57341..8fa3c03e40 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassBigger.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassBigger.java
@@ -17,10 +17,10 @@
  */
 package org.apache.hadoop.hive.serde2.lazybinary;
 
-import java.math.BigDecimal;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
 
 /**
@@ -35,7 +35,7 @@ public class MyTestClassBigger {
     Float myFloat;
     Double myDouble;
     String myString;
-    BigDecimal myDecimal;
+    HiveDecimal myDecimal;
     MyTestInnerStruct myStruct;
     List<Integer> myList;
     byte[] myBA;
@@ -45,7 +45,7 @@ public MyTestClassBigger() {
     }
 
     public MyTestClassBigger(Byte b, Short s, Integer i, Long l, Float f,
-			     Double d, String st, BigDecimal bd, MyTestInnerStruct is, List<Integer> li,
+			     Double d, String st, HiveDecimal bd, MyTestInnerStruct is, List<Integer> li,
 			     byte[] ba, Map<String, List<MyTestInnerStruct>> mp) {
 	myByte = b;
 	myShort = s;
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassSmaller.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassSmaller.java
index f692d9921d..52e6145b12 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassSmaller.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/MyTestClassSmaller.java
@@ -17,7 +17,7 @@
  */
 package org.apache.hadoop.hive.serde2.lazybinary;
 
-import java.math.BigDecimal;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct;
 
 public class MyTestClassSmaller {
@@ -28,14 +28,14 @@ public class MyTestClassSmaller {
     Float myFloat;
     Double myDouble;
     String myString;
-    BigDecimal myDecimal;
+    HiveDecimal myDecimal;
     MyTestInnerStruct myStruct;
 
     public MyTestClassSmaller() {
     }
     
     public MyTestClassSmaller(Byte b, Short s, Integer i, Long l, Float f,
-			      Double d, String st, BigDecimal bd, MyTestInnerStruct is) {
+			      Double d, String st, HiveDecimal bd, MyTestInnerStruct is) {
 	myByte = b;
 	myShort = s;
 	myInt = i;
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
index 030bc09ef2..b4f414e994 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinarySerDe.java
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.hive.serde2.lazybinary;
 
-import java.math.BigDecimal;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -30,6 +29,7 @@
 import junit.framework.TestCase;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDe;
 import org.apache.hadoop.hive.serde2.SerDeUtils;
@@ -201,7 +201,7 @@ private void testShorterSchemaDeserialization(Random r) throws Throwable {
       Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
       String st = randField > 6 ? null : TestBinarySortableSerDe
           .getRandString(r);
-      BigDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);
+      HiveDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);
       MyTestInnerStruct is = randField > 7 ? null : new MyTestInnerStruct(r
           .nextInt(5) - 2, r.nextInt(5) - 2);
       List<Integer> li = randField > 8 ? null : TestBinarySortableSerDe
@@ -269,7 +269,7 @@ private void testShorterSchemaDeserialization1(Random r) throws Throwable {
       Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
       String st = randField > 6 ? null : TestBinarySortableSerDe
           .getRandString(r);
-      BigDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);
+      HiveDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);
       MyTestInnerStruct is = randField > 8 ? null : new MyTestInnerStruct(r
           .nextInt(5) - 2, r.nextInt(5) - 2);
       List<Integer> li = randField > 9 ? null : TestBinarySortableSerDe
@@ -325,7 +325,7 @@ void testLongerSchemaDeserialization(Random r) throws Throwable {
       Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
       String st = randField > 6 ? null : TestBinarySortableSerDe
           .getRandString(r);
-      BigDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);
+      HiveDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);
       MyTestInnerStruct is = randField > 8 ? null : new MyTestInnerStruct(r
           .nextInt(5) - 2, r.nextInt(5) - 2);
       List<Integer> li = randField > 9 ? null : TestBinarySortableSerDe
@@ -381,7 +381,7 @@ void testLongerSchemaDeserialization1(Random r) throws Throwable {
       Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
       String st = randField > 6 ? null : TestBinarySortableSerDe
           .getRandString(r);
-      BigDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);
+      HiveDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);
       MyTestInnerStruct is = randField > 7 ? null : new MyTestInnerStruct(r
           .nextInt(5) - 2, r.nextInt(5) - 2);
 
@@ -500,7 +500,7 @@ public void testLazyBinarySerDe() throws Throwable {
         Double d = randField > 5 ? null : Double.valueOf(r.nextDouble());
         String st = randField > 6 ? null : TestBinarySortableSerDe
             .getRandString(r);
-        BigDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandBigDecimal(r);      
+        HiveDecimal bd = randField > 7 ? null : TestBinarySortableSerDe.getRandHiveDecimal(r);      
         MyTestInnerStruct is = randField > 8 ? null : new MyTestInnerStruct(r
             .nextInt(5) - 2, r.nextInt(5) - 2);
         List<Integer> li = randField > 9 ? null : TestBinarySortableSerDe
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java b/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
index 6fda3c2424..a93a66cd08 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
@@ -19,8 +19,8 @@
 
 import junit.framework.TestCase;
 
-import java.math.BigDecimal;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
@@ -129,10 +129,10 @@ public void testObjectInspectorConverters() throws Throwable {
       assertEquals("TextConverter", new Text("hive"), textConverter
 	  .convert(new String("hive")));
       textConverter = ObjectInspectorConverters.getConverter(
-          PrimitiveObjectInspectorFactory.javaBigDecimalObjectInspector,
+          PrimitiveObjectInspectorFactory.javaHiveDecimalObjectInspector,
           PrimitiveObjectInspectorFactory.writableStringObjectInspector);
       assertEquals("TextConverter", new Text("100.001"), textConverter
-	  .convert(new BigDecimal("100.001")));
+	  .convert(new HiveDecimal("100.001")));
 
       // Binary
       Converter baConverter = ObjectInspectorConverters.getConverter(
diff --git a/service/src/java/org/apache/hive/service/cli/ColumnValue.java b/service/src/java/org/apache/hive/service/cli/ColumnValue.java
index c3327d3c5e..56aa67d67b 100644
--- a/service/src/java/org/apache/hive/service/cli/ColumnValue.java
+++ b/service/src/java/org/apache/hive/service/cli/ColumnValue.java
@@ -18,9 +18,9 @@
 
 package org.apache.hive.service.cli;
 
-import java.math.BigDecimal;
 import java.sql.Timestamp;
 
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hive.service.cli.thrift.TBoolValue;
 import org.apache.hive.service.cli.thrift.TByteValue;
 import org.apache.hive.service.cli.thrift.TColumnValue;
@@ -119,10 +119,10 @@ public static ColumnValue timestampValue(Timestamp value) {
     return new ColumnValue(TColumnValue.stringVal(tStringValue));
   }
 
-  public static ColumnValue stringValue(BigDecimal value) {
+  public static ColumnValue stringValue(HiveDecimal value) {
     TStringValue tStrValue = new TStringValue();
     if (value != null) {
-      tStrValue.setValue(((BigDecimal)value).toString());
+      tStrValue.setValue(((HiveDecimal)value).toString());
     }
     return new ColumnValue(TColumnValue.stringVal(tStrValue));
   }
@@ -148,7 +148,7 @@ public static ColumnValue newColumnValue(Type type, Object value) {
     case TIMESTAMP_TYPE:
       return timestampValue((Timestamp)value);
     case DECIMAL_TYPE:
-      return stringValue(((BigDecimal)value));
+      return stringValue(((HiveDecimal)value));
     case BINARY_TYPE:
     case ARRAY_TYPE:
     case MAP_TYPE:
