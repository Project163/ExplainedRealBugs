diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
index 53d913435c..50e022da57 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java
@@ -380,6 +380,7 @@ public void configureTableJobProperties(
     // do this for reconciling HBaseStorageHandler for use in HCatalog
     // check to see if this an input job or an outputjob
     if (this.configureInputJobProps) {
+      LOG.info("Configuring input job properties");
       String snapshotName = HiveConf.getVar(jobConf, HiveConf.ConfVars.HIVE_HBASE_SNAPSHOT_NAME);
       if (snapshotName != null) {
         HBaseTableSnapshotInputFormatUtil.assertSupportsTableSnapshots();
@@ -428,6 +429,7 @@ public void configureTableJobProperties(
       } //input job properties
     }
     else {
+      LOG.info("Configuring output job properties");
       if (isHBaseGenerateHFiles(jobConf)) {
         // only support bulkload when a hfile.family.path has been specified.
         // TODO: support detecting cf's from column mapping
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 182626698d..553113e9aa 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -23,6 +23,7 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.io.Serializable;
+import java.io.StringWriter;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
@@ -30,6 +31,7 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Properties;
 import java.util.Set;
 import java.util.concurrent.Future;
 
@@ -59,6 +61,7 @@
 import org.apache.hadoop.hive.ql.plan.ListBucketingCtx;
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
 import org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionTaskIndependent;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
@@ -339,7 +342,12 @@ protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveExc
       taskId = Utilities.getTaskId(hconf);
       initializeSpecPath();
       fs = specPath.getFileSystem(hconf);
-      hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(hconf, conf.getTableInfo());
+      try {
+        createHiveOutputFormat(hconf);
+      } catch (HiveException ex) {
+        logOutputFormatError(hconf, ex);
+        throw ex;
+      }
       isCompressed = conf.getCompressed();
       parent = Utilities.toTempPath(conf.getDirName());
       statsCollectRawDataSize = conf.isStatsCollectRawDataSize();
@@ -440,6 +448,26 @@ protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveExc
     return result;
   }
 
+  private void logOutputFormatError(Configuration hconf, HiveException ex) {
+    StringWriter errorWriter = new StringWriter();
+    errorWriter.append("Failed to create output format; configuration: ");
+    try {
+      Configuration.dumpConfiguration(hconf, errorWriter);
+    } catch (IOException ex2) {
+      errorWriter.append("{ failed to dump configuration: " + ex2.getMessage() + " }");
+    }
+    Properties tdp = null;
+    if (this.conf.getTableInfo() != null
+        && (tdp = this.conf.getTableInfo().getProperties()) != null) {
+      errorWriter.append(";\n table properties: { ");
+      for (Map.Entry<Object, Object> e : tdp.entrySet()) {
+        errorWriter.append(e.getKey() + ": " + e.getValue() + ", ");
+      }
+      errorWriter.append('}');
+    }
+    LOG.error(errorWriter.toString(), ex);
+  }
+
   /**
    * Initialize list bucketing information
    */
@@ -1082,10 +1110,10 @@ public void augmentPlan() {
 
   public void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException {
     if (hiveOutputFormat == null) {
-      Utilities.copyTableJobPropertiesToConf(conf.getTableInfo(), job);
       try {
-        hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(job, getConf().getTableInfo());
-      } catch (Exception ex) {
+        createHiveOutputFormat(job);
+      } catch (HiveException ex) {
+        logOutputFormatError(job, ex);
         throw new IOException(ex);
       }
     }
@@ -1101,6 +1129,17 @@ public void checkOutputSpecs(FileSystem ignored, JobConf job) throws IOException
     }
   }
 
+  private void createHiveOutputFormat(Configuration hconf) throws HiveException {
+    if (hiveOutputFormat == null) {
+      Utilities.copyTableJobPropertiesToConf(conf.getTableInfo(), hconf);
+    }
+    try {
+      hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(hconf, getConf().getTableInfo());
+    } catch (Throwable t) {
+      throw (t instanceof HiveException) ? (HiveException)t : new HiveException(t);
+    }
+  }
+
   private void publishStats() throws HiveException {
     boolean isStatsReliable = conf.isStatsReliable();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 0cceb7fb75..32fb219d47 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -2398,7 +2398,7 @@ public static int getDefaultNotificationInterval(Configuration hconf) {
    * @param job
    *          configuration which receives configured properties
    */
-  public static void copyTableJobPropertiesToConf(TableDesc tbl, JobConf job) {
+  public static void copyTableJobPropertiesToConf(TableDesc tbl, Configuration job) {
     Properties tblProperties = tbl.getProperties();
     for(String name: tblProperties.stringPropertyNames()) {
       if (job.get(name) == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java
index b76d26e489..cdc0372f56 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java
@@ -74,7 +74,6 @@ public RecordReaderImpl.Index readRowIndex(StripeInformation stripe, OrcProto.St
       // filter and combine the io to read row index and bloom filters for that column together
       if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.ROW_INDEX)) {
         boolean readBloomFilter = false;
-        // TODO#: HERE
         if (sargColumns != null && sargColumns[col] &&
             nextStream.getKind() == OrcProto.Stream.Kind.BLOOM_FILTER) {
           len += nextStream.getLength();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
index 24226bd1fe..e2455be4ed 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
@@ -812,7 +812,7 @@ private static void configureJobPropertiesForStorageHandler(boolean input,
                   tableDesc,
                   jobProperties);
             } catch(AbstractMethodError e) {
-                LOG.debug("configureInputJobProperties not found "+
+                LOG.info("configureInputJobProperties not found "+
                     "using configureTableJobProperties",e);
                 storageHandler.configureTableJobProperties(tableDesc, jobProperties);
             }
@@ -823,7 +823,7 @@ private static void configureJobPropertiesForStorageHandler(boolean input,
                   tableDesc,
                   jobProperties);
             } catch(AbstractMethodError e) {
-                LOG.debug("configureOutputJobProperties not found"+
+                LOG.info("configureOutputJobProperties not found"+
                     "using configureTableJobProperties",e);
                 storageHandler.configureTableJobProperties(tableDesc, jobProperties);
             }
