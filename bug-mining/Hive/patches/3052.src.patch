diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
index 351ef00054..322834e85e 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/InjectableDummyAuthenticator.java
@@ -82,12 +82,9 @@ public Configuration getConf() {
   public void setConf(Configuration config) {
     try {
       hmap = hmapClass.newInstance();
-    } catch (InstantiationException e) {
+    } catch (Exception e) {
       throw new RuntimeException("Whoops, could not create an Authenticator of class " +
-          hmapClass.getName());
-    } catch (IllegalAccessException e) {
-      throw new RuntimeException("Whoops, could not create an Authenticator of class " +
-          hmapClass.getName());
+          hmapClass.getName(), e);
     }
 
     hmap.setConf(config);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
index 2227e6ffe7..3a2a6ee57a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
@@ -242,7 +242,7 @@ public int execute(DriverContext driverContext) {
       job.setPartitionerClass((Class<? extends Partitioner>) (Class.forName(HiveConf.getVar(job,
           HiveConf.ConfVars.HIVEPARTITIONER))));
     } catch (ClassNotFoundException e) {
-      throw new RuntimeException(e.getMessage());
+      throw new RuntimeException(e.getMessage(), e);
     }
 
     if (mWork.getNumMapTasks() != null) {
@@ -288,7 +288,7 @@ public int execute(DriverContext driverContext) {
     try {
       job.setInputFormat((Class<? extends InputFormat>) (Class.forName(inpFormat)));
     } catch (ClassNotFoundException e) {
-      throw new RuntimeException(e.getMessage());
+      throw new RuntimeException(e.getMessage(), e);
     }
 
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java
index 297ce44c06..f7612d6674 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java
@@ -234,7 +234,7 @@ public void onRootVertexInitialized(String inputName, InputDescriptor inputDescr
         try {
           fileSplit = getFileSplitFromEvent(diEvent);
         } catch (IOException e) {
-          throw new RuntimeException("Failed to get file split for event: " + diEvent);
+          throw new RuntimeException("Failed to get file split for event: " + diEvent, e);
         }
         Set<FileSplit> fsList =
             pathFileSplitsMap.get(Utilities.getBucketFileNameFromPathSubString(fileSplit.getPath()
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index d06bdb9921..c9029f2ee4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -578,7 +578,7 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,
         }
       } catch (IOException e) {
         throw new RuntimeException(
-            "Can't make path " + outputPath + " : " + e.getMessage());
+            "Can't make path " + outputPath + " : " + e.getMessage(), e);
       }
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java
index 33788429bb..243a807ffb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java
@@ -20,7 +20,6 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalToStringUnaryUDF;
 
 /**
  * To support vectorized cast of decimal to string.
@@ -49,9 +48,8 @@ protected void func(BytesColumnVector outV, DecimalColumnVector inV, int i) {
     try {
       b = s.getBytes("UTF-8");
     } catch (Exception e) {
-
       // This should never happen. If it does, there is a bug.
-      throw new RuntimeException("Internal error:  unable to convert decimal to string");
+      throw new RuntimeException("Internal error:  unable to convert decimal to string", e);
     }
     assign(outV, i, b, b.length);
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFArgDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFArgDesc.java
index e3c5b7ffef..e113980ffc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFArgDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFArgDesc.java
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.udf;
 
-import java.io.IOException;
 import java.io.Serializable;
 
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
@@ -107,7 +106,7 @@ public DeferredObject getDeferredJavaObject(int row, VectorizedRowBatch b, int a
         o = writers[argPosition].writeValue(cv, row);
         return new GenericUDF.DeferredJavaObject(o);
       } catch (HiveException e) {
-        throw new RuntimeException("Unable to get Java object from VectorizedRowBatch");
+        throw new RuntimeException("Unable to get Java object from VectorizedRowBatch", e);
       }
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java
index c52624c5e1..c62add0097 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java
@@ -185,7 +185,7 @@ public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
         }
       } catch (HiveException e) {
         throw new RuntimeException(
-            "Unable to get metadata for input table split" + split.getPath());
+            "Unable to get metadata for input table split" + split.getPath(), e);
       }
     }
     InputSplit retA[] = newSplits.toArray((new FileSplit[newSplits.size()]));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
index 8f58c65b68..41b5f1c3b0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java
@@ -66,8 +66,8 @@ public void write(final ArrayWritable record) {
         writeGroupFields(record, schema);
       } catch (RuntimeException e) {
         String errorMessage = "Parquet record is malformed: " + e.getMessage();
-        LOG.error(errorMessage);
-        throw new RuntimeException(errorMessage);
+        LOG.error(errorMessage, e);
+        throw new RuntimeException(errorMessage, e);
       }
       recordConsumer.endMessage();
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
index 57a9bcc8d5..0f7e833e63 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
@@ -51,7 +51,6 @@
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
-import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapred.FileInputFormat;
@@ -148,7 +147,7 @@ public int execute(DriverContext driverContext) {
       job.setInputFormat((Class<? extends InputFormat>) (Class
           .forName(inpFormat)));
     } catch (ClassNotFoundException e) {
-      throw new RuntimeException(e.getMessage());
+      throw new RuntimeException(e.getMessage(), e);
     }
 
     job.setOutputKeyClass(NullWritable.class);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java
index 65785fe34b..73c6dcc2eb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java
@@ -39,7 +39,6 @@
 import org.apache.hadoop.hive.ql.plan.MapredWork;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapred.FileInputFormat;
@@ -124,7 +123,7 @@ public int execute(DriverContext driverContext) {
       job.setInputFormat((Class<? extends InputFormat>) (Class
           .forName(inpFormat)));
     } catch (ClassNotFoundException e) {
-      throw new RuntimeException(e.getMessage());
+      throw new RuntimeException(e.getMessage(), e);
     }
 
     Path outputPath = this.work.getOutputDir();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java
index 6d57a8dcb4..1a50a46a30 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/RelOptHiveTable.java
@@ -273,8 +273,8 @@ private void updateColStats(Set<Integer> projIndxLst) {
           }
         } catch (HiveException e) {
           String logMsg = "Collecting stats failed.";
-          LOG.error(logMsg);
-          throw new RuntimeException(logMsg);
+          LOG.error(logMsg, e);
+          throw new RuntimeException(logMsg, e);
         }
       }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/xml/UDFXPathUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/xml/UDFXPathUtil.java
index a5a8d644f6..7fc0ae582e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/xml/UDFXPathUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/xml/UDFXPathUtil.java
@@ -71,7 +71,7 @@ public Object eval(String xml, String path, QName qname) {
     try {
       return expression.evaluate(inputSource, qname);
     } catch (XPathExpressionException e) {
-      throw new RuntimeException ("Invalid expression '" + oldPath + "'");
+      throw new RuntimeException ("Invalid expression '" + oldPath + "'", e);
     }
   }
 
