diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 8e2e90ba5d..0b4dad9a29 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -384,6 +384,9 @@ public static enum ConfVars {
     // whether session is running in silent mode or not
     HIVESESSIONSILENT("hive.session.silent", false),
 
+    // Whether to enable history for this session
+    HIVE_SESSION_HISTORY_ENABLED("hive.session.history.enabled", false),
+
     // query being executed (multiple per session)
     HIVEQUERYSTRING("hive.query.string", ""),
 
diff --git a/conf/hive-default.xml.template b/conf/hive-default.xml.template
index 8a4d89f13c..d02ac3d14e 100644
--- a/conf/hive-default.xml.template
+++ b/conf/hive-default.xml.template
@@ -446,6 +446,12 @@
   <description>Portion of total memory to be used by map-side grup aggregation hash table</description>
 </property>
 
+<property>
+  <name>hive.session.history.enabled</name>
+  <value>false</value>
+  <description>Whether to log hive query, query plan, runtime statistics etc </description>
+</property>
+
 <property>
   <name>hive.map.aggr.hash.min.reduction</name>
   <value>0.5</value>
diff --git a/hbase-handler/src/test/templates/TestHBaseCliDriver.vm b/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
index c59e882a0c..8421e096af 100644
--- a/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
+++ b/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
@@ -26,10 +26,6 @@ import java.util.*;
 
 import org.apache.hadoop.hive.hbase.HBaseQTestUtil;
 import org.apache.hadoop.hive.hbase.HBaseTestSetup;
-import org.apache.hadoop.hive.ql.history.HiveHistoryViewer;
-import org.apache.hadoop.hive.ql.history.HiveHistory.QueryInfo;
-import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
-import org.apache.hadoop.hive.ql.history.HiveHistory.TaskInfo;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 public class $className extends TestCase {
@@ -114,21 +110,6 @@ public class $className extends TestCase {
       if (ecode != 0) {
         fail("Client Execution failed with error code = " + ecode);
       }
-      if (SessionState.get() != null) {
-        HiveHistoryViewer hv = new HiveHistoryViewer(SessionState.get()
-          .getHiveHistory().getHistFileName());
-        Map<String, QueryInfo> jobInfoMap = hv.getJobInfoMap();
-        Map<String, TaskInfo> taskInfoMap = hv.getTaskInfoMap();
-
-        if (jobInfoMap.size() != 0) {
-          String cmd = (String)jobInfoMap.keySet().toArray()[0];
-          QueryInfo ji = jobInfoMap.get(cmd);
-
-          if (!ji.hm.get(Keys.QUERY_RET_CODE.name()).equals("0")) {
-              fail("Wrong return code in hive history");
-          }
-        }
-      }
 
       ecode = qt.checkCliDriverResults(fname);
       if (ecode != 0) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java
index 97436c5bcd..7b0d978aa3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java
@@ -18,55 +18,21 @@
 
 package org.apache.hadoop.hive.ql.history;
 
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.PrintWriter;
 import java.io.Serializable;
 import java.util.HashMap;
 import java.util.Map;
-import java.util.Random;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.QueryPlan;
 import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
-import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.Counters.Counter;
-import org.apache.hadoop.mapred.Counters.Group;
 
 /**
- * HiveHistory.
- *
+ * HiveHistory. Logs information such as query, query plan, runtime statistics
+ * into a file.
+ * Each session uses a new object, which creates a new file.
  */
-public class HiveHistory {
-
-  PrintWriter histStream; // History File stream
-
-  String histFileName; // History file name
-
-  private static final Log LOG = LogFactory.getLog("hive.ql.exec.HiveHistory");
-
-  private LogHelper console;
-
-  private Map<String, String> idToTableMap = null;
-
-  // Job Hash Map
-  private final HashMap<String, QueryInfo> queryInfoMap = new HashMap<String, QueryInfo>();
-
-  // Task Hash Map
-  private final HashMap<String, TaskInfo> taskInfoMap = new HashMap<String, TaskInfo>();
-
-  private static final String DELIMITER = " ";
+public interface HiveHistory {
 
   /**
    * RecordTypes.
@@ -105,83 +71,17 @@ public static enum Keys {
     ROWS_INSERTED
   };
 
-  private static final String KEY = "(\\w+)";
-  private static final String VALUE = "[[^\"]?]+"; // anything but a " in ""
-  private static final String ROW_COUNT_PATTERN = "TABLE_ID_(\\d+)_ROWCOUNT";
-
-  private static final Pattern pattern = Pattern.compile(KEY + "=" + "\""
-      + VALUE + "\"");
-
-  private static final Pattern rowCountPattern = Pattern.compile(ROW_COUNT_PATTERN);
-
-  // temp buffer for parsed dataa
-  private static Map<String, String> parseBuffer = new HashMap<String, String>();
-
   /**
-   * Listner interface Parser will call handle function for each record type.
+   * Listener interface.
+   * Parser will call handle function for each history record row, specifying
+   * the record type and its values
+   *
    */
   public static interface Listener {
 
     void handle(RecordTypes recType, Map<String, String> values) throws IOException;
   }
 
-  /**
-   * Parses history file and calls call back functions.
-   *
-   * @param path
-   * @param l
-   * @throws IOException
-   */
-  public static void parseHiveHistory(String path, Listener l) throws IOException {
-    FileInputStream fi = new FileInputStream(path);
-    BufferedReader reader = new BufferedReader(new InputStreamReader(fi));
-    try {
-      String line = null;
-      StringBuilder buf = new StringBuilder();
-      while ((line = reader.readLine()) != null) {
-        buf.append(line);
-        // if it does not end with " then it is line continuation
-        if (!line.trim().endsWith("\"")) {
-          continue;
-        }
-        parseLine(buf.toString(), l);
-        buf = new StringBuilder();
-      }
-    } finally {
-      try {
-        reader.close();
-      } catch (IOException ex) {
-      }
-    }
-  }
-
-  /**
-   * Parse a single line of history.
-   *
-   * @param line
-   * @param l
-   * @throws IOException
-   */
-  private static void parseLine(String line, Listener l) throws IOException {
-    // extract the record type
-    int idx = line.indexOf(' ');
-    String recType = line.substring(0, idx);
-    String data = line.substring(idx + 1, line.length());
-
-    Matcher matcher = pattern.matcher(data);
-
-    while (matcher.find()) {
-      String tuple = matcher.group(0);
-      String[] parts = tuple.split("=");
-
-      parseBuffer.put(parts[0], parts[1].substring(1, parts[1].length() - 1));
-    }
-
-    l.handle(RecordTypes.valueOf(recType), parseBuffer);
-
-    parseBuffer.clear();
-  }
-
   /**
    * Info.
    *
@@ -216,122 +116,25 @@ public static class TaskInfo extends Info {
 
   };
 
-  /**
-   * Construct HiveHistory object an open history log file.
-   *
-   * @param ss
-   */
-  public HiveHistory(SessionState ss) {
-
-    try {
-      console = new LogHelper(LOG);
-      String conf_file_loc = ss.getConf().getVar(
-          HiveConf.ConfVars.HIVEHISTORYFILELOC);
-      if ((conf_file_loc == null) || conf_file_loc.length() == 0) {
-        console.printError("No history file location given");
-        return;
-      }
-
-      // Create directory
-      File f = new File(conf_file_loc);
-      if (!f.exists()) {
-        if (!f.mkdirs()) {
-          console.printError("Unable to create log directory " + conf_file_loc);
-          return;
-        }
-      }
-      Random randGen = new Random();
-      do {
-        histFileName = conf_file_loc + "/hive_job_log_" + ss.getSessionId() + "_"
-          + Math.abs(randGen.nextInt()) + ".txt";
-      } while (new File(histFileName).exists());
-      console.printInfo("Hive history file=" + histFileName);
-      histStream = new PrintWriter(histFileName);
-
-      HashMap<String, String> hm = new HashMap<String, String>();
-      hm.put(Keys.SESSION_ID.name(), ss.getSessionId());
-      log(RecordTypes.SessionStart, hm);
-    } catch (FileNotFoundException e) {
-      console.printError("FAILED: Failed to open Query Log : " + histFileName
-          + " " + e.getMessage(), "\n"
-          + org.apache.hadoop.util.StringUtils.stringifyException(e));
-    }
-
-  }
 
   /**
    * @return historyFileName
    */
-  public String getHistFileName() {
-    return histFileName;
-  }
-
-  /**
-   * Write the a history record to history file.
-   *
-   * @param rt
-   * @param keyValMap
-   */
-  void log(RecordTypes rt, Map<String, String> keyValMap) {
-
-    if (histStream == null) {
-      return;
-    }
-
-    StringBuilder sb = new StringBuilder();
-    sb.append(rt.name());
-
-    for (Map.Entry<String, String> ent : keyValMap.entrySet()) {
-
-      sb.append(DELIMITER);
-      String key = ent.getKey();
-      String val = ent.getValue();
-      if(val != null) {
-        val = val.replace(System.getProperty("line.separator"), " ");
-      }
-      sb.append(key + "=\"" + val + "\"");
-
-    }
-    sb.append(DELIMITER);
-    sb.append(Keys.TIME.name() + "=\"" + System.currentTimeMillis() + "\"");
-    histStream.println(sb);
-    histStream.flush();
-
-  }
+  public String getHistFileName();
 
   /**
-   * Called at the start of job Driver.execute().
+   * Called at the start of query execution in Driver.execute().
    */
-  public void startQuery(String cmd, String id) {
-    SessionState ss = SessionState.get();
-    if (ss == null) {
-      return;
-    }
-    QueryInfo ji = new QueryInfo();
-
-    ji.hm.put(Keys.QUERY_ID.name(), id);
-    ji.hm.put(Keys.QUERY_STRING.name(), cmd);
-
-    queryInfoMap.put(id, ji);
-
-    log(RecordTypes.QueryStart, ji.hm);
-
-  }
+  public void startQuery(String cmd, String id);
 
   /**
-   * Used to set job status and other attributes of a job.
+   * Used to set query status and other attributes of a query
    *
    * @param queryId
    * @param propName
    * @param propValue
    */
-  public void setQueryProperty(String queryId, Keys propName, String propValue) {
-    QueryInfo ji = queryInfoMap.get(queryId);
-    if (ji == null) {
-      return;
-    }
-    ji.hm.put(propName.name(), propValue);
-  }
+  public void setQueryProperty(String queryId, Keys propName, String propValue);
 
   /**
    * Used to set task properties.
@@ -341,14 +144,7 @@ public void setQueryProperty(String queryId, Keys propName, String propValue) {
    * @param propValue
    */
   public void setTaskProperty(String queryId, String taskId, Keys propName,
-      String propValue) {
-    String id = queryId + ":" + taskId;
-    TaskInfo ti = taskInfoMap.get(id);
-    if (ti == null) {
-      return;
-    }
-    ti.hm.put(propName.name(), propValue);
-  }
+      String propValue);
 
   /**
    * Serialize the task counters and set as a task property.
@@ -357,190 +153,62 @@ public void setTaskProperty(String queryId, String taskId, Keys propName,
    * @param taskId
    * @param ctrs
    */
-  public void setTaskCounters(String queryId, String taskId, Counters ctrs) {
-    String id = queryId + ":" + taskId;
-    QueryInfo ji = queryInfoMap.get(queryId);
-    StringBuilder sb1 = new StringBuilder("");
-    TaskInfo ti = taskInfoMap.get(id);
-    if ((ti == null) || (ctrs == null)) {
-      return;
-    }
-    StringBuilder sb = new StringBuilder("");
-    try {
-
-      boolean first = true;
-      for (Group group : ctrs) {
-        for (Counter counter : group) {
-          if (first) {
-            first = false;
-          } else {
-            sb.append(',');
-          }
-          sb.append(group.getDisplayName());
-          sb.append('.');
-          sb.append(counter.getDisplayName());
-          sb.append(':');
-          sb.append(counter.getCounter());
-          String tab = getRowCountTableName(counter.getDisplayName());
-          if (tab != null) {
-            if (sb1.length() > 0) {
-              sb1.append(",");
-            }
-            sb1.append(tab);
-            sb1.append('~');
-            sb1.append(counter.getCounter());
-            ji.rowCountMap.put(tab, counter.getCounter());
-
-          }
-        }
-      }
-
-    } catch (Exception e) {
-      LOG.warn(org.apache.hadoop.util.StringUtils.stringifyException(e));
-    }
-    if (sb1.length() > 0) {
-      taskInfoMap.get(id).hm.put(Keys.ROWS_INSERTED.name(), sb1.toString());
-      queryInfoMap.get(queryId).hm.put(Keys.ROWS_INSERTED.name(), sb1
-          .toString());
-    }
-    if (sb.length() > 0) {
-      taskInfoMap.get(id).hm.put(Keys.TASK_COUNTERS.name(), sb.toString());
-    }
-  }
+  public void setTaskCounters(String queryId, String taskId, Counters ctrs);
 
-  public void printRowCount(String queryId) {
-    QueryInfo ji = queryInfoMap.get(queryId);
-    if (ji == null) {
-      return;
-    }
-    for (String tab : ji.rowCountMap.keySet()) {
-      console.printInfo(ji.rowCountMap.get(tab) + " Rows loaded to " + tab);
-    }
-  }
+  public void printRowCount(String queryId);
 
   /**
-   * Called at the end of Job. A Job is sql query.
+   * Called at the end of a query
    *
    * @param queryId
    */
-  public void endQuery(String queryId) {
-    QueryInfo ji = queryInfoMap.get(queryId);
-    if (ji == null) {
-      return;
-    }
-    log(RecordTypes.QueryEnd, ji.hm);
-    queryInfoMap.remove(queryId);
-  }
+  public void endQuery(String queryId);
 
   /**
-   * Called at the start of a task. Called by Driver.run() A Job can have
+   * Called at the start of a task. Called by Driver.run() A query can have
    * multiple tasks. Tasks will have multiple operator.
    *
    * @param task
    */
   public void startTask(String queryId, Task<? extends Serializable> task,
-      String taskName) {
-    SessionState ss = SessionState.get();
-    if (ss == null) {
-      return;
-    }
-    TaskInfo ti = new TaskInfo();
-
-    ti.hm.put(Keys.QUERY_ID.name(), ss.getQueryId());
-    ti.hm.put(Keys.TASK_ID.name(), task.getId());
-    ti.hm.put(Keys.TASK_NAME.name(), taskName);
-
-    String id = queryId + ":" + task.getId();
-    taskInfoMap.put(id, ti);
-
-    log(RecordTypes.TaskStart, ti.hm);
-
-  }
+      String taskName);
 
   /**
    * Called at the end of a task.
    *
    * @param task
    */
-  public void endTask(String queryId, Task<? extends Serializable> task) {
-    String id = queryId + ":" + task.getId();
-    TaskInfo ti = taskInfoMap.get(id);
-
-    if (ti == null) {
-      return;
-    }
-    log(RecordTypes.TaskEnd, ti.hm);
-    taskInfoMap.remove(id);
-  }
+  public void endTask(String queryId, Task<? extends Serializable> task);
 
   /**
-   * Called at the end of a task.
+   * Logs progress of a task if ConfVars.HIVE_LOG_INCREMENTAL_PLAN_PROGRESS is
+   * set to true
    *
    * @param task
    */
-  public void progressTask(String queryId, Task<? extends Serializable> task) {
-    String id = queryId + ":" + task.getId();
-    TaskInfo ti = taskInfoMap.get(id);
-    if (ti == null) {
-      return;
-    }
-    log(RecordTypes.TaskProgress, ti.hm);
+  public void progressTask(String queryId, Task<? extends Serializable> task);
 
-  }
 
   /**
-   * write out counters.
+   * Logs the current plan state
+   * @param plan
+   * @throws IOException
    */
-  static ThreadLocal<Map<String,String>> ctrMapFactory =
-      new ThreadLocal<Map<String, String>>() {
-        @Override
-        protected Map<String,String> initialValue() {
-          return new HashMap<String,String>();
-        }
-      };
-
-  public void logPlanProgress(QueryPlan plan) throws IOException {
-    Map<String,String> ctrmap = ctrMapFactory.get();
-    ctrmap.put("plan", plan.toString());
-    log(RecordTypes.Counters, ctrmap);
-  }
+  public void logPlanProgress(QueryPlan plan) throws IOException;
+
 
   /**
-   * Set the table to id map.
+   * Set the id to table name map
    *
    * @param map
    */
-  public void setIdToTableMap(Map<String, String> map) {
-    idToTableMap = map;
-  }
+  public void setIdToTableMap(Map<String, String> map);
 
   /**
-   * Returns table name for the counter name.
-   *
-   * @param name
-   * @return tableName
+   * Close the log file stream
    */
-  String getRowCountTableName(String name) {
-    if (idToTableMap == null) {
-      return null;
-    }
-    Matcher m = rowCountPattern.matcher(name);
-
-    if (m.find()) {
-      String tuple = m.group(1);
-      return idToTableMap.get(tuple);
-    }
-    return null;
+  public void closeStream();
 
-  }
 
-  public void closeStream() {
-    IOUtils.cleanup(LOG, histStream);
-  }
 
-  @Override
-  public void finalize() throws Throwable {
-    closeStream();
-    super.finalize();
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
new file mode 100644
index 0000000000..f75b70e4f0
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryImpl.java
@@ -0,0 +1,363 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.history;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintWriter;
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.QueryPlan;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+import org.apache.hadoop.io.IOUtils;
+import org.apache.hadoop.mapred.Counters;
+import org.apache.hadoop.mapred.Counters.Counter;
+import org.apache.hadoop.mapred.Counters.Group;
+
+/**
+ * HiveHistory. Logs information such as query, query plan, runtime statistics
+ * into a file.
+ * Each session uses a new object, which creates a new file.
+ */
+public class HiveHistoryImpl implements HiveHistory{
+
+  PrintWriter histStream; // History File stream
+
+  String histFileName; // History file name
+
+  private static final Log LOG = LogFactory.getLog("hive.ql.exec.HiveHistoryImpl");
+
+  private static final Random randGen = new Random();
+
+  private LogHelper console;
+
+  private Map<String, String> idToTableMap = null;
+
+  // Job Hash Map
+  private final HashMap<String, QueryInfo> queryInfoMap = new HashMap<String, QueryInfo>();
+
+  // Task Hash Map
+  private final HashMap<String, TaskInfo> taskInfoMap = new HashMap<String, TaskInfo>();
+
+  private static final String DELIMITER = " ";
+
+  private static final String ROW_COUNT_PATTERN = "TABLE_ID_(\\d+)_ROWCOUNT";
+
+  private static final Pattern rowCountPattern = Pattern.compile(ROW_COUNT_PATTERN);
+
+  /**
+   * Construct HiveHistoryImpl object and open history log file.
+   *
+   * @param ss
+   */
+  public HiveHistoryImpl(SessionState ss) {
+
+    try {
+      console = new LogHelper(LOG);
+      String conf_file_loc = ss.getConf().getVar(
+          HiveConf.ConfVars.HIVEHISTORYFILELOC);
+      if ((conf_file_loc == null) || conf_file_loc.length() == 0) {
+        console.printError("No history file location given");
+        return;
+      }
+
+      // Create directory
+      File histDir = new File(conf_file_loc);
+      if (!histDir.exists()) {
+        if (!histDir.mkdirs()) {
+          console.printError("Unable to create log directory " + conf_file_loc);
+          return;
+        }
+      }
+
+      do {
+        histFileName = conf_file_loc + File.separator + "hive_job_log_" + ss.getSessionId() + "_"
+          + Math.abs(randGen.nextInt()) + ".txt";
+      } while (! new File(histFileName).createNewFile());
+      console.printInfo("Hive history file=" + histFileName);
+      histStream = new PrintWriter(histFileName);
+
+      HashMap<String, String> hm = new HashMap<String, String>();
+      hm.put(Keys.SESSION_ID.name(), ss.getSessionId());
+      log(RecordTypes.SessionStart, hm);
+    } catch (IOException e) {
+      console.printError("FAILED: Failed to open Query Log : " + histFileName
+          + " " + e.getMessage(), "\n"
+          + org.apache.hadoop.util.StringUtils.stringifyException(e));
+    }
+
+  }
+
+  @Override
+  public String getHistFileName() {
+    return histFileName;
+  }
+
+  /**
+   * Write the a history record to history file.
+   *
+   * @param rt
+   * @param keyValMap
+   */
+  void log(RecordTypes rt, Map<String, String> keyValMap) {
+
+    if (histStream == null) {
+      return;
+    }
+
+    StringBuilder sb = new StringBuilder();
+    sb.append(rt.name());
+
+    for (Map.Entry<String, String> ent : keyValMap.entrySet()) {
+
+      sb.append(DELIMITER);
+      String key = ent.getKey();
+      String val = ent.getValue();
+      if(val != null) {
+        val = val.replace(System.getProperty("line.separator"), " ");
+      }
+      sb.append(key + "=\"" + val + "\"");
+
+    }
+    sb.append(DELIMITER);
+    sb.append(Keys.TIME.name() + "=\"" + System.currentTimeMillis() + "\"");
+    histStream.println(sb);
+    histStream.flush();
+
+  }
+
+  @Override
+  public void startQuery(String cmd, String id) {
+    SessionState ss = SessionState.get();
+    if (ss == null) {
+      return;
+    }
+    QueryInfo ji = new QueryInfo();
+
+    ji.hm.put(Keys.QUERY_ID.name(), id);
+    ji.hm.put(Keys.QUERY_STRING.name(), cmd);
+
+    queryInfoMap.put(id, ji);
+
+    log(RecordTypes.QueryStart, ji.hm);
+
+  }
+
+
+  @Override
+  public void setQueryProperty(String queryId, Keys propName, String propValue) {
+    QueryInfo ji = queryInfoMap.get(queryId);
+    if (ji == null) {
+      return;
+    }
+    ji.hm.put(propName.name(), propValue);
+  }
+
+  @Override
+  public void setTaskProperty(String queryId, String taskId, Keys propName,
+      String propValue) {
+    String id = queryId + ":" + taskId;
+    TaskInfo ti = taskInfoMap.get(id);
+    if (ti == null) {
+      return;
+    }
+    ti.hm.put(propName.name(), propValue);
+  }
+
+  @Override
+  public void setTaskCounters(String queryId, String taskId, Counters ctrs) {
+    String id = queryId + ":" + taskId;
+    QueryInfo ji = queryInfoMap.get(queryId);
+    StringBuilder sb1 = new StringBuilder("");
+    TaskInfo ti = taskInfoMap.get(id);
+    if ((ti == null) || (ctrs == null)) {
+      return;
+    }
+    StringBuilder sb = new StringBuilder("");
+    try {
+
+      boolean first = true;
+      for (Group group : ctrs) {
+        for (Counter counter : group) {
+          if (first) {
+            first = false;
+          } else {
+            sb.append(',');
+          }
+          sb.append(group.getDisplayName());
+          sb.append('.');
+          sb.append(counter.getDisplayName());
+          sb.append(':');
+          sb.append(counter.getCounter());
+          String tab = getRowCountTableName(counter.getDisplayName());
+          if (tab != null) {
+            if (sb1.length() > 0) {
+              sb1.append(",");
+            }
+            sb1.append(tab);
+            sb1.append('~');
+            sb1.append(counter.getCounter());
+            ji.rowCountMap.put(tab, counter.getCounter());
+
+          }
+        }
+      }
+
+    } catch (Exception e) {
+      LOG.warn(org.apache.hadoop.util.StringUtils.stringifyException(e));
+    }
+    if (sb1.length() > 0) {
+      taskInfoMap.get(id).hm.put(Keys.ROWS_INSERTED.name(), sb1.toString());
+      queryInfoMap.get(queryId).hm.put(Keys.ROWS_INSERTED.name(), sb1
+          .toString());
+    }
+    if (sb.length() > 0) {
+      taskInfoMap.get(id).hm.put(Keys.TASK_COUNTERS.name(), sb.toString());
+    }
+  }
+
+  @Override
+  public void printRowCount(String queryId) {
+    QueryInfo ji = queryInfoMap.get(queryId);
+    if (ji == null) {
+      return;
+    }
+    for (String tab : ji.rowCountMap.keySet()) {
+      console.printInfo(ji.rowCountMap.get(tab) + " Rows loaded to " + tab);
+    }
+  }
+
+  @Override
+  public void endQuery(String queryId) {
+    QueryInfo ji = queryInfoMap.get(queryId);
+    if (ji == null) {
+      return;
+    }
+    log(RecordTypes.QueryEnd, ji.hm);
+    queryInfoMap.remove(queryId);
+  }
+
+  @Override
+  public void startTask(String queryId, Task<? extends Serializable> task,
+      String taskName) {
+    SessionState ss = SessionState.get();
+    if (ss == null) {
+      return;
+    }
+    TaskInfo ti = new TaskInfo();
+
+    ti.hm.put(Keys.QUERY_ID.name(), ss.getQueryId());
+    ti.hm.put(Keys.TASK_ID.name(), task.getId());
+    ti.hm.put(Keys.TASK_NAME.name(), taskName);
+
+    String id = queryId + ":" + task.getId();
+    taskInfoMap.put(id, ti);
+
+    log(RecordTypes.TaskStart, ti.hm);
+
+  }
+
+  @Override
+  public void endTask(String queryId, Task<? extends Serializable> task) {
+    String id = queryId + ":" + task.getId();
+    TaskInfo ti = taskInfoMap.get(id);
+
+    if (ti == null) {
+      return;
+    }
+    log(RecordTypes.TaskEnd, ti.hm);
+    taskInfoMap.remove(id);
+  }
+
+  @Override
+  public void progressTask(String queryId, Task<? extends Serializable> task) {
+    String id = queryId + ":" + task.getId();
+    TaskInfo ti = taskInfoMap.get(id);
+    if (ti == null) {
+      return;
+    }
+    log(RecordTypes.TaskProgress, ti.hm);
+
+  }
+
+  /**
+   * write out counters.
+   */
+  static ThreadLocal<Map<String,String>> ctrMapFactory =
+      new ThreadLocal<Map<String, String>>() {
+    @Override
+    protected Map<String,String> initialValue() {
+      return new HashMap<String,String>();
+    }
+  };
+
+  @Override
+  public void logPlanProgress(QueryPlan plan) throws IOException {
+    Map<String,String> ctrmap = ctrMapFactory.get();
+    ctrmap.put("plan", plan.toString());
+    log(RecordTypes.Counters, ctrmap);
+  }
+
+  @Override
+  public void setIdToTableMap(Map<String, String> map) {
+    idToTableMap = map;
+  }
+
+  /**
+   * Returns table name for the counter name.
+   *
+   * @param name
+   * @return tableName
+   */
+  String getRowCountTableName(String name) {
+    if (idToTableMap == null) {
+      return null;
+    }
+    Matcher m = rowCountPattern.matcher(name);
+
+    if (m.find()) {
+      String tuple = m.group(1);
+      return idToTableMap.get(tuple);
+    }
+    return null;
+
+  }
+
+  @Override
+  public void closeStream() {
+    IOUtils.cleanup(LOG, histStream);
+  }
+
+  @Override
+  public void finalize() throws Throwable {
+    closeStream();
+    super.finalize();
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryProxyHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryProxyHandler.java
new file mode 100644
index 0000000000..31c465ace9
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryProxyHandler.java
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.history;
+
+/**
+ * Proxy handler for HiveHistory to do nothing
+ * Used when HiveHistory is disabled.
+ */
+import java.lang.reflect.InvocationHandler;
+import java.lang.reflect.Method;
+import java.lang.reflect.Proxy;
+
+public class HiveHistoryProxyHandler implements InvocationHandler {
+
+  public static HiveHistory getNoOpHiveHistoryProxy() {
+    return (HiveHistory)Proxy.newProxyInstance(HiveHistory.class.getClassLoader(),
+        new Class<?>[] {HiveHistory.class},
+        new HiveHistoryProxyHandler());
+  }
+
+  @Override
+  public Object invoke(Object arg0, final Method method, final Object[] args){
+    //do nothing
+    return null;
+  }
+
+}
+
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryUtil.java
new file mode 100644
index 0000000000..a7ca165a84
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryUtil.java
@@ -0,0 +1,103 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.history;
+
+import java.io.BufferedReader;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import org.apache.hadoop.hive.ql.history.HiveHistory.Listener;
+import org.apache.hadoop.hive.ql.history.HiveHistory.RecordTypes;
+
+public class HiveHistoryUtil {
+  /**
+  * Parses history file and calls call back functions. Also look at
+  *  HiveHistoryViewer
+  *
+  * @param path
+  * @param l
+  * @throws IOException
+  */
+ public static void parseHiveHistory(String path, Listener l) throws IOException {
+   if(path == null){
+     return;
+   }
+   FileInputStream fi = new FileInputStream(path);
+   BufferedReader reader = new BufferedReader(new InputStreamReader(fi));
+   try {
+     String line = null;
+     StringBuilder buf = new StringBuilder();
+     while ((line = reader.readLine()) != null) {
+       buf.append(line);
+       // if it does not end with " then it is line continuation
+       if (!line.trim().endsWith("\"")) {
+         continue;
+       }
+       parseLine(buf.toString(), l);
+       buf = new StringBuilder();
+     }
+   } finally {
+     try {
+       reader.close();
+     } catch (IOException ex) {
+     }
+   }
+ }
+
+
+ private static final String KEY = "(\\w+)";
+ private static final String VALUE = "[[^\"]?]+"; // anything but a " in ""
+ private static final Pattern pattern = Pattern.compile(KEY + "=" + "\""
+     + VALUE + "\"");
+
+ // temp buffer for parsed dataa
+ private static Map<String, String> parseBuffer = new HashMap<String, String>();
+
+ /**
+  * Parse a single line of history.
+  *
+  * @param line
+  * @param l
+  * @throws IOException
+  */
+ private static void parseLine(String line, Listener l) throws IOException {
+   // extract the record type
+   int idx = line.indexOf(' ');
+   String recType = line.substring(0, idx);
+   String data = line.substring(idx + 1, line.length());
+
+   Matcher matcher = pattern.matcher(data);
+
+   while (matcher.find()) {
+     String tuple = matcher.group(0);
+     String[] parts = tuple.split("=");
+
+     parseBuffer.put(parts[0], parts[1].substring(1, parts[1].length() - 1));
+   }
+
+   l.handle(RecordTypes.valueOf(recType), parseBuffer);
+
+   parseBuffer.clear();
+ }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryViewer.java b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryViewer.java
index fdd56dbdc2..1b357de5c5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryViewer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistoryViewer.java
@@ -22,6 +22,8 @@
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
 import org.apache.hadoop.hive.ql.history.HiveHistory.Listener;
 import org.apache.hadoop.hive.ql.history.HiveHistory.QueryInfo;
@@ -35,8 +37,8 @@
 public class HiveHistoryViewer implements Listener {
 
   String historyFile;
-
   String sessionId;
+  private static final Log LOG = LogFactory.getLog(HiveHistoryViewer.class);
 
   // Job Hash Map
   private final HashMap<String, QueryInfo> jobInfoMap = new HashMap<String, QueryInfo>();
@@ -65,19 +67,18 @@ public Map<String, TaskInfo> getTaskInfoMap() {
    * Parse history files.
    */
   void init() {
-
     try {
-      HiveHistory.parseHiveHistory(historyFile, this);
+      HiveHistoryUtil.parseHiveHistory(historyFile, this);
     } catch (IOException e) {
-      // TODO Auto-generated catch block
+      // TODO pass on this exception
       e.printStackTrace();
+      LOG.error("Error parsing hive history log file", e);
     }
-
   }
 
   /**
-   * Implementation Listner interface function.
-   * 
+   * Implementation Listener interface function.
+   *
    * @see org.apache.hadoop.hive.ql.history.HiveHistory.Listener#handle(org.apache.hadoop.hive.ql.history.HiveHistory.RecordTypes,
    *      java.util.Map)
    */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index ab369f0ab3..0d5522f394 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -44,6 +44,8 @@
 import org.apache.hadoop.hive.ql.MapRedStats;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.history.HiveHistory;
+import org.apache.hadoop.hive.ql.history.HiveHistoryImpl;
+import org.apache.hadoop.hive.ql.history.HiveHistoryProxyHandler;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
@@ -251,19 +253,19 @@ public static SessionState start(SessionState startSs) {
 
     tss.set(startSs);
 
-    if (startSs.hiveHist == null) {
-      startSs.hiveHist = new HiveHistory(startSs);
+    if(startSs.hiveHist == null){
+      if (startSs.getConf().getBoolVar(HiveConf.ConfVars.HIVE_SESSION_HISTORY_ENABLED)) {
+        startSs.hiveHist = new HiveHistoryImpl(startSs);
+      }else {
+        //Hive history is disabled, create a no-op proxy
+        startSs.hiveHist = HiveHistoryProxyHandler.getNoOpHiveHistoryProxy();
+      }
     }
 
     if (startSs.getTmpOutputFile() == null) {
-      // per-session temp file containing results to be sent from HiveServer to HiveClient
-      File tmpDir = new File(
-          HiveConf.getVar(startSs.getConf(), HiveConf.ConfVars.HIVEHISTORYFILELOC));
-      String sessionID = startSs.getConf().getVar(HiveConf.ConfVars.HIVESESSIONID);
+      // set temp file containing results to be sent to HiveClient
       try {
-        File tmpFile = File.createTempFile(sessionID, ".pipeout", tmpDir);
-        tmpFile.deleteOnExit();
-        startSs.setTmpOutputFile(tmpFile);
+        startSs.setTmpOutputFile(createTempFile(startSs.getConf()));
       } catch (IOException e) {
         throw new RuntimeException(e);
       }
@@ -284,6 +286,33 @@ public static SessionState start(SessionState startSs) {
     return startSs;
   }
 
+  /**
+   * @param conf
+   * @return per-session temp file
+   * @throws IOException
+   */
+  private static File createTempFile(HiveConf conf) throws IOException {
+    String hHistDir =
+        HiveConf.getVar(conf, HiveConf.ConfVars.HIVEHISTORYFILELOC);
+
+    File tmpDir = new File(hHistDir);
+    String sessionID = conf.getVar(HiveConf.ConfVars.HIVESESSIONID);
+    if (!tmpDir.exists()) {
+      if (!tmpDir.mkdirs()) {
+        //Do another exists to check to handle possible race condition
+        // Another thread might have created the dir, if that is why
+        // mkdirs returned false, that is fine
+        if(!tmpDir.exists()){
+          throw new RuntimeException("Unable to create log directory "
+              + hHistDir);
+        }
+      }
+    }
+    File tmpFile = File.createTempFile(sessionID, ".pipeout", tmpDir);
+    tmpFile.deleteOnExit();
+    return tmpFile;
+  }
+
   /**
    * get the current session.
    */
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java b/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
index a783303904..8b8c276075 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/history/TestHiveHistory.java
@@ -20,6 +20,7 @@
 
 import java.io.PrintStream;
 import java.io.UnsupportedEncodingException;
+import java.lang.reflect.Proxy;
 import java.util.LinkedList;
 import java.util.Map;
 
@@ -31,9 +32,9 @@
 import org.apache.hadoop.hive.common.LogUtils;
 import org.apache.hadoop.hive.common.LogUtils.LogInitializationException;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.ql.Driver;
-import org.apache.hadoop.hive.ql.QTestUtil.QTestSetup;
 import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
 import org.apache.hadoop.hive.ql.history.HiveHistory.QueryInfo;
 import org.apache.hadoop.hive.ql.history.HiveHistory.TaskInfo;
@@ -75,7 +76,7 @@ protected void setUp() {
               + tmpdir);
         }
       }
-      
+
       conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);
 
       // copy the test files into hadoop if required.
@@ -124,8 +125,9 @@ public void testSimpleQuery() {
         LogUtils.initHiveLog4j();
       } catch (LogInitializationException e) {
       }
-
-      CliSessionState ss = new CliSessionState(new HiveConf(SessionState.class));
+      HiveConf hconf = new HiveConf(SessionState.class);
+      hconf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
+      CliSessionState ss = new CliSessionState(hconf);
       ss.in = System.in;
       try {
         ss.out = new PrintStream(System.out, true, "UTF-8");
@@ -179,7 +181,7 @@ public void testQueryloglocParentDirNotExist() throws Exception {
       HiveConf conf = new HiveConf(SessionState.class);
       conf.set(HiveConf.ConfVars.HIVEHISTORYFILELOC.toString(), actualDir);
       SessionState ss = new CliSessionState(conf);
-      HiveHistory hiveHistory = new HiveHistory(ss);
+      HiveHistory hiveHistory = new HiveHistoryImpl(ss);
       Path actualPath = new Path(actualDir);
       if (!fs.exists(actualPath)) {
         fail("Query location path is not exist :" + actualPath.toString());
@@ -192,4 +194,38 @@ public void testQueryloglocParentDirNotExist() throws Exception {
     }
   }
 
+  /**
+   * Check if HiveHistoryImpl class is returned when hive history is enabled
+   * @throws Exception
+   */
+  public void testHiveHistoryConfigEnabled() throws Exception {
+      HiveConf conf = new HiveConf(SessionState.class);
+      conf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, true);
+      SessionState ss = new CliSessionState(conf);
+      SessionState.start(ss);
+      HiveHistory hHistory = ss.getHiveHistory();
+      assertEquals("checking hive history class when history is enabled",
+          hHistory.getClass(), HiveHistoryImpl.class);
+  }
+  /**
+   * Check if HiveHistory class is a Proxy class when hive history is disabled
+   * @throws Exception
+   */
+  public void testHiveHistoryConfigDisabled() throws Exception {
+    HiveConf conf = new HiveConf(SessionState.class);
+    conf.setBoolVar(ConfVars.HIVE_SESSION_HISTORY_ENABLED, false);
+    SessionState ss = new CliSessionState(conf);
+    SessionState.start(ss);
+    HiveHistory hHistory = ss.getHiveHistory();
+    assertTrue("checking hive history class when history is disabled",
+        hHistory.getClass() != HiveHistoryImpl.class);
+    System.err.println("hHistory.getClass" + hHistory.getClass());
+    assertTrue("verifying proxy class is used when history is disabled",
+        Proxy.isProxyClass(hHistory.getClass()));
+
+  }
+
+
+
+
 }
diff --git a/ql/src/test/templates/TestCliDriver.vm b/ql/src/test/templates/TestCliDriver.vm
index a6ae6c3f6d..ea24c372bf 100644
--- a/ql/src/test/templates/TestCliDriver.vm
+++ b/ql/src/test/templates/TestCliDriver.vm
@@ -25,10 +25,6 @@ import java.io.*;
 import java.util.*;
 
 import org.apache.hadoop.hive.ql.QTestUtil;
-import org.apache.hadoop.hive.ql.history.HiveHistoryViewer;
-import org.apache.hadoop.hive.ql.history.HiveHistory.QueryInfo;
-import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;
-import org.apache.hadoop.hive.ql.history.HiveHistory.TaskInfo;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 public class $className extends TestCase {
@@ -136,22 +132,6 @@ public class $className extends TestCase {
       if (ecode != 0) {
         fail("Client Execution failed with error code = " + ecode + debugHint);
       }
-      if (SessionState.get() != null) {
-        HiveHistoryViewer hv = new HiveHistoryViewer(SessionState.get()
-          .getHiveHistory().getHistFileName());
-        Map<String, QueryInfo> jobInfoMap = hv.getJobInfoMap();
-        Map<String, TaskInfo> taskInfoMap = hv.getTaskInfoMap();
-
-        if(jobInfoMap.size() != 0) {
-          String cmd = (String)jobInfoMap.keySet().toArray()[0];
-          QueryInfo ji = jobInfoMap.get(cmd);
-
-          if (!ji.hm.get(Keys.QUERY_RET_CODE.name()).equals("0")) {
-              fail("Wrong return code in hive history" + debugHint);
-          }
-        }
-      }
-
       ecode = qt.checkCliDriverResults(fname);
       if (ecode != 0) {
         fail("Client execution results failed with error code = " + ecode
