diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index 43ab9e607b..877a8979bd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -758,7 +758,14 @@ private Vertex createVertexFromMergeWork(JobConf conf, MergeJoinWork mergeJoinWo
       Vertex mergeVx = createVertexFromMapWork(
           conf, mapWork, mrScratchDir, vertexType);
 
-      conf.setClass("mapred.input.format.class", HiveInputFormat.class, InputFormat.class);
+      Class<?> inputFormatClass = conf.getClass("mapred.input.format.class",
+              HiveInputFormat.class);
+      if (inputFormatClass != BucketizedHiveInputFormat.class &&
+              inputFormatClass != HiveInputFormat.class) {
+        // As of now only these two formats are supported.
+        inputFormatClass = HiveInputFormat.class;
+      }
+      conf.setClass("mapred.input.format.class", inputFormatClass, InputFormat.class);
       // mapreduce.tez.input.initializer.serialize.event.payload should be set
       // to false when using this plug-in to avoid getting a serialized event at run-time.
       conf.setBoolean("mapreduce.tez.input.initializer.serialize.event.payload", false);
@@ -769,7 +776,7 @@ private Vertex createVertexFromMergeWork(JobConf conf, MergeJoinWork mergeJoinWo
         conf.set(Utilities.INPUT_NAME, mapWork.getName());
         LOG.info("Going through each work and adding MultiMRInput");
         mergeVx.addDataSource(mapWork.getName(),
-            MultiMRInput.createConfigBuilder(conf, HiveInputFormat.class).build());
+            MultiMRInput.createConfigBuilder(conf, inputFormatClass).build());
       }
 
       // To be populated for SMB joins only for all the small tables
@@ -835,8 +842,12 @@ private Vertex createVertexFromMapWork(JobConf conf, MapWork mapWork, Path mrScr
       groupSplitsInInputInitializer = false;
       // grouping happens in execution phase. The input payload should not enable grouping here,
       // it will be enabled in the CustomVertex.
-      inputFormatClass = HiveInputFormat.class;
-      conf.setClass("mapred.input.format.class", HiveInputFormat.class, InputFormat.class);
+      if (inputFormatClass != BucketizedHiveInputFormat.class &&
+              inputFormatClass != HiveInputFormat.class) {
+        // As of now only these two formats are supported.
+        inputFormatClass = HiveInputFormat.class;
+      }
+      conf.setClass("mapred.input.format.class", inputFormatClass, InputFormat.class);
       // mapreduce.tez.input.initializer.serialize.event.payload should be set to false when using
       // this plug-in to avoid getting a serialized event at run-time.
       conf.setBoolean("mapreduce.tez.input.initializer.serialize.event.payload", false);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java
index 2295edcd46..6b8aec1fa2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/SplitGrouper.java
@@ -32,6 +32,8 @@
 import java.util.concurrent.ConcurrentHashMap;
 
 import org.apache.hadoop.hive.ql.io.AcidUtils;
+import org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
+import org.apache.hadoop.mapred.InputFormat;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -94,11 +96,17 @@ public Multimap<Integer, InputSplit> group(Configuration conf,
     // use the tez grouper to combine splits once per bucket
     for (int bucketId : bucketSplitMultimap.keySet()) {
       Collection<InputSplit> inputSplitCollection = bucketSplitMultimap.get(bucketId);
-
+      Class inputFormatClass = conf.getClass("mapred.input.format.class",
+              HiveInputFormat.class);
+      if (inputFormatClass != BucketizedHiveInputFormat.class &&
+              inputFormatClass != HiveInputFormat.class) {
+        // As of now only these two formats are supported.
+        inputFormatClass = HiveInputFormat.class;
+      }
       InputSplit[] rawSplits = inputSplitCollection.toArray(new InputSplit[0]);
       InputSplit[] groupedSplits =
           tezGrouper.getGroupedSplits(conf, rawSplits, bucketTaskMap.get(bucketId),
-              HiveInputFormat.class.getName(), new ColumnarSplitSizeEstimator(), splitLocationProvider);
+                  inputFormatClass.getName(), new ColumnarSplitSizeEstimator(), splitLocationProvider);
 
       LOG.info("Original split count is " + rawSplits.length + " grouped split count is "
           + groupedSplits.length + ", for bucket: " + bucketId);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
index 52732fff91..84e349cf01 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
@@ -130,6 +130,7 @@
 import org.apache.hadoop.hive.ql.plan.GroupByDesc;
 import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
 import org.apache.hadoop.hive.ql.plan.MapWork;
+import org.apache.hadoop.hive.ql.plan.MergeJoinWork;
 import org.apache.hadoop.hive.ql.plan.MoveWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.Statistics;
@@ -700,20 +701,32 @@ protected void generateTaskTree(List<Task<?>> rootTasks, ParseContext pCtx,
     perfLogger.perfLogEnd(this.getClass().getName(), PerfLogger.TEZ_COMPILER, "generateTaskTree");
   }
 
+  void setInputFormatForMapWork(BaseWork work) {
+    if (work instanceof MapWork) {
+      MapWork mapWork = (MapWork) work;
+      Map<String, Operator<? extends OperatorDesc>> opMap = mapWork.getAliasToWork();
+      if (!opMap.isEmpty()) {
+        for (Operator<? extends OperatorDesc> op : opMap.values()) {
+          setInputFormat(mapWork, op);
+        }
+      }
+    }
+  }
+
   @Override
   protected void setInputFormat(Task<?> task) {
     if (task instanceof TezTask) {
       TezWork work = ((TezTask)task).getWork();
       List<BaseWork> all = work.getAllWork();
       for (BaseWork w: all) {
-        if (w instanceof MapWork) {
-          MapWork mapWork = (MapWork) w;
-          Map<String, Operator<? extends OperatorDesc>> opMap = mapWork.getAliasToWork();
-          if (!opMap.isEmpty()) {
-            for (Operator<? extends OperatorDesc> op : opMap.values()) {
-              setInputFormat(mapWork, op);
-            }
+        if (w instanceof MergeJoinWork) {
+          MergeJoinWork mj = (MergeJoinWork)w;
+          setInputFormatForMapWork(mj.getMainWork());
+          for (BaseWork bw : mj.getBaseWorkList()) {
+            setInputFormatForMapWork(bw);
           }
+        } else {
+          setInputFormatForMapWork(w);
         }
       }
     } else if (task instanceof ConditionalTask) {
diff --git a/ql/src/test/queries/clientpositive/auto_sortmerge_join_10.q b/ql/src/test/queries/clientpositive/auto_sortmerge_join_10.q
index 453823d8e8..429d08b898 100644
--- a/ql/src/test/queries/clientpositive/auto_sortmerge_join_10.q
+++ b/ql/src/test/queries/clientpositive/auto_sortmerge_join_10.q
@@ -63,3 +63,20 @@ select count(*) from
     join
   (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
   on subq1.key = subq2.key;
+
+
+set hive.auto.convert.sortmerge.join=true;
+set hive.optimize.semijoin.conversion = false;
+
+explain
+select count(*) from
+  (select a.key as key, count(*) as value from tbl1_n5 a where key < 6 group by a.key) subq1
+    join
+  (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
+  on subq1.key = subq2.key;
+
+select count(*) from
+  (select a.key as key, count(*) as value from tbl1_n5 a where key < 6 group by a.key) subq1
+    join
+  (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
+  on subq1.key = subq2.key;
diff --git a/ql/src/test/results/clientpositive/llap/auto_sortmerge_join_10.q.out b/ql/src/test/results/clientpositive/llap/auto_sortmerge_join_10.q.out
index 8c713b8e81..bb8bdde769 100644
--- a/ql/src/test/results/clientpositive/llap/auto_sortmerge_join_10.q.out
+++ b/ql/src/test/results/clientpositive/llap/auto_sortmerge_join_10.q.out
@@ -363,3 +363,122 @@ POSTHOOK: Input: default@tbl1_n5
 POSTHOOK: Input: default@tbl2_n4
 #### A masked pattern was here ####
 8
+PREHOOK: query: explain
+select count(*) from
+  (select a.key as key, count(*) as value from tbl1_n5 a where key < 6 group by a.key) subq1
+    join
+  (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
+  on subq1.key = subq2.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl1_n5
+PREHOOK: Input: default@tbl2_n4
+#### A masked pattern was here ####
+POSTHOOK: query: explain
+select count(*) from
+  (select a.key as key, count(*) as value from tbl1_n5 a where key < 6 group by a.key) subq1
+    join
+  (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
+  on subq1.key = subq2.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl1_n5
+POSTHOOK: Input: default@tbl2_n4
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  filterExpr: (key < 6) (type: boolean)
+                  Statistics: Num rows: 10 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: (key < 6) (type: boolean)
+                    Statistics: Num rows: 7 Data size: 28 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 7 Data size: 28 Basic stats: COMPLETE Column stats: COMPLETE
+                      Dummy Store
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  filterExpr: (key < 6) (type: boolean)
+                  Statistics: Num rows: 10 Data size: 40 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: (key < 6) (type: boolean)
+                    Statistics: Num rows: 7 Data size: 28 Basic stats: COMPLETE Column stats: COMPLETE
+                    Group By Operator
+                      keys: key (type: int)
+                      mode: final
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 5 Data size: 20 Basic stats: COMPLETE Column stats: COMPLETE
+                      Merge Join Operator
+                        condition map:
+                             Inner Join 0 to 1
+                        keys:
+                          0 _col0 (type: int)
+                          1 _col0 (type: int)
+                        Statistics: Num rows: 7 Data size: 56 Basic stats: COMPLETE Column stats: COMPLETE
+                        Group By Operator
+                          aggregations: count()
+                          minReductionHashAggr: 0.85714287
+                          mode: hash
+                          outputColumnNames: _col0
+                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                          Reduce Output Operator
+                            null sort order: 
+                            sort order: 
+                            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                            value expressions: _col0 (type: bigint)
+            Execution mode: llap
+        Reducer 2 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select count(*) from
+  (select a.key as key, count(*) as value from tbl1_n5 a where key < 6 group by a.key) subq1
+    join
+  (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
+  on subq1.key = subq2.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl1_n5
+PREHOOK: Input: default@tbl2_n4
+#### A masked pattern was here ####
+POSTHOOK: query: select count(*) from
+  (select a.key as key, count(*) as value from tbl1_n5 a where key < 6 group by a.key) subq1
+    join
+  (select a.key as key, a.value as value from tbl2_n4 a where key < 6) subq2
+  on subq1.key = subq2.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl1_n5
+POSTHOOK: Input: default@tbl2_n4
+#### A masked pattern was here ####
+8
