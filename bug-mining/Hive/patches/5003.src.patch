diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java
index b48de3e141..4a5e8cf072 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java
@@ -38,26 +38,24 @@ public class JobMetricsListener extends SparkListener {
 
   private final Map<Integer, int[]> jobIdToStageId = Maps.newHashMap();
   private final Map<Integer, Integer> stageIdToJobId = Maps.newHashMap();
-  private final Map<Integer, Map<String, List<TaskMetrics>>> allJobMetrics = Maps.newHashMap();
+  private final Map<Integer, Map<Integer, List<TaskMetrics>>> allJobMetrics = Maps.newHashMap();
 
   @Override
   public synchronized void onTaskEnd(SparkListenerTaskEnd taskEnd) {
     int stageId = taskEnd.stageId();
-    int stageAttemptId = taskEnd.stageAttemptId();
-    String stageIdentifier = stageId + "_" + stageAttemptId;
     Integer jobId = stageIdToJobId.get(stageId);
     if (jobId == null) {
       LOG.warn("Can not find job id for stage[" + stageId + "].");
     } else {
-      Map<String, List<TaskMetrics>> jobMetrics = allJobMetrics.get(jobId);
+      Map<Integer, List<TaskMetrics>> jobMetrics = allJobMetrics.get(jobId);
       if (jobMetrics == null) {
         jobMetrics = Maps.newHashMap();
         allJobMetrics.put(jobId, jobMetrics);
       }
-      List<TaskMetrics> stageMetrics = jobMetrics.get(stageIdentifier);
+      List<TaskMetrics> stageMetrics = jobMetrics.get(stageId);
       if (stageMetrics == null) {
         stageMetrics = Lists.newLinkedList();
-        jobMetrics.put(stageIdentifier, stageMetrics);
+        jobMetrics.put(stageId, stageMetrics);
       }
       stageMetrics.add(taskEnd.taskMetrics());
     }
@@ -76,7 +74,7 @@ public synchronized void onJobStart(SparkListenerJobStart jobStart) {
     jobIdToStageId.put(jobId, intStageIds);
   }
 
-  public synchronized  Map<String, List<TaskMetrics>> getJobMetric(int jobId) {
+  public synchronized  Map<Integer, List<TaskMetrics>> getJobMetric(int jobId) {
     return allJobMetrics.get(jobId);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/LocalSparkJobStatus.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/LocalSparkJobStatus.java
index d4819d943f..4e93979f52 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/LocalSparkJobStatus.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/LocalSparkJobStatus.java
@@ -36,14 +36,8 @@
 import org.apache.spark.SparkStageInfo;
 import org.apache.spark.api.java.JavaFutureAction;
 import org.apache.spark.api.java.JavaSparkContext;
-import org.apache.spark.executor.ShuffleReadMetrics;
-import org.apache.spark.executor.ShuffleWriteMetrics;
 import org.apache.spark.executor.TaskMetrics;
 
-import scala.Option;
-
-import com.google.common.collect.Maps;
-
 public class LocalSparkJobStatus implements SparkJobStatus {
 
   private final JavaSparkContext sparkContext;
@@ -132,22 +126,21 @@ public SparkStatistics getSparkStatistics() {
     sparkStatisticsBuilder.add(sparkCounters);
     // add spark job metrics.
     String jobIdentifier = "Spark Job[" + jobId + "] Metrics";
-    Map<String, List<TaskMetrics>> jobMetric = jobMetricsListener.getJobMetric(jobId);
+    Map<Integer, List<TaskMetrics>> jobMetric = jobMetricsListener.getJobMetric(jobId);
     if (jobMetric == null) {
       return null;
     }
 
     MetricsCollection metricsCollection = new MetricsCollection();
-    Set<String> stageIds = jobMetric.keySet();
-    for (String stageId : stageIds) {
+    Set<Integer> stageIds = jobMetric.keySet();
+    for (int stageId : stageIds) {
       List<TaskMetrics> taskMetrics = jobMetric.get(stageId);
       for (TaskMetrics taskMetric : taskMetrics) {
         Metrics metrics = new Metrics(taskMetric);
-        metricsCollection.addMetrics(jobId, Integer.parseInt(stageId), 0, metrics);
+        metricsCollection.addMetrics(jobId, stageId, 0, metrics);
       }
     }
-    SparkJobUtils sparkJobUtils = new SparkJobUtils();
-    Map<String, Long> flatJobMetric = sparkJobUtils.collectMetrics(metricsCollection
+    Map<String, Long> flatJobMetric = SparkMetricsUtils.collectMetrics(metricsCollection
         .getAllMetrics());
     for (Map.Entry<String, Long> entry : flatJobMetric.entrySet()) {
       sparkStatisticsBuilder.add(jobIdentifier, entry.getKey(), Long.toString(entry.getValue()));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java
index 2c6818f9b8..9fc717f8ae 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/RemoteSparkJobStatus.java
@@ -24,8 +24,6 @@
 import org.apache.hadoop.hive.ql.exec.spark.Statistic.SparkStatisticsBuilder;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hive.spark.client.MetricsCollection;
-import org.apache.hive.spark.client.metrics.Metrics;
-import org.apache.hive.spark.client.metrics.ShuffleReadMetrics;
 import org.apache.hive.spark.counter.SparkCounters;
 import org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus;
 import org.apache.hadoop.hive.ql.exec.spark.status.SparkStageProgress;
@@ -40,7 +38,6 @@
 
 import java.io.Serializable;
 import java.util.HashMap;
-import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.Future;
@@ -125,8 +122,8 @@ public SparkStatistics getSparkStatistics() {
     // add spark job metrics.
     String jobIdentifier = "Spark Job[" + jobHandle.getClientJobId() + "] Metrics";
 
-    SparkJobUtils sparkJobUtils = new SparkJobUtils();
-    Map<String, Long> flatJobMetric = sparkJobUtils.collectMetrics(metricsCollection.getAllMetrics());
+    Map<String, Long> flatJobMetric = SparkMetricsUtils.collectMetrics(
+        metricsCollection.getAllMetrics());
     for (Map.Entry<String, Long> entry : flatJobMetric.entrySet()) {
       sparkStatisticsBuilder.add(jobIdentifier, entry.getKey(), Long.toString(entry.getValue()));
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/SparkJobUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/SparkMetricsUtils.java
similarity index 92%
rename from ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/SparkJobUtils.java
rename to ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/SparkMetricsUtils.java
index eff208ade8..16ce073400 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/SparkJobUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/SparkMetricsUtils.java
@@ -23,7 +23,7 @@
 import org.apache.hive.spark.client.metrics.Metrics;
 import org.apache.hive.spark.client.metrics.ShuffleReadMetrics;
 
-public class SparkJobUtils {
+final class SparkMetricsUtils {
 
   private final static String EXECUTOR_DESERIALIZE_TIME = "ExecutorDeserializeTime";
   private final static String EXECUTOR_RUN_TIME = "ExecutorRunTime";
@@ -31,7 +31,7 @@ public class SparkJobUtils {
   private final static String JVM_GC_TIME = "JvmGCTime";
   private final static String RESULT_SERIALIZATION_TIME = "ResultSerializationTime";
   private final static String MEMORY_BYTES_SPLIED = "MemoryBytesSpilled";
-  private final static String DISK_BYTES_SPLIED = "DiskBytesSpilled";
+  private final static String DISK_BYTES_SPILLED = "DiskBytesSpilled";
   private final static String BYTES_READ = "BytesRead";
   private final static String REMOTE_BLOCKS_FETCHED = "RemoteBlocksFetched";
   private final static String LOCAL_BLOCKS_FETCHED = "LocalBlocksFetched";
@@ -41,7 +41,9 @@ public class SparkJobUtils {
   private final static String SHUFFLE_BYTES_WRITTEN = "ShuffleBytesWritten";
   private final static String SHUFFLE_WRITE_TIME = "ShuffleWriteTime";
 
-  public Map<String, Long> collectMetrics(Metrics allMetrics) {
+  private SparkMetricsUtils(){}
+
+  static Map<String, Long> collectMetrics(Metrics allMetrics) {
     Map<String, Long> results = new LinkedHashMap<String, Long>();
     results.put(EXECUTOR_DESERIALIZE_TIME, allMetrics.executorDeserializeTime);
     results.put(EXECUTOR_RUN_TIME, allMetrics.executorRunTime);
@@ -49,7 +51,7 @@ public Map<String, Long> collectMetrics(Metrics allMetrics) {
     results.put(JVM_GC_TIME, allMetrics.jvmGCTime);
     results.put(RESULT_SERIALIZATION_TIME, allMetrics.resultSerializationTime);
     results.put(MEMORY_BYTES_SPLIED, allMetrics.memoryBytesSpilled);
-    results.put(DISK_BYTES_SPLIED, allMetrics.diskBytesSpilled);
+    results.put(DISK_BYTES_SPILLED, allMetrics.diskBytesSpilled);
     if (allMetrics.inputMetrics != null) {
       results.put(BYTES_READ, allMetrics.inputMetrics.bytesRead);
     }
