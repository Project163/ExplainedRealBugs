diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/DefaultAccumuloRowIdFactory.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/DefaultAccumuloRowIdFactory.java
index 6d96d9b370..b51ff9b581 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/DefaultAccumuloRowIdFactory.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/serde/DefaultAccumuloRowIdFactory.java
@@ -77,7 +77,7 @@ public LazyObjectBase createRowId(ObjectInspector inspector) throws SerDeExcepti
 //    return LazyFactory.createLazyObject(inspector,
 //            ColumnEncoding.BINARY == rowIdMapping.getEncoding());
     return LazyFactory.createLazyObject(inspector,
-        inspector.getTypeName() != TypeInfoFactory.stringTypeInfo.getTypeName()
+            !TypeInfoFactory.stringTypeInfo.getTypeName().equals(inspector.getTypeName())
             && ColumnEncoding.BINARY == rowIdMapping.getEncoding());
   }
 
diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java
index ff10b05128..edd92e8d6f 100644
--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java
+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java
@@ -40,8 +40,7 @@ public static boolean isAuthorizationEnabled(Configuration conf) {
     // The recommended configuration is to use storage based authorization in metastore server.
     // However, if user define a custom V1 authorization, it will be honored.
     if (SessionState.get().getAuthorizer() == null ||
-        HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER)
-        == DefaultHiveAuthorizationProvider.class.getName()) {
+        DefaultHiveAuthorizationProvider.class.getName().equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER))) {
       LOG.info("Metastore authorizer is skipped for V2 authorizer or"
         + " DefaultHiveAuthorizationProvider");
       return false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableDummyOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableDummyOperator.java
index 2075d9b57a..537e39b7ed 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableDummyOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableDummyOperator.java
@@ -79,10 +79,9 @@ public OperatorType getType() {
 
   @Override
   public boolean equals(Object obj) {
-    return super.equals(obj) || (obj instanceof HashTableDummyOperator)
-        && (((HashTableDummyOperator)obj).operatorId == operatorId);
+    return super.equals(obj) || (obj instanceof HashTableDummyOperator) && ((HashTableDummyOperator)obj).operatorId.equals(operatorId);
   }
-  
+
   @Override
   public int hashCode() {
     return operatorId.hashCode();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/errors/ErrorAndSolution.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/errors/ErrorAndSolution.java
index bc07035de6..bc07b2b53b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/errors/ErrorAndSolution.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/errors/ErrorAndSolution.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.errors;
 
+import java.util.Objects;
+
 /**
  * Immutable class for storing a possible error and a resolution suggestion.
  */
@@ -45,8 +47,7 @@ public boolean equals(Object o) {
       return false;
     }
     ErrorAndSolution e = (ErrorAndSolution)o;
-
-    return e.error == this.error && e.solution == this.solution;
+    return Objects.equals(e.error, error) && Objects.equals(e.solution, solution);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
index 9a7e9d93c9..5256c46bc9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java
@@ -610,7 +610,7 @@ private List<CombineFileSplit> sampleSplits(List<CombineFileSplit> splits) {
         // 2. the alias it serves is not sampled
         // 3. it serves different alias than another path for the same split
         if (l.size() != 1 || !nameToSamples.containsKey(l.get(0)) ||
-            (alias != null && l.get(0) != alias)) {
+            (alias != null && !alias.equals(l.get(0)))) {
           alias = null;
           break;
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java
index 56c016311d..5dff242a17 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMaterializedViewsRegistry.java
@@ -24,6 +24,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
@@ -74,7 +75,7 @@
 
 import com.google.common.collect.ImmutableList;
 
-/** 
+/**
  * Registry for materialized views. The goal of this cache is to avoid parsing and creating
  * logical plans for the materialized views at query runtime. When a query arrives, we will
  * just need to consult this cache and extract the logical plans for the views (which had
@@ -358,8 +359,7 @@ public boolean equals(Object obj) {
         return false;
       }
       ViewKey viewKey = (ViewKey) obj;
-      return creationDate == viewKey.creationDate &&
-          (viewName == viewKey.viewName || (viewName != null && viewName.equals(viewKey.viewName)));
+      return creationDate == viewKey.creationDate && Objects.equals(viewName, viewKey.viewName);
     }
 
     @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
index c6b34d46c4..a9198a1c70 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java
@@ -127,9 +127,9 @@ public static GenericUDF getHiveUDF(SqlOperator op, RelDataType dt, int argsLeng
     }
     // Make sure we handle unary + and - correctly.
     if (argsLength == 1) {
-      if (name == "+") {
+      if ("+".equals(name)) {
         name = FunctionRegistry.UNARY_PLUS_FUNC_NAME;
-      } else if (name == "-") {
+      } else if ("-".equals(name)) {
         name = FunctionRegistry.UNARY_MINUS_FUNC_NAME;
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
index 2b575b571d..1671773d4a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
@@ -913,7 +913,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       GroupByDesc gbDesc = gbOp.getConf();
       ArrayList<AggregationDesc> aggregationDescs = gbDesc.getAggregators();
       for (AggregationDesc agg : aggregationDescs) {
-        if (agg.getGenericUDAFName() != "bloom_filter") {
+        if (!"bloom_filter".equals(agg.getGenericUDAFName())) {
           continue;
         }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java
index d5c3a1a288..5d62c9159b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java
@@ -23,6 +23,7 @@
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Set;
 
 import org.slf4j.Logger;
@@ -104,7 +105,7 @@ private void initUserRoles() throws HiveAuthzPluginException {
     // the interface. this helps in being able to switch the user within a session.
     // so we need to check if the user has changed
     String newUserName = authenticator.getUserName();
-    if(currentUserName == newUserName){
+    if (Objects.equals(currentUserName, newUserName)) {
       //no need to (re-)initialize the currentUserName, currentRoles fields
       return;
     }
diff --git a/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java b/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java
index b87ec55c67..19b121c54a 100644
--- a/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java
+++ b/vector-code-gen/src/org/apache/hadoop/hive/tools/GenVectorCode.java
@@ -1428,10 +1428,10 @@ private void generateFilterTruncStringColumnBetween(String[] tdesc) throws IOExc
     String truncStringTypeName = tdesc[1];
     String truncStringHiveType;
     String truncStringHiveGetBytes;
-    if (truncStringTypeName == "Char") {
+    if ("Char".equals(truncStringTypeName)) {
       truncStringHiveType = "HiveChar";
       truncStringHiveGetBytes = "getStrippedValue().getBytes()";
-    } else if (truncStringTypeName == "VarChar") {
+    } else if ("VarChar".equals(truncStringTypeName)) {
       truncStringHiveType = "HiveVarchar";
       truncStringHiveGetBytes = "getValue().getBytes()";
     } else {
@@ -2048,10 +2048,10 @@ private void generateStringCompareTruncStringScalar(String[] tdesc, String class
     String truncStringTypeName = tdesc[1];
     String truncStringHiveType;
     String truncStringHiveGetBytes;
-    if (truncStringTypeName == "Char") {
+    if ("Char".equals(truncStringTypeName)) {
       truncStringHiveType = "HiveChar";
       truncStringHiveGetBytes = "getStrippedValue().getBytes()";
-    } else if (truncStringTypeName == "VarChar") {
+    } else if ("VarChar".equals(truncStringTypeName)) {
       truncStringHiveType = "HiveVarchar";
       truncStringHiveGetBytes = "getValue().getBytes()";
     } else {
