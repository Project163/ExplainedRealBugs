diff --git a/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index 5b5dbdac37..643f537cf9 100644
--- a/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/common/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -608,8 +608,6 @@ public enum ErrorMsg {
   SPARK_JOB_RUNTIME_ERROR(40001, "Spark job failed due to: {0}", true),
   SPARK_TASK_RUNTIME_ERROR(40002, "Spark job failed due to task failures: {0}", true),
   REPL_DATABASE_IS_TARGET_OF_REPLICATION(40003, "Cannot dump database as it is a Target of replication."),
-  REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION(40004,
-                                               "Source of replication (repl.source.for) is not set in the database properties."),
   REPL_INVALID_DB_OR_TABLE_PATTERN(40005,
                                      "Invalid pattern for the DB or table name in the replication policy. "
                                      + "It should be a valid regex enclosed within single or double quotes."),
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
index 4b5452535f..05f52e716b 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java
@@ -4083,21 +4083,6 @@ public void testDumpWithPartitionDirMissing() throws IOException {
     run("drop database " + dbName, true, driver);
   }
 
-  @Test
-  public void testDumpNonReplDatabase() throws IOException {
-    String dbName = createDBNonRepl(testName.getMethodName(), driver);
-    verifyFail("REPL DUMP " + dbName, driver);
-    verifyFail("REPL DUMP " + dbName, driver);
-    assertTrue(run("REPL DUMP " + dbName + " with ('hive.repl.dump.metadata.only' = 'true')",
-            true, driver));
-    //Dump again before load will print a warning
-    assertTrue(run("REPL DUMP " + dbName + " with ('hive.repl.dump.metadata.only' = 'true')",
-            true, driver));
-    dbName = createDBNonRepl(testName.getMethodName() + "_case", driver);
-    run("alter database " + dbName + " set dbproperties ('repl.source.for' = '1, 2, 3')", driver);
-    assertTrue(run("REPL DUMP " + dbName, true, driver));
-  }
-
   @Test
   public void testRecycleFileNonReplDatabase() throws IOException {
     String dbName = createDBNonRepl(testName.getMethodName(), driver);
@@ -4220,6 +4205,48 @@ public void testMoveOptimizationIncremental() throws IOException {
     verifyRun("SELECT count(*) from " + replDbName + ".unptned_late ", "3", driverMirror);
   }
 
+  @Test
+  public void testPolicyIdImplicitly() throws Exception {
+    // Create a database.
+    String name = testName.getMethodName();
+    String dbName = createDB(name, driver);
+
+    // Remove SOURCE_OF_REPLICATION property.
+    run("ALTER DATABASE " + name + " Set DBPROPERTIES ( '"
+        + SOURCE_OF_REPLICATION + "' = '')", driver);
+
+    // Create a table with some data.
+    run("CREATE TABLE " + dbName + ".dataTable(a string) STORED AS TEXTFILE",
+        driver);
+
+    String[] unptn_data = new String[] {"eleven", "twelve"};
+    String unptn_locn = new Path(TEST_PATH, name + "_unptn").toUri().getPath();
+    createTestDataFile(unptn_locn, unptn_data);
+
+    run("LOAD DATA LOCAL INPATH '" + unptn_locn + "' OVERWRITE INTO TABLE "
+        + dbName + ".dataTable", driver);
+
+    // Perform Dump & Load and verify the data is replicated properly.
+    String replicatedDbName = dbName + "_dupe";
+
+    Tuple bootstrapDump = bootstrapLoadAndVerify(dbName, replicatedDbName);
+    FileSystem fs = new Path(bootstrapDump.dumpLocation).getFileSystem(hconf);
+    Path dumpPath =
+        new Path(bootstrapDump.dumpLocation, ReplUtils.REPL_HIVE_BASE_DIR);
+    assertTrue(fs.exists(new Path(dumpPath, DUMP_ACKNOWLEDGEMENT.toString())));
+    assertTrue(fs.exists(new Path(dumpPath, LOAD_ACKNOWLEDGEMENT.toString())));
+    verifyRun("SELECT * from " + replicatedDbName + ".dataTable", unptn_data,
+        driverMirror);
+
+    // Check the value of SOURCE_OF_REPLICATION in the database, it should
+    // get set automatically.
+    run("DESCRIBE DATABASE EXTENDED " + dbName, driver);
+    List<String> result = getOutput(driver);
+    System.out.print(result);
+    assertTrue(result.get(0),
+        result.get(0).contains("repl.source.for=default_REPL DUMP " + dbName));
+  }
+
   private static String createDB(String name, IDriver myDriver) {
     LOG.info("Testing " + name);
     String mgdLocation = System.getProperty("test.warehouse.dir", "file:/tmp/warehouse/managed");
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
index 51e9ee90da..0ee0241278 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestScheduledReplicationScenarios.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService;
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.test.GenericTestUtils;
 import org.junit.Before;
 import org.junit.After;
 import org.junit.Assert;
@@ -53,6 +54,9 @@
 import java.util.List;
 import java.util.ArrayList;
 
+import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;
+import static org.junit.Assert.assertTrue;
+
 
 /**
  * TestScheduledReplicationScenarios - test scheduled replication .
@@ -245,6 +249,72 @@ public void testExternalTablesReplLoadBootstrapIncr() throws Throwable {
     }
   }
 
+  @Test
+  public void testSetPolicyId() throws Throwable {
+    String withClause =
+        " WITH('" + HiveConf.ConfVars.HIVE_IN_TEST + "' = 'true'" + ",'"
+            + HiveConf.ConfVars.REPL_SOURCE_CLUSTER_NAME + "' = 'cluster0'"
+            + ",'" + HiveConf.ConfVars.REPL_TARGET_CLUSTER_NAME
+            + "' = 'cluster1')";
+
+    // Create a table with some data at source DB.
+    primary.run("use " + primaryDbName).run("create table t2 (id int)")
+        .run("insert into t2 values(1)").run("insert into t2 values(2)");
+
+    // Remove the SOURCE_OF_REPLICATION property from the database.
+    primary.run("ALTER DATABASE " + primaryDbName + " Set DBPROPERTIES ( '"
+        + SOURCE_OF_REPLICATION + "' = '')");
+
+    // Schedule Dump & Load and verify the data is replicated properly.
+    try (ScheduledQueryExecutionService schqS = ScheduledQueryExecutionService
+        .startScheduledQueryExecutorService(primary.hiveConf)) {
+      int next = -1;
+      ReplDumpWork.injectNextDumpDirForTest(String.valueOf(next), true);
+      primary.run("create scheduled query s1_t2 every 5 seconds as repl dump "
+          + primaryDbName + withClause);
+      replica.run("create scheduled query s2_t2 every 5 seconds as repl load "
+          + primaryDbName + " INTO " + replicatedDbName + withClause);
+      Path dumpRoot =
+          new Path(primary.hiveConf.getVar(HiveConf.ConfVars.REPLDIR),
+              Base64.getEncoder().encodeToString(primaryDbName.toLowerCase()
+                  .getBytes(StandardCharsets.UTF_8.name())));
+      FileSystem fs = FileSystem.get(dumpRoot.toUri(), primary.hiveConf);
+      next = Integer.parseInt(ReplDumpWork.getTestInjectDumpDir()) + 1;
+      Path ackPath = new Path(dumpRoot,
+          String.valueOf(next) + File.separator + ReplUtils.REPL_HIVE_BASE_DIR
+              + File.separator + ReplAck.LOAD_ACKNOWLEDGEMENT.toString());
+      waitForAck(fs, ackPath, DEFAULT_PROBE_TIMEOUT);
+      replica.run("use " + replicatedDbName).run("show tables like 't2'")
+          .verifyResult("t2").run("select id from t2 order by id")
+          .verifyResults(new String[] {"1", "2"});
+
+      // Check the database got the SOURCE_OF_REPLICATION property set.
+      primary.run("DESCRIBE DATABASE EXTENDED " + primaryDbName);
+      String result = primary.getOutput().get(0);
+      assertTrue(result, result.contains("repl.source.for=s1_t2"));
+
+      // Test the new policy id is appended
+      primary.run("drop scheduled query s1_t2");
+      fs.delete(new Path(dumpRoot, String.valueOf(next)), true);
+      primary.run(
+          "create scheduled query s1_t2_new every 5 seconds as repl " + "dump "
+              + primaryDbName + withClause);
+
+      GenericTestUtils.waitFor(() -> {
+        try {
+          primary.run("DESCRIBE DATABASE EXTENDED " + primaryDbName);
+          return primary.getOutput().get(0)
+              .contains("repl.source.for=s1_t2, s1_t2_new");
+        } catch (Throwable e) {
+          return false;
+        }
+      }, 100, 10000);
+    } finally {
+      primary.run("drop scheduled query s1_t2_new");
+      replica.run("drop scheduled query s2_t2");
+    }
+  }
+
   private void checkMetrics(List<ReplicationMetric> expectedReplicationMetrics,
                             List<ReplicationMetric> actualMetrics) {
     Assert.assertEquals(expectedReplicationMetrics.size(), actualMetrics.size());
diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
index 613887c609..c6d896874a 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
@@ -3109,20 +3109,6 @@ public void testCreateTableExecAsync() throws Exception {
     stmt.close();
   }
 
-  @Test
-  public void testReplErrorScenarios() throws Exception {
-    HiveStatement stmt = (HiveStatement) con.createStatement();
-
-    try {
-      // source of replication not set
-      stmt.execute("repl dump default");
-    } catch(SQLException e){
-      assertTrue(e.getErrorCode() == ErrorMsg.REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION.getErrorCode());
-    }
-
-    stmt.close();
-  }
-
   /**
    * Test {@link HiveStatement#executeAsync(String)} for an insert overwrite into a table
    * @throws Exception
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
index 6c64b5a6d1..8c2bd0543b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.hive.metastore.messaging.event.filters.ReplEventFilter;
 import org.apache.hadoop.hive.metastore.utils.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.utils.SecurityUtils;
+import org.apache.hadoop.hive.metastore.utils.StringUtils;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
@@ -110,7 +111,10 @@
 import java.util.concurrent.Callable;
 import java.util.concurrent.TimeUnit;
 
+import static org.apache.hadoop.hive.conf.Constants.SCHEDULED_QUERY_SCHEDULENAME;
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_BOOTSTRAP_DUMP_ABORT_WRITE_TXN_AFTER_TIMEOUT;
+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY;
+import static org.apache.hadoop.hive.metastore.ReplChangeManager.getReplPolicyIdString;
 import static org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.Writer;
 import static org.apache.hadoop.hive.ql.exec.repl.ReplAck.LOAD_ACKNOWLEDGEMENT;
 import static org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.RANGER_AUTHORIZER;
@@ -430,7 +434,7 @@ private boolean previousReplScopeModified() {
    */
   private boolean shouldDumpExternalTableLocation() {
     return conf.getBoolVar(HiveConf.ConfVars.REPL_INCLUDE_EXTERNAL_TABLES)
-            && (!conf.getBoolVar(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY) &&
+            && (!conf.getBoolVar(REPL_DUMP_METADATA_ONLY) &&
             !conf.getBoolVar(HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY_FOR_EXTERNAL_TABLE));
   }
 
@@ -798,7 +802,7 @@ private void dumpEvent(NotificationEvent ev, Path evRoot, Path dumpRoot, Path cm
   private ReplicationSpec getNewEventOnlyReplicationSpec(Long eventId) {
     ReplicationSpec rspec =
         getNewReplicationSpec(eventId.toString(), eventId.toString(), conf.getBoolean(
-            HiveConf.ConfVars.REPL_DUMP_METADATA_ONLY.varname, false));
+            REPL_DUMP_METADATA_ONLY.varname, false));
     rspec.setReplSpecType(ReplicationSpec.Type.INCREMENTAL_DUMP);
     return rspec;
   }
@@ -873,6 +877,27 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)
           throw new HiveException("Replication dump not allowed for replicated database" +
                   " with first incremental dump pending : " + dbName);
         }
+
+        if (db != null && !HiveConf.getBoolVar(conf, REPL_DUMP_METADATA_ONLY)) {
+          if (!ReplChangeManager.isSourceOfReplication(db)) {
+            // Check if the schedule name is available else set the query value
+            // as default.
+            String value = conf.get(SCHEDULED_QUERY_SCHEDULENAME,
+                "default_" + getQueryState().getQueryString());
+            updateReplSourceFor(hiveDb, dbName, db, value);
+          } else {
+            // If a schedule name is available and that isn't part of the
+            // existing conf, append the schedule name to the conf.
+            String scheduleQuery = conf.get(SCHEDULED_QUERY_SCHEDULENAME);
+            if (!StringUtils.isEmpty(scheduleQuery)) {
+              if (!getReplPolicyIdString(db).contains(scheduleQuery)) {
+                updateReplSourceFor(hiveDb, dbName, db,
+                    getReplPolicyIdString(db) + ", " + scheduleQuery);
+              }
+            }
+          }
+        }
+
         int estimatedNumTables = Utils.getAllTables(hiveDb, dbName, work.replScope).size();
         int estimatedNumFunctions = hiveDb.getFunctions(dbName, "*").size();
         replLogger = new BootstrapDumpLogger(dbName, dumpRoot.toString(),
@@ -961,6 +986,18 @@ Long bootStrapDump(Path dumpRoot, DumpMetaData dmd, Path cmRoot, Hive hiveDb)
     }
   }
 
+  private void updateReplSourceFor(Hive hiveDb, String dbName, Database db,
+      String value) throws HiveException {
+    Map<String, String> params = db.getParameters();
+    if (params != null) {
+      params.put("repl.source.for", value);
+      db.setParameters(params);
+    } else {
+      db.setParameters(Collections.singletonMap("repl.source.for", value));
+    }
+    hiveDb.alterDatabase(dbName, db);
+  }
+
   private FileList createTableFileList(Path dumpRoot, String fileName, int cacheSize) {
     Path backingFile = new Path(dumpRoot, fileName);
     return new FileList(backingFile, cacheSize, conf);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
index 60ada73ae9..5562f5a549 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ReplicationSemanticAnalyzer.java
@@ -177,11 +177,6 @@ private void initReplDump(ASTNode ast) throws HiveException {
     for (String dbName : Utils.matchesDb(db, dbNameOrPattern)) {
       Database database = db.getDatabase(dbName);
       if (database != null) {
-        if (!isMetaDataOnly && !ReplChangeManager.isSourceOfReplication(database)) {
-          LOG.error("Cannot dump database " + dbNameOrPattern +
-                  " as it is not a source of replication (repl.source.for)");
-          throw new SemanticException(ErrorMsg.REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION.getMsg());
-        }
         if (ReplUtils.isTargetOfReplication(database)) {
           LOG.error("Cannot dump database " + dbNameOrPattern + " as it is a target of replication (repl.target.for)");
           throw new SemanticException(ErrorMsg.REPL_DATABASE_IS_TARGET_OF_REPLICATION.getMsg());
