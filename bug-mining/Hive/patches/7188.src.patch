diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index 90cb007057..05bdb24cf3 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -540,6 +540,7 @@ minillaplocal.query.files=\
   get_splits_0.q,\
   groupby2.q,\
   groupby_groupingset_bug.q,\
+  groupby_rollup_empty2.q,\
   hybridgrace_hashjoin_1.q,\
   hybridgrace_hashjoin_2.q,\
   is_distinct_from.q,\
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java
index e9f93d4c02..ca1472219b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DynamicPartitionPruner.java
@@ -43,6 +43,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.MapWork;
@@ -263,12 +264,12 @@ protected void prunePartitionSingleSource(String source, SourceInfo si)
     ExprNodeEvaluator eval = ExprNodeEvaluatorFactory.get(si.partKey);
     eval.initialize(soi);
 
-    applyFilterToPartitions(converter, eval, columnName, values);
+    applyFilterToPartitions(converter, eval, columnName, values, si.mustKeepOnePartition);
   }
 
   @SuppressWarnings("rawtypes")
   private void applyFilterToPartitions(Converter converter, ExprNodeEvaluator eval,
-      String columnName, Set<Object> values) throws HiveException {
+      String columnName, Set<Object> values, boolean mustKeepOnePartition) throws HiveException {
 
     Object[] row = new Object[1];
 
@@ -297,7 +298,7 @@ private void applyFilterToPartitions(Converter converter, ExprNodeEvaluator eval
         LOG.debug("part key expr applied: " + partValue);
       }
 
-      if (!values.contains(partValue)) {
+      if (!values.contains(partValue) && (!mustKeepOnePartition || work.getPathToPartitionInfo().size() > 1)) {
         LOG.info("Pruning path: " + p);
         it.remove();
         // work.removePathToPartitionInfo(p);
@@ -328,6 +329,7 @@ static class SourceInfo {
     public AtomicBoolean skipPruning = new AtomicBoolean();
     public final String columnName;
     public final String columnType;
+    private boolean mustKeepOnePartition;
 
     @VisibleForTesting // Only used for testing.
     SourceInfo(TableDesc table, ExprNodeDesc partKey, String columnName, String columnType, JobConf jobConf, Object forTesting) {
@@ -349,6 +351,7 @@ public SourceInfo(TableDesc table, ExprNodeDesc partKey, String columnName, Stri
 
       this.columnName = columnName;
       this.columnType = columnType;
+      this.mustKeepOnePartition = jobConf.getBoolean(Utilities.ENSURE_OPERATORS_EXECUTED, false);
 
       deserializer = ReflectionUtils.newInstance(table.getDeserializerClass(), null);
       deserializer.initialize(jobConf, table.getProperties());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
index 086e59f255..4bd4a24195 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java
@@ -54,7 +54,6 @@
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
@@ -546,7 +545,9 @@ public static JobConf createConfForMmOriginalsSplit(
 
   protected ValidWriteIdList getMmValidWriteIds(
       JobConf conf, TableDesc table, ValidWriteIdList validWriteIdList) throws IOException {
-    if (!AcidUtils.isInsertOnlyTable(table.getProperties())) return null;
+    if (!AcidUtils.isInsertOnlyTable(table.getProperties())) {
+      return null;
+    }
     if (validWriteIdList == null) {
       validWriteIdList = AcidUtils.getTableValidWriteIdList( conf, table.getTableName());
       if (validWriteIdList == null) {
@@ -635,7 +636,7 @@ private static void processForWriteIds(Path dir, Configuration conf,
       }
     }
   }
- 
+
 
   Path[] getInputPaths(JobConf job) throws IOException {
     Path[] dirs;
@@ -819,7 +820,7 @@ public static void pushFilters(JobConf jobConf, TableScanOperator tableScan,
     Utilities.setColumnNameList(jobConf, tableScan);
     Utilities.setColumnTypeList(jobConf, tableScan);
     // push down filters
-    ExprNodeGenericFuncDesc filterExpr = (ExprNodeGenericFuncDesc)scanDesc.getFilterExpr();
+    ExprNodeGenericFuncDesc filterExpr = scanDesc.getFilterExpr();
     if (filterExpr == null) {
       return;
     }
diff --git a/ql/src/test/queries/clientpositive/groupby_rollup_empty2.q b/ql/src/test/queries/clientpositive/groupby_rollup_empty2.q
new file mode 100644
index 0000000000..4b79f5be89
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/groupby_rollup_empty2.q
@@ -0,0 +1,18 @@
+set hive.auto.convert.join=true;
+drop table if exists store_sales_s0;
+drop table if exists store_s0;
+
+CREATE TABLE store_sales_s0 (ss_item_sk int,payload string,payload2 string,payload3 string) PARTITIONED BY (ss_store_sk int) stored as orc TBLPROPERTIES( 'transactional'='false');
+CREATE TABLE store_s0 (s_item_sk int,s_store_sk int,s_state string) stored as orc TBLPROPERTIES( 'transactional'='false');
+
+insert into store_s0 values
+    (1,10,'XX'),
+    (2,20,'AA'),
+    (3,30,'ZZ')
+    ;
+insert into store_sales_s0 partition(ss_store_sk=9) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx');
+insert into store_sales_s0 partition(ss_store_sk=39) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx');
+
+explain select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state);
+select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state);
+
diff --git a/ql/src/test/results/clientpositive/llap/groupby_rollup_empty2.q.out b/ql/src/test/results/clientpositive/llap/groupby_rollup_empty2.q.out
new file mode 100644
index 0000000000..2420e379ac
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/groupby_rollup_empty2.q.out
@@ -0,0 +1,207 @@
+PREHOOK: query: drop table if exists store_sales_s0
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists store_sales_s0
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table if exists store_s0
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists store_s0
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE store_sales_s0 (ss_item_sk int,payload string,payload2 string,payload3 string) PARTITIONED BY (ss_store_sk int) stored as orc TBLPROPERTIES( 'transactional'='false')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@store_sales_s0
+POSTHOOK: query: CREATE TABLE store_sales_s0 (ss_item_sk int,payload string,payload2 string,payload3 string) PARTITIONED BY (ss_store_sk int) stored as orc TBLPROPERTIES( 'transactional'='false')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@store_sales_s0
+PREHOOK: query: CREATE TABLE store_s0 (s_item_sk int,s_store_sk int,s_state string) stored as orc TBLPROPERTIES( 'transactional'='false')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@store_s0
+POSTHOOK: query: CREATE TABLE store_s0 (s_item_sk int,s_store_sk int,s_state string) stored as orc TBLPROPERTIES( 'transactional'='false')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@store_s0
+PREHOOK: query: insert into store_s0 values
+    (1,10,'XX'),
+    (2,20,'AA'),
+    (3,30,'ZZ')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@store_s0
+POSTHOOK: query: insert into store_s0 values
+    (1,10,'XX'),
+    (2,20,'AA'),
+    (3,30,'ZZ')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@store_s0
+POSTHOOK: Lineage: store_s0.s_item_sk SCRIPT []
+POSTHOOK: Lineage: store_s0.s_state SCRIPT []
+POSTHOOK: Lineage: store_s0.s_store_sk SCRIPT []
+PREHOOK: query: insert into store_sales_s0 partition(ss_store_sk=9) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@store_sales_s0@ss_store_sk=9
+POSTHOOK: query: insert into store_sales_s0 partition(ss_store_sk=9) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@store_sales_s0@ss_store_sk=9
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=9).payload SCRIPT []
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=9).payload2 SCRIPT []
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=9).payload3 SCRIPT []
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=9).ss_item_sk SCRIPT []
+PREHOOK: query: insert into store_sales_s0 partition(ss_store_sk=39) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@store_sales_s0@ss_store_sk=39
+POSTHOOK: query: insert into store_sales_s0 partition(ss_store_sk=39) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@store_sales_s0@ss_store_sk=39
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=39).payload SCRIPT []
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=39).payload2 SCRIPT []
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=39).payload3 SCRIPT []
+POSTHOOK: Lineage: store_sales_s0 PARTITION(ss_store_sk=39).ss_item_sk SCRIPT []
+PREHOOK: query: explain select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@store_s0
+PREHOOK: Input: default@store_sales_s0
+PREHOOK: Input: default@store_sales_s0@ss_store_sk=39
+PREHOOK: Input: default@store_sales_s0@ss_store_sk=9
+#### A masked pattern was here ####
+POSTHOOK: query: explain select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@store_s0
+POSTHOOK: Input: default@store_sales_s0
+POSTHOOK: Input: default@store_sales_s0@ss_store_sk=39
+POSTHOOK: Input: default@store_sales_s0@ss_store_sk=9
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Map 2 <- Map 1 (BROADCAST_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: store_s0
+                  filterExpr: ((s_state) IN ('SD', 'FL', 'MI', 'LA', 'MO', 'SC') and s_store_sk is not null) (type: boolean)
+                  Statistics: Num rows: 3 Data size: 270 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: ((s_state) IN ('SD', 'FL', 'MI', 'LA', 'MO', 'SC') and s_store_sk is not null) (type: boolean)
+                    Statistics: Num rows: 3 Data size: 270 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: s_store_sk (type: int), s_state (type: string)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 3 Data size: 270 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 3 Data size: 270 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col1 (type: string)
+                      Select Operator
+                        expressions: _col0 (type: int)
+                        outputColumnNames: _col0
+                        Statistics: Num rows: 3 Data size: 270 Basic stats: COMPLETE Column stats: COMPLETE
+                        Group By Operator
+                          keys: _col0 (type: int)
+                          minReductionHashAggr: 0.6666666
+                          mode: hash
+                          outputColumnNames: _col0
+                          Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+                          Dynamic Partitioning Event Operator
+                            Target column: ss_store_sk (int)
+                            Target Input: store_sales_s0
+                            Partition key expr: ss_store_sk
+                            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+                            Target Vertex: Map 2
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: store_sales_s0
+                  filterExpr: ss_store_sk is not null (type: boolean)
+                  Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: ss_item_sk (type: int), ss_store_sk (type: int)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE
+                    Map Join Operator
+                      condition map:
+                           Inner Join 0 to 1
+                      keys:
+                        0 _col0 (type: int)
+                        1 _col1 (type: int)
+                      outputColumnNames: _col1, _col2
+                      input vertices:
+                        0 Map 1
+                      Statistics: Num rows: 10 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
+                      Select Operator
+                        expressions: _col2 (type: int), _col1 (type: string)
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 10 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE
+                        Group By Operator
+                          keys: _col0 (type: int), _col1 (type: string), 0L (type: bigint)
+                          minReductionHashAggr: 0.19999999
+                          mode: hash
+                          outputColumnNames: _col0, _col1, _col2
+                          Statistics: Num rows: 15 Data size: 1470 Basic stats: COMPLETE Column stats: COMPLETE
+                          Reduce Output Operator
+                            key expressions: _col0 (type: int), _col1 (type: string), _col2 (type: bigint)
+                            sort order: +++
+                            Map-reduce partition columns: _col0 (type: int), _col1 (type: string), _col2 (type: bigint)
+                            Statistics: Num rows: 15 Data size: 1470 Basic stats: COMPLETE Column stats: COMPLETE
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+        Reducer 3 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: int), KEY._col1 (type: string), KEY._col2 (type: bigint)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 15 Data size: 1470 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: grouping(_col2, 0L) (type: bigint)
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 15 Data size: 120 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 15 Data size: 120 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@store_s0
+PREHOOK: Input: default@store_sales_s0
+PREHOOK: Input: default@store_sales_s0@ss_store_sk=39
+PREHOOK: Input: default@store_sales_s0@ss_store_sk=9
+#### A masked pattern was here ####
+POSTHOOK: query: select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@store_s0
+POSTHOOK: Input: default@store_sales_s0
+POSTHOOK: Input: default@store_sales_s0@ss_store_sk=39
+POSTHOOK: Input: default@store_sales_s0@ss_store_sk=9
+#### A masked pattern was here ####
+1
