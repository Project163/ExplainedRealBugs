diff --git a/itests/hive-unit-hadoop2/pom.xml b/itests/hive-unit-hadoop2/pom.xml
index fbf1c736ed..44135d62e3 100644
--- a/itests/hive-unit-hadoop2/pom.xml
+++ b/itests/hive-unit-hadoop2/pom.xml
@@ -180,6 +180,12 @@
       <version>${hadoop.version}</version>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-common</artifactId>
+      <version>${hadoop.version}</version>
+      <scope>test</scope>
+    </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-server</artifactId>
diff --git a/itests/hive-unit/pom.xml b/itests/hive-unit/pom.xml
index b241daa4b2..cd209b4532 100644
--- a/itests/hive-unit/pom.xml
+++ b/itests/hive-unit/pom.xml
@@ -211,6 +211,12 @@
       <version>${hadoop.version}</version>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-common</artifactId>
+      <version>${hadoop.version}</version>
+      <scope>test</scope>
+    </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-server</artifactId>
@@ -352,6 +358,18 @@
       <version>${hadoop.version}</version>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-common</artifactId>
+      <version>${hadoop.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-api</artifactId>
+      <version>${hadoop.version}</version>
+      <scope>test</scope>
+    </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-yarn-registry</artifactId>
diff --git a/itests/qtest/pom.xml b/itests/qtest/pom.xml
index ed44bb8681..7fc72b9ee3 100644
--- a/itests/qtest/pom.xml
+++ b/itests/qtest/pom.xml
@@ -247,6 +247,18 @@
       <version>${hadoop.version}</version>
       <scope>test</scope>
     </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-common</artifactId>
+      <version>${hadoop.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-yarn-api</artifactId>
+      <version>${hadoop.version}</version>
+      <scope>test</scope>
+    </dependency>
     <dependency>
       <groupId>org.apache.hbase</groupId>
       <artifactId>hbase-common</artifactId>
diff --git a/pom.xml b/pom.xml
index 9ed1c19f3e..4c41200ffc 100644
--- a/pom.xml
+++ b/pom.xml
@@ -132,7 +132,7 @@
     <dropwizard-metrics-hadoop-metrics2-reporter.version>0.1.2</dropwizard-metrics-hadoop-metrics2-reporter.version>
     <guava.version>14.0.1</guava.version>
     <groovy.version>2.4.4</groovy.version>
-    <hadoop.version>2.6.1</hadoop.version>
+    <hadoop.version>2.7.2</hadoop.version>
     <hadoop.bin.path>${basedir}/${hive.path.to.root}/testutils/hadoop</hadoop.bin.path>
     <hbase.version>1.1.1</hbase.version>
     <!-- required for logging test to avoid including hbase which pulls disruptor transitively -->
@@ -141,7 +141,7 @@
     <httpcomponents.client.version>4.4</httpcomponents.client.version>
     <httpcomponents.core.version>4.4</httpcomponents.core.version>
     <ivy.version>2.4.0</ivy.version>
-    <jackson.version>1.9.2</jackson.version>
+    <jackson.version>1.9.13</jackson.version>
     <!-- jackson 1 and 2 lines can coexist without issue, as they have different artifactIds -->
     <jackson.new.version>2.4.2</jackson.new.version>
     <jasper.version>5.5.23</jasper.version>
@@ -187,7 +187,7 @@
     <zookeeper.version>3.4.6</zookeeper.version>
     <jpam.version>1.1</jpam.version>
     <felix.version>2.4.0</felix.version>
-    <curator.version>2.6.0</curator.version>
+    <curator.version>2.7.1</curator.version>
     <jsr305.version>3.0.0</jsr305.version>
     <tephra.version>0.6.0</tephra.version>
     <gson.version>2.2.4</gson.version>
diff --git a/ql/src/test/queries/clientpositive/encryption_move_tbl.q b/ql/src/test/queries/clientpositive/encryption_move_tbl.q
index 5a8c0361d7..a25d9554f8 100644
--- a/ql/src/test/queries/clientpositive/encryption_move_tbl.q
+++ b/ql/src/test/queries/clientpositive/encryption_move_tbl.q
@@ -5,16 +5,27 @@
 set hive.cli.errors.ignore=true;
 
 DROP TABLE IF EXISTS encrypted_table PURGE;
+DROP DATABASE IF EXISTS encrypted_db;
 CREATE TABLE encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';
 CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
 CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;
 
+CREATE DATABASE encrypted_db LOCATION '${hiveconf:hive.metastore.warehouse.dir}/encrypted_db';
+CRYPTO CREATE_KEY --keyName key_128_2 --bitLength 128;
+CRYPTO CREATE_ZONE --keyName key_128_2 --path ${hiveconf:hive.metastore.warehouse.dir}/encrypted_db;
+
 INSERT OVERWRITE TABLE encrypted_table SELECT * FROM src;
 SHOW TABLES;
+-- should fail
+ALTER TABLE default.encrypted_table RENAME TO encrypted_db.encrypted_table_2;
+SHOW TABLES;
+-- should succeed in Hadoop 2.7 but fail in 2.6  (HDFS-7530)
 ALTER TABLE default.encrypted_table RENAME TO default.plain_table;
 SHOW TABLES;
 
-DROP TABLE encrypted_table PURGE;
 
+DROP TABLE encrypted_table PURGE;
+DROP TABLE default.plain_table PURGE;
+DROP DATABASE encrypted_db;
 CRYPTO DELETE_KEY --keyName key_128;
-
+CRYPTO DELETE_KEY --keyName key_128_2;
diff --git a/ql/src/test/results/clientpositive/encrypted/encryption_move_tbl.q.out b/ql/src/test/results/clientpositive/encrypted/encryption_move_tbl.q.out
index 1106880433..910ce25572 100644
--- a/ql/src/test/results/clientpositive/encrypted/encryption_move_tbl.q.out
+++ b/ql/src/test/results/clientpositive/encrypted/encryption_move_tbl.q.out
@@ -2,6 +2,10 @@ PREHOOK: query: DROP TABLE IF EXISTS encrypted_table PURGE
 PREHOOK: type: DROPTABLE
 POSTHOOK: query: DROP TABLE IF EXISTS encrypted_table PURGE
 POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP DATABASE IF EXISTS encrypted_db
+PREHOOK: type: DROPDATABASE
+POSTHOOK: query: DROP DATABASE IF EXISTS encrypted_db
+POSTHOOK: type: DROPDATABASE
 #### A masked pattern was here ####
 PREHOOK: type: CREATETABLE
 #### A masked pattern was here ####
@@ -14,6 +18,15 @@ POSTHOOK: Output: database:default
 POSTHOOK: Output: default@encrypted_table
 Encryption key created: 'key_128'
 Encryption zone created: '/build/ql/test/data/warehouse/default/encrypted_table' using key: 'key_128'
+#### A masked pattern was here ####
+PREHOOK: type: CREATEDATABASE
+PREHOOK: Output: database:encrypted_db
+#### A masked pattern was here ####
+POSTHOOK: type: CREATEDATABASE
+POSTHOOK: Output: database:encrypted_db
+#### A masked pattern was here ####
+Encryption key created: 'key_128_2'
+Encryption zone created: '/build/ql/test/data/warehouse/encrypted_db' using key: 'key_128_2'
 PREHOOK: query: INSERT OVERWRITE TABLE encrypted_table SELECT * FROM src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
@@ -32,11 +45,12 @@ POSTHOOK: type: SHOWTABLES
 POSTHOOK: Input: database:default
 encrypted_table
 src
-PREHOOK: query: ALTER TABLE default.encrypted_table RENAME TO default.plain_table
+PREHOOK: query: -- should fail
+ALTER TABLE default.encrypted_table RENAME TO encrypted_db.encrypted_table_2
 PREHOOK: type: ALTERTABLE_RENAME
 PREHOOK: Input: default@encrypted_table
 PREHOOK: Output: default@encrypted_table
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. Alter Table operation for default.encrypted_table failed to move data due to: '/build/ql/test/data/warehouse/default/encrypted_table can't be moved from an encryption zone.' See hive log file for details.
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. Alter Table operation for default.encrypted_table failed to move data due to: '/build/ql/test/data/warehouse/default/encrypted_table can't be moved from encryption zone /build/ql/test/data/warehouse/default/encrypted_table to encryption zone /build/ql/test/data/warehouse/encrypted_db.' See hive log file for details.
 PREHOOK: query: SHOW TABLES
 PREHOOK: type: SHOWTABLES
 PREHOOK: Input: database:default
@@ -45,11 +59,42 @@ POSTHOOK: type: SHOWTABLES
 POSTHOOK: Input: database:default
 encrypted_table
 src
-PREHOOK: query: DROP TABLE encrypted_table PURGE
-PREHOOK: type: DROPTABLE
+PREHOOK: query: -- should succeed in Hadoop 2.7 but fail in 2.6  (HDFS-7530)
+ALTER TABLE default.encrypted_table RENAME TO default.plain_table
+PREHOOK: type: ALTERTABLE_RENAME
 PREHOOK: Input: default@encrypted_table
 PREHOOK: Output: default@encrypted_table
-POSTHOOK: query: DROP TABLE encrypted_table PURGE
-POSTHOOK: type: DROPTABLE
+POSTHOOK: query: -- should succeed in Hadoop 2.7 but fail in 2.6  (HDFS-7530)
+ALTER TABLE default.encrypted_table RENAME TO default.plain_table
+POSTHOOK: type: ALTERTABLE_RENAME
 POSTHOOK: Input: default@encrypted_table
 POSTHOOK: Output: default@encrypted_table
+POSTHOOK: Output: default@plain_table
+PREHOOK: query: SHOW TABLES
+PREHOOK: type: SHOWTABLES
+PREHOOK: Input: database:default
+POSTHOOK: query: SHOW TABLES
+POSTHOOK: type: SHOWTABLES
+POSTHOOK: Input: database:default
+plain_table
+src
+PREHOOK: query: DROP TABLE encrypted_table PURGE
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE encrypted_table PURGE
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE default.plain_table PURGE
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@plain_table
+PREHOOK: Output: default@plain_table
+POSTHOOK: query: DROP TABLE default.plain_table PURGE
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@plain_table
+POSTHOOK: Output: default@plain_table
+PREHOOK: query: DROP DATABASE encrypted_db
+PREHOOK: type: DROPDATABASE
+PREHOOK: Input: database:encrypted_db
+PREHOOK: Output: database:encrypted_db
+POSTHOOK: query: DROP DATABASE encrypted_db
+POSTHOOK: type: DROPDATABASE
+POSTHOOK: Input: database:encrypted_db
+POSTHOOK: Output: database:encrypted_db
diff --git a/shims/0.23/pom.xml b/shims/0.23/pom.xml
index cb1999e07f..d0d1d5fbda 100644
--- a/shims/0.23/pom.xml
+++ b/shims/0.23/pom.xml
@@ -91,6 +91,22 @@
           </exclusion>
         </exclusions>
    </dependency>
+  <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-mapreduce-client-common</artifactId>
+      <version>${hadoop.version}</version>
+      <optional>true</optional>
+          <exclusions>
+            <exclusion>
+            <groupId>org.slf4j</groupId>
+            <artifactId>slf4j-log4j12</artifactId>
+          </exclusion>
+          <exclusion>
+            <groupId>commmons-logging</groupId>
+            <artifactId>commons-logging</artifactId>
+          </exclusion>
+        </exclusions>
+   </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java b/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
index 28d3e4832f..f712c3c02e 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java
@@ -20,6 +20,8 @@
 import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;
 import org.apache.hadoop.util.VersionInfo;
 import org.apache.log4j.AppenderSkeleton;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import java.util.HashMap;
 import java.util.Map;
@@ -29,6 +31,7 @@
  *
  */
 public abstract class ShimLoader {
+  private static final Logger LOG = LoggerFactory.getLogger(ShimLoader.class);
   public static String HADOOP23VERSIONNAME = "0.23";
 
   private static volatile HadoopShims hadoopShims;
@@ -92,7 +95,12 @@ public static HadoopShims getHadoopShims() {
     if (hadoopShims == null) {
       synchronized (ShimLoader.class) {
         if (hadoopShims == null) {
-          hadoopShims = loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
+          try {
+            hadoopShims = loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
+          } catch (Throwable t) {
+            LOG.error("Error loading shims", t);
+            throw new RuntimeException(t);
+          }
         }
       }
     }
diff --git a/spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java b/spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java
index ea83125347..b95cd7a05d 100644
--- a/spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java
+++ b/spark-client/src/test/java/org/apache/hive/spark/client/TestSparkClient.java
@@ -17,6 +17,20 @@
 
 package org.apache.hive.spark.client;
 
+import org.apache.hive.spark.client.JobHandle.Listener;
+
+import org.slf4j.Logger;
+
+import org.slf4j.LoggerFactory;
+
+import org.mockito.invocation.InvocationOnMock;
+
+import org.mockito.stubbing.Answer;
+
+import org.mockito.stubbing.Answer;
+
+import org.mockito.Mockito;
+
 import java.io.File;
 import java.io.FileInputStream;
 import java.io.FileOutputStream;
@@ -258,13 +272,42 @@ public void call(SparkClient client) throws Exception {
     });
   }
 
+  private static final Logger LOG = LoggerFactory.getLogger(TestSparkClient.class);
+
   private <T extends Serializable> JobHandle.Listener<T> newListener() {
     @SuppressWarnings("unchecked")
-    JobHandle.Listener<T> listener =
-      (JobHandle.Listener<T>) mock(JobHandle.Listener.class);
+    JobHandle.Listener<T> listener = mock(JobHandle.Listener.class);
+    answerWhen(listener, "cancelled").onJobCancelled(Mockito.<JobHandle<T>>any());
+    answerWhen(listener, "queued").onJobQueued(Mockito.<JobHandle<T>>any());
+    answerWhen(listener, "started").onJobStarted(Mockito.<JobHandle<T>>any());
+    answerWhen(listener, "succeeded").onJobSucceeded(
+        Mockito.<JobHandle<T>>any(), Mockito.<T>any());
+    answerWhen(listener, "job started").onSparkJobStarted(
+        Mockito.<JobHandle<T>>any(), Mockito.anyInt());
+    Mockito.doAnswer(new Answer<Void>() {
+      public Void answer(InvocationOnMock invocation) throws Throwable {
+        @SuppressWarnings("rawtypes")
+        JobHandleImpl arg = ((JobHandleImpl)invocation.getArguments()[0]);
+        LOG.info("Job failed " + arg.getClientJobId(),
+            (Throwable)invocation.getArguments()[1]);
+        return null;
+      };
+    }).when(listener).onJobFailed(Mockito.<JobHandle<T>>any(), Mockito.<Throwable>any());
     return listener;
   }
 
+  protected <T extends Serializable> Listener<T> answerWhen(
+      Listener<T> listener, final String logStr) {
+    return Mockito.doAnswer(new Answer<Void>() {
+      public Void answer(InvocationOnMock invocation) throws Throwable {
+        @SuppressWarnings("rawtypes")
+        JobHandleImpl arg = ((JobHandleImpl)invocation.getArguments()[0]);
+        LOG.info("Job " + logStr + " " + arg.getClientJobId());
+        return null;
+      };
+    }).when(listener);
+  }
+
   private void runTest(boolean local, TestFunction test) throws Exception {
     Map<String, String> conf = createConf(local);
     SparkClientFactory.initialize(conf);
