diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index 4710b8f786..a854f9f870 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -697,7 +697,9 @@ public static TypeInfo getTypeInfoForPrimitiveCategory(
   }
 
   /**
-   * Find a common class for union-all operator
+   * Find a common type for union-all operator. Only the common types for the same
+   * type group will resolve to a common type. No implicit conversion across different
+   * type groups will be done.
    */
   public static TypeInfo getCommonClassForUnionAll(TypeInfo a, TypeInfo b) {
     if (a.equals(b)) {
@@ -716,26 +718,21 @@ public static TypeInfo getCommonClassForUnionAll(TypeInfo a, TypeInfo b) {
 
     PrimitiveGrouping pgA = PrimitiveObjectInspectorUtils.getPrimitiveGrouping(pcA);
     PrimitiveGrouping pgB = PrimitiveObjectInspectorUtils.getPrimitiveGrouping(pcB);
-    // handle string types properly
-    if (pgA == PrimitiveGrouping.STRING_GROUP && pgB == PrimitiveGrouping.STRING_GROUP) {
-      return getTypeInfoForPrimitiveCategory(
-          (PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b,PrimitiveCategory.STRING);
+    if (pgA != pgB) {
+      return null;
     }
 
-    if (TypeInfoUtils.implicitConvertible(a, b)) {
-      return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, pcB);
-    }
-    if (TypeInfoUtils.implicitConvertible(b, a)) {
-      return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, pcA);
-    }
-    for (PrimitiveCategory t : TypeInfoUtils.numericTypeList) {
-      if (TypeInfoUtils.implicitConvertible(pcA, t)
-          && TypeInfoUtils.implicitConvertible(pcB, t)) {
-        return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, t);
-      }
+    switch(pgA) {
+    case STRING_GROUP:
+      return getTypeInfoForPrimitiveCategory(
+          (PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b,PrimitiveCategory.STRING);
+    case NUMERIC_GROUP:
+      return TypeInfoUtils.implicitConvertible(a, b) ? b : a;
+    case DATE_GROUP:
+      return TypeInfoFactory.timestampTypeInfo;
+    default:
+      return null;
     }
-
-    return null;
   }
 
   /**
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
index 59ecd1ec6e..d2d5a1b038 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFunctionRegistry.java
@@ -327,14 +327,12 @@ private void unionAll(TypeInfo a, TypeInfo b, TypeInfo result) {
   }
 
   public void testCommonClassUnionAll() {
+    unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.intTypeInfo,
+        TypeInfoFactory.doubleTypeInfo);
     unionAll(TypeInfoFactory.intTypeInfo, TypeInfoFactory.decimalTypeInfo,
         TypeInfoFactory.decimalTypeInfo);
-    unionAll(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo,
-        TypeInfoFactory.stringTypeInfo);
     unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.decimalTypeInfo,
         TypeInfoFactory.doubleTypeInfo);
-    unionAll(TypeInfoFactory.doubleTypeInfo, TypeInfoFactory.stringTypeInfo,
-        TypeInfoFactory.stringTypeInfo);
 
     unionAll(varchar5, varchar10, varchar10);
     unionAll(varchar10, varchar5, varchar10);
@@ -346,8 +344,13 @@ public void testCommonClassUnionAll() {
     unionAll(char10, TypeInfoFactory.stringTypeInfo, TypeInfoFactory.stringTypeInfo);
     unionAll(TypeInfoFactory.stringTypeInfo, char10, TypeInfoFactory.stringTypeInfo);
 
-    // common class for char/varchar is string?
-    comparison(char10, varchar5, TypeInfoFactory.stringTypeInfo);
+    unionAll(TypeInfoFactory.timestampTypeInfo, TypeInfoFactory.dateTypeInfo,
+        TypeInfoFactory.timestampTypeInfo);
+
+    // Invalid cases
+    unionAll(TypeInfoFactory.stringTypeInfo, TypeInfoFactory.decimalTypeInfo, null);
+    unionAll(TypeInfoFactory.doubleTypeInfo, varchar10, null);
+
   }
 
   public void testGetTypeInfoForPrimitiveCategory() {
diff --git a/ql/src/test/queries/clientpositive/alter_partition_change_col.q b/ql/src/test/queries/clientpositive/alter_partition_change_col.q
index 360f4d20ba..23de3d7d76 100644
--- a/ql/src/test/queries/clientpositive/alter_partition_change_col.q
+++ b/ql/src/test/queries/clientpositive/alter_partition_change_col.q
@@ -12,7 +12,7 @@ create table alter_partition_change_col1 (c1 string, c2 string) partitioned by (
 insert overwrite table alter_partition_change_col1 partition (p1, p2)
   select c1, c2, 'abc', '123' from alter_partition_change_col0
   union all
-  select c1, c2, null, '123' from alter_partition_change_col0;
+  select c1, c2, cast(null as string), '123' from alter_partition_change_col0;
   
 show partitions alter_partition_change_col1;
 select * from alter_partition_change_col1 where p1='abc';
diff --git a/ql/src/test/queries/clientpositive/alter_table_cascade.q b/ql/src/test/queries/clientpositive/alter_table_cascade.q
index acca4e8675..288fe4a8ce 100644
--- a/ql/src/test/queries/clientpositive/alter_table_cascade.q
+++ b/ql/src/test/queries/clientpositive/alter_table_cascade.q
@@ -15,7 +15,7 @@ create table alter_table_cascade (c1 string) partitioned by (p1 string, p2 strin
 insert overwrite table alter_table_cascade partition (p1, p2)
   select c1, 'abc', '123' from alter_table_src
   union all
-  select c1, null, '123' from alter_table_src;
+  select c1, cast(null as string), '123' from alter_table_src;
 
 show partitions alter_table_cascade;
 describe alter_table_cascade;
@@ -92,7 +92,7 @@ create table alter_table_restrict (c1 string) partitioned by (p1 string, p2 stri
 insert overwrite table alter_table_restrict partition (p1, p2)
   select c1, 'abc', '123' from alter_table_src
   union all
-  select c1, null, '123' from alter_table_src;
+  select c1, cast(null as string), '123' from alter_table_src;
 
 show partitions alter_table_restrict;
 describe alter_table_restrict;
diff --git a/ql/src/test/queries/clientpositive/groupby_sort_1_23.q b/ql/src/test/queries/clientpositive/groupby_sort_1_23.q
index 67fdd234b4..f0a00fbd54 100644
--- a/ql/src/test/queries/clientpositive/groupby_sort_1_23.q
+++ b/ql/src/test/queries/clientpositive/groupby_sort_1_23.q
@@ -134,14 +134,14 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1;
 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1;
 
 SELECT * FROM outputTbl1;
diff --git a/ql/src/test/queries/clientpositive/groupby_sort_skew_1_23.q b/ql/src/test/queries/clientpositive/groupby_sort_skew_1_23.q
index 39b9420ac8..38384dca65 100644
--- a/ql/src/test/queries/clientpositive/groupby_sort_skew_1_23.q
+++ b/ql/src/test/queries/clientpositive/groupby_sort_skew_1_23.q
@@ -100,12 +100,12 @@ SELECT * FROM outputTbl3;
 -- group by followed by another group by
 EXPLAIN EXTENDED 
 INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key;
 
 INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key;
 
@@ -135,14 +135,14 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1;
 
 INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1;
 
 SELECT * FROM outputTbl1;
diff --git a/ql/src/test/queries/clientpositive/union32.q b/ql/src/test/queries/clientpositive/union32.q
index b18d48446a..4d9de6a25a 100644
--- a/ql/src/test/queries/clientpositive/union32.q
+++ b/ql/src/test/queries/clientpositive/union32.q
@@ -51,13 +51,13 @@ SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key) a
 -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 ;
 
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 ;
@@ -67,11 +67,11 @@ EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 ;
 
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 ;
diff --git a/ql/src/test/queries/clientpositive/union33.q b/ql/src/test/queries/clientpositive/union33.q
index 017442e6b3..994060ac37 100644
--- a/ql/src/test/queries/clientpositive/union33.q
+++ b/ql/src/test/queries/clientpositive/union33.q
@@ -11,7 +11,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a;
  
@@ -20,7 +20,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a;
  
@@ -28,7 +28,7 @@ SELECT COUNT(*) FROM test_src;
  
 EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -37,7 +37,7 @@ UNION ALL
  
 INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -45,4 +45,4 @@ UNION ALL
 )a;
  
 SELECT COUNT(*) FROM test_src;
- 
\ No newline at end of file
+ 
diff --git a/ql/src/test/queries/clientpositive/union36.q b/ql/src/test/queries/clientpositive/union36.q
index b79ff0fc63..f050e1a5e9 100644
--- a/ql/src/test/queries/clientpositive/union36.q
+++ b/ql/src/test/queries/clientpositive/union36.q
@@ -2,9 +2,9 @@ set hive.mapred.mode=nonstrict;
 set hive.cbo.enable=false;
 
 -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u;
+select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast('100000000' as decimal(10,0)) x from (select * from src limit 2) s3)u;
 
-select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u;
+select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as decimal(10,0)) x from (select * from src limit 2) s3)u;
 
 
 
diff --git a/ql/src/test/queries/clientpositive/unionDistinct_1.q b/ql/src/test/queries/clientpositive/unionDistinct_1.q
index 5c52d9ba95..9792267b18 100644
--- a/ql/src/test/queries/clientpositive/unionDistinct_1.q
+++ b/ql/src/test/queries/clientpositive/unionDistinct_1.q
@@ -911,13 +911,13 @@ SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key) a
 -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 ;
 
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 ;
@@ -927,13 +927,13 @@ EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 ;
 
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 ;
 -- union33.q
 
@@ -950,7 +950,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a;
  
@@ -959,7 +959,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a;
  
@@ -967,7 +967,7 @@ SELECT COUNT(*) FROM test_src;
  
 EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -976,7 +976,7 @@ UNION DISTINCT
  
 INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
diff --git a/ql/src/test/queries/clientpositive/union_date_trim.q b/ql/src/test/queries/clientpositive/union_date_trim.q
index 6842e56441..51f5997309 100644
--- a/ql/src/test/queries/clientpositive/union_date_trim.q
+++ b/ql/src/test/queries/clientpositive/union_date_trim.q
@@ -4,4 +4,4 @@ insert into table testDate select 1, '2014-04-07' from src where key=100 limit 1
 insert into table testDate select 2, '2014-04-08' from src where key=100 limit 1;
 insert into table testDate select 3, '2014-04-09' from src where key=100 limit 1;
 --- without the fix following query will throw HiveException: Incompatible types for union operator
-insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, trim(Cast (dt as string)) as tm from testDate where id = 3 ) a;
+insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, cast(trim(Cast (dt as string)) as date) as tm from testDate where id = 3 ) a;
diff --git a/ql/src/test/queries/clientpositive/union_null.q b/ql/src/test/queries/clientpositive/union_null.q
index 23da07abbb..45448b4e7c 100644
--- a/ql/src/test/queries/clientpositive/union_null.q
+++ b/ql/src/test/queries/clientpositive/union_null.q
@@ -1,10 +1,10 @@
 -- SORT_BEFORE_DIFF
 
 -- HIVE-2901
-select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a;
-set hive.cbo.returnpath.hiveop=true; 
-select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a;
+select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a;
+set hive.cbo.returnpath.hiveop=true;
+select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a;
 
 set hive.cbo.returnpath.hiveop=false;
 -- HIVE-4837
-select * from (select * from (select null as N from src1 group by key)a UNION ALL select * from (select null as N from src1 group by key)b ) a;
+select * from (select * from (select cast(null as string) as N from src1 group by key)a UNION ALL select * from (select cast(null as string) as N from src1 group by key)b ) a;
diff --git a/ql/src/test/queries/clientpositive/union_remove_12.q b/ql/src/test/queries/clientpositive/union_remove_12.q
index b665666b03..6bfb991de2 100644
--- a/ql/src/test/queries/clientpositive/union_remove_12.q
+++ b/ql/src/test/queries/clientpositive/union_remove_12.q
@@ -34,7 +34,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c;
 
@@ -43,7 +43,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c;
 
diff --git a/ql/src/test/queries/clientpositive/union_remove_13.q b/ql/src/test/queries/clientpositive/union_remove_13.q
index 11077fd848..4d59b6bc85 100644
--- a/ql/src/test/queries/clientpositive/union_remove_13.q
+++ b/ql/src/test/queries/clientpositive/union_remove_13.q
@@ -34,7 +34,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c;
 
@@ -43,7 +43,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c;
 
diff --git a/ql/src/test/queries/clientpositive/union_remove_14.q b/ql/src/test/queries/clientpositive/union_remove_14.q
index b559b35696..3ffb985ac5 100644
--- a/ql/src/test/queries/clientpositive/union_remove_14.q
+++ b/ql/src/test/queries/clientpositive/union_remove_14.q
@@ -35,7 +35,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c;
 
@@ -44,7 +44,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c;
 
diff --git a/ql/src/test/queries/clientpositive/union_type_chk.q b/ql/src/test/queries/clientpositive/union_type_chk.q
deleted file mode 100644
index ff2e7cfcae..0000000000
--- a/ql/src/test/queries/clientpositive/union_type_chk.q
+++ /dev/null
@@ -1,7 +0,0 @@
-set hive.mapred.mode=nonstrict;
-set hive.cbo.enable=false;
-
--- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u;
-
-select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u;
diff --git a/ql/src/test/queries/clientpositive/unionall_join_nullconstant.q b/ql/src/test/queries/clientpositive/unionall_join_nullconstant.q
index 4f0ffa6ff5..6d6fa66d9c 100644
--- a/ql/src/test/queries/clientpositive/unionall_join_nullconstant.q
+++ b/ql/src/test/queries/clientpositive/unionall_join_nullconstant.q
@@ -21,7 +21,7 @@ CREATE TABLE table_b2
 CREATE VIEW a_view AS
 SELECT
 substring(a1.composite_key, 1, locate('|',a1.composite_key) - 1) AS autoname,
-NULL AS col1
+cast(NULL as string) AS col1
 FROM table_a1 a1
 FULL OUTER JOIN table_a2 a2
 ON a1.composite_key = a2.composite_key
diff --git a/ql/src/test/results/clientpositive/alter_partition_change_col.q.out b/ql/src/test/results/clientpositive/alter_partition_change_col.q.out
index 23febee6a4..fff987c9dc 100644
--- a/ql/src/test/results/clientpositive/alter_partition_change_col.q.out
+++ b/ql/src/test/results/clientpositive/alter_partition_change_col.q.out
@@ -29,14 +29,14 @@ POSTHOOK: Output: default@alter_partition_change_col1
 PREHOOK: query: insert overwrite table alter_partition_change_col1 partition (p1, p2)
   select c1, c2, 'abc', '123' from alter_partition_change_col0
   union all
-  select c1, c2, null, '123' from alter_partition_change_col0
+  select c1, c2, cast(null as string), '123' from alter_partition_change_col0
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_partition_change_col0
 PREHOOK: Output: default@alter_partition_change_col1
 POSTHOOK: query: insert overwrite table alter_partition_change_col1 partition (p1, p2)
   select c1, c2, 'abc', '123' from alter_partition_change_col0
   union all
-  select c1, c2, null, '123' from alter_partition_change_col0
+  select c1, c2, cast(null as string), '123' from alter_partition_change_col0
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_partition_change_col0
 POSTHOOK: Output: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__/p2=123
diff --git a/ql/src/test/results/clientpositive/alter_table_cascade.q.out b/ql/src/test/results/clientpositive/alter_table_cascade.q.out
index 1d8204ceab..5b9c9ee864 100644
--- a/ql/src/test/results/clientpositive/alter_table_cascade.q.out
+++ b/ql/src/test/results/clientpositive/alter_table_cascade.q.out
@@ -37,14 +37,14 @@ POSTHOOK: Output: default@alter_table_cascade
 PREHOOK: query: insert overwrite table alter_table_cascade partition (p1, p2)
   select c1, 'abc', '123' from alter_table_src
   union all
-  select c1, null, '123' from alter_table_src
+  select c1, cast(null as string), '123' from alter_table_src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_table_src
 PREHOOK: Output: default@alter_table_cascade
 POSTHOOK: query: insert overwrite table alter_table_cascade partition (p1, p2)
   select c1, 'abc', '123' from alter_table_src
   union all
-  select c1, null, '123' from alter_table_src
+  select c1, cast(null as string), '123' from alter_table_src
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_table_src
 POSTHOOK: Output: default@alter_table_cascade@p1=__HIVE_DEFAULT_PARTITION__/p2=123
@@ -902,14 +902,14 @@ POSTHOOK: Output: default@alter_table_restrict
 PREHOOK: query: insert overwrite table alter_table_restrict partition (p1, p2)
   select c1, 'abc', '123' from alter_table_src
   union all
-  select c1, null, '123' from alter_table_src
+  select c1, cast(null as string), '123' from alter_table_src
 PREHOOK: type: QUERY
 PREHOOK: Input: default@alter_table_src
 PREHOOK: Output: default@alter_table_restrict
 POSTHOOK: query: insert overwrite table alter_table_restrict partition (p1, p2)
   select c1, 'abc', '123' from alter_table_src
   union all
-  select c1, null, '123' from alter_table_src
+  select c1, cast(null as string), '123' from alter_table_src
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@alter_table_src
 POSTHOOK: Output: default@alter_table_restrict@p1=__HIVE_DEFAULT_PARTITION__/p2=123
diff --git a/ql/src/test/results/clientpositive/groupby_sort_1_23.q.out b/ql/src/test/results/clientpositive/groupby_sort_1_23.q.out
index 81fe0d9a50..e70f912fdb 100644
--- a/ql/src/test/results/clientpositive/groupby_sort_1_23.q.out
+++ b/ql/src/test/results/clientpositive/groupby_sort_1_23.q.out
@@ -2791,7 +2791,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a union where one of the sub-queries is map-side group by
@@ -2800,7 +2800,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
@@ -2902,23 +2902,27 @@ STAGE PLANS:
           mode: mergepartial
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
+          Select Operator
+            expressions: UDFToString(_col0) (type: string), _col1 (type: bigint)
+            outputColumnNames: _col0, _col1
+            Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
 #### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0,_col1
-                  columns.types double,bigint
-                  escape.delim \
-                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
+              NumFilesPerFileSink: 1
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  properties:
+                    columns _col0,_col1
+                    columns.types string,bigint
+                    escape.delim \
+                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+              TotalFiles: 1
+              GatherStats: false
+              MultiFileSpray: false
 
   Stage: Stage-2
     Map Reduce
@@ -2937,47 +2941,43 @@ STAGE PLANS:
                 mode: final
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-                Select Operator
-                  expressions: UDFToDouble(_col0) (type: double), _col1 (type: bigint)
-                  outputColumnNames: _col0, _col1
-                  Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-                  Union
+                Union
+                  Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                    outputColumnNames: _col0, _col1
                     Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
-                      outputColumnNames: _col0, _col1
+                    File Output Operator
+                      compressed: false
+                      GlobalTableId: 1
+#### A masked pattern was here ####
+                      NumFilesPerFileSink: 1
                       Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        GlobalTableId: 1
-#### A masked pattern was here ####
-                        NumFilesPerFileSink: 1
-                        Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            properties:
-                              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-                              bucket_count -1
-                              columns key,cnt
-                              columns.comments 
-                              columns.types int:int
-#### A masked pattern was here ####
-                              name default.outputtbl1
-                              numFiles 1
-                              numRows 10
-                              rawDataSize 30
-                              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                              serialization.format 1
-                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              totalSize 40
-#### A masked pattern was here ####
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            name: default.outputtbl1
-                        TotalFiles: 1
-                        GatherStats: true
-                        MultiFileSpray: false
+#### A masked pattern was here ####
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          properties:
+                            COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
+                            bucket_count -1
+                            columns key,cnt
+                            columns.comments 
+                            columns.types int:int
+#### A masked pattern was here ####
+                            name default.outputtbl1
+                            numFiles 1
+                            numRows 10
+                            rawDataSize 30
+                            serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                            serialization.format 1
+                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                            totalSize 40
+#### A masked pattern was here ####
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.outputtbl1
+                      TotalFiles: 1
+                      GatherStats: true
+                      MultiFileSpray: false
           TableScan
             GatherStats: false
             Union
@@ -3027,7 +3027,7 @@ STAGE PLANS:
             output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
             properties:
               columns _col0,_col1
-              columns.types double,bigint
+              columns.types string,bigint
               escape.delim \
               serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
             serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
@@ -3036,7 +3036,7 @@ STAGE PLANS:
               output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
               properties:
                 columns _col0,_col1
-                columns.types double,bigint
+                columns.types string,bigint
                 escape.delim \
                 serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
               serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
@@ -3310,7 +3310,7 @@ PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
@@ -3319,7 +3319,7 @@ POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
diff --git a/ql/src/test/results/clientpositive/groupby_sort_skew_1_23.q.out b/ql/src/test/results/clientpositive/groupby_sort_skew_1_23.q.out
index 5cf0ea208f..fc5298452b 100644
--- a/ql/src/test/results/clientpositive/groupby_sort_skew_1_23.q.out
+++ b/ql/src/test/results/clientpositive/groupby_sort_skew_1_23.q.out
@@ -2330,7 +2330,7 @@ PREHOOK: query: -- it should not matter what follows the group by
 -- group by followed by another group by
 EXPLAIN EXTENDED 
 INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 PREHOOK: type: QUERY
@@ -2340,7 +2340,7 @@ POSTHOOK: query: -- it should not matter what follows the group by
 -- group by followed by another group by
 EXPLAIN EXTENDED 
 INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 POSTHOOK: type: QUERY
@@ -2514,7 +2514,7 @@ STAGE PLANS:
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
           Select Operator
-            expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+            expressions: UDFToInteger(UDFToString(_col0)) (type: int), UDFToInteger(_col1) (type: int)
             outputColumnNames: _col0, _col1
             Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
@@ -2581,14 +2581,14 @@ STAGE PLANS:
 #### A masked pattern was here ####
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Output: default@outputtbl1
 POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 POSTHOOK: type: QUERY
@@ -3055,7 +3055,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a union where one of the sub-queries is map-side group by
@@ -3064,7 +3064,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
@@ -3232,23 +3232,27 @@ STAGE PLANS:
           mode: final
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 0
+          Select Operator
+            expressions: UDFToString(_col0) (type: string), _col1 (type: bigint)
+            outputColumnNames: _col0, _col1
+            Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
 #### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            table:
-                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                properties:
-                  columns _col0,_col1
-                  columns.types double,bigint
-                  escape.delim \
-                  serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-            TotalFiles: 1
-            GatherStats: false
-            MultiFileSpray: false
+              NumFilesPerFileSink: 1
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  properties:
+                    columns _col0,_col1
+                    columns.types string,bigint
+                    escape.delim \
+                    serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+              TotalFiles: 1
+              GatherStats: false
+              MultiFileSpray: false
 
   Stage: Stage-2
     Map Reduce
@@ -3267,47 +3271,43 @@ STAGE PLANS:
                 mode: final
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-                Select Operator
-                  expressions: UDFToDouble(_col0) (type: double), _col1 (type: bigint)
-                  outputColumnNames: _col0, _col1
-                  Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-                  Union
+                Union
+                  Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                    outputColumnNames: _col0, _col1
                     Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
-                      outputColumnNames: _col0, _col1
+                    File Output Operator
+                      compressed: false
+                      GlobalTableId: 1
+#### A masked pattern was here ####
+                      NumFilesPerFileSink: 1
                       Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        GlobalTableId: 1
-#### A masked pattern was here ####
-                        NumFilesPerFileSink: 1
-                        Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            properties:
-                              COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-                              bucket_count -1
-                              columns key,cnt
-                              columns.comments 
-                              columns.types int:int
-#### A masked pattern was here ####
-                              name default.outputtbl1
-                              numFiles 1
-                              numRows 10
-                              rawDataSize 30
-                              serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                              serialization.format 1
-                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                              totalSize 40
-#### A masked pattern was here ####
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            name: default.outputtbl1
-                        TotalFiles: 1
-                        GatherStats: true
-                        MultiFileSpray: false
+#### A masked pattern was here ####
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          properties:
+                            COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
+                            bucket_count -1
+                            columns key,cnt
+                            columns.comments 
+                            columns.types int:int
+#### A masked pattern was here ####
+                            name default.outputtbl1
+                            numFiles 1
+                            numRows 10
+                            rawDataSize 30
+                            serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                            serialization.format 1
+                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                            totalSize 40
+#### A masked pattern was here ####
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.outputtbl1
+                      TotalFiles: 1
+                      GatherStats: true
+                      MultiFileSpray: false
           TableScan
             GatherStats: false
             Union
@@ -3357,7 +3357,7 @@ STAGE PLANS:
             output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
             properties:
               columns _col0,_col1
-              columns.types double,bigint
+              columns.types string,bigint
               escape.delim \
               serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
             serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
@@ -3366,7 +3366,7 @@ STAGE PLANS:
               output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
               properties:
                 columns _col0,_col1
-                columns.types double,bigint
+                columns.types string,bigint
                 escape.delim \
                 serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
               serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
@@ -3640,7 +3640,7 @@ PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
@@ -3649,7 +3649,7 @@ POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
diff --git a/ql/src/test/results/clientpositive/llap/unionDistinct_1.q.out b/ql/src/test/results/clientpositive/llap/unionDistinct_1.q.out
index 52af8fd636..624d8860aa 100644
--- a/ql/src/test/results/clientpositive/llap/unionDistinct_1.q.out
+++ b/ql/src/test/results/clientpositive/llap/unionDistinct_1.q.out
@@ -13532,14 +13532,14 @@ POSTHOOK: Input: default@t2
 PREHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -13580,7 +13580,7 @@ STAGE PLANS:
                           1 Map 4
                         Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                         Select Operator
-                          expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(UDFToDouble(_col1)) (type: string)
+                          expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(CAST( _col1 AS varchar(20))) (type: string)
                           outputColumnNames: _col0, _col1
                           Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                           Group By Operator
@@ -13660,7 +13660,7 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
@@ -13668,7 +13668,7 @@ PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -13676,30 +13676,24 @@ POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 #### A masked pattern was here ####
 0.0	0
-0.0	0.0
 2.0	2
-2.0	2.0
 4.0	4
-4.0	4.0
 5.0	5
-5.0	5.0
 8.0	8
-8.0	8.0
 9.0	9
-9.0	9.0
 PREHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -13721,18 +13715,18 @@ STAGE PLANS:
                   alias: t2
                   Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: UDFToDouble(key) (type: double), UDFToDouble(key) (type: double)
+                    expressions: UDFToDouble(key) (type: double), key (type: string)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
-                      keys: _col0 (type: double), _col1 (type: double)
+                      keys: _col0 (type: double), _col1 (type: string)
                       mode: hash
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col0 (type: double), _col1 (type: double)
+                        key expressions: _col0 (type: double), _col1 (type: string)
                         sort order: ++
-                        Map-reduce partition columns: _col0 (type: double), _col1 (type: double)
+                        Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                         Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
             Execution mode: llap
             LLAP IO: no inputs
@@ -13759,18 +13753,18 @@ STAGE PLANS:
                           1 Map 5
                         Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                         Select Operator
-                          expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToDouble(_col1) (type: double)
+                          expressions: UDFToDouble(UDFToLong(_col0)) (type: double), _col1 (type: string)
                           outputColumnNames: _col0, _col1
                           Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                           Group By Operator
-                            keys: _col0 (type: double), _col1 (type: double)
+                            keys: _col0 (type: double), _col1 (type: string)
                             mode: hash
                             outputColumnNames: _col0, _col1
                             Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
                             Reduce Output Operator
-                              key expressions: _col0 (type: double), _col1 (type: double)
+                              key expressions: _col0 (type: double), _col1 (type: string)
                               sort order: ++
-                              Map-reduce partition columns: _col0 (type: double), _col1 (type: double)
+                              Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                               Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
             Execution mode: llap
             LLAP IO: no inputs
@@ -13797,7 +13791,7 @@ STAGE PLANS:
             Execution mode: llap
             Reduce Operator Tree:
               Group By Operator
-                keys: KEY._col0 (type: double), KEY._col1 (type: double)
+                keys: KEY._col0 (type: double), KEY._col1 (type: string)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
@@ -13820,7 +13814,7 @@ STAGE PLANS:
 PREHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
@@ -13828,17 +13822,17 @@ PREHOOK: Input: default@t2
 POSTHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 #### A masked pattern was here ####
-0.0	0.0
-2.0	2.0
-4.0	4.0
-5.0	5.0
-8.0	8.0
-9.0	9.0
+0.0	0
+2.0	2
+4.0	4
+5.0	5
+8.0	8
+9.0	9
 PREHOOK: query: -- union33.q
 
 -- SORT_BEFORE_DIFF
@@ -13868,7 +13862,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -13877,7 +13871,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -14007,7 +14001,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -14018,7 +14012,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -14037,7 +14031,7 @@ POSTHOOK: Input: default@test_src
 310
 PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -14046,7 +14040,7 @@ UNION DISTINCT
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -14176,7 +14170,7 @@ STAGE PLANS:
 
 PREHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -14187,7 +14181,7 @@ PREHOOK: Input: default@src
 PREHOOK: Output: default@test_src
 POSTHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
diff --git a/ql/src/test/results/clientpositive/llap/union_type_chk.q.out b/ql/src/test/results/clientpositive/llap/union_type_chk.q.out
deleted file mode 100644
index 1eb0182297..0000000000
--- a/ql/src/test/results/clientpositive/llap/union_type_chk.q.out
+++ /dev/null
@@ -1,30 +0,0 @@
-PREHOOK: query: -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-0.4999999900000002
-0.4999999900000002
-4.999999900000002E-9
-4.999999900000002E-9
-4.999999900000002E-9
-4.999999900000002E-9
-PREHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-0.25
-0.25
-0.25
-0.25
-NULL
-NULL
diff --git a/ql/src/test/results/clientpositive/spark/groupby_sort_1_23.q.out b/ql/src/test/results/clientpositive/spark/groupby_sort_1_23.q.out
index 408c1b9b6f..c6a7982871 100644
--- a/ql/src/test/results/clientpositive/spark/groupby_sort_1_23.q.out
+++ b/ql/src/test/results/clientpositive/spark/groupby_sort_1_23.q.out
@@ -1941,7 +1941,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a union where one of the sub-queries is map-side group by
@@ -1950,7 +1950,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
@@ -1982,44 +1982,40 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                       Select Operator
-                        expressions: UDFToDouble(_col0) (type: double), _col1 (type: bigint)
+                        expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
                         outputColumnNames: _col0, _col1
-                        Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-                        Select Operator
-                          expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
-                          outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                        File Output Operator
+                          compressed: false
+                          GlobalTableId: 1
+#### A masked pattern was here ####
+                          NumFilesPerFileSink: 1
                           Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                          File Output Operator
-                            compressed: false
-                            GlobalTableId: 1
 #### A masked pattern was here ####
-                            NumFilesPerFileSink: 1
-                            Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                          table:
+                              input format: org.apache.hadoop.mapred.TextInputFormat
+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                              properties:
+                                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
+                                bucket_count -1
+                                columns key,cnt
+                                columns.comments 
+                                columns.types int:int
 #### A masked pattern was here ####
-                            table:
-                                input format: org.apache.hadoop.mapred.TextInputFormat
-                                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                properties:
-                                  COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-                                  bucket_count -1
-                                  columns key,cnt
-                                  columns.comments 
-                                  columns.types int:int
-#### A masked pattern was here ####
-                                  name default.outputtbl1
-                                  numFiles 4
-                                  numRows 10
-                                  rawDataSize 30
-                                  serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                                  serialization.format 1
-                                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  totalSize 40
+                                name default.outputtbl1
+                                numFiles 4
+                                numRows 10
+                                rawDataSize 30
+                                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                                serialization.format 1
+                                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                                totalSize 40
 #### A masked pattern was here ####
-                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                name: default.outputtbl1
-                            TotalFiles: 1
-                            GatherStats: true
-                            MultiFileSpray: false
+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                              name: default.outputtbl1
+                          TotalFiles: 1
+                          GatherStats: true
+                          MultiFileSpray: false
             Path -> Alias:
 #### A masked pattern was here ####
             Path -> Partition:
@@ -2161,40 +2157,44 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                  expressions: UDFToString(_col0) (type: string), _col1 (type: bigint)
                   outputColumnNames: _col0, _col1
-                  Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                  File Output Operator
-                    compressed: false
-                    GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
+                  Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                    outputColumnNames: _col0, _col1
                     Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      GlobalTableId: 1
 #### A masked pattern was here ####
-                    table:
-                        input format: org.apache.hadoop.mapred.TextInputFormat
-                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-                          bucket_count -1
-                          columns key,cnt
-                          columns.comments 
-                          columns.types int:int
-#### A masked pattern was here ####
-                          name default.outputtbl1
-                          numFiles 4
-                          numRows 10
-                          rawDataSize 30
-                          serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 40
+                      NumFilesPerFileSink: 1
+                      Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
 #### A masked pattern was here ####
-                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        name: default.outputtbl1
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          properties:
+                            COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
+                            bucket_count -1
+                            columns key,cnt
+                            columns.comments 
+                            columns.types int:int
+#### A masked pattern was here ####
+                            name default.outputtbl1
+                            numFiles 4
+                            numRows 10
+                            rawDataSize 30
+                            serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                            serialization.format 1
+                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                            totalSize 40
+#### A masked pattern was here ####
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.outputtbl1
+                      TotalFiles: 1
+                      GatherStats: true
+                      MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
@@ -2231,7 +2231,7 @@ PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
@@ -2240,7 +2240,7 @@ POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
diff --git a/ql/src/test/results/clientpositive/spark/groupby_sort_skew_1_23.q.out b/ql/src/test/results/clientpositive/spark/groupby_sort_skew_1_23.q.out
index 63258894ec..a43812486f 100644
--- a/ql/src/test/results/clientpositive/spark/groupby_sort_skew_1_23.q.out
+++ b/ql/src/test/results/clientpositive/spark/groupby_sort_skew_1_23.q.out
@@ -1467,7 +1467,7 @@ PREHOOK: query: -- it should not matter what follows the group by
 -- group by followed by another group by
 EXPLAIN EXTENDED 
 INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 PREHOOK: type: QUERY
@@ -1477,7 +1477,7 @@ POSTHOOK: query: -- it should not matter what follows the group by
 -- group by followed by another group by
 EXPLAIN EXTENDED 
 INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 POSTHOOK: type: QUERY
@@ -1610,7 +1610,7 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                  expressions: UDFToInteger(UDFToString(_col0)) (type: int), UDFToInteger(_col1) (type: int)
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
@@ -1677,14 +1677,14 @@ STAGE PLANS:
 #### A masked pattern was here ####
 
 PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Output: default@outputtbl1
 POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
-SELECT key + key, sum(cnt) from
+SELECT cast(key + key as string), sum(cnt) from
 (SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
 group by key + key
 POSTHOOK: type: QUERY
@@ -2017,7 +2017,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 POSTHOOK: query: -- group by followed by a union where one of the sub-queries is map-side group by
@@ -2026,7 +2026,7 @@ INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
@@ -2059,44 +2059,40 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                       Select Operator
-                        expressions: UDFToDouble(_col0) (type: double), _col1 (type: bigint)
+                        expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
                         outputColumnNames: _col0, _col1
-                        Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
-                        Select Operator
-                          expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
-                          outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                        File Output Operator
+                          compressed: false
+                          GlobalTableId: 1
+#### A masked pattern was here ####
+                          NumFilesPerFileSink: 1
                           Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                          File Output Operator
-                            compressed: false
-                            GlobalTableId: 1
 #### A masked pattern was here ####
-                            NumFilesPerFileSink: 1
-                            Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                          table:
+                              input format: org.apache.hadoop.mapred.TextInputFormat
+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                              properties:
+                                COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
+                                bucket_count -1
+                                columns key,cnt
+                                columns.comments 
+                                columns.types int:int
 #### A masked pattern was here ####
-                            table:
-                                input format: org.apache.hadoop.mapred.TextInputFormat
-                                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                                properties:
-                                  COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-                                  bucket_count -1
-                                  columns key,cnt
-                                  columns.comments 
-                                  columns.types int:int
-#### A masked pattern was here ####
-                                  name default.outputtbl1
-                                  numFiles 4
-                                  numRows 10
-                                  rawDataSize 30
-                                  serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                                  serialization.format 1
-                                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                  totalSize 40
+                                name default.outputtbl1
+                                numFiles 4
+                                numRows 10
+                                rawDataSize 30
+                                serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                                serialization.format 1
+                                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                                totalSize 40
 #### A masked pattern was here ####
-                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                                name: default.outputtbl1
-                            TotalFiles: 1
-                            GatherStats: true
-                            MultiFileSpray: false
+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                              name: default.outputtbl1
+                          TotalFiles: 1
+                          GatherStats: true
+                          MultiFileSpray: false
             Path -> Alias:
 #### A masked pattern was here ####
             Path -> Partition:
@@ -2256,40 +2252,44 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                  expressions: UDFToString(_col0) (type: string), _col1 (type: bigint)
                   outputColumnNames: _col0, _col1
-                  Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
-                  File Output Operator
-                    compressed: false
-                    GlobalTableId: 1
-#### A masked pattern was here ####
-                    NumFilesPerFileSink: 1
+                  Statistics: Num rows: 3 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: UDFToInteger(_col0) (type: int), UDFToInteger(_col1) (type: int)
+                    outputColumnNames: _col0, _col1
                     Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                    File Output Operator
+                      compressed: false
+                      GlobalTableId: 1
 #### A masked pattern was here ####
-                    table:
-                        input format: org.apache.hadoop.mapred.TextInputFormat
-                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                        properties:
-                          COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
-                          bucket_count -1
-                          columns key,cnt
-                          columns.comments 
-                          columns.types int:int
-#### A masked pattern was here ####
-                          name default.outputtbl1
-                          numFiles 4
-                          numRows 10
-                          rawDataSize 30
-                          serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
-                          serialization.format 1
-                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                          totalSize 40
+                      NumFilesPerFileSink: 1
+                      Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
 #### A masked pattern was here ####
-                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        name: default.outputtbl1
-                    TotalFiles: 1
-                    GatherStats: true
-                    MultiFileSpray: false
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          properties:
+                            COLUMN_STATS_ACCURATE {"BASIC_STATS":"true"}
+                            bucket_count -1
+                            columns key,cnt
+                            columns.comments 
+                            columns.types int:int
+#### A masked pattern was here ####
+                            name default.outputtbl1
+                            numFiles 4
+                            numRows 10
+                            rawDataSize 30
+                            serialization.ddl struct outputtbl1 { i32 key, i32 cnt}
+                            serialization.format 1
+                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                            totalSize 40
+#### A masked pattern was here ####
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.outputtbl1
+                      TotalFiles: 1
+                      GatherStats: true
+                      MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
@@ -2326,7 +2326,7 @@ PREHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
@@ -2335,7 +2335,7 @@ POSTHOOK: query: INSERT OVERWRITE TABLE outputTbl1
 SELECT * FROM (
 SELECT key, count(1) as cnt FROM T1 GROUP BY key
   UNION ALL
-SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
+SELECT cast(key + key as string) as key, count(1) as cnt FROM T1 GROUP BY key + key
 ) subq1
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
diff --git a/ql/src/test/results/clientpositive/spark/union32.q.out b/ql/src/test/results/clientpositive/spark/union32.q.out
index 16cb243929..755e936e37 100644
--- a/ql/src/test/results/clientpositive/spark/union32.q.out
+++ b/ql/src/test/results/clientpositive/spark/union32.q.out
@@ -429,14 +429,14 @@ POSTHOOK: Input: default@t2
 PREHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -509,7 +509,7 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(UDFToDouble(_col1)) (type: string)
+                  expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(CAST( _col1 AS CHAR(20)) (type: string)
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
@@ -527,7 +527,7 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
@@ -535,7 +535,7 @@ PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -545,48 +545,48 @@ POSTHOOK: Input: default@t2
 0.0	0
 0.0	0
 0.0	0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
 2.0	2
-2.0	2.0
+2.0	2
+4.0	4
 4.0	4
-4.0	4.0
 5.0	5
 5.0	5
 5.0	5
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+8.0	8
 8.0	8
-8.0	8.0
 9.0	9
-9.0	9.0
+9.0	9
 PREHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -605,7 +605,7 @@ STAGE PLANS:
                   alias: t2
                   Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: UDFToDouble(key) (type: double), UDFToDouble(key) (type: double)
+                    expressions: UDFToDouble(key) (type: double), key (type: string)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
                     File Output Operator
@@ -657,7 +657,7 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
-                  expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToDouble(_col1) (type: double)
+                  expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(CAST( _col1 AS CHAR(20)) (type: string)
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
@@ -677,7 +677,7 @@ STAGE PLANS:
 PREHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
@@ -685,40 +685,40 @@ PREHOOK: Input: default@t2
 POSTHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 #### A masked pattern was here ####
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-2.0	2.0
-2.0	2.0
-4.0	4.0
-4.0	4.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-8.0	8.0
-8.0	8.0
-9.0	9.0
-9.0	9.0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+2.0	2
+2.0	2
+4.0	4
+4.0	4
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+8.0	8
+8.0	8
+9.0	9
+9.0	9
diff --git a/ql/src/test/results/clientpositive/spark/union33.q.out b/ql/src/test/results/clientpositive/spark/union33.q.out
index a61a8df38f..d0d2567aa2 100644
--- a/ql/src/test/results/clientpositive/spark/union33.q.out
+++ b/ql/src/test/results/clientpositive/spark/union33.q.out
@@ -19,7 +19,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -28,7 +28,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -139,7 +139,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -150,7 +150,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -169,7 +169,7 @@ POSTHOOK: Input: default@test_src
 312
 PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -178,7 +178,7 @@ UNION ALL
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -289,7 +289,7 @@ STAGE PLANS:
 
 PREHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -300,7 +300,7 @@ PREHOOK: Input: default@src
 PREHOOK: Output: default@test_src
 POSTHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
diff --git a/ql/src/test/results/clientpositive/spark/union_date_trim.q.out b/ql/src/test/results/clientpositive/spark/union_date_trim.q.out
index 324e8b7ddb..daa7987f19 100644
--- a/ql/src/test/results/clientpositive/spark/union_date_trim.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_date_trim.q.out
@@ -41,12 +41,12 @@ POSTHOOK: Output: default@testdate
 POSTHOOK: Lineage: testdate.dt EXPRESSION []
 POSTHOOK: Lineage: testdate.id SIMPLE []
 PREHOOK: query: --- without the fix following query will throw HiveException: Incompatible types for union operator
-insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, trim(Cast (dt as string)) as tm from testDate where id = 3 ) a
+insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, cast(trim(Cast (dt as string)) as date) as tm from testDate where id = 3 ) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@testdate
 PREHOOK: Output: default@testdate
 POSTHOOK: query: --- without the fix following query will throw HiveException: Incompatible types for union operator
-insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, trim(Cast (dt as string)) as tm from testDate where id = 3 ) a
+insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, cast(trim(Cast (dt as string)) as date) as tm from testDate where id = 3 ) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@testdate
 POSTHOOK: Output: default@testdate
diff --git a/ql/src/test/results/clientpositive/spark/union_null.q.out b/ql/src/test/results/clientpositive/spark/union_null.q.out
index d93a9d9dbc..6d06e1d701 100644
--- a/ql/src/test/results/clientpositive/spark/union_null.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_null.q.out
@@ -1,14 +1,14 @@
 PREHOOK: query: -- SORT_BEFORE_DIFF
 
 -- HIVE-2901
-select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
 POSTHOOK: query: -- SORT_BEFORE_DIFF
 
 -- HIVE-2901
-select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
@@ -22,11 +22,11 @@ NULL
 NULL
 NULL
 NULL
-PREHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+PREHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-POSTHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+POSTHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
@@ -41,12 +41,12 @@ NULL
 NULL
 NULL
 PREHOOK: query: -- HIVE-4837
-select * from (select * from (select null as N from src1 group by key)a UNION ALL select * from (select null as N from src1 group by key)b ) a
+select * from (select * from (select cast(null as string) as N from src1 group by key)a UNION ALL select * from (select cast(null as string) as N from src1 group by key)b ) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src1
 #### A masked pattern was here ####
 POSTHOOK: query: -- HIVE-4837
-select * from (select * from (select null as N from src1 group by key)a UNION ALL select * from (select null as N from src1 group by key)b ) a
+select * from (select * from (select cast(null as string) as N from src1 group by key)a UNION ALL select * from (select cast(null as string) as N from src1 group by key)b ) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src1
 #### A masked pattern was here ####
diff --git a/ql/src/test/results/clientpositive/spark/union_remove_12.q.out b/ql/src/test/results/clientpositive/spark/union_remove_12.q.out
index 7487cd3377..8ca1432749 100644
--- a/ql/src/test/results/clientpositive/spark/union_remove_12.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_remove_12.q.out
@@ -56,7 +56,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -66,7 +66,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
@@ -114,21 +114,17 @@ STAGE PLANS:
                   alias: inputtbl1
                   Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: key (type: string), '1' (type: string)
+                    expressions: key (type: string), 1 (type: bigint)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint)
-                      outputColumnNames: _col0, _col1
+                    File Output Operator
+                      compressed: false
                       Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                        table:
-                            input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                            serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                            name: default.outputtbl1
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                          name: default.outputtbl1
         Map 2 
             Map Operator Tree:
                 TableScan
@@ -152,21 +148,17 @@ STAGE PLANS:
                           1 Map 3
                         Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
                         Select Operator
-                          expressions: _col0 (type: string), _col2 (type: string)
+                          expressions: _col0 (type: string), UDFToLong(_col2) (type: bigint)
                           outputColumnNames: _col0, _col1
                           Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
-                          Select Operator
-                            expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint)
-                            outputColumnNames: _col0, _col1
+                          File Output Operator
+                            compressed: false
                             Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                            File Output Operator
-                              compressed: false
-                              Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                              table:
-                                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                                  serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                                  name: default.outputtbl1
+                            table:
+                                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                                name: default.outputtbl1
             Local Work:
               Map Reduce Local Work
 
@@ -222,7 +214,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -233,7 +225,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/spark/union_remove_13.q.out b/ql/src/test/results/clientpositive/spark/union_remove_13.q.out
index 550c24b98e..2954f7b58e 100644
--- a/ql/src/test/results/clientpositive/spark/union_remove_13.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_remove_13.q.out
@@ -56,7 +56,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -66,7 +66,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
@@ -154,21 +154,17 @@ STAGE PLANS:
                           1 Map 4
                         Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
                         Select Operator
-                          expressions: _col0 (type: string), _col2 (type: string)
+                          expressions: _col0 (type: string), UDFToLong(_col2) (type: bigint)
                           outputColumnNames: _col0, _col1
                           Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
-                          Select Operator
-                            expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint)
-                            outputColumnNames: _col0, _col1
+                          File Output Operator
+                            compressed: false
                             Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                            File Output Operator
-                              compressed: false
-                              Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                              table:
-                                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                                  serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                                  name: default.outputtbl1
+                            table:
+                                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                                name: default.outputtbl1
             Local Work:
               Map Reduce Local Work
         Reducer 2 
@@ -179,22 +175,14 @@ STAGE PLANS:
                 mode: mergepartial
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-                Select Operator
-                  expressions: _col0 (type: string), UDFToString(_col1) (type: string)
-                  outputColumnNames: _col0, _col1
-                  Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-                  Select Operator
-                    expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint)
-                    outputColumnNames: _col0, _col1
-                    Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                    File Output Operator
-                      compressed: false
-                      Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                      table:
-                          input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                          output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                          serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                          name: default.outputtbl1
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                      name: default.outputtbl1
 
   Stage: Stage-6
     Conditional Operator
@@ -248,7 +236,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -259,7 +247,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/spark/union_remove_14.q.out b/ql/src/test/results/clientpositive/spark/union_remove_14.q.out
index 9002223aec..07bd1bb3bd 100644
--- a/ql/src/test/results/clientpositive/spark/union_remove_14.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_remove_14.q.out
@@ -58,7 +58,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -68,7 +68,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
@@ -116,21 +116,17 @@ STAGE PLANS:
                   alias: inputtbl1
                   Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: key (type: string), '1' (type: string)
+                    expressions: key (type: string), 1 (type: bigint)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint)
-                      outputColumnNames: _col0, _col1
+                    File Output Operator
+                      compressed: false
                       Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                        table:
-                            input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                            serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                            name: default.outputtbl1
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                          name: default.outputtbl1
         Map 2 
             Map Operator Tree:
                 TableScan
@@ -154,21 +150,17 @@ STAGE PLANS:
                           1 Map 3
                         Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
                         Select Operator
-                          expressions: _col0 (type: string), _col2 (type: string)
+                          expressions: _col0 (type: string), UDFToLong(_col2) (type: bigint)
                           outputColumnNames: _col0, _col1
                           Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
-                          Select Operator
-                            expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint)
-                            outputColumnNames: _col0, _col1
+                          File Output Operator
+                            compressed: false
                             Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                            File Output Operator
-                              compressed: false
-                              Statistics: Num rows: 2 Data size: 63 Basic stats: COMPLETE Column stats: NONE
-                              table:
-                                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                                  serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                                  name: default.outputtbl1
+                            table:
+                                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                                name: default.outputtbl1
             Local Work:
               Map Reduce Local Work
 
@@ -224,7 +216,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -235,7 +227,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/union32.q.out b/ql/src/test/results/clientpositive/union32.q.out
index 136a1c3e6d..73d934065a 100644
--- a/ql/src/test/results/clientpositive/union32.q.out
+++ b/ql/src/test/results/clientpositive/union32.q.out
@@ -446,14 +446,14 @@ POSTHOOK: Input: default@t2
 PREHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -506,7 +506,7 @@ STAGE PLANS:
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
           Select Operator
-            expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(UDFToDouble(_col1)) (type: string)
+            expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(CAST( _col1 AS CHAR(20)) (type: string)
             outputColumnNames: _col0, _col1
             Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
@@ -553,7 +553,7 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
@@ -561,7 +561,7 @@ PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION ALL
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -571,48 +571,48 @@ POSTHOOK: Input: default@t2
 0.0	0
 0.0	0
 0.0	0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
 2.0	2
-2.0	2.0
+2.0	2
+4.0	4
 4.0	4
-4.0	4.0
 5.0	5
 5.0	5
 5.0	5
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+8.0	8
 8.0	8
-8.0	8.0
 9.0	9
-9.0	9.0
+9.0	9
 PREHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-3 is a root stage
@@ -663,7 +663,7 @@ STAGE PLANS:
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
           Select Operator
-            expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToDouble(_col1) (type: double)
+            expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(CAST( _col1 AS CHAR(20)) (type: string)
             outputColumnNames: _col0, _col1
             Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
@@ -680,7 +680,7 @@ STAGE PLANS:
             alias: t2
             Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: UDFToDouble(key) (type: double), UDFToDouble(key) (type: double)
+              expressions: UDFToDouble(key) (type: double), key (type: string)
               outputColumnNames: _col0, _col1
               Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
               Union
@@ -712,7 +712,7 @@ STAGE PLANS:
 PREHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
@@ -720,40 +720,40 @@ PREHOOK: Input: default@t2
 POSTHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION ALL
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 #### A masked pattern was here ####
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-0.0	0.0
-2.0	2.0
-2.0	2.0
-4.0	4.0
-4.0	4.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-5.0	5.0
-8.0	8.0
-8.0	8.0
-9.0	9.0
-9.0	9.0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+0.0	0
+2.0	2
+2.0	2
+4.0	4
+4.0	4
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+5.0	5
+8.0	8
+8.0	8
+9.0	9
+9.0	9
diff --git a/ql/src/test/results/clientpositive/union33.q.out b/ql/src/test/results/clientpositive/union33.q.out
index a91e74cd1e..f8a6e00997 100644
--- a/ql/src/test/results/clientpositive/union33.q.out
+++ b/ql/src/test/results/clientpositive/union33.q.out
@@ -19,7 +19,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -28,7 +28,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -201,7 +201,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -212,7 +212,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION ALL
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -231,7 +231,7 @@ POSTHOOK: Input: default@test_src
 312
 PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -240,7 +240,7 @@ UNION ALL
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -413,7 +413,7 @@ STAGE PLANS:
 
 PREHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
@@ -424,7 +424,7 @@ PREHOOK: Input: default@src
 PREHOOK: Output: default@test_src
 POSTHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION ALL
  	SELECT key, value FROM src 
diff --git a/ql/src/test/results/clientpositive/union36.q.out b/ql/src/test/results/clientpositive/union36.q.out
index 1eb0182297..e12590cc07 100644
--- a/ql/src/test/results/clientpositive/union36.q.out
+++ b/ql/src/test/results/clientpositive/union36.q.out
@@ -1,30 +1,30 @@
 PREHOOK: query: -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u
+select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast('100000000' as decimal(10,0)) x from (select * from src limit 2) s3)u
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
 POSTHOOK: query: -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u
+select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast('100000000' as decimal(10,0)) x from (select * from src limit 2) s3)u
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
-0.4999999900000002
-0.4999999900000002
-4.999999900000002E-9
-4.999999900000002E-9
-4.999999900000002E-9
-4.999999900000002E-9
-PREHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u
+0.000000004999999900000
+0.000000004999999900000
+0.000000004999999900000
+0.000000004999999900000
+0.499999990000000200000
+0.499999990000000200000
+PREHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as decimal(10,0)) x from (select * from src limit 2) s3)u
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-POSTHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u
+POSTHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as decimal(10,0)) x from (select * from src limit 2) s3)u
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
-0.25
-0.25
-0.25
-0.25
+0.250000000000000000000
+0.250000000000000000000
+0.250000000000000000000
+0.250000000000000000000
 NULL
 NULL
diff --git a/ql/src/test/results/clientpositive/unionDistinct_1.q.out b/ql/src/test/results/clientpositive/unionDistinct_1.q.out
index 8c9ce5efee..f17f1d7dd1 100644
--- a/ql/src/test/results/clientpositive/unionDistinct_1.q.out
+++ b/ql/src/test/results/clientpositive/unionDistinct_1.q.out
@@ -14144,14 +14144,14 @@ POSTHOOK: Input: default@t2
 PREHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the left selecting multiple columns
 EXPLAIN
 SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS STRING) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -14225,7 +14225,7 @@ STAGE PLANS:
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(UDFToDouble(_col1)) (type: string)
+                    expressions: UDFToDouble(UDFToLong(_col0)) (type: double), _col1 (type: string)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                     Union
@@ -14263,7 +14263,7 @@ STAGE PLANS:
         ListSink
 
 PREHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 PREHOOK: type: QUERY
@@ -14271,7 +14271,7 @@ PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
 #### A masked pattern was here ####
 POSTHOOK: query: SELECT * FROM 
-(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
+(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS CHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key
 UNION DISTINCT
 SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
 POSTHOOK: type: QUERY
@@ -14279,30 +14279,24 @@ POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 #### A masked pattern was here ####
 0.0	0
-0.0	0.0
 2.0	2
-2.0	2.0
 4.0	4
-4.0	4.0
 5.0	5
-5.0	5.0
 8.0	8
-8.0	8.0
 9.0	9
-9.0	9.0
 PREHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 POSTHOOK: query: -- Test union with join on the right selecting multiple columns
 EXPLAIN
 SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-6 is a root stage
@@ -14340,20 +14334,20 @@ STAGE PLANS:
             alias: t2
             Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: UDFToDouble(key) (type: double), UDFToDouble(key) (type: double)
+              expressions: UDFToDouble(key) (type: double), key (type: string)
               outputColumnNames: _col0, _col1
               Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
               Union
                 Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
                 Group By Operator
-                  keys: _col0 (type: double), _col1 (type: double)
+                  keys: _col0 (type: double), _col1 (type: string)
                   mode: hash
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
-                    key expressions: _col0 (type: double), _col1 (type: double)
+                    key expressions: _col0 (type: double), _col1 (type: string)
                     sort order: ++
-                    Map-reduce partition columns: _col0 (type: double), _col1 (type: double)
+                    Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                     Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
           TableScan
             alias: b
@@ -14374,26 +14368,26 @@ STAGE PLANS:
                   outputColumnNames: _col0, _col1
                   Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToDouble(_col1) (type: double)
+                    expressions: UDFToDouble(UDFToLong(_col0)) (type: double), UDFToString(CAST( _col1 AS varchar(20))) (type: string)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 11 Data size: 77 Basic stats: COMPLETE Column stats: NONE
                     Union
                       Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
-                        keys: _col0 (type: double), _col1 (type: double)
+                        keys: _col0 (type: double), _col1 (type: string)
                         mode: hash
                         outputColumnNames: _col0, _col1
                         Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
-                          key expressions: _col0 (type: double), _col1 (type: double)
+                          key expressions: _col0 (type: double), _col1 (type: string)
                           sort order: ++
-                          Map-reduce partition columns: _col0 (type: double), _col1 (type: double)
+                          Map-reduce partition columns: _col0 (type: double), _col1 (type: string)
                           Statistics: Num rows: 21 Data size: 147 Basic stats: COMPLETE Column stats: NONE
       Local Work:
         Map Reduce Local Work
       Reduce Operator Tree:
         Group By Operator
-          keys: KEY._col0 (type: double), KEY._col1 (type: double)
+          keys: KEY._col0 (type: double), KEY._col1 (type: string)
           mode: mergepartial
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 10 Data size: 70 Basic stats: COMPLETE Column stats: NONE
@@ -14414,7 +14408,7 @@ STAGE PLANS:
 PREHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@t1
 PREHOOK: Input: default@t2
@@ -14422,17 +14416,17 @@ PREHOOK: Input: default@t2
 POSTHOOK: query: SELECT * FROM 
 (SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
 UNION DISTINCT
-SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
+SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS VARCHAR(20)) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@t1
 POSTHOOK: Input: default@t2
 #### A masked pattern was here ####
-0.0	0.0
-2.0	2.0
-4.0	4.0
-5.0	5.0
-8.0	8.0
-9.0	9.0
+0.0	0
+2.0	2
+4.0	4
+5.0	5
+8.0	8
+9.0	9
 PREHOOK: query: -- union33.q
 
 -- SORT_BEFORE_DIFF
@@ -14462,7 +14456,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -14471,7 +14465,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -14593,7 +14587,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 PREHOOK: type: QUERY
@@ -14604,7 +14598,7 @@ SELECT key, value FROM (
 	SELECT key, value FROM src 
 	WHERE key = 0
 UNION DISTINCT
- 	SELECT key, COUNT(*) AS value FROM src
+ 	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 )a
 POSTHOOK: type: QUERY
@@ -14623,7 +14617,7 @@ POSTHOOK: Input: default@test_src
 310
 PREHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -14632,7 +14626,7 @@ UNION DISTINCT
 PREHOOK: type: QUERY
 POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -14754,7 +14748,7 @@ STAGE PLANS:
 
 PREHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
@@ -14765,7 +14759,7 @@ PREHOOK: Input: default@src
 PREHOOK: Output: default@test_src
 POSTHOOK: query: INSERT OVERWRITE TABLE test_src 
 SELECT key, value FROM (
-	SELECT key, COUNT(*) AS value FROM src
+	SELECT key, cast(COUNT(*) as string) AS value FROM src
  	GROUP BY key
 UNION DISTINCT
  	SELECT key, value FROM src 
diff --git a/ql/src/test/results/clientpositive/union_date_trim.q.out b/ql/src/test/results/clientpositive/union_date_trim.q.out
index 324e8b7ddb..daa7987f19 100644
--- a/ql/src/test/results/clientpositive/union_date_trim.q.out
+++ b/ql/src/test/results/clientpositive/union_date_trim.q.out
@@ -41,12 +41,12 @@ POSTHOOK: Output: default@testdate
 POSTHOOK: Lineage: testdate.dt EXPRESSION []
 POSTHOOK: Lineage: testdate.id SIMPLE []
 PREHOOK: query: --- without the fix following query will throw HiveException: Incompatible types for union operator
-insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, trim(Cast (dt as string)) as tm from testDate where id = 3 ) a
+insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, cast(trim(Cast (dt as string)) as date) as tm from testDate where id = 3 ) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@testdate
 PREHOOK: Output: default@testdate
 POSTHOOK: query: --- without the fix following query will throw HiveException: Incompatible types for union operator
-insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, trim(Cast (dt as string)) as tm from testDate where id = 3 ) a
+insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, cast(trim(Cast (dt as string)) as date) as tm from testDate where id = 3 ) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@testdate
 POSTHOOK: Output: default@testdate
diff --git a/ql/src/test/results/clientpositive/union_null.q.out b/ql/src/test/results/clientpositive/union_null.q.out
index a3407d48f0..519d40f770 100644
--- a/ql/src/test/results/clientpositive/union_null.q.out
+++ b/ql/src/test/results/clientpositive/union_null.q.out
@@ -1,14 +1,14 @@
 PREHOOK: query: -- SORT_BEFORE_DIFF
 
 -- HIVE-2901
-select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
 POSTHOOK: query: -- SORT_BEFORE_DIFF
 
 -- HIVE-2901
-select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
@@ -22,11 +22,11 @@ NULL
 NULL
 NULL
 NULL
-PREHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+PREHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
 #### A masked pattern was here ####
-POSTHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a
+POSTHOOK: query: select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select cast(NULL as string) as x from src limit 5)b )a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 #### A masked pattern was here ####
@@ -41,12 +41,12 @@ val_0
 val_10
 val_100
 PREHOOK: query: -- HIVE-4837
-select * from (select * from (select null as N from src1 group by key)a UNION ALL select * from (select null as N from src1 group by key)b ) a
+select * from (select * from (select cast(null as string) as N from src1 group by key)a UNION ALL select * from (select cast(null as string) as N from src1 group by key)b ) a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src1
 #### A masked pattern was here ####
 POSTHOOK: query: -- HIVE-4837
-select * from (select * from (select null as N from src1 group by key)a UNION ALL select * from (select null as N from src1 group by key)b ) a
+select * from (select * from (select cast(null as string) as N from src1 group by key)a UNION ALL select * from (select cast(null as string) as N from src1 group by key)b ) a
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src1
 #### A masked pattern was here ####
diff --git a/ql/src/test/results/clientpositive/union_remove_12.q.out b/ql/src/test/results/clientpositive/union_remove_12.q.out
index 46b6895a10..26887ead68 100644
--- a/ql/src/test/results/clientpositive/union_remove_12.q.out
+++ b/ql/src/test/results/clientpositive/union_remove_12.q.out
@@ -56,7 +56,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -66,7 +66,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
@@ -204,7 +204,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -215,7 +215,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/union_remove_13.q.out b/ql/src/test/results/clientpositive/union_remove_13.q.out
index 46396022fc..d013464d5b 100644
--- a/ql/src/test/results/clientpositive/union_remove_13.q.out
+++ b/ql/src/test/results/clientpositive/union_remove_13.q.out
@@ -56,7 +56,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -66,7 +66,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
@@ -111,18 +111,14 @@ STAGE PLANS:
           mode: mergepartial
           outputColumnNames: _col0, _col1
           Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-          Select Operator
-            expressions: _col0 (type: string), UDFToLong(UDFToString(_col1)) (type: bigint)
-            outputColumnNames: _col0, _col1
+          File Output Operator
+            compressed: false
             Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-            File Output Operator
-              compressed: false
-              Statistics: Num rows: 1 Data size: 30 Basic stats: COMPLETE Column stats: NONE
-              table:
-                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                  name: default.outputtbl1
+            table:
+                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                name: default.outputtbl1
 
   Stage: Stage-6
     Conditional Operator
@@ -227,7 +223,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -238,7 +234,7 @@ SELECT * FROM
 (
 select key, count(1) as `values` from inputTbl1 group by key
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/union_remove_14.q.out b/ql/src/test/results/clientpositive/union_remove_14.q.out
index fcb091e449..3a76b9590e 100644
--- a/ql/src/test/results/clientpositive/union_remove_14.q.out
+++ b/ql/src/test/results/clientpositive/union_remove_14.q.out
@@ -58,7 +58,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -68,7 +68,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
@@ -206,7 +206,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 PREHOOK: type: QUERY
@@ -217,7 +217,7 @@ SELECT * FROM
 (
 select key, 1 as `values` from inputTbl1
 union all
-select a.key as key, b.val as `values`
+select a.key as key, cast(b.val as bigint) as `values`
 FROM inputTbl1 a join inputTbl1 b on a.key=b.key
 )c
 POSTHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/union_type_chk.q.out b/ql/src/test/results/clientpositive/union_type_chk.q.out
deleted file mode 100644
index 1eb0182297..0000000000
--- a/ql/src/test/results/clientpositive/union_type_chk.q.out
+++ /dev/null
@@ -1,30 +0,0 @@
-PREHOOK: query: -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: -- SORT_QUERY_RESULTS
-select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-0.4999999900000002
-0.4999999900000002
-4.999999900000002E-9
-4.999999900000002E-9
-4.999999900000002E-9
-4.999999900000002E-9
-PREHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u
-PREHOOK: type: QUERY
-PREHOOK: Input: default@src
-#### A masked pattern was here ####
-POSTHOOK: query: select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u
-POSTHOOK: type: QUERY
-POSTHOOK: Input: default@src
-#### A masked pattern was here ####
-0.25
-0.25
-0.25
-0.25
-NULL
-NULL
diff --git a/ql/src/test/results/clientpositive/unionall_join_nullconstant.q.out b/ql/src/test/results/clientpositive/unionall_join_nullconstant.q.out
index fca26b4139..cced114fb3 100644
--- a/ql/src/test/results/clientpositive/unionall_join_nullconstant.q.out
+++ b/ql/src/test/results/clientpositive/unionall_join_nullconstant.q.out
@@ -61,7 +61,7 @@ POSTHOOK: Output: default@table_b2
 PREHOOK: query: CREATE VIEW a_view AS
 SELECT
 substring(a1.composite_key, 1, locate('|',a1.composite_key) - 1) AS autoname,
-NULL AS col1
+cast(NULL as string) AS col1
 FROM table_a1 a1
 FULL OUTER JOIN table_a2 a2
 ON a1.composite_key = a2.composite_key
@@ -82,7 +82,7 @@ PREHOOK: Output: default@a_view
 POSTHOOK: query: CREATE VIEW a_view AS
 SELECT
 substring(a1.composite_key, 1, locate('|',a1.composite_key) - 1) AS autoname,
-NULL AS col1
+cast(NULL as string) AS col1
 FROM table_a1 a1
 FULL OUTER JOIN table_a2 a2
 ON a1.composite_key = a2.composite_key
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
index 8f7b799400..14349fabca 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
@@ -874,6 +874,9 @@ public static void registerNumericType(PrimitiveCategory primitiveCategory, int
     numericTypes.put(primitiveCategory, level);
   }
 
+  /**
+   * Test if it's implicitly convertible for data comparison.
+   */
   public static boolean implicitConvertible(PrimitiveCategory from, PrimitiveCategory to) {
     if (from == to) {
       return true;
