diff --git a/CHANGES.txt b/CHANGES.txt
index 7553103a71..45a80e8e93 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -590,6 +590,9 @@ Trunk -  Unreleased
     HIVE-1508 Add cleanup method for HiveHistory
     (Edward Capriolo via namit)
 
+    HIVE-1830 Mappers in group by followed by map-join may die OOM
+    (Liyin Tang via namit)
+
   TESTS
 
     HIVE-1464. improve  test query performance
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 393de11854..1acf492a5d 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -202,6 +202,8 @@ public static enum ConfVars {
     HIVEMAPJOINCACHEROWS("hive.mapjoin.cache.numrows", 25000),
     HIVEGROUPBYMAPINTERVAL("hive.groupby.mapaggr.checkinterval", 100000),
     HIVEMAPAGGRHASHMEMORY("hive.map.aggr.hash.percentmemory", (float) 0.5),
+    HIVEMAPJOINFOLLOWEDBYMAPAGGRHASHMEMORY("hive.mapjoin.followby.map.aggr.hash.percentmemory", (float) 0.3),
+    HIVEMAPAGGRMEMORYTHRESHOLD("hive.map.aggr.hash.force.flush.memory.threshold", (float) 0.9),
     HIVEMAPAGGRHASHMINREDUCTION("hive.map.aggr.hash.min.reduction", (float) 0.5),
 
     // for hive udtf operator
@@ -256,6 +258,7 @@ public static enum ConfVars {
     HIVEMAXMAPJOINSIZE("hive.mapjoin.maxsize", 100000),
     HIVEHASHTABLETHRESHOLD("hive.hashtable.initialCapacity", 100000),
     HIVEHASHTABLELOADFACTOR("hive.hashtable.loadfactor", (float) 0.75),
+    HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE("hive.mapjoin.followby.gby.localtask.max.memory.usage", (float) 0.55),
     HIVEHASHTABLEMAXMEMORYUSAGE("hive.mapjoin.localtask.max.memory.usage", (float) 0.90),
     HIVEHASHTABLESCALE("hive.mapjoin.check.memory.rows", (long)100000),
 
@@ -318,7 +321,7 @@ public static enum ConfVars {
     HIVEVARIABLESUBSTITUTE("hive.variable.substitute", true),
 
     SEMANTIC_ANALYZER_HOOK("hive.semantic.analyzer.hook",null),
-    
+
     // Print column names in output
     HIVE_CLI_PRINT_HEADER("hive.cli.print.header", false);
     ;
diff --git a/conf/hive-default.xml b/conf/hive-default.xml
index 4b9997d909..ddfc6858ef 100644
--- a/conf/hive-default.xml
+++ b/conf/hive-default.xml
@@ -252,6 +252,18 @@
   <description>For local mode, memory of the mappers/reducers</description>
 </property>
 
+<property>
+  <name>hive.mapjoin.followby.map.aggr.hash.percentmemory</name>
+  <value>0.3</value>
+  <description>Portion of total memory to be used by map-side grup aggregation hash table, when this group by is followed by map join</description>
+</property>
+
+<property>
+  <name>hive.map.aggr.hash.force.flush.memory.threshold</name>
+  <value>0.9</value>
+  <description>The max memory to be used by map-side grup aggregation hash table, if the memory usage is higher than this number, force to flush data</description>
+</property>
+
 <property>
   <name>hive.map.aggr.hash.percentmemory</name>
   <value>0.5</value>
@@ -504,6 +516,12 @@
   <description>This number means how much memory the local task can take to hold the key/value into in-memory hash table; If the local task's memory usage is more than this number, the local task will be abort by themself. It means the data of small table is too large to be hold in the memory.</description>
 </property>
 
+<property>
+  <name>hive.mapjoin.followby.gby.localtask.max.memory.usage</name>
+  <value>0.55</value>
+  <description>This number means how much memory the local task can take to hold the key/value into in-memory hash table when this map join followed by a group by; If the local task's memory usage is more than this number, the local task will be abort by themself. It means the data of small table is too large to be hold in the memory.</description>
+</property>
+
 <property>
   <name>hive.mapjoin.check.memory.rows</name>
   <value>100000</value>
@@ -649,7 +667,6 @@
   <description>Maximum number of HDFS files created by all mappers/reducers in a MapReduce job.</description>
 </property>
 
-
 <property>
   <name>hive.exec.default.partition.name</name>
   <value>__HIVE_DEFAULT_PARTITION__</value>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 8423178a65..fa1eb35f07 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -19,6 +19,8 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
 import java.lang.reflect.Field;
 import java.util.ArrayList;
 import java.util.Arrays;
@@ -133,6 +135,9 @@ public class GroupByOperator extends Operator<GroupByDesc> implements
   // new Key ObjectInspectors are objectInspectors from the parent
   transient StructObjectInspector newKeyObjectInspector;
   transient StructObjectInspector currentKeyObjectInspector;
+  public static MemoryMXBean memoryMXBean;
+  private long maxMemory;
+  private float memoryThreshold;
 
   /**
    * This is used to store the position and field names for variable length
@@ -373,6 +378,9 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     if (hashAggr) {
       computeMaxEntriesHashAggr(hconf);
     }
+    memoryMXBean = ManagementFactory.getMemoryMXBean();
+    maxMemory = memoryMXBean.getHeapMemoryUsage().getMax();
+    memoryThreshold = this.getConf().getMemoryThreshold();
     initializeChildren(hconf);
   }
 
@@ -386,8 +394,8 @@ protected void initializeOp(Configuration hconf) throws HiveException {
    *         aggregation only
    **/
   private void computeMaxEntriesHashAggr(Configuration hconf) throws HiveException {
-    maxHashTblMemory = (long) (HiveConf.getFloatVar(hconf,
-        HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY) * Runtime.getRuntime().maxMemory());
+    float memoryPercentage = this.getConf().getGroupByMemoryUsage();
+    maxHashTblMemory = (long) (memoryPercentage * Runtime.getRuntime().maxMemory());
     estimateRowSize();
   }
 
@@ -824,10 +832,18 @@ private void processAggr(Object row, ObjectInspector rowInspector,
    **/
   private boolean shouldBeFlushed(KeyWrapper newKeys) {
     int numEntries = hashAggregations.size();
+    long usedMemory;
+    float rate;
 
     // The fixed size for the aggregation class is already known. Get the
     // variable portion of the size every NUMROWSESTIMATESIZE rows.
     if ((numEntriesHashTable == 0) || ((numEntries % NUMROWSESTIMATESIZE) == 0)) {
+      //check how much memory left memory
+      usedMemory = memoryMXBean.getHeapMemoryUsage().getUsed();
+      rate = (float) usedMemory / (float) maxMemory;
+      if(rate > memoryThreshold){
+        return true;
+      }
       for (Integer pos : keyPositionsSize) {
         Object key = newKeys.getKeyArray()[pos.intValue()];
         // Ignore nulls
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
index 4cf055f99d..13892b5079 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/HashTableSinkOperator.java
@@ -118,7 +118,6 @@ public class HashTableSinkOperator extends TerminalOperator<HashTableSinkDesc> i
   private long hashTableScale;
   private boolean isAbort = false;
 
-
   public static class HashTableSinkObjectCtx {
     ObjectInspector standardOI;
     SerDe serde;
@@ -244,14 +243,13 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     for (int pos = 0; pos < numAliases; pos++) {
       metadataValueTag[pos] = -1;
     }
-
     mapJoinTables = new HashMap<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>>();
 
     int hashTableThreshold = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEHASHTABLETHRESHOLD);
     float hashTableLoadFactor = HiveConf.getFloatVar(hconf,
         HiveConf.ConfVars.HIVEHASHTABLELOADFACTOR);
-    float hashTableMaxMemoryUsage = HiveConf.getFloatVar(hconf,
-        HiveConf.ConfVars.HIVEHASHTABLEMAXMEMORYUSAGE);
+    float hashTableMaxMemoryUsage = this.getConf().getHashtableMemoryUsage();
+
     hashTableScale = HiveConf.getLongVar(hconf, HiveConf.ConfVars.HIVEHASHTABLESCALE);
     if (hashTableScale <= 0) {
       hashTableScale = 1;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
index 4a2c383ad6..cafc9952ba 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
@@ -161,7 +161,6 @@ public boolean isAbort(long numRows,LogHelper console) {
     int size = mHash.size();
     long usedMemory = memoryMXBean.getHeapMemoryUsage().getUsed();
     double rate = (double) usedMemory / (double) maxMemory;
-    long mem1 = Runtime.getRuntime().totalMemory() - Runtime.getRuntime().freeMemory();
     console.printInfo(Utilities.now() + "\tProcessing rows:\t" + numRows + "\tHashtable size:\t"
         + size + "\tMemory usage:\t" + usedMemory + "\trate:\t" + num.format(rate));
     if (rate > (double) maxMemoryUsage) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinResolver.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinResolver.java
index 8ef76f0681..39420cd410 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinResolver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/CommonJoinResolver.java
@@ -92,7 +92,6 @@ private ConditionalTask processCurrentTask(MapRedTask currTask, ConditionalTask
       JoinDesc joinDesc = joinOp.getConf();
       Byte[] order = joinDesc.getTagOrder();
       int numAliases = order.length;
-
       try {
         HashSet<Integer> smallTableOnlySet = MapJoinProcessor.getSmallTableOnlySet(joinDesc
             .getConds());
@@ -115,7 +114,6 @@ private ConditionalTask processCurrentTask(MapRedTask currTask, ConditionalTask
           if (smallTableOnlySet.contains(i)) {
             continue;
           }
-
           // create map join task and set big table as i
           // deep copy a new mapred work from xml
           InputStream in = new ByteArrayInputStream(xml.getBytes("UTF-8"));
@@ -148,9 +146,7 @@ private ConditionalTask processCurrentTask(MapRedTask currTask, ConditionalTask
               aliasToPath.put(bigTableAlias, path);
             }
           }
-
         }
-
       } catch (Exception e) {
         e.printStackTrace();
         throw new SemanticException("Generate Map Join Task Error: " + e.getMessage());
@@ -212,7 +208,6 @@ private void replaceTaskWithConditionalTask(Task<? extends Serializable> currTas
       }
     }
 
-
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
@@ -243,19 +238,16 @@ public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
       return null;
     }
 
-
     private JoinOperator getJoinOp(MapRedTask task) throws SemanticException {
       if (task.getWork() == null) {
         return null;
       }
-
       Operator<? extends Serializable> reducerOp = task.getWork().getReducer();
       if (reducerOp instanceof JoinOperator) {
         return (JoinOperator) reducerOp;
       } else {
         return null;
       }
-
     }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
index c59c5ab250..63e45b3de5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/LocalMapJoinProcFactory.java
@@ -20,9 +20,13 @@
 
 import java.io.Serializable;
 import java.util.ArrayList;
+import java.util.LinkedHashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Stack;
 
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.GroupByOperator;
 import org.apache.hadoop.hive.ql.exec.HashTableDummyOperator;
 import org.apache.hadoop.hive.ql.exec.HashTableSinkOperator;
 import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
@@ -30,9 +34,15 @@
 import org.apache.hadoop.hive.ql.exec.OperatorFactory;
 import org.apache.hadoop.hive.ql.exec.RowSchema;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
+import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
+import org.apache.hadoop.hive.ql.lib.Dispatcher;
+import org.apache.hadoop.hive.ql.lib.GraphWalker;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
+import org.apache.hadoop.hive.ql.lib.Rule;
+import org.apache.hadoop.hive.ql.lib.RuleRegExp;
 import org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.LocalMapJoinProcCtx;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.HashTableDummyDesc;
@@ -44,129 +54,150 @@
  * Node processor factory for skew join resolver.
  */
 public final class LocalMapJoinProcFactory {
-
-
-
   public static NodeProcessor getJoinProc() {
     return new LocalMapJoinProcessor();
   }
-  public static NodeProcessor getMapJoinMapJoinProc() {
-    return new MapJoinMapJoinProc();
+
+  public static NodeProcessor getGroupByProc() {
+    return new MapJoinFollowByProcessor();
   }
+
   public static NodeProcessor getDefaultProc() {
     return new NodeProcessor() {
       @Override
-      public Object process(Node nd, Stack<Node> stack,
-          NodeProcessorCtx procCtx, Object... nodeOutputs)
-          throws SemanticException {
+      public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+          Object... nodeOutputs) throws SemanticException {
         return null;
       }
     };
   }
 
+  /**
+   * MapJoinFollowByProcessor.
+   *
+   */
+  public static class MapJoinFollowByProcessor implements NodeProcessor {
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs)
+        throws SemanticException {
+      LocalMapJoinProcCtx context = (LocalMapJoinProcCtx) ctx;
+      if (!nd.getName().equals("GBY")) {
+        return null;
+      }
+      context.setFollowedByGroupBy(true);
+      GroupByOperator groupByOp = (GroupByOperator) nd;
+      float groupByMemoryUsage = context.getParseCtx().getConf().getFloatVar(
+          HiveConf.ConfVars.HIVEMAPJOINFOLLOWEDBYMAPAGGRHASHMEMORY);
+      groupByOp.getConf().setGroupByMemoryUsage(groupByMemoryUsage);
+      return null;
+    }
+  }
+
   /**
    * LocalMapJoinProcessor.
    *
    */
   public static class LocalMapJoinProcessor implements NodeProcessor {
-    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
-        Object... nodeOutputs) throws SemanticException {
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx, Object... nodeOutputs)
+        throws SemanticException {
       LocalMapJoinProcCtx context = (LocalMapJoinProcCtx) ctx;
-
-      if(!nd.getName().equals("MAPJOIN")){
+      if (!nd.getName().equals("MAPJOIN")) {
         return null;
       }
       MapJoinOperator mapJoinOp = (MapJoinOperator) nd;
+      try {
+        hasGroupBy(mapJoinOp, context);
+      } catch (Exception e) {
+        e.printStackTrace();
+      }
 
       HashTableSinkDesc hashTableSinkDesc = new HashTableSinkDesc(mapJoinOp.getConf());
-      HashTableSinkOperator hashTableSinkOp =(HashTableSinkOperator)OperatorFactory.get(hashTableSinkDesc);
+      HashTableSinkOperator hashTableSinkOp = (HashTableSinkOperator) OperatorFactory
+          .get(hashTableSinkDesc);
+
+      // set hashtable memory usage
+      float hashtableMemoryUsage;
+      if (context.isFollowedByGroupBy()) {
+        hashtableMemoryUsage = context.getParseCtx().getConf().getFloatVar(
+            HiveConf.ConfVars.HIVEHASHTABLEFOLLOWBYGBYMAXMEMORYUSAGE);
+      } else {
+        hashtableMemoryUsage = context.getParseCtx().getConf().getFloatVar(
+            HiveConf.ConfVars.HIVEHASHTABLEMAXMEMORYUSAGE);
+      }
+      hashTableSinkOp.getConf().setHashtableMemoryUsage(hashtableMemoryUsage);
 
-      //get the last operator for processing big tables
+      // get the last operator for processing big tables
       int bigTable = mapJoinOp.getConf().getPosBigTable();
       Byte[] order = mapJoinOp.getConf().getTagOrder();
-      int bigTableAlias=(int)order[bigTable];
-
+      int bigTableAlias = (int) order[bigTable];
       Operator<? extends Serializable> bigOp = mapJoinOp.getParentOperators().get(bigTable);
 
-      //the parent ops for hashTableSinkOp
-      List<Operator<?extends Serializable>> smallTablesParentOp= new ArrayList<Operator<?extends Serializable>>();
-
-      List<Operator<?extends Serializable>> dummyOperators= new ArrayList<Operator<?extends Serializable>>();
-      //get all parents
-      List<Operator<? extends Serializable> >  parentsOp = mapJoinOp.getParentOperators();
-      for(int i = 0; i<parentsOp.size();i++){
-        if(i == bigTableAlias){
+      // the parent ops for hashTableSinkOp
+      List<Operator<? extends Serializable>> smallTablesParentOp = new ArrayList<Operator<? extends Serializable>>();
+      List<Operator<? extends Serializable>> dummyOperators = new ArrayList<Operator<? extends Serializable>>();
+      // get all parents
+      List<Operator<? extends Serializable>> parentsOp = mapJoinOp.getParentOperators();
+      for (int i = 0; i < parentsOp.size(); i++) {
+        if (i == bigTableAlias) {
           smallTablesParentOp.add(null);
           continue;
         }
-
         Operator<? extends Serializable> parent = parentsOp.get(i);
-        //let hashtable Op be the child of this parent
+        // let hashtable Op be the child of this parent
         parent.replaceChild(mapJoinOp, hashTableSinkOp);
-        //keep the parent id correct
+        // keep the parent id correct
         smallTablesParentOp.add(parent);
 
-        //create an new operator: HashTable DummyOpeator, which share the table desc
+        // create an new operator: HashTable DummyOpeator, which share the table desc
         HashTableDummyDesc desc = new HashTableDummyDesc();
-        HashTableDummyOperator dummyOp =(HashTableDummyOperator)OperatorFactory.get(desc);
+        HashTableDummyOperator dummyOp = (HashTableDummyOperator) OperatorFactory.get(desc);
         TableDesc tbl;
 
-        if(parent.getSchema()==null){
-          if(parent instanceof TableScanOperator ){
-            tbl = ((TableScanOperator)parent).getTableDesc();
-         }else{
-           throw new SemanticException();
-         }
-        }else{
-          //get parent schema
+        if (parent.getSchema() == null) {
+          if (parent instanceof TableScanOperator) {
+            tbl = ((TableScanOperator) parent).getTableDesc();
+          } else {
+            throw new SemanticException();
+          }
+        } else {
+          // get parent schema
           RowSchema rowSchema = parent.getSchema();
-          tbl = PlanUtils.getIntermediateFileTableDesc(PlanUtils
-              .getFieldSchemasFromRowSchema(rowSchema, ""));
+          tbl = PlanUtils.getIntermediateFileTableDesc(PlanUtils.getFieldSchemasFromRowSchema(
+              rowSchema, ""));
         }
-
-
         dummyOp.getConf().setTbl(tbl);
-
-        //let the dummy op  be the parent of mapjoin op
+        // let the dummy op be the parent of mapjoin op
         mapJoinOp.replaceParent(parent, dummyOp);
         List<Operator<? extends Serializable>> dummyChildren = new ArrayList<Operator<? extends Serializable>>();
         dummyChildren.add(mapJoinOp);
         dummyOp.setChildOperators(dummyChildren);
-
-        //add this dummy op to the dummp operator list
+        // add this dummy op to the dummp operator list
         dummyOperators.add(dummyOp);
-
       }
-
       hashTableSinkOp.setParentOperators(smallTablesParentOp);
-      for(Operator<? extends Serializable> op: dummyOperators){
+      for (Operator<? extends Serializable> op : dummyOperators) {
         context.addDummyParentOp(op);
       }
       return null;
     }
 
-  }
-
-  /**
-   * LocalMapJoinProcessor.
-   *
-   */
-  public static class MapJoinMapJoinProc implements NodeProcessor {
-    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,
-        Object... nodeOutputs) throws SemanticException {
-      LocalMapJoinProcCtx context = (LocalMapJoinProcCtx) ctx;
-      if(!nd.getName().equals("MAPJOIN")){
-        return null;
-      }
-      System.out.println("Mapjoin * MapJoin");
-
-      return null;
+    public void hasGroupBy(Operator<? extends Serializable> mapJoinOp,
+        LocalMapJoinProcCtx localMapJoinProcCtx) throws Exception {
+      List<Operator<? extends Serializable>> childOps = mapJoinOp.getChildOperators();
+      Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
+      opRules.put(new RuleRegExp("R1", "GBY%"), LocalMapJoinProcFactory.getGroupByProc());
+      // The dispatcher fires the processor corresponding to the closest
+      // matching rule and passes the context along
+      Dispatcher disp = new DefaultRuleDispatcher(LocalMapJoinProcFactory.getDefaultProc(),
+          opRules, localMapJoinProcCtx);
+      GraphWalker ogw = new DefaultGraphWalker(disp);
+      // iterator the reducer operator tree
+      ArrayList<Node> topNodes = new ArrayList<Node>();
+      topNodes.addAll(childOps);
+      ogw.startWalking(topNodes, null);
     }
   }
 
-
   private LocalMapJoinProcFactory() {
     // prevent instantiation
   }
- }
-
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MapJoinResolver.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MapJoinResolver.java
index 79ac3b11a3..09235621c6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MapJoinResolver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MapJoinResolver.java
@@ -55,31 +55,32 @@
 import org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.ConditionalResolverSkewJoinCtx;
 
 /**
- * An implementation of PhysicalPlanResolver. It iterator each MapRedTask to see whether the task has a local map work
- * if it has, it will move the local work to a new local map join task. Then it will make this new generated task depends on
- * current task's parent task and make current task depends on this new generated task.
+ * An implementation of PhysicalPlanResolver. It iterator each MapRedTask to see whether the task
+ * has a local map work if it has, it will move the local work to a new local map join task. Then it
+ * will make this new generated task depends on current task's parent task and make current task
+ * depends on this new generated task.
  */
 public class MapJoinResolver implements PhysicalPlanResolver {
   @Override
   public PhysicalContext resolve(PhysicalContext pctx) throws SemanticException {
 
-    //create dispatcher and graph walker
+    // create dispatcher and graph walker
     Dispatcher disp = new LocalMapJoinTaskDispatcher(pctx);
     TaskGraphWalker ogw = new TaskGraphWalker(disp);
 
-    //get all the tasks nodes from root task
+    // get all the tasks nodes from root task
     ArrayList<Node> topNodes = new ArrayList<Node>();
     topNodes.addAll(pctx.rootTasks);
 
-    //begin to walk through the task tree.
+    // begin to walk through the task tree.
     ogw.startWalking(topNodes, null);
     return pctx;
   }
 
   /**
-   * Iterator each tasks. If this task has a local work,create a new task for this local work, named MapredLocalTask.
-   * then make this new generated task depends on current task's parent task, and make current task
-   * depends on this new generated task
+   * Iterator each tasks. If this task has a local work,create a new task for this local work, named
+   * MapredLocalTask. then make this new generated task depends on current task's parent task, and
+   * make current task depends on this new generated task
    */
   class LocalMapJoinTaskDispatcher implements Dispatcher {
 
@@ -91,195 +92,160 @@ public LocalMapJoinTaskDispatcher(PhysicalContext context) {
     }
 
     private void processCurrentTask(Task<? extends Serializable> currTask,
-        ConditionalTask conditionalTask) throws SemanticException{
-
-
-      //get current mapred work and its local work
+        ConditionalTask conditionalTask) throws SemanticException {
+      // get current mapred work and its local work
       MapredWork mapredWork = (MapredWork) currTask.getWork();
       MapredLocalWork localwork = mapredWork.getMapLocalWork();
-
-
-      if(localwork != null){
-        //get the context info and set up the shared tmp URI
+      if (localwork != null) {
+        // get the context info and set up the shared tmp URI
         Context ctx = physicalContext.getContext();
         String tmpFileURI = Utilities.generateTmpURI(ctx.getLocalTmpFileURI(), currTask.getId());
         localwork.setTmpFileURI(tmpFileURI);
         String hdfsTmpURI = Utilities.generateTmpURI(ctx.getMRTmpFileURI(), currTask.getId());
         mapredWork.setTmpHDFSFileURI(hdfsTmpURI);
-        //create a task for this local work; right now, this local work is shared
-        //by the original MapredTask and this new generated MapredLocalTask.
-        MapredLocalTask localTask = (MapredLocalTask) TaskFactory.get(localwork,
-            physicalContext.getParseContext().getConf());
+        // create a task for this local work; right now, this local work is shared
+        // by the original MapredTask and this new generated MapredLocalTask.
+        MapredLocalTask localTask = (MapredLocalTask) TaskFactory.get(localwork, physicalContext
+            .getParseContext().getConf());
 
-        //set the backup task from curr task
+        // set the backup task from curr task
         localTask.setBackupTask(currTask.getBackupTask());
         localTask.setBackupChildrenTasks(currTask.getBackupChildrenTasks());
         currTask.setBackupChildrenTasks(null);
         currTask.setBackupTask(null);
 
-        if(currTask.getTaskTag() == Task.CONVERTED_MAPJOIN) {
+        if (currTask.getTaskTag() == Task.CONVERTED_MAPJOIN) {
           localTask.setTaskTag(Task.CONVERTED_LOCAL_MAPJOIN);
         } else {
           localTask.setTaskTag(Task.LOCAL_MAPJOIN);
         }
+        // replace the map join operator to local_map_join operator in the operator tree
+        // and return all the dummy parent
+        LocalMapJoinProcCtx  localMapJoinProcCtx= adjustLocalTask(localTask);
+        List<Operator<? extends Serializable>> dummyOps = localMapJoinProcCtx.getDummyParentOp();
 
-        //replace the map join operator to local_map_join operator in the operator tree
-        //and return all the dummy parent
-        List<Operator<? extends Serializable>>  dummyOps= adjustLocalTask(localTask);
-
-        //create new local work and setup the dummy ops
+        // create new local work and setup the dummy ops
         MapredLocalWork newLocalWork = new MapredLocalWork();
         newLocalWork.setDummyParentOp(dummyOps);
         newLocalWork.setTmpFileURI(tmpFileURI);
         newLocalWork.setInputFileChangeSensitive(localwork.getInputFileChangeSensitive());
         mapredWork.setMapLocalWork(newLocalWork);
-
-        //get all parent tasks
+        // get all parent tasks
         List<Task<? extends Serializable>> parentTasks = currTask.getParentTasks();
         currTask.setParentTasks(null);
         if (parentTasks != null) {
-
           for (Task<? extends Serializable> tsk : parentTasks) {
-            //make new generated task depends on all the  parent tasks of current task.
+            // make new generated task depends on all the parent tasks of current task.
             tsk.addDependentTask(localTask);
-            //remove the current task from its original parent task's dependent task
+            // remove the current task from its original parent task's dependent task
             tsk.removeDependentTask(currTask);
           }
-
-        }else{
-          //in this case, current task is in the root tasks
-          //so add this new task into root tasks and remove the current task from root tasks
-          if(conditionalTask== null){
+        } else {
+          // in this case, current task is in the root tasks
+          // so add this new task into root tasks and remove the current task from root tasks
+          if (conditionalTask == null) {
             physicalContext.addToRootTask(localTask);
             physicalContext.removeFromRootTask(currTask);
-          }else{
-            //set list task
+          } else {
+            // set list task
             List<Task<? extends Serializable>> listTask = conditionalTask.getListTasks();
-            ConditionalWork conditionalWork= conditionalTask.getWork();
+            ConditionalWork conditionalWork = conditionalTask.getWork();
             int index = listTask.indexOf(currTask);
             listTask.set(index, localTask);
-
-            //set list work
-            List<Serializable> listWork = (List<Serializable>)conditionalWork.getListWorks();
+            // set list work
+            List<Serializable> listWork = (List<Serializable>) conditionalWork.getListWorks();
             index = listWork.indexOf(mapredWork);
-            listWork.set(index,(Serializable)localwork);
+            listWork.set(index, (Serializable) localwork);
             conditionalWork.setListWorks(listWork);
-
             ConditionalResolver resolver = conditionalTask.getResolver();
-            if(resolver instanceof ConditionalResolverSkewJoin){
-              //get bigKeysDirToTaskMap
-              ConditionalResolverSkewJoinCtx context  =
-                (ConditionalResolverSkewJoinCtx) conditionalTask.getResolverCtx();
-              HashMap<String, Task<? extends Serializable>> bigKeysDirToTaskMap =
-                context.getDirToTaskMap();
-
-              //to avoid concurrent modify the hashmap
-              HashMap<String, Task<? extends Serializable>> newbigKeysDirToTaskMap =
-                new HashMap<String, Task<? extends Serializable>>();
-
-
-              //reset the resolver
-              for(Map.Entry<String, Task<? extends Serializable>> entry: bigKeysDirToTaskMap.entrySet()){
+            if (resolver instanceof ConditionalResolverSkewJoin) {
+              // get bigKeysDirToTaskMap
+              ConditionalResolverSkewJoinCtx context = (ConditionalResolverSkewJoinCtx) conditionalTask
+                  .getResolverCtx();
+              HashMap<String, Task<? extends Serializable>> bigKeysDirToTaskMap = context
+                  .getDirToTaskMap();
+              // to avoid concurrent modify the hashmap
+              HashMap<String, Task<? extends Serializable>> newbigKeysDirToTaskMap = new HashMap<String, Task<? extends Serializable>>();
+              // reset the resolver
+              for (Map.Entry<String, Task<? extends Serializable>> entry : bigKeysDirToTaskMap
+                  .entrySet()) {
                 Task<? extends Serializable> task = entry.getValue();
                 String key = entry.getKey();
-                if(task.equals(currTask)){
+                if (task.equals(currTask)) {
                   newbigKeysDirToTaskMap.put(key, localTask);
-                }else{
+                } else {
                   newbigKeysDirToTaskMap.put(key, task);
                 }
               }
-
               context.setDirToTaskMap(newbigKeysDirToTaskMap);
               conditionalTask.setResolverCtx(context);
-
-            }else if(resolver instanceof ConditionalResolverCommonJoin){
-              //get bigKeysDirToTaskMap
-              ConditionalResolverCommonJoinCtx context  =
-                (ConditionalResolverCommonJoinCtx) conditionalTask.getResolverCtx();
-              HashMap<String, Task<? extends Serializable>> aliasToWork =
-                context.getAliasToTask();
-
-              //to avoid concurrent modify the hashmap
-              HashMap<String, Task<? extends Serializable>> newAliasToWork =
-                new HashMap<String, Task<? extends Serializable>>();
-
-              //reset the resolver
-              for(Map.Entry<String, Task<? extends Serializable>> entry: aliasToWork.entrySet()){
+            } else if (resolver instanceof ConditionalResolverCommonJoin) {
+              // get bigKeysDirToTaskMap
+              ConditionalResolverCommonJoinCtx context = (ConditionalResolverCommonJoinCtx) conditionalTask
+                  .getResolverCtx();
+              HashMap<String, Task<? extends Serializable>> aliasToWork = context.getAliasToTask();
+              // to avoid concurrent modify the hashmap
+              HashMap<String, Task<? extends Serializable>> newAliasToWork = new HashMap<String, Task<? extends Serializable>>();
+              // reset the resolver
+              for (Map.Entry<String, Task<? extends Serializable>> entry : aliasToWork.entrySet()) {
                 Task<? extends Serializable> task = entry.getValue();
                 String key = entry.getKey();
 
-                if(task.equals(currTask)){
+                if (task.equals(currTask)) {
                   newAliasToWork.put(key, localTask);
-                }else{
+                } else {
                   newAliasToWork.put(key, task);
                 }
               }
-
               context.setAliasToTask(newAliasToWork);
               conditionalTask.setResolverCtx(context);
-
-            }else{
-
             }
-
-
           }
         }
-
-        //make current task depends on this new generated localMapJoinTask
-        //now localTask is the parent task of the current task
+        // make current task depends on this new generated localMapJoinTask
+        // now localTask is the parent task of the current task
         localTask.addDependentTask(currTask);
-
       }
-
     }
 
     @Override
     public Object dispatch(Node nd, Stack<Node> stack, Object... nodeOutputs)
         throws SemanticException {
       Task<? extends Serializable> currTask = (Task<? extends Serializable>) nd;
-      //not map reduce task or not conditional task, just skip
-      if(currTask.isMapRedTask() ){
-        if(currTask instanceof ConditionalTask){
-          //get the list of task
-          List<Task<? extends Serializable>> taskList =  ((ConditionalTask) currTask).getListTasks();
-          for(Task<? extends Serializable> tsk : taskList){
-            if(tsk.isMapRedTask()){
-              this.processCurrentTask(tsk,((ConditionalTask) currTask));
+      // not map reduce task or not conditional task, just skip
+      if (currTask.isMapRedTask()) {
+        if (currTask instanceof ConditionalTask) {
+          // get the list of task
+          List<Task<? extends Serializable>> taskList = ((ConditionalTask) currTask).getListTasks();
+          for (Task<? extends Serializable> tsk : taskList) {
+            if (tsk.isMapRedTask()) {
+              this.processCurrentTask(tsk, ((ConditionalTask) currTask));
             }
           }
-        }else{
-          this.processCurrentTask(currTask,null);
+        } else {
+          this.processCurrentTask(currTask, null);
         }
       }
       return null;
     }
 
-    //replace the map join operator to local_map_join operator in the operator tree
-    private List<Operator<? extends Serializable>> adjustLocalTask(MapredLocalTask task) throws SemanticException {
-
-      LocalMapJoinProcCtx localMapJoinProcCtx = new LocalMapJoinProcCtx(task,
-          physicalContext.getParseContext());
-
+    // replace the map join operator to local_map_join operator in the operator tree
+    private LocalMapJoinProcCtx adjustLocalTask(MapredLocalTask task)
+        throws SemanticException {
+      LocalMapJoinProcCtx localMapJoinProcCtx = new LocalMapJoinProcCtx(task, physicalContext
+          .getParseContext());
       Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
-      //opRules.put(new RuleRegExp("R1", "MAPJOIN%.*MAPJOIN%"),
-          //LocalMapJoinProcFactory.getMapJoinMapJoinProc());
       opRules.put(new RuleRegExp("R1", "MAPJOIN%"), LocalMapJoinProcFactory.getJoinProc());
-
       // The dispatcher fires the processor corresponding to the closest
       // matching rule and passes the context along
-      Dispatcher disp = new DefaultRuleDispatcher(LocalMapJoinProcFactory
-          .getDefaultProc(), opRules, localMapJoinProcCtx);
+      Dispatcher disp = new DefaultRuleDispatcher(LocalMapJoinProcFactory.getDefaultProc(),
+          opRules, localMapJoinProcCtx);
       GraphWalker ogw = new DefaultGraphWalker(disp);
-
       // iterator the reducer operator tree
       ArrayList<Node> topNodes = new ArrayList<Node>();
-
       topNodes.addAll(task.getWork().getAliasToWork().values());
       ogw.startWalking(topNodes, null);
-
-      return localMapJoinProcCtx.getDummyParentOp();
-
+      return localMapJoinProcCtx;
     }
 
     public PhysicalContext getPhysicalContext() {
@@ -290,6 +256,7 @@ public void setPhysicalContext(PhysicalContext physicalContext) {
       this.physicalContext = physicalContext;
     }
   }
+
   /**
    * A container of current task and parse context.
    */
@@ -297,12 +264,13 @@ public static class LocalMapJoinProcCtx implements NodeProcessorCtx {
     private Task<? extends Serializable> currentTask;
     private ParseContext parseCtx;
     private List<Operator<? extends Serializable>> dummyParentOp = null;
+    private boolean isFollowedByGroupBy;
 
-    public LocalMapJoinProcCtx(Task<? extends Serializable> task,
-        ParseContext parseCtx) {
+    public LocalMapJoinProcCtx(Task<? extends Serializable> task, ParseContext parseCtx) {
       currentTask = task;
       this.parseCtx = parseCtx;
       dummyParentOp = new ArrayList<Operator<? extends Serializable>>();
+      isFollowedByGroupBy = false;
     }
 
     public Task<? extends Serializable> getCurrentTask() {
@@ -313,6 +281,13 @@ public void setCurrentTask(Task<? extends Serializable> currentTask) {
       this.currentTask = currentTask;
     }
 
+    public boolean isFollowedByGroupBy() {
+      return isFollowedByGroupBy;
+    }
+
+    public void setFollowedByGroupBy(boolean isFollowedByGroupBy) {
+      this.isFollowedByGroupBy = isFollowedByGroupBy;
+    }
     public ParseContext getParseCtx() {
       return parseCtx;
     }
@@ -321,17 +296,16 @@ public void setParseCtx(ParseContext parseCtx) {
       this.parseCtx = parseCtx;
     }
 
-    public void setDummyParentOp(List<Operator<? extends Serializable>> op){
-      this.dummyParentOp=op;
+    public void setDummyParentOp(List<Operator<? extends Serializable>> op) {
+      this.dummyParentOp = op;
     }
 
-    public List<Operator<? extends Serializable>> getDummyParentOp(){
+    public List<Operator<? extends Serializable>> getDummyParentOp() {
       return this.dummyParentOp;
     }
-    public void addDummyParentOp(Operator<? extends Serializable> op){
-       this.dummyParentOp.add(op);
-    }
 
+    public void addDummyParentOp(Operator<? extends Serializable> op) {
+      this.dummyParentOp.add(op);
+    }
   }
 }
-
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 633ff60942..c667a63d68 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -82,12 +82,12 @@
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.Rule;
 import org.apache.hadoop.hive.ql.lib.RuleRegExp;
+import org.apache.hadoop.hive.ql.metadata.DummyPartition;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
 import org.apache.hadoop.hive.ql.metadata.InvalidTableException;
 import org.apache.hadoop.hive.ql.metadata.Partition;
-import org.apache.hadoop.hive.ql.metadata.DummyPartition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
 import org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1;
@@ -2274,10 +2274,11 @@ private Operator genGroupByPlanGroupByOperator(QBParseInfo parseInfo,
         genericUDAFEvaluators.put(entry.getKey(), genericUDAFEvaluator);
       }
     }
-
+    float groupByMemoryUsage = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);
+    float memoryThreshold = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);
     Operator op = putOpInsertMap(OperatorFactory.getAndMakeChild(
         new GroupByDesc(mode, outputColumnNames, groupByKeys, aggregations,
-        false), new RowSchema(groupByOutputRowResolver.getColumnInfos()),
+        false,groupByMemoryUsage,memoryThreshold), new RowSchema(groupByOutputRowResolver.getColumnInfos()),
         reduceSinkOperatorInfo), groupByOutputRowResolver);
     op.setColumnExprMap(colExprMap);
     return op;
@@ -2422,10 +2423,11 @@ private Operator genGroupByPlanGroupByOperator1(QBParseInfo parseInfo,
       groupByOutputRowResolver.putExpression(value, new ColumnInfo(
           field, udaf.returnType, "", false));
     }
-
+    float groupByMemoryUsage = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);
+    float memoryThreshold = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);
     Operator op = putOpInsertMap(OperatorFactory.getAndMakeChild(
         new GroupByDesc(mode, outputColumnNames, groupByKeys, aggregations,
-        distPartAgg), new RowSchema(groupByOutputRowResolver
+        distPartAgg,groupByMemoryUsage,memoryThreshold), new RowSchema(groupByOutputRowResolver
         .getColumnInfos()), reduceSinkOperatorInfo),
         groupByOutputRowResolver);
     op.setColumnExprMap(colExprMap);
@@ -2540,10 +2542,11 @@ private Operator genGroupByPlanMapGroupByOperator(QB qb, String dest,
         genericUDAFEvaluators.put(entry.getKey(), genericUDAFEvaluator);
       }
     }
-
+    float groupByMemoryUsage = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);
+    float memoryThreshold = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);
     Operator op = putOpInsertMap(OperatorFactory.getAndMakeChild(
         new GroupByDesc(mode, outputColumnNames, groupByKeys, aggregations,
-        false), new RowSchema(groupByOutputRowResolver.getColumnInfos()),
+        false,groupByMemoryUsage,memoryThreshold), new RowSchema(groupByOutputRowResolver.getColumnInfos()),
         inputOperatorInfo), groupByOutputRowResolver);
     op.setColumnExprMap(colExprMap);
     return op;
@@ -2847,10 +2850,11 @@ private Operator genGroupByPlanGroupByOperator2MR(QBParseInfo parseInfo,
       groupByOutputRowResolver2.putExpression(value, new ColumnInfo(
           field, udaf.returnType, "", false));
     }
-
+    float groupByMemoryUsage = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);
+    float memoryThreshold = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);
     Operator op = putOpInsertMap(OperatorFactory.getAndMakeChild(
         new GroupByDesc(mode, outputColumnNames, groupByKeys, aggregations,
-        false), new RowSchema(groupByOutputRowResolver2.getColumnInfos()),
+        false,groupByMemoryUsage,memoryThreshold), new RowSchema(groupByOutputRowResolver2.getColumnInfos()),
         reduceSinkOperatorInfo2), groupByOutputRowResolver2);
     op.setColumnExprMap(colExprMap);
     return op;
@@ -4578,9 +4582,11 @@ private Operator genMapGroupByForSemijoin(QB qb, ArrayList<ASTNode> fields, // t
     }
 
     // Generate group-by operator
+    float groupByMemoryUsage = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);
+    float memoryThreshold = HiveConf.getFloatVar(conf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);
     Operator op = putOpInsertMap(OperatorFactory.getAndMakeChild(
         new GroupByDesc(mode, outputColumnNames, groupByKeys, aggregations,
-        false), new RowSchema(groupByOutputRowResolver.getColumnInfos()),
+        false,groupByMemoryUsage,memoryThreshold), new RowSchema(groupByOutputRowResolver.getColumnInfos()),
         inputOperatorInfo), groupByOutputRowResolver);
 
     op.setColumnExprMap(colExprMap);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
index c1941f9957..9ebbb3363d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/GroupByDesc.java
@@ -52,6 +52,8 @@ public static enum Mode {
   private java.util.ArrayList<ExprNodeDesc> keys;
   private java.util.ArrayList<org.apache.hadoop.hive.ql.plan.AggregationDesc> aggregators;
   private java.util.ArrayList<java.lang.String> outputColumnNames;
+  private float groupByMemoryUsage;
+  private float memoryThreshold;
 
   public GroupByDesc() {
   }
@@ -61,9 +63,9 @@ public GroupByDesc(
       final java.util.ArrayList<java.lang.String> outputColumnNames,
       final java.util.ArrayList<ExprNodeDesc> keys,
       final java.util.ArrayList<org.apache.hadoop.hive.ql.plan.AggregationDesc> aggregators,
-      final boolean groupKeyNotReductionKey) {
+      final boolean groupKeyNotReductionKey,float groupByMemoryUsage, float memoryThreshold) {
     this(mode, outputColumnNames, keys, aggregators, groupKeyNotReductionKey,
-        false);
+        false, groupByMemoryUsage, memoryThreshold);
   }
 
   public GroupByDesc(
@@ -71,13 +73,15 @@ public GroupByDesc(
       final java.util.ArrayList<java.lang.String> outputColumnNames,
       final java.util.ArrayList<ExprNodeDesc> keys,
       final java.util.ArrayList<org.apache.hadoop.hive.ql.plan.AggregationDesc> aggregators,
-      final boolean groupKeyNotReductionKey, final boolean bucketGroup) {
+      final boolean groupKeyNotReductionKey, final boolean bucketGroup,float groupByMemoryUsage, float memoryThreshold) {
     this.mode = mode;
     this.outputColumnNames = outputColumnNames;
     this.keys = keys;
     this.aggregators = aggregators;
     this.groupKeyNotReductionKey = groupKeyNotReductionKey;
     this.bucketGroup = bucketGroup;
+    this.groupByMemoryUsage = groupByMemoryUsage;
+    this.memoryThreshold = memoryThreshold;
   }
 
   public Mode getMode() {
@@ -129,6 +133,22 @@ public void setOutputColumnNames(
     this.outputColumnNames = outputColumnNames;
   }
 
+  public float getGroupByMemoryUsage() {
+    return groupByMemoryUsage;
+  }
+
+  public void setGroupByMemoryUsage(float groupByMemoryUsage) {
+    this.groupByMemoryUsage = groupByMemoryUsage;
+  }
+
+  public float getMemoryThreshold() {
+    return memoryThreshold;
+  }
+
+  public void setMemoryThreshold(float memoryThreshold) {
+    this.memoryThreshold = memoryThreshold;
+  }
+
   @Explain(displayName = "aggregations")
   public java.util.ArrayList<org.apache.hadoop.hive.ql.plan.AggregationDesc> getAggregators() {
     return aggregators;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java
index f9724755ed..4a729e8d58 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/HashTableSinkDesc.java
@@ -79,6 +79,7 @@ public class HashTableSinkDesc extends JoinDesc implements Serializable {
 
   private LinkedHashMap<String, LinkedHashMap<String, ArrayList<String>>> aliasBucketFileNameMapping;
   private LinkedHashMap<String, Integer> bucketFileNameMapping;
+  private float hashtableMemoryUsage;
 
   public HashTableSinkDesc() {
     bucketFileNameMapping = new LinkedHashMap<String, Integer>();
@@ -125,6 +126,14 @@ private void initRetainExprList() {
     }
   }
 
+  public float getHashtableMemoryUsage() {
+    return hashtableMemoryUsage;
+  }
+
+  public void setHashtableMemoryUsage(float hashtableMemoryUsage) {
+    this.hashtableMemoryUsage = hashtableMemoryUsage;
+  }
+
   public boolean isHandleSkewJoin() {
     return handleSkewJoin;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java
index 708300ae60..752c8e4abc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredLocalWork.java
@@ -45,7 +45,6 @@ public class MapredLocalWork implements Serializable {
   private String tmpFileURI;
   private String stageID;
 
-
   private List<Operator<? extends Serializable>> dummyParentOp ;
 
   public MapredLocalWork() {
@@ -90,7 +89,6 @@ public void setStageID(String stageID) {
     this.stageID = stageID;
   }
 
-
   public void setAliasToWork(
       final LinkedHashMap<String, Operator<? extends Serializable>> aliasToWork) {
     this.aliasToWork = aliasToWork;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java
index 0ba6ca94c3..62b6e727af 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapredWork.java
@@ -75,7 +75,6 @@ public class MapredWork implements Serializable {
 
   private QBJoinTree joinTree;
 
-
   public MapredWork() {
     aliasToPartnInfo = new LinkedHashMap<String, PartitionDesc>();
   }
diff --git a/ql/src/test/queries/clientpositive/auto_join26.q b/ql/src/test/queries/clientpositive/auto_join26.q
new file mode 100644
index 0000000000..1a0ae9d194
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/auto_join26.q
@@ -0,0 +1,10 @@
+CREATE TABLE dest_j1(key INT, cnt INT);
+set hive.auto.convert.join = true;
+EXPLAIN
+INSERT OVERWRITE TABLE dest_j1 
+SELECT x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
+
+INSERT OVERWRITE TABLE dest_j1 
+SELECT  x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;
+
+select * from dest_j1 x order by x.key;
diff --git a/ql/src/test/results/clientpositive/auto_join26.q.out b/ql/src/test/results/clientpositive/auto_join26.q.out
new file mode 100644
index 0000000000..337af32ce3
--- /dev/null
+++ b/ql/src/test/results/clientpositive/auto_join26.q.out
@@ -0,0 +1,315 @@
+PREHOOK: query: CREATE TABLE dest_j1(key INT, cnt INT)
+PREHOOK: type: CREATETABLE
+POSTHOOK: query: CREATE TABLE dest_j1(key INT, cnt INT)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: default@dest_j1
+PREHOOK: query: EXPLAIN
+INSERT OVERWRITE TABLE dest_j1 
+SELECT x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+INSERT OVERWRITE TABLE dest_j1 
+SELECT x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF src1 x) (TOK_TABREF src y) (= (. (TOK_TABLE_OR_COL x) key) (. (TOK_TABLE_OR_COL y) key)))) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_j1)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL x) key)) (TOK_SELEXPR (TOK_FUNCTION count 1))) (TOK_GROUPBY (. (TOK_TABLE_OR_COL x) key))))
+
+STAGE DEPENDENCIES:
+  Stage-7 is a root stage , consists of Stage-8, Stage-9, Stage-1
+  Stage-8 has a backup stage: Stage-1
+  Stage-5 depends on stages: Stage-8
+  Stage-2 depends on stages: Stage-1, Stage-5, Stage-6
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+  Stage-9 has a backup stage: Stage-1
+  Stage-6 depends on stages: Stage-9
+  Stage-1
+
+STAGE PLANS:
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-8
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        y 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        y 
+          TableScan
+            alias: y
+            HashTable Sink Operator
+              condition expressions:
+                0 {key}
+                1 
+              handleSkewJoin: false
+              keys:
+                0 [Column[key]]
+                1 [Column[key]]
+              Position of Big Table: 0
+
+  Stage: Stage-5
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+          TableScan
+            alias: x
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+              condition expressions:
+                0 {key}
+                1 
+              handleSkewJoin: false
+              keys:
+                0 [Column[key]]
+                1 [Column[key]]
+              outputColumnNames: _col0
+              Position of Big Table: 0
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                outputColumnNames: _col0
+                Group By Operator
+                  aggregations:
+                        expr: count(1)
+                  bucketGroup: false
+                  keys:
+                        expr: _col0
+                        type: string
+                  mode: hash
+                  outputColumnNames: _col0, _col1
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 0
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        file:/tmp/liyintang/hive_2010-12-06_15-13-36_873_2449071063337197304/-mr-10002 
+            Reduce Output Operator
+              key expressions:
+                    expr: _col0
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: _col0
+                    type: string
+              tag: -1
+              value expressions:
+                    expr: _col1
+                    type: bigint
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations:
+                expr: count(VALUE._col0)
+          bucketGroup: false
+          keys:
+                expr: KEY._col0
+                type: string
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+                  expr: _col1
+                  type: bigint
+            outputColumnNames: _col0, _col1
+            Select Operator
+              expressions:
+                    expr: UDFToInteger(_col0)
+                    type: int
+                    expr: UDFToInteger(_col1)
+                    type: int
+              outputColumnNames: _col0, _col1
+              File Output Operator
+                compressed: false
+                GlobalTableId: 1
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: dest_j1
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: dest_j1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-9
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        x 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        x 
+          TableScan
+            alias: x
+            HashTable Sink Operator
+              condition expressions:
+                0 {key}
+                1 
+              handleSkewJoin: false
+              keys:
+                0 [Column[key]]
+                1 [Column[key]]
+              Position of Big Table: 1
+
+  Stage: Stage-6
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y 
+          TableScan
+            alias: y
+            Map Join Operator
+              condition map:
+                   Inner Join 0 to 1
+              condition expressions:
+                0 {key}
+                1 
+              handleSkewJoin: false
+              keys:
+                0 [Column[key]]
+                1 [Column[key]]
+              outputColumnNames: _col0
+              Position of Big Table: 1
+              Select Operator
+                expressions:
+                      expr: _col0
+                      type: string
+                outputColumnNames: _col0
+                Group By Operator
+                  aggregations:
+                        expr: count(1)
+                  bucketGroup: false
+                  keys:
+                        expr: _col0
+                        type: string
+                  mode: hash
+                  outputColumnNames: _col0, _col1
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 0
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+      Local Work:
+        Map Reduce Local Work
+
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+          TableScan
+            alias: x
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key
+                    type: string
+        y 
+          TableScan
+            alias: y
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 1
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 {VALUE._col0}
+            1 
+          handleSkewJoin: false
+          outputColumnNames: _col0
+          Select Operator
+            expressions:
+                  expr: _col0
+                  type: string
+            outputColumnNames: _col0
+            Group By Operator
+              aggregations:
+                    expr: count(1)
+              bucketGroup: false
+              keys:
+                    expr: _col0
+                    type: string
+              mode: hash
+              outputColumnNames: _col0, _col1
+              File Output Operator
+                compressed: false
+                GlobalTableId: 0
+                table:
+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+
+
+PREHOOK: query: INSERT OVERWRITE TABLE dest_j1 
+SELECT  x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Input: default@src1
+PREHOOK: Output: default@dest_j1
+POSTHOOK: query: INSERT OVERWRITE TABLE dest_j1 
+SELECT  x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Input: default@src1
+POSTHOOK: Output: default@dest_j1
+POSTHOOK: Lineage: dest_j1.cnt EXPRESSION [(src1)x.null, (src)y.null, ]
+POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
+PREHOOK: query: select * from dest_j1 x order by x.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@dest_j1
+PREHOOK: Output: file:/tmp/liyintang/hive_2010-12-06_15-13-51_429_1293784409064103219/-mr-10000
+POSTHOOK: query: select * from dest_j1 x order by x.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@dest_j1
+POSTHOOK: Output: file:/tmp/liyintang/hive_2010-12-06_15-13-51_429_1293784409064103219/-mr-10000
+POSTHOOK: Lineage: dest_j1.cnt EXPRESSION [(src1)x.null, (src)y.null, ]
+POSTHOOK: Lineage: dest_j1.key EXPRESSION [(src1)x.FieldSchema(name:key, type:string, comment:default), ]
+66	1
+98	2
+128	3
+146	2
+150	1
+213	2
+224	2
+238	2
+255	2
+273	3
+278	2
+311	3
+369	3
+401	5
+406	4
diff --git a/ql/src/test/results/compiler/plan/groupby1.q.xml b/ql/src/test/results/compiler/plan/groupby1.q.xml
index c782350c21..507b84f81b 100755
--- a/ql/src/test/results/compiler/plan/groupby1.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby1.q.xml
@@ -22,7 +22,7 @@
           <void property="work"> 
            <object class="org.apache.hadoop.hive.ql.plan.StatsWork"> 
             <void property="aggKey"> 
-             <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-11-17_08-31-24_521_562459283286922391/-ext-10000/</string> 
+             <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/scratchdir/hive_2010-12-08_21-56-35_737_7151386379653921790/-ext-10000/</string> 
             </void> 
            </object> 
           </void> 
@@ -58,7 +58,7 @@
            <boolean>true</boolean> 
           </void> 
           <void property="sourceDir"> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-11-17_08-31-24_521_562459283286922391/-ext-10000</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/scratchdir/hive_2010-12-08_21-56-35_737_7151386379653921790/-ext-10000</string> 
           </void> 
           <void property="table"> 
            <object id="TableDesc0" class="org.apache.hadoop.hive.ql.plan.TableDesc"> 
@@ -111,11 +111,11 @@
               </void> 
               <void method="put"> 
                <string>location</string> 
-               <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/dest1</string> 
+               <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/dest1</string> 
               </void> 
               <void method="put"> 
                <string>transient_lastDdlTime</string> 
-               <string>1290011484</string> 
+               <string>1291874195</string> 
               </void> 
              </object> 
             </void> 
@@ -125,7 +125,7 @@
            </object> 
           </void> 
           <void property="tmpDir"> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-11-17_08-31-24_521_562459283286922391/-ext-10001</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/scratchdir/hive_2010-12-08_21-56-35_737_7151386379653921790/-ext-10001</string> 
           </void> 
          </object> 
         </void> 
@@ -196,11 +196,11 @@
          </void> 
          <void method="put"> 
           <string>location</string> 
-          <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+          <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
          </void> 
          <void method="put"> 
           <string>transient_lastDdlTime</string> 
-          <string>1290011481</string> 
+          <string>1291874193</string> 
          </void> 
         </object> 
        </void> 
@@ -536,6 +536,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="groupByMemoryUsage"> 
+                   <float>0.5</float> 
+                  </void> 
                   <void property="keys"> 
                    <object class="java.util.ArrayList"> 
                     <void method="add"> 
@@ -543,6 +546,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="memoryThreshold"> 
+                   <float>0.9</float> 
+                  </void> 
                   <void property="mode"> 
                    <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
                     <string>HASH</string> 
@@ -840,7 +846,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -852,7 +858,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.PartitionDesc"> 
         <void property="baseFileName"> 
          <string>src</string> 
@@ -909,11 +915,11 @@
           </void> 
           <void method="put"> 
            <string>location</string> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
           </void> 
           <void method="put"> 
            <string>transient_lastDdlTime</string> 
-           <string>1290011481</string> 
+           <string>1291874193</string> 
           </void> 
          </object> 
         </void> 
@@ -959,7 +965,7 @@
                  <int>1</int> 
                 </void> 
                 <void property="dirName"> 
-                 <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-11-17_08-31-24_521_562459283286922391/-ext-10000</string> 
+                 <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/scratchdir/hive_2010-12-08_21-56-35_737_7151386379653921790/-ext-10000</string> 
                 </void> 
                 <void property="gatherStats"> 
                  <boolean>true</boolean> 
@@ -968,7 +974,7 @@
                  <int>1</int> 
                 </void> 
                 <void property="statsAggPrefix"> 
-                 <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/scratchdir/hive_2010-11-17_08-31-24_521_562459283286922391/-ext-10000/</string> 
+                 <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/scratchdir/hive_2010-12-08_21-56-35_737_7151386379653921790/-ext-10000/</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object idref="TableDesc0"/> 
@@ -1206,6 +1212,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="groupByMemoryUsage"> 
+         <float>0.5</float> 
+        </void> 
         <void property="keys"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -1213,6 +1222,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="memoryThreshold"> 
+         <float>0.9</float> 
+        </void> 
         <void property="mode"> 
          <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
           <string>MERGEPARTIAL</string> 
diff --git a/ql/src/test/results/compiler/plan/groupby2.q.xml b/ql/src/test/results/compiler/plan/groupby2.q.xml
index 5b15b4cce9..93cf55f473 100755
--- a/ql/src/test/results/compiler/plan/groupby2.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby2.q.xml
@@ -62,11 +62,11 @@
          </void> 
          <void method="put"> 
           <string>location</string> 
-          <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+          <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
          </void> 
          <void method="put"> 
           <string>transient_lastDdlTime</string> 
-          <string>1290011493</string> 
+          <string>1291874204</string> 
          </void> 
         </object> 
        </void> 
@@ -620,6 +620,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="groupByMemoryUsage"> 
+                   <float>0.5</float> 
+                  </void> 
                   <void property="keys"> 
                    <object class="java.util.ArrayList"> 
                     <void method="add"> 
@@ -630,6 +633,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="memoryThreshold"> 
+                   <float>0.9</float> 
+                  </void> 
                   <void property="mode"> 
                    <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
                     <string>HASH</string> 
@@ -952,7 +958,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -964,7 +970,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.PartitionDesc"> 
         <void property="baseFileName"> 
          <string>src</string> 
@@ -1021,11 +1027,11 @@
           </void> 
           <void method="put"> 
            <string>location</string> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
           </void> 
           <void method="put"> 
            <string>transient_lastDdlTime</string> 
-           <string>1290011493</string> 
+           <string>1291874204</string> 
           </void> 
          </object> 
         </void> 
@@ -1068,13 +1074,13 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.FileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-31-35_965_1361173781945581739/-ext-10001</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-56-47_166_8686062841890784236/-ext-10001</string> 
                 </void> 
                 <void property="numFiles"> 
                  <int>1</int> 
                 </void> 
                 <void property="statsAggPrefix"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-31-35_965_1361173781945581739/-ext-10001/</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-56-47_166_8686062841890784236/-ext-10001/</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.TableDesc"> 
@@ -1456,6 +1462,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="groupByMemoryUsage"> 
+         <float>0.5</float> 
+        </void> 
         <void property="keys"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -1463,6 +1472,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="memoryThreshold"> 
+         <float>0.9</float> 
+        </void> 
         <void property="mode"> 
          <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
           <string>MERGEPARTIAL</string> 
diff --git a/ql/src/test/results/compiler/plan/groupby3.q.xml b/ql/src/test/results/compiler/plan/groupby3.q.xml
index e1e746ef07..472569a345 100644
--- a/ql/src/test/results/compiler/plan/groupby3.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby3.q.xml
@@ -62,11 +62,11 @@
          </void> 
          <void method="put"> 
           <string>location</string> 
-          <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+          <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
          </void> 
          <void method="put"> 
           <string>transient_lastDdlTime</string> 
-          <string>1290011504</string> 
+          <string>1291874216</string> 
          </void> 
         </object> 
        </void> 
@@ -809,6 +809,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="groupByMemoryUsage"> 
+                   <float>0.5</float> 
+                  </void> 
                   <void property="keys"> 
                    <object class="java.util.ArrayList"> 
                     <void method="add"> 
@@ -816,6 +819,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="memoryThreshold"> 
+                   <float>0.9</float> 
+                  </void> 
                   <void property="mode"> 
                    <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
                     <string>HASH</string> 
@@ -1148,7 +1154,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1160,7 +1166,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.PartitionDesc"> 
         <void property="baseFileName"> 
          <string>src</string> 
@@ -1217,11 +1223,11 @@
           </void> 
           <void method="put"> 
            <string>location</string> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
           </void> 
           <void method="put"> 
            <string>transient_lastDdlTime</string> 
-           <string>1290011504</string> 
+           <string>1291874216</string> 
           </void> 
          </object> 
         </void> 
@@ -1264,13 +1270,13 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.FileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-31-47_349_1076341287549233934/-ext-10001</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-56-58_827_2958066279964475091/-ext-10001</string> 
                 </void> 
                 <void property="numFiles"> 
                  <int>1</int> 
                 </void> 
                 <void property="statsAggPrefix"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-31-47_349_1076341287549233934/-ext-10001/</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-56-58_827_2958066279964475091/-ext-10001/</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.TableDesc"> 
@@ -1779,9 +1785,15 @@
           </void> 
          </object> 
         </void> 
+        <void property="groupByMemoryUsage"> 
+         <float>0.5</float> 
+        </void> 
         <void property="keys"> 
          <object class="java.util.ArrayList"/> 
         </void> 
+        <void property="memoryThreshold"> 
+         <float>0.9</float> 
+        </void> 
         <void property="mode"> 
          <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
           <string>MERGEPARTIAL</string> 
diff --git a/ql/src/test/results/compiler/plan/groupby4.q.xml b/ql/src/test/results/compiler/plan/groupby4.q.xml
index e37e265e0c..2a09e512ac 100644
--- a/ql/src/test/results/compiler/plan/groupby4.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby4.q.xml
@@ -62,11 +62,11 @@
          </void> 
          <void method="put"> 
           <string>location</string> 
-          <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+          <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
          </void> 
          <void method="put"> 
           <string>transient_lastDdlTime</string> 
-          <string>1290011516</string> 
+          <string>1291874228</string> 
          </void> 
         </object> 
        </void> 
@@ -347,6 +347,9 @@
                   <void property="aggregators"> 
                    <object class="java.util.ArrayList"/> 
                   </void> 
+                  <void property="groupByMemoryUsage"> 
+                   <float>0.5</float> 
+                  </void> 
                   <void property="keys"> 
                    <object class="java.util.ArrayList"> 
                     <void method="add"> 
@@ -354,6 +357,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="memoryThreshold"> 
+                   <float>0.9</float> 
+                  </void> 
                   <void property="mode"> 
                    <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
                     <string>HASH</string> 
@@ -610,7 +616,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -622,7 +628,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.PartitionDesc"> 
         <void property="baseFileName"> 
          <string>src</string> 
@@ -679,11 +685,11 @@
           </void> 
           <void method="put"> 
            <string>location</string> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
           </void> 
           <void method="put"> 
            <string>transient_lastDdlTime</string> 
-           <string>1290011516</string> 
+           <string>1291874228</string> 
           </void> 
          </object> 
         </void> 
@@ -726,13 +732,13 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.FileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-31-59_508_6444245139394408713/-ext-10001</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-57-10_793_471111125633992980/-ext-10001</string> 
                 </void> 
                 <void property="numFiles"> 
                  <int>1</int> 
                 </void> 
                 <void property="statsAggPrefix"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-31-59_508_6444245139394408713/-ext-10001/</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-57-10_793_471111125633992980/-ext-10001/</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.TableDesc"> 
@@ -926,6 +932,9 @@
         <void property="aggregators"> 
          <object class="java.util.ArrayList"/> 
         </void> 
+        <void property="groupByMemoryUsage"> 
+         <float>0.5</float> 
+        </void> 
         <void property="keys"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -933,6 +942,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="memoryThreshold"> 
+         <float>0.9</float> 
+        </void> 
         <void property="mode"> 
          <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
           <string>MERGEPARTIAL</string> 
diff --git a/ql/src/test/results/compiler/plan/groupby5.q.xml b/ql/src/test/results/compiler/plan/groupby5.q.xml
index 3d8a85129e..aee72eac88 100644
--- a/ql/src/test/results/compiler/plan/groupby5.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby5.q.xml
@@ -62,11 +62,11 @@
          </void> 
          <void method="put"> 
           <string>location</string> 
-          <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+          <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
          </void> 
          <void method="put"> 
           <string>transient_lastDdlTime</string> 
-          <string>1290011528</string> 
+          <string>1291874240</string> 
          </void> 
         </object> 
        </void> 
@@ -402,6 +402,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="groupByMemoryUsage"> 
+                   <float>0.5</float> 
+                  </void> 
                   <void property="keys"> 
                    <object class="java.util.ArrayList"> 
                     <void method="add"> 
@@ -409,6 +412,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="memoryThreshold"> 
+                   <float>0.9</float> 
+                  </void> 
                   <void property="mode"> 
                    <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
                     <string>HASH</string> 
@@ -703,7 +709,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -715,7 +721,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.PartitionDesc"> 
         <void property="baseFileName"> 
          <string>src</string> 
@@ -772,11 +778,11 @@
           </void> 
           <void method="put"> 
            <string>location</string> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
           </void> 
           <void method="put"> 
            <string>transient_lastDdlTime</string> 
-           <string>1290011528</string> 
+           <string>1291874240</string> 
           </void> 
          </object> 
         </void> 
@@ -819,13 +825,13 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.FileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-32-11_590_7081640435930246433/-ext-10001</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-57-22_726_3776737499696649834/-ext-10001</string> 
                 </void> 
                 <void property="numFiles"> 
                  <int>1</int> 
                 </void> 
                 <void property="statsAggPrefix"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-32-11_590_7081640435930246433/-ext-10001/</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-57-22_726_3776737499696649834/-ext-10001/</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.TableDesc"> 
@@ -1095,6 +1101,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="groupByMemoryUsage"> 
+         <float>0.5</float> 
+        </void> 
         <void property="keys"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -1102,6 +1111,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="memoryThreshold"> 
+         <float>0.9</float> 
+        </void> 
         <void property="mode"> 
          <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
           <string>MERGEPARTIAL</string> 
diff --git a/ql/src/test/results/compiler/plan/groupby6.q.xml b/ql/src/test/results/compiler/plan/groupby6.q.xml
index c1e91d9f52..74169e87d3 100644
--- a/ql/src/test/results/compiler/plan/groupby6.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby6.q.xml
@@ -62,11 +62,11 @@
          </void> 
          <void method="put"> 
           <string>location</string> 
-          <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+          <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
          </void> 
          <void method="put"> 
           <string>transient_lastDdlTime</string> 
-          <string>1290011540</string> 
+          <string>1291874251</string> 
          </void> 
         </object> 
        </void> 
@@ -347,6 +347,9 @@
                   <void property="aggregators"> 
                    <object class="java.util.ArrayList"/> 
                   </void> 
+                  <void property="groupByMemoryUsage"> 
+                   <float>0.5</float> 
+                  </void> 
                   <void property="keys"> 
                    <object class="java.util.ArrayList"> 
                     <void method="add"> 
@@ -354,6 +357,9 @@
                     </void> 
                    </object> 
                   </void> 
+                  <void property="memoryThreshold"> 
+                   <float>0.9</float> 
+                  </void> 
                   <void property="mode"> 
                    <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
                     <string>HASH</string> 
@@ -610,7 +616,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -622,7 +628,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+       <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.PartitionDesc"> 
         <void property="baseFileName"> 
          <string>src</string> 
@@ -679,11 +685,11 @@
           </void> 
           <void method="put"> 
            <string>location</string> 
-           <string>pfile:/data/users/njain/hive_commit1/hive_commit1/build/ql/test/data/warehouse/src</string> 
+           <string>pfile:/data/users/njain/hive_commit2/hive_commit2/build/ql/test/data/warehouse/src</string> 
           </void> 
           <void method="put"> 
            <string>transient_lastDdlTime</string> 
-           <string>1290011540</string> 
+           <string>1291874251</string> 
           </void> 
          </object> 
         </void> 
@@ -726,13 +732,13 @@
               <void property="conf"> 
                <object class="org.apache.hadoop.hive.ql.plan.FileSinkDesc"> 
                 <void property="dirName"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-32-23_681_2824395582243541248/-ext-10001</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-57-34_504_8277508781558017027/-ext-10001</string> 
                 </void> 
                 <void property="numFiles"> 
                  <int>1</int> 
                 </void> 
                 <void property="statsAggPrefix"> 
-                 <string>file:/tmp/njain/hive_2010-11-17_08-32-23_681_2824395582243541248/-ext-10001/</string> 
+                 <string>file:/tmp/njain/hive_2010-12-08_21-57-34_504_8277508781558017027/-ext-10001/</string> 
                 </void> 
                 <void property="tableInfo"> 
                  <object class="org.apache.hadoop.hive.ql.plan.TableDesc"> 
@@ -926,6 +932,9 @@
         <void property="aggregators"> 
          <object class="java.util.ArrayList"/> 
         </void> 
+        <void property="groupByMemoryUsage"> 
+         <float>0.5</float> 
+        </void> 
         <void property="keys"> 
          <object class="java.util.ArrayList"> 
           <void method="add"> 
@@ -933,6 +942,9 @@
           </void> 
          </object> 
         </void> 
+        <void property="memoryThreshold"> 
+         <float>0.9</float> 
+        </void> 
         <void property="mode"> 
          <object class="org.apache.hadoop.hive.ql.plan.GroupByDesc$Mode" method="valueOf"> 
           <string>MERGEPARTIAL</string> 
