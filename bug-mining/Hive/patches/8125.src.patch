diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/RandomTypeUtil.java b/serde/src/java/org/apache/hadoop/hive/serde2/RandomTypeUtil.java
index ad9de4cd37..c0e0583a33 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/RandomTypeUtil.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/RandomTypeUtil.java
@@ -131,7 +131,7 @@ public static Date getRandDate(Random r) {
 
   public static final long NANOSECONDS_PER_SECOND = TimeUnit.SECONDS.toNanos(1);
   public static final long MILLISECONDS_PER_SECOND = TimeUnit.SECONDS.toMillis(1);
-  public static final long NANOSECONDS_PER_MILLISSECOND = TimeUnit.MILLISECONDS.toNanos(1);
+  public static final long NANOSECONDS_PER_MILLISECOND = TimeUnit.MILLISECONDS.toNanos(1);
 
   private static final ThreadLocal<DateFormat> DATE_FORMAT =
       new ThreadLocal<DateFormat>() {
@@ -172,12 +172,12 @@ public static Timestamp getRandTimestamp(Random r, int minYear, int maxYear) {
     case 2:
       // Limit to milliseconds only...
       optionalNanos = String.format(".%09d",
-          Integer.valueOf(r.nextInt((int) MILLISECONDS_PER_SECOND)) * NANOSECONDS_PER_MILLISSECOND);
+          Integer.valueOf(r.nextInt((int) MILLISECONDS_PER_SECOND)) * NANOSECONDS_PER_MILLISECOND);
       break;
     case 3:
       // Limit to below milliseconds only...
       optionalNanos = String.format(".%09d",
-          Integer.valueOf(r.nextInt((int) NANOSECONDS_PER_MILLISSECOND)));
+          Integer.valueOf(r.nextInt((int) NANOSECONDS_PER_MILLISECOND)));
       break;
     }
     String timestampStr = String.format("%04d-%02d-%02d %02d:%02d:%02d%s",
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeStatsStruct.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeStatsStruct.java
index cf86e5e671..8f0fa9dee2 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeStatsStruct.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeStatsStruct.java
@@ -21,7 +21,7 @@
 public interface SerDeStatsStruct {
 
   /**
-   * Rerurns the serialized size of the object.
+   * Returns the serialized size of the object.
    */
   public long getRawDataSerializedSize();
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
index 8156fee7ce..849425ea39 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java
@@ -112,7 +112,7 @@ public static String escapeString(String str) {
         escape.append('t');
         break;
       default:
-        // Control characeters! According to JSON RFC u0020
+        // Control characters! According to JSON RFC u0020
         if (c < ' ') {
           String hex = Integer.toHexString(c);
           escape.append('\\');
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java b/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
index 60ac3f370e..11d66277ef 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java
@@ -158,7 +158,7 @@ public GenericRecord reencode(GenericRecord r)
    * @param writable Instance of GenericAvroWritable to deserialize
    * @param readerSchema Schema of the writable to deserialize
    * @return A list of objects suitable for Hive to work with further
-   * @throws AvroSerdeException For any exception during deseriliazation
+   * @throws AvroSerdeException For any exception during deserialization
    */
   public Object deserialize(List<String> columnNames, List<TypeInfo> columnTypes,
                             Writable writable, Schema readerSchema) throws AvroSerdeException {
@@ -418,16 +418,16 @@ private Object deserializeUnion(Object datum, Schema fileSchema, Schema recordSc
     // and we would end up doing calculations twice to get the same tag
     int fsTag = GenericData.get().resolveUnion(fileSchema, datum); // Determine index of value from fileSchema
     int rsTag = GenericData.get().resolveUnion(recordSchema, datum); // Determine index of value from recordSchema
-    Object desered = worker(datum, fileSchema == null ? null : fileSchema.getTypes().get(fsTag),
+    Object desired = worker(datum, fileSchema == null ? null : fileSchema.getTypes().get(fsTag),
         recordSchema.getTypes().get(rsTag), columnType.getAllUnionObjectTypeInfos().get(rsTag));
-    return new StandardUnionObjectInspector.StandardUnion((byte)rsTag, desered);
+    return new StandardUnionObjectInspector.StandardUnion((byte)rsTag, desired);
   }
 
   private Object deserializeList(Object datum, Schema fileSchema, Schema recordSchema,
                                  ListTypeInfo columnType) throws AvroSerdeException {
     // Need to check the original schema to see if this is actually a Fixed.
     if(recordSchema.getType().equals(Schema.Type.FIXED)) {
-    // We're faking out Hive to work through a type system impedence mismatch.
+    // We're faking out Hive to work through a type system impedance mismatch.
     // Pull out the backing array and convert to a list.
       GenericData.Fixed fixed = (GenericData.Fixed) datum;
       List<Byte> asList = new ArrayList<Byte>(fixed.bytes().length);
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java b/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java
index eb04701457..409ad9d23f 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java
@@ -161,7 +161,7 @@ private Object serialize(TypeInfo typeInfo, ObjectInspector fieldOI, Object stru
 
   /** private cache to avoid lots of EnumSymbol creation while serializing.
    *  Two levels because the enum symbol is specific to a schema.
-   *  Object because we want to avoid the overhead of repeated toString calls while maintaining compatability.
+   *  Object because we want to avoid the overhead of repeated toString calls while maintaining compatibility.
    *  Provided there are few enum types per record, and few symbols per enum, memory use should be moderate.
    *  eg 20 types with 50 symbols each as length-10 Strings should be on the order of 100KB per AvroSerializer.
    */
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java b/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
index 5e8277d3c0..fe678e770a 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/columnar/ColumnarSerDe.java
@@ -88,7 +88,7 @@ public void initialize(Configuration configuration, Properties tableProperties,
     serdeParams = new LazySerDeParameters(configuration, properties, getClass().getName());
 
     // Create the ObjectInspectors for the fields. Note: Currently
-    // ColumnarObject uses same ObjectInpector as LazyStruct
+    // ColumnarObject uses same ObjectInspector as LazyStruct
     cachedObjectInspector = LazyFactory.createColumnarStructInspector(
         serdeParams.getColumnNames(), serdeParams.getColumnTypes(), serdeParams);
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/io/HiveCharWritable.java b/serde/src/java/org/apache/hadoop/hive/serde2/io/HiveCharWritable.java
index 70f907f32e..fcd14e5ec8 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/io/HiveCharWritable.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/io/HiveCharWritable.java
@@ -26,7 +26,7 @@
 /**
  * HiveCharWritable.
  * String values will be padded to full char length.
- * Character legnth, comparison, hashCode should ignore trailing spaces.
+ * Character length, comparison, hashCode should ignore trailing spaces.
  */
 public class HiveCharWritable extends HiveBaseCharWritable 
     implements WritableComparable<HiveCharWritable> {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonReader.java b/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonReader.java
index b24ca0c5e2..6e9bb4e226 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonReader.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonReader.java
@@ -469,7 +469,7 @@ private byte[] getByteValue(final JsonNode binaryNode) throws SerDeException {
   /**
    * Matches the JSON object's field name with the Hive data type.
    *
-   * @param oi The ObjectInsepctor to lookup the matching in
+   * @param oi The ObjectInspector to lookup the matching in
    * @param fieldName The name of the field parsed from the JSON text
    * @return The meta data of regarding this field
    * @throws SerDeException The SerDe is not configured correctly
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonWriter.java b/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonWriter.java
index cacc70bd30..cf5c56b8c3 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonWriter.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/json/HiveJsonWriter.java
@@ -171,7 +171,7 @@ private JsonNode walkObjectGraph(final ObjectInspector oi, final Object o,
   /**
    * Visit a vertex in the graph that is a Java Map.
    *
-   * @param oi The map's OjectInspector
+   * @param oi The map's ObjectInspector
    * @param o The Map object
    * @return A JsonNode representation of the Map
    * @throws SerDeException The Map cannot be parsed
@@ -199,7 +199,7 @@ private ObjectNode visitMap(final ObjectInspector oi, final Object o)
   /**
    * Visit a vertex in the graph that is a Java List.
    *
-   * @param oi The list's OjectInspector
+   * @param oi The list's ObjectInspector
    * @param o The List object
    * @return A JsonNode representation of the List
    * @throws SerDeException The List cannot be parsed
@@ -225,7 +225,7 @@ private ContainerNode<?> visitList(final ObjectInspector oi, final Object o)
    * represented as a Java List where the name associated with each element in
    * the list is stored in the ObjectInspector.
    *
-   * @param oi The struct's OjectInspector
+   * @param oi The struct's ObjectInspector
    * @param o The List object
    * @param fieldNames List of names to override the default field names
    * @return A JsonNode representation of the List
@@ -256,7 +256,7 @@ private ObjectNode visitStruct(final ObjectInspector oi, final Object o,
   /**
    * Visit a vertex in the graph that is a union data type.
    *
-   * @param oi The union's OjectInspector
+   * @param oi The union's ObjectInspector
    * @param o The Union object
    * @return A JsonNode representation of the union
    * @throws SerDeException The union cannot be parsed
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
index 883ba88df6..8b9c012d9b 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyFactory.java
@@ -84,7 +84,7 @@ public final class LazyFactory {
    * of the lazy object.
    *
    * @param poi PrimitiveObjectInspector
-   * @param typeBinary a switch to return either a LazyPrimtive class or it's binary
+   * @param typeBinary a switch to return either a LazyPrimitive class or it's binary
    *        companion
    * @return LazyPrimitive&lt;? extends ObjectInspector, ? extends Writable&gt;
    */
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
index 2ca023c5c1..093872d158 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java
@@ -58,7 +58,7 @@ public LazyHiveDecimal(LazyHiveDecimal copy) {
   }
 
   /**
-   * Initilizes LazyHiveDecimal object by interpreting the input bytes
+   * Initializes LazyHiveDecimal object by interpreting the input bytes
    * as a numeric string
    *
    * @param bytes
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java
index b4f8718ad8..2218942f41 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyTimestamp.java
@@ -46,7 +46,7 @@ public LazyTimestamp(LazyTimestamp copy) {
   }
 
   /**
-   * Initilizes LazyTimestamp object by interpreting the input bytes
+   * Initializes LazyTimestamp object by interpreting the input bytes
    * as a JDBC timestamp string
    *
    * @param bytes
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
index 348faaca2a..816a98ff28 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/StringToDouble.java
@@ -71,7 +71,7 @@ public static double strtod(byte[] utf8, int offset, int length)
     int c;
     int exp = 0;		/* Exponent read from "EX" field. */
     int fracExp = 0;		/* Exponent that derives from the fractional
-				 * part.  Under normal circumstatnces, it is
+				 * part.  Under normal circumstances, it is
 				 * the negative of the number of digits in F.
 				 * However, if I is very long, the last digits
 				 * of I get dropped (otherwise a long I with a
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryArray.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryArray.java
index f741a960c6..4e67b39a09 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryArray.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryArray.java
@@ -135,9 +135,9 @@ private void parse() {
     // adjust arrays
     adjustArraySize(arraySize);
     // find out the null-bytes
-    int arryByteStart = start + vInt.length;
-    int nullByteCur = arryByteStart;
-    int nullByteEnd = arryByteStart + (arraySize + 7) / 8;
+    int arrayByteStart = start + vInt.length;
+    int nullByteCur = arrayByteStart;
+    int nullByteEnd = arrayByteStart + (arraySize + 7) / 8;
     // the begin the real elements
     int lastElementByteEnd = nullByteEnd;
     // the list element object inspector
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe2.java b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe2.java
index 7a6e166ae0..f04d790a43 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe2.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe2.java
@@ -65,7 +65,7 @@
 /**
  * Subclass of LazyBinarySerDe with faster serialization, initializing a serializer based on the
  * row columns rather than checking the ObjectInspector category/primitiveType for every value.
- * This appears to be around 3x faster than the LazyBinarSerDe serialization.
+ * This appears to be around 3x faster than the LazyBinarySerDe serialization.
  */
 @SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS, serdeConstants.LIST_COLUMN_TYPES})
 public class LazyBinarySerDe2 extends LazyBinarySerDe {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ListObjectsEqualComparer.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ListObjectsEqualComparer.java
index a87459b65c..e90c5f601d 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ListObjectsEqualComparer.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ListObjectsEqualComparer.java
@@ -32,7 +32,7 @@
  * Compare two list of objects.
  * Two lists are expected to have same types. Type information for every object is
  * passed when calling Constructor to avoid the step of figuring out types from
- * ObjectInspetor and determine how to compare the types when comparing.
+ * ObjectInspector and determine how to compare the types when comparing.
  * Also, for string and text elements, it performs slightly better than
  * using ObjectInspectorUtils.compare() == 0, which instead of calling .compare()
  * calls .equalTo(), which compares size before byte by byte comparison.
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
index 9129177375..e7e0473a40 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java
@@ -439,7 +439,7 @@ public static class UnionConverter implements Converter {
     UnionObjectInspector inputOI;
     SettableUnionObjectInspector outputOI;
 
-    // Object inspectors for the tags for the input and output unionss
+    // Object inspectors for the tags for the input and output unions
     List<? extends ObjectInspector> inputTagsOIs;
     List<? extends ObjectInspector> outputTagsOIs;
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
index bf547def68..747564ad08 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java
@@ -631,7 +631,7 @@ public static String getObjectInspectorName(ObjectInspector oi) {
   /**
    * Computes the bucket number to which the bucketFields belong to
    * @param bucketFields  the bucketed fields of the row
-   * @param bucketFieldInspectors  the ObjectInpsectors for each of the bucketed fields
+   * @param bucketFieldInspectors  the ObjectInspectors for each of the bucketed fields
    * @param totalBuckets the number of buckets in the table
    * @return the bucket number using Murmur hash
    */
@@ -642,7 +642,7 @@ public static int getBucketNumber(Object[] bucketFields, ObjectInspector[] bucke
   /**
    * Computes the bucket number to which the bucketFields belong to
    * @param bucketFields  the bucketed fields of the row
-   * @param bucketFieldInspectors  the ObjectInpsectors for each of the bucketed fields
+   * @param bucketFieldInspectors  the ObjectInspectors for each of the bucketed fields
    * @param totalBuckets the number of buckets in the table
    * @return the bucket number
    */
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
index 23c7a0dd43..c61b4c3c58 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java
@@ -1418,7 +1418,7 @@ public static enum PrimitiveGrouping {
    * Based on the PrimitiveCategory of a type, return the PrimitiveGrouping
    * that the PrimitiveCategory belongs to (numeric, string, date, etc).
    * @param primitiveCategory Primitive category of the type
-   * @return PrimitveGrouping corresponding to the PrimitiveCategory,
+   * @return PrimitiveGrouping corresponding to the PrimitiveCategory,
    *         or UNKNOWN_GROUP if the type does not match to a grouping.
    */
   public static PrimitiveGrouping getPrimitiveGrouping(PrimitiveCategory primitiveCategory) {
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataInputStream.java b/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataInputStream.java
index 9e19da7828..a85b350545 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataInputStream.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataInputStream.java
@@ -136,7 +136,7 @@ public Date readDate() throws IOException, ParseException {
    * Read CHAR(N).
    * The representation of char in Teradata binary format is
    * the byte number to read is based on the [charLength] * [bytePerChar] &lt;- totalLength,
-   * bytePerChar is decided by the charset: LATAIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
+   * bytePerChar is decided by the charset: LATIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
    * the null char will use space to pad.
    *
    * @param totalLength the total length
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataOutputStream.java b/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataOutputStream.java
index 4cd2ac7d5f..c2648c0da2 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataOutputStream.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinaryDataOutputStream.java
@@ -165,7 +165,7 @@ public void writeLong(long l) throws IOException {
    * Write CHAR(N).
    * The representation of char in Teradata binary format is:
    * the byte number to read is based on the [charLength] * [bytePerChar] &lt;- totalLength,
-   * bytePerChar is decided by the charset: LATAIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
+   * bytePerChar is decided by the charset: LATIN charset is 2 bytes per char and UNICODE charset is 3 bytes per char.
    * the null char will use space to pad.
    *
    * @param writable the writable
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java b/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java
index 480d029de0..543ae14f9e 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/teradata/TeradataBinarySerde.java
@@ -412,8 +412,8 @@ private void serializeField(Object objectForField, ObjectInspector oi, TypeInfo
   private Object deserializeField(TeradataBinaryDataInputStream in, TypeInfo type, Object reuse, boolean isNull)
       throws IOException, ParseException, SerDeException {
     // isNull:
-    // In the Teradata Binary file, even the field is null (isNull=true),
-    // thd data still has some default values to pad the record.
+    // In the Teradata Binary file, even if the field is null (isNull=true),
+    // the data still has some default values to pad the record.
     // In this case, you cannot avoid reading the bytes even it is not used.
     switch (type.getCategory()) {
     case PRIMITIVE:
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TCTLSeparatedProtocol.java b/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TCTLSeparatedProtocol.java
index b04c3e72cf..d4590fccfa 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TCTLSeparatedProtocol.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/thrift/TCTLSeparatedProtocol.java
@@ -182,7 +182,7 @@ public String getMapSeparator() {
   protected final TTransport innerTransport;
 
   /**
-   * Strings used to lookup the various configurable paramaters of this
+   * Strings used to lookup the various configurable parameters of this
    * protocol.
    */
   public static final String ReturnNullsKey = "separators.return_nulls";
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TimestampLocalTZTypeInfo.java b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TimestampLocalTZTypeInfo.java
index a4f3af1112..e1f9a2699a 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TimestampLocalTZTypeInfo.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TimestampLocalTZTypeInfo.java
@@ -45,7 +45,7 @@ public String getTypeName() {
 
   @Override
   public void setTypeName(String typeName) {
-    // No need to set type name, it should always be timestamplocaltz
+    // No need to set type name, it should always be {@link serdeConstants.TIMESTAMPLOCALTZ_TYPE_NAME}
     return;
   }
 
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
index 24cd5d00b0..b23e3cfc44 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoUtils.java
@@ -712,9 +712,9 @@ public static ObjectInspector getStandardJavaObjectInspectorFromTypeInfo(
         break;
       }
       case STRUCT: {
-        StructTypeInfo strucTypeInfo = (StructTypeInfo) typeInfo;
-        List<String> fieldNames = strucTypeInfo.getAllStructFieldNames();
-        List<TypeInfo> fieldTypeInfos = strucTypeInfo
+        StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
+        List<String> fieldNames = structTypeInfo.getAllStructFieldNames();
+        List<TypeInfo> fieldTypeInfos = structTypeInfo
             .getAllStructFieldTypeInfos();
         List<ObjectInspector> fieldObjectInspectors = new ArrayList<ObjectInspector>(
             fieldTypeInfos.size());
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/SerdeRandomRowSource.java b/serde/src/test/org/apache/hadoop/hive/serde2/SerdeRandomRowSource.java
index e77083c5dd..8ed0db8a34 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/SerdeRandomRowSource.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/SerdeRandomRowSource.java
@@ -457,7 +457,7 @@ private void chooseSchema(SupportedTypes supportedTypes, int maxComplexDepth) {
         primitiveObjectInspectorList.add(null);
         break;
       default:
-        throw new RuntimeException("Unexpected catagory " + category);
+        throw new RuntimeException("Unexpected category " + category);
       }
       objectInspectorList.add(objectInspector);
 
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/TestJsonSerDe.java b/serde/src/test/org/apache/hadoop/hive/serde2/TestJsonSerDe.java
index 1b24fd98e5..1135f9f384 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/TestJsonSerDe.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/TestJsonSerDe.java
@@ -49,7 +49,7 @@
 public class TestJsonSerDe {
 
   @Test
-  public void testPrimativeDataTypes() throws Exception {
+  public void testPrimitiveDataTypes() throws Exception {
     Properties props = new Properties();
     props.setProperty(serdeConstants.LIST_COLUMNS,
         "name,height,weight,endangered,born");
@@ -168,7 +168,7 @@ public void testArray() throws Exception {
   /**
    * Test when a map has a key defined as a numeric value. Technically, JSON
    * does not support this because each key in a map must be a quoted string.
-   * Unquoted strings (hence an int value) is allowed by Javascript, but not by
+   * Unquoted strings (hence an int value) is allowed by JavaScript, but not by
    * JSON specification. For Hive, the int map key type is stored as a string
    * and must be converted back into an int type.
    */
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/TestTCTLSeparatedProtocol.java b/serde/src/test/org/apache/hadoop/hive/serde2/TestTCTLSeparatedProtocol.java
index d5d6566136..9c87ecbe29 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/TestTCTLSeparatedProtocol.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/TestTCTLSeparatedProtocol.java
@@ -231,12 +231,12 @@ public void testWrites() throws Exception {
     assertTrue(prot.readString().equals("bye!"));
     prot.readFieldEnd();
 
-    // shouldl return nulls at end
+    // should return nulls at end
     prot.readFieldBegin();
     assertNull(prot.readString());
     prot.readFieldEnd();
 
-    // shouldl return nulls at end
+    // should return nulls at end
     prot.readFieldBegin();
     assertNull(prot.readString());
     prot.readFieldEnd();
@@ -299,12 +299,12 @@ public void testQuotedWrites() throws Exception {
     prot.readListEnd();
     prot.readFieldEnd();
 
-    // shouldl return nulls at end
+    // should return nulls at end
     prot.readFieldBegin();
     assertNull(prot.readString());
     prot.readFieldEnd();
 
-    // shouldl return nulls at end
+    // should return nulls at end
     prot.readFieldBegin();
     assertNull(prot.readString());
     prot.readFieldEnd();
@@ -530,7 +530,7 @@ public void checkReadBytesAvailable(long l) throws TTransportException {
     separatedProtocol.initialize(null, new Properties());
     try {
       separatedProtocol.readStructBegin();
-      fail("Runtime Exception is expected if the intialization of tokenizer failed.");
+      fail("Runtime Exception is expected if the initialization of tokenizer failed.");
     } catch (Exception e) {
       assertTrue(e.getCause() instanceof TTransportException);
     }
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java b/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java
index f9ac97a6d1..24800dad5a 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java
@@ -510,7 +510,7 @@ public void canDeserializeEnums() throws SerDeException, IOException {
     assertEquals("DALEKS", finalValue);
   }
 
-  @Test // Fixed doesn't exist in Hive. Fixeds go in, lists of bytes go out.
+  @Test // Fixed doesn't exist in Hive. Fixed go in, lists of bytes go out.
   public void canDeserializeFixed() throws SerDeException, IOException {
     Schema s = AvroSerdeUtils.getSchemaFor(TestAvroObjectInspectorGenerator.FIXED_SCHEMA);
     GenericData.Record record = new GenericData.Record(s);
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java b/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java
index 0029c3f253..048d3d970d 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java
@@ -162,7 +162,7 @@ public class TestAvroObjectInspectorGenerator {
       "  \"fields\": [\n" +
       "   {\n" +
       "      \"name\":\"baddies\",\n" +
-      "      \"type\":{\"type\":\"enum\",\"name\":\"villians\", \"symbols\": " +
+      "      \"type\":{\"type\":\"enum\",\"name\":\"villains\", \"symbols\": " +
           "[\"DALEKS\", \"CYBERMEN\", \"SLITHEEN\", \"JAGRAFESS\"]}\n" +
       "      \n" +
       "      \n" +
@@ -206,7 +206,7 @@ public class TestAvroObjectInspectorGenerator {
       "  \"fields\": [\n" +
       "   {\n" +
       "      \"name\":\"nullableEnum\",\n" +
-      "      \"type\": [\"null\", {\"type\":\"enum\",\"name\":\"villians\", \"symbols\": " +
+      "      \"type\": [\"null\", {\"type\":\"enum\",\"name\":\"villains\", \"symbols\": " +
           "[\"DALEKS\", \"CYBERMEN\", \"SLITHEEN\", \"JAGRAFESS\"]}]\n" +
       "      \n" +
       "      \n" +
@@ -412,8 +412,8 @@ public void canHandleMapsWithPrimitiveValueTypes() throws SerDeException {
   /**
    * Check a given AvroObjectInspectorGenerator to verify that it matches our test
    * schema's expected map.
-   * @param aoig should already have been intitialized, may not be null
-   * @param fieldName name of the contianed column, will always fail if null.
+   * @param aoig should already have been initialized, may not be null
+   * @param fieldName name of the contained column, will always fail if null.
    */
   private void verifyMap(final AvroObjectInspectorGenerator aoig, final String fieldName) {
     // Column names
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java b/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java
index 4e1ee69fe3..3ef802d605 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerdeUtils.java
@@ -48,7 +48,7 @@ public class TestAvroSerdeUtils {
       "    {\"name\":\"mayBeNull\", \"type\":[\"string\", \"null\"]}\n" +
       "  ]\n" +
       "}";
-  // Same union, order reveresed
+  // Same union, order reversed
   private final String NULLABLE_UNION2 = "{\n" +
     "  \"type\": \"record\", \n" +
     "  \"name\": \"nullTest\",\n" +
@@ -151,7 +151,7 @@ public void determineSchemaFindsLiterals() throws Exception {
   }
 
   @Test
-  public void detemineSchemaTriesToOpenUrl() throws AvroSerdeException, IOException {
+  public void determineSchemaTriesToOpenUrl() throws AvroSerdeException, IOException {
     Configuration conf = new Configuration();
     Properties props = new Properties();
     props.put(AvroTableProperties.SCHEMA_URL.getPropName(), "not:///a.real.url");
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
index 68ba2c25fe..dbf84da2f3 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
@@ -338,7 +338,7 @@ private void verifyRead(BinarySortableDeserializeRead binarySortableDeserializeR
         }
       }
       if (!VerifyLazy.lazyCompare(typeInfo, complexFieldObj, expectedObject)) {
-        fail("Comparision failed typeInfo " + typeInfo.toString());
+        fail("Comparison failed typeInfo " + typeInfo.toString());
       }
     }
   }
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleFast.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleFast.java
index 8187419f3f..92b6176e88 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleFast.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazy/TestLazySimpleFast.java
@@ -244,7 +244,7 @@ private void verifyRead(LazySimpleDeserializeRead lazySimpleDeserializeRead,
         }
       }
       if (!VerifyLazy.lazyCompare(typeInfo, complexFieldObj, expectedObject)) {
-        fail("Comparision failed typeInfo " + typeInfo.toString());
+        fail("Comparison failed typeInfo " + typeInfo.toString());
       }
     }
   }
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
index ae23809278..fd1f23e401 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/lazybinary/TestLazyBinaryFast.java
@@ -240,7 +240,7 @@ private void verifyRead(LazyBinaryDeserializeRead lazyBinaryDeserializeRead,
         }
       }
       if (!VerifyLazy.lazyCompare(typeInfo, complexFieldObj, expectedObject)) {
-        fail("Comparision failed typeInfo " + typeInfo.toString());
+        fail("Comparison failed typeInfo " + typeInfo.toString());
       }
     }
   }
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java b/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
index f829f9412f..08a4f2d253 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestObjectInspectorConverters.java
@@ -85,7 +85,7 @@ public void testObjectInspectorConverters() throws Throwable {
       convertText();
 
       // Binary
-      converBinary();
+      convertBinary();
 
       // Union
       convertUnion();
@@ -185,7 +185,7 @@ private void convertUnion() {
       assertEquals(expectedObjectExtra, convertedObjectExtra); // we should get back null
 }
 
-private void converBinary() {
+private void convertBinary() {
 	Converter baConverter = ObjectInspectorConverters.getConverter(
           PrimitiveObjectInspectorFactory.javaStringObjectInspector,
           PrimitiveObjectInspectorFactory.writableBinaryObjectInspector);
