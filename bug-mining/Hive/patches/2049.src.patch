diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 551639fc4f..d6441035c7 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -989,7 +989,8 @@ public static enum ConfVars {
     // Setting to 0.12:
     //   Maintains division behavior: int / int => double
     // Setting to 0.13:
-    HIVE_COMPAT("hive.compat", HiveCompat.DEFAULT_COMPAT_LEVEL)
+    HIVE_COMPAT("hive.compat", HiveCompat.DEFAULT_COMPAT_LEVEL),
+    HIVE_CONVERT_JOIN_BUCKET_MAPJOIN_TEZ("hive.convert.join.bucket.mapjoin.tez", false)
     ;
 
     public final String varname;
diff --git a/itests/qtest/pom.xml b/itests/qtest/pom.xml
index 666d3b8deb..dc93c1c73d 100644
--- a/itests/qtest/pom.xml
+++ b/itests/qtest/pom.xml
@@ -38,7 +38,7 @@
     <execute.beeline.tests>false</execute.beeline.tests>
     <minimr.query.files>stats_counter_partitioned.q,list_bucket_dml_10.q,input16_cc.q,scriptfile1.q,scriptfile1_win.q,bucket4.q,bucketmapjoin6.q,disable_merge_for_bucketing.q,reduce_deduplicate.q,smb_mapjoin_8.q,join1.q,groupby2.q,bucketizedhiveinputformat.q,bucketmapjoin7.q,optrstat_groupby.q,bucket_num_reducers.q,bucket5.q,load_fs2.q,bucket_num_reducers2.q,infer_bucket_sort_merge.q,infer_bucket_sort_reducers_power_two.q,infer_bucket_sort_dyn_part.q,infer_bucket_sort_bucketed_table.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_num_buckets.q,leftsemijoin_mr.q,schemeAuthority.q,schemeAuthority2.q,truncate_column_buckets.q,remote_script.q,,load_hdfs_file_with_space_in_the_name.q,parallel_orderby.q,import_exported_table.q,stats_counter.q,auto_sortmerge_join_16.q,quotedid_smb.q,file_with_header_footer.q,external_table_with_space_in_location_path.q,root_dir_external_table.q,index_bitmap3.q,ql_rewrite_gbtoidx.q,index_bitmap_auto.q,udf_using.q</minimr.query.files>
     <minimr.query.negative.files>cluster_tasklog_retrieval.q,minimr_broken_pipe.q,mapreduce_stack_trace.q,mapreduce_stack_trace_turnoff.q,mapreduce_stack_trace_hadoop20.q,mapreduce_stack_trace_turnoff_hadoop20.q,file_with_header_footer_negative.q,udf_local_resource.q</minimr.query.negative.files>
-    <minitez.query.files>mapjoin_decimal.q,tez_join_tests.q,tez_joins_explain.q,mrr.q,tez_dml.q,tez_insert_overwrite_local_directory_1.q,tez_union.q</minitez.query.files>
+    <minitez.query.files>mapjoin_decimal.q,tez_join_tests.q,tez_joins_explain.q,mrr.q,tez_dml.q,tez_insert_overwrite_local_directory_1.q,tez_union.q,bucket_map_join_tez1.q,bucket_map_join_tez2.q</minitez.query.files>
     <minitez.query.files.shared>dynpart_sort_opt_vectorization.q,dynpart_sort_optimization.q,orc_analyze.q,join0.q,join1.q,auto_join0.q,auto_join1.q,bucket2.q,bucket3.q,bucket4.q,count.q,create_merge_compressed.q,cross_join.q,ctas.q,custom_input_output_format.q,disable_merge_for_bucketing.q,enforce_order.q,filter_join_breaktask.q,filter_join_breaktask2.q,groupby1.q,groupby2.q,groupby3.q,having.q,insert1.q,insert_into1.q,insert_into2.q,leftsemijoin.q,limit_pushdown.q,load_dyn_part1.q,load_dyn_part2.q,load_dyn_part3.q,mapjoin_mapjoin.q,mapreduce1.q,mapreduce2.q,merge1.q,merge2.q,metadata_only_queries.q,sample1.q,subquery_in.q,subquery_exists.q,vectorization_15.q,ptf.q,stats_counter.q,stats_noscan_1.q,stats_counter_partitioned.q,union2.q,union3.q,union4.q,union5.q,union6.q,union7.q,union8.q,union9.q</minitez.query.files.shared>
     <beeline.positive.exclude>add_part_exist.q,alter1.q,alter2.q,alter4.q,alter5.q,alter_rename_partition.q,alter_rename_partition_authorization.q,archive.q,archive_corrupt.q,archive_multi.q,archive_mr_1806.q,archive_multi_mr_1806.q,authorization_1.q,authorization_2.q,authorization_4.q,authorization_5.q,authorization_6.q,authorization_7.q,ba_table1.q,ba_table2.q,ba_table3.q,ba_table_udfs.q,binary_table_bincolserde.q,binary_table_colserde.q,cluster.q,columnarserde_create_shortcut.q,combine2.q,constant_prop.q,create_nested_type.q,create_or_replace_view.q,create_struct_table.q,create_union_table.q,database.q,database_location.q,database_properties.q,ddltime.q,describe_database_json.q,drop_database_removes_partition_dirs.q,escape1.q,escape2.q,exim_00_nonpart_empty.q,exim_01_nonpart.q,exim_02_00_part_empty.q,exim_02_part.q,exim_03_nonpart_over_compat.q,exim_04_all_part.q,exim_04_evolved_parts.q,exim_05_some_part.q,exim_06_one_part.q,exim_07_all_part_over_nonoverlap.q,exim_08_nonpart_rename.q,exim_09_part_spec_nonoverlap.q,exim_10_external_managed.q,exim_11_managed_external.q,exim_12_external_location.q,exim_13_managed_location.q,exim_14_managed_location_over_existing.q,exim_15_external_part.q,exim_16_part_external.q,exim_17_part_managed.q,exim_18_part_external.q,exim_19_00_part_external_location.q,exim_19_part_external_location.q,exim_20_part_managed_location.q,exim_21_export_authsuccess.q,exim_22_import_exist_authsuccess.q,exim_23_import_part_authsuccess.q,exim_24_import_nonexist_authsuccess.q,global_limit.q,groupby_complex_types.q,groupby_complex_types_multi_single_reducer.q,index_auth.q,index_auto.q,index_auto_empty.q,index_bitmap.q,index_bitmap1.q,index_bitmap2.q,index_bitmap3.q,index_bitmap_auto.q,index_bitmap_rc.q,index_compact.q,index_compact_1.q,index_compact_2.q,index_compact_3.q,index_stale_partitioned.q,init_file.q,input16.q,input16_cc.q,input46.q,input_columnarserde.q,input_dynamicserde.q,input_lazyserde.q,input_testxpath3.q,input_testxpath4.q,insert2_overwrite_partitions.q,insertexternal1.q,join_thrift.q,lateral_view.q,load_binary_data.q,load_exist_part_authsuccess.q,load_nonpart_authsuccess.q,load_part_authsuccess.q,loadpart_err.q,lock1.q,lock2.q,lock3.q,lock4.q,merge_dynamic_partition.q,multi_insert.q,multi_insert_move_tasks_share_dependencies.q,null_column.q,ppd_clusterby.q,query_with_semi.q,rename_column.q,sample6.q,sample_islocalmode_hook.q,set_processor_namespaces.q,show_tables.q,source.q,split_sample.q,str_to_map.q,transform1.q,udaf_collect_set.q,udaf_context_ngrams.q,udaf_histogram_numeric.q,udaf_ngrams.q,udaf_percentile_approx.q,udf_array.q,udf_bitmap_and.q,udf_bitmap_or.q,udf_explode.q,udf_format_number.q,udf_map.q,udf_map_keys.q,udf_map_values.q,udf_max.q,udf_min.q,udf_named_struct.q,udf_percentile.q,udf_printf.q,udf_sentences.q,udf_sort_array.q,udf_split.q,udf_struct.q,udf_substr.q,udf_translate.q,udf_union.q,udf_xpath.q,udtf_stack.q,view.q,virtual_column.q</beeline.positive.exclude>
   </properties>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
index 3ea9c96a03..1104a2b1a1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java
@@ -100,8 +100,10 @@ protected void initializeOp(Configuration hconf) throws HiveException {
     mapJoinTables = (MapJoinTableContainer[]) cache.retrieve(tableKey);
     mapJoinTableSerdes = (MapJoinTableContainerSerDe[]) cache.retrieve(serdeKey);
     hashTblInitedOnce = true;
+    LOG.info("Try to retrieve from cache");
 
     if (mapJoinTables == null || mapJoinTableSerdes == null) {
+      LOG.info("Did not find tables in cache");
       mapJoinTables = new MapJoinTableContainer[tagLen];
       mapJoinTableSerdes = new MapJoinTableContainerSerDe[tagLen];
       hashTblInitedOnce = false;
@@ -148,8 +150,19 @@ private void loadHashTable() throws HiveException {
     perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);
     loader.init(getExecContext(), hconf, this);
     loader.load(mapJoinTables, mapJoinTableSerdes);
-    cache.cache(tableKey, mapJoinTables);
-    cache.cache(serdeKey, mapJoinTableSerdes);
+    if (conf.isBucketMapJoin() == false) {
+      /*
+       * The issue with caching in case of bucket map join is that different tasks
+       * process different buckets and if the container is reused to join a different bucket,
+       * join results can be incorrect. The cache is keyed on operator id and for bucket map join
+       * the operator does not change but data needed is different. For a proper fix, this
+       * requires changes in the Tez API with regard to finding bucket id and 
+       * also ability to schedule tasks to re-use containers that have cached the specific bucket.
+       */
+      LOG.info("This is not bucket map join, so cache");
+      cache.cache(tableKey, mapJoinTables);
+      cache.cache(serdeKey, mapJoinTableSerdes);
+    }
     perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.LOAD_HASHTABLE);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index f1a8ee18f0..36671b6dff 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -39,6 +39,7 @@
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 import org.apache.hadoop.hive.ql.plan.Explain;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.OpTraits;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.Statistics;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
@@ -1246,6 +1247,25 @@ public Statistics getStatistics() {
     }
     return null;
   }
+  
+  public OpTraits getOpTraits() {
+    if (conf != null) {
+      return conf.getOpTraits();
+    }
+    
+    return null;
+  }
+  
+  public void setOpTraits(OpTraits metaInfo) {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Setting traits ("+metaInfo+") on "+this);
+    }
+    if (conf != null) {
+      conf.setOpTraits(metaInfo);
+    } else {
+      LOG.warn("Cannot set traits when there's no descriptor: "+this);
+    }
+  }
 
   public void setStatistics(Statistics stats) {
     if (LOG.isDebugEnabled()) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomEdgeConfiguration.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomEdgeConfiguration.java
new file mode 100644
index 0000000000..dec47a94f9
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomEdgeConfiguration.java
@@ -0,0 +1,92 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.tez;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map.Entry;
+
+import org.apache.hadoop.io.Writable;
+
+import com.google.common.collect.LinkedListMultimap;
+import com.google.common.collect.Multimap;
+
+class CustomEdgeConfiguration implements Writable {
+  boolean vertexInited = false;
+  int numBuckets = -1;
+  Multimap<Integer, Integer> bucketToTaskMap = null;
+  
+  public CustomEdgeConfiguration() {
+  }
+  
+  public CustomEdgeConfiguration(int numBuckets, Multimap<Integer, Integer> routingTable) {
+    this.bucketToTaskMap = routingTable;
+    this.numBuckets = numBuckets;
+    if (routingTable != null) {
+      vertexInited = true;
+    }
+  }
+  
+  @Override
+  public void write(DataOutput out) throws IOException {
+    out.writeBoolean(vertexInited);
+    out.writeInt(numBuckets);
+    if (bucketToTaskMap == null) {
+      return;
+    }
+    
+    out.writeInt(bucketToTaskMap.size());
+    for (Entry<Integer, Collection<Integer>> entry : bucketToTaskMap.asMap().entrySet()) {
+      int bucketNum = entry.getKey();
+      for (Integer taskId : entry.getValue()) {
+        out.writeInt(bucketNum);
+        out.writeInt(taskId);
+      }
+    }
+  }
+
+  @Override
+  public void readFields(DataInput in) throws IOException {
+    this.vertexInited = in.readBoolean();
+    this.numBuckets = in.readInt();
+    if (this.vertexInited == false) {
+      return;
+    }
+
+    int count = in.readInt();
+    bucketToTaskMap = LinkedListMultimap.create();
+    for (int i = 0; i < count; i++) {
+      bucketToTaskMap.put(in.readInt(), in.readInt());
+    }
+
+    if (count != bucketToTaskMap.size()) {
+      throw new IOException("Was not a clean translation. Some records are missing");
+    }
+  }
+
+  public Multimap<Integer, Integer> getRoutingTable() {
+    return bucketToTaskMap;
+  }
+
+  public int getNumBuckets() {
+    return numBuckets;
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionEdge.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionEdge.java
new file mode 100644
index 0000000000..74c5429587
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionEdge.java
@@ -0,0 +1,112 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.tez;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.io.DataInputBuffer;
+import org.apache.tez.dag.api.EdgeManager;
+import org.apache.tez.dag.api.EdgeManagerContext;
+import org.apache.tez.runtime.api.events.DataMovementEvent;
+import org.apache.tez.runtime.api.events.InputReadErrorEvent;
+
+import com.google.common.collect.Multimap;
+
+public class CustomPartitionEdge implements EdgeManager {
+
+  private static final Log LOG = LogFactory.getLog(CustomPartitionEdge.class.getName());
+
+  CustomEdgeConfiguration conf = null;
+
+  // used by the framework at runtime. initialize is the real initializer at runtime
+  public CustomPartitionEdge() {  
+  }
+
+  @Override
+  public int getNumDestinationTaskPhysicalInputs(int numSourceTasks, 
+      int destinationTaskIndex) {
+    return numSourceTasks;
+  }
+
+  @Override
+  public int getNumSourceTaskPhysicalOutputs(int numDestinationTasks, 
+      int sourceTaskIndex) {
+    return conf.getNumBuckets();
+  }
+
+  @Override
+  public int getNumDestinationConsumerTasks(int sourceTaskIndex, int numDestinationTasks) {
+    return numDestinationTasks;
+  }
+
+  // called at runtime to initialize the custom edge.
+  @Override
+  public void initialize(EdgeManagerContext context) {
+    byte[] payload = context.getUserPayload();
+    LOG.info("Initializing the edge, payload: " + payload);
+    if (payload == null) {
+      throw new RuntimeException("Invalid payload");
+    }
+    // De-serialization code
+    DataInputBuffer dib = new DataInputBuffer();
+    dib.reset(payload, payload.length);
+    conf = new CustomEdgeConfiguration();
+    try {
+      conf.readFields(dib);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+
+    LOG.info("Routing table: " + conf.getRoutingTable() + " num Buckets: " + conf.getNumBuckets());
+  }
+
+  @Override
+  public void routeDataMovementEventToDestination(DataMovementEvent event,
+      int sourceTaskIndex, int numDestinationTasks, Map<Integer, List<Integer>> mapDestTaskIndices) {
+    int srcIndex = event.getSourceIndex();
+    List<Integer> destTaskIndices = new ArrayList<Integer>();
+    destTaskIndices.addAll(conf.getRoutingTable().get(srcIndex));
+    mapDestTaskIndices.put(new Integer(sourceTaskIndex), destTaskIndices);
+  }
+
+  @Override
+  public void routeInputSourceTaskFailedEventToDestination(int sourceTaskIndex, 
+      int numDestinationTasks, Map<Integer, List<Integer>> mapDestTaskIndices) {
+    List<Integer> destTaskIndices = new ArrayList<Integer>();
+    addAllDestinationTaskIndices(numDestinationTasks, destTaskIndices);
+    mapDestTaskIndices.put(new Integer(sourceTaskIndex), destTaskIndices);
+  }
+
+  @Override
+  public int routeInputErrorEventToSource(InputReadErrorEvent event, 
+      int destinationTaskIndex) {
+    return event.getIndex();
+  }
+
+  void addAllDestinationTaskIndices(int numDestinationTasks, List<Integer> taskIndices) {
+    for(int i=0; i<numDestinationTasks; ++i) {
+      taskIndices.add(new Integer(i));
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java
new file mode 100644
index 0000000000..04c497a4c7
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java
@@ -0,0 +1,402 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.tez;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.TreeMap;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.ql.io.HiveInputFormat;
+import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.serializer.SerializationFactory;
+import org.apache.hadoop.mapred.FileSplit;
+import org.apache.hadoop.mapred.InputSplit;
+import org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat;
+import org.apache.hadoop.mapred.split.TezMapredSplitsGrouper;
+import org.apache.tez.dag.api.EdgeManagerDescriptor;
+import org.apache.tez.dag.api.EdgeProperty;
+import org.apache.tez.dag.api.EdgeProperty.DataMovementType;
+import org.apache.tez.dag.api.VertexLocationHint.TaskLocationHint;
+import org.apache.tez.dag.api.InputDescriptor;
+import org.apache.tez.dag.api.TezConfiguration;
+import org.apache.tez.dag.api.VertexLocationHint;
+import org.apache.tez.dag.api.VertexManagerPlugin;
+import org.apache.tez.dag.api.VertexManagerPluginContext;
+import org.apache.tez.mapreduce.hadoop.MRHelpers;
+import org.apache.tez.mapreduce.protos.MRRuntimeProtos.MRInputUserPayloadProto;
+import org.apache.tez.mapreduce.protos.MRRuntimeProtos.MRSplitProto;
+import org.apache.tez.runtime.api.Event;
+import org.apache.tez.runtime.api.events.RootInputConfigureVertexTasksEvent;
+import org.apache.tez.runtime.api.events.RootInputDataInformationEvent;
+import org.apache.tez.runtime.api.events.RootInputUpdatePayloadEvent;
+import org.apache.tez.runtime.api.events.VertexManagerEvent;
+
+import com.google.common.base.Function;
+import com.google.common.base.Preconditions;
+import com.google.common.collect.ArrayListMultimap;
+import com.google.common.collect.HashMultimap;
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Lists;
+import com.google.common.collect.Maps;
+import com.google.common.collect.Multimap;
+
+
+/*
+ * Only works with old mapred API
+ * Will only work with a single MRInput for now.
+ */
+public class CustomPartitionVertex implements VertexManagerPlugin {
+
+  private static final Log LOG = LogFactory.getLog(CustomPartitionVertex.class.getName());
+
+
+  VertexManagerPluginContext context;
+
+  private Multimap<Integer, Integer> bucketToTaskMap = HashMultimap.<Integer, Integer>create();
+  private Multimap<Integer, InputSplit> bucketToInitialSplitMap = 
+      ArrayListMultimap.<Integer, InputSplit>create();
+
+  private RootInputConfigureVertexTasksEvent configureVertexTaskEvent;
+  private List<RootInputDataInformationEvent> dataInformationEvents;
+  private Map<Path, List<FileSplit>> pathFileSplitsMap = new TreeMap<Path, List<FileSplit>>();
+  private int numBuckets = -1;
+  private Configuration conf = null;
+  private boolean rootVertexInitialized = false;
+  Multimap<Integer, InputSplit> bucketToGroupedSplitMap;
+
+
+  private Map<Integer, Integer> bucketToNumTaskMap = new HashMap<Integer, Integer>();
+
+  public CustomPartitionVertex() {
+  }
+
+  @Override
+  public void initialize(VertexManagerPluginContext context) {
+    this.context = context; 
+    ByteBuffer byteBuf = ByteBuffer.wrap(context.getUserPayload());
+    this.numBuckets = byteBuf.getInt();
+  }
+
+  @Override
+  public void onVertexStarted(Map<String, List<Integer>> completions) {
+    int numTasks = context.getVertexNumTasks(context.getVertexName());
+    List<Integer> scheduledTasks = new ArrayList<Integer>(numTasks);
+    for (int i=0; i<numTasks; ++i) {
+      scheduledTasks.add(new Integer(i));
+    }
+    context.scheduleVertexTasks(scheduledTasks);
+  }
+
+  @Override
+  public void onSourceTaskCompleted(String srcVertexName, Integer attemptId) {
+  }
+
+  @Override
+  public void onVertexManagerEventReceived(VertexManagerEvent vmEvent) {
+  }
+
+  // One call per root Input - and for now only one is handled.
+  @Override
+  public void onRootVertexInitialized(String inputName, InputDescriptor inputDescriptor,
+      List<Event> events) {
+
+    // Ideally, since there's only 1 Input expected at the moment -
+    // ensure this method is called only once. Tez will call it once per Root Input.
+    Preconditions.checkState(rootVertexInitialized == false);
+    rootVertexInitialized = true;
+    try {
+      // This is using the payload from the RootVertexInitializer corresponding
+      // to InputName. Ideally it should be using it's own configuration class - but that
+      // means serializing another instance.
+      MRInputUserPayloadProto protoPayload = 
+          MRHelpers.parseMRInputPayload(inputDescriptor.getUserPayload());
+      this.conf = MRHelpers.createConfFromByteString(protoPayload.getConfigurationBytes());
+
+      /*
+       * Currently in tez, the flow of events is thus: "Generate Splits -> Initialize Vertex"
+       * (with parallelism info obtained from the generate splits phase). The generate splits
+       * phase groups splits using the TezGroupedSplitsInputFormat. However, for bucket map joins
+       * the grouping done by this input format results in incorrect results as the grouper has no
+       * knowledge of buckets. So, we initially set the input format to be HiveInputFormat
+       * (in DagUtils) for the case of bucket map joins so as to obtain un-grouped splits.
+       * We then group the splits corresponding to buckets using the tez grouper which returns
+       * TezGroupedSplits.
+       */
+
+      // This assumes that Grouping will always be used. 
+      // Changing the InputFormat - so that the correct one is initialized in MRInput.
+      this.conf.set("mapred.input.format.class", TezGroupedSplitsInputFormat.class.getName());
+      MRInputUserPayloadProto updatedPayload = MRInputUserPayloadProto
+          .newBuilder(protoPayload)
+          .setConfigurationBytes(MRHelpers.createByteStringFromConf(conf))
+          .build();
+      inputDescriptor.setUserPayload(updatedPayload.toByteArray());
+    } catch (IOException e) {
+      e.printStackTrace();
+      throw new RuntimeException(e);
+    }
+    boolean dataInformationEventSeen = false;
+    for (Event event : events) {
+      if (event instanceof RootInputConfigureVertexTasksEvent) {
+        // No tasks should have been started yet. Checked by initial state check.
+        Preconditions.checkState(dataInformationEventSeen == false);
+        Preconditions
+        .checkState(
+            context.getVertexNumTasks(context.getVertexName()) == -1,
+            "Parallelism for the vertex should be set to -1 if the InputInitializer is setting parallelism");
+        RootInputConfigureVertexTasksEvent cEvent = (RootInputConfigureVertexTasksEvent) event;
+
+        // The vertex cannot be configured until all DataEvents are seen - to build the routing table.
+        configureVertexTaskEvent = cEvent;
+        dataInformationEvents = Lists.newArrayListWithCapacity(configureVertexTaskEvent.getNumTasks());
+      }
+      if (event instanceof RootInputUpdatePayloadEvent) {
+        // this event can never occur. If it does, fail.
+        Preconditions.checkState(false);
+      } else if (event instanceof RootInputDataInformationEvent) {
+        dataInformationEventSeen = true;
+        RootInputDataInformationEvent diEvent = (RootInputDataInformationEvent) event;
+        dataInformationEvents.add(diEvent);
+        FileSplit fileSplit;
+        try {
+          fileSplit = getFileSplitFromEvent(diEvent);
+        } catch (IOException e) {
+          throw new RuntimeException("Failed to get file split for event: " + diEvent);
+        }
+        List<FileSplit> fsList = pathFileSplitsMap.get(fileSplit.getPath()); 
+        if (fsList == null) {
+          fsList = new ArrayList<FileSplit>();
+          pathFileSplitsMap.put(fileSplit.getPath(), fsList);
+        }
+        fsList.add(fileSplit);
+      }
+    }
+
+    setBucketNumForPath(pathFileSplitsMap);
+    try {
+      groupSplits();
+      processAllEvents(inputName);
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  private void processAllEvents(String inputName) throws IOException {
+
+    List<InputSplit> finalSplits = Lists.newLinkedList();
+    int taskCount = 0;
+    for (Entry<Integer, Collection<InputSplit>> entry : bucketToGroupedSplitMap.asMap().entrySet()) {
+      int bucketNum = entry.getKey();
+      Collection<InputSplit> initialSplits = entry.getValue();
+      finalSplits.addAll(initialSplits);
+      for (int i = 0; i < initialSplits.size(); i++) {
+        bucketToTaskMap.put(bucketNum, taskCount);
+        taskCount++;
+      }
+    }
+
+    // Construct the EdgeManager descriptor to be used by all edges which need the routing table.
+    EdgeManagerDescriptor hiveEdgeManagerDesc = new EdgeManagerDescriptor(
+        CustomPartitionEdge.class.getName());    
+    byte[] payload = getBytePayload(bucketToTaskMap);
+    hiveEdgeManagerDesc.setUserPayload(payload);
+
+    Map<String, EdgeManagerDescriptor> emMap = Maps.newHashMap();
+
+    // Replace the edge manager for all vertices which have routing type custom.
+    for (Entry<String, EdgeProperty> edgeEntry : context.getInputVertexEdgeProperties().entrySet()) {
+      if (edgeEntry.getValue().getDataMovementType() == DataMovementType.CUSTOM
+          && edgeEntry.getValue().getEdgeManagerDescriptor().getClassName()
+              .equals(CustomPartitionEdge.class.getName())) {
+        emMap.put(edgeEntry.getKey(), hiveEdgeManagerDesc);
+      }
+    }
+
+    LOG.info("Task count is " + taskCount);
+
+    List<RootInputDataInformationEvent> taskEvents = Lists.newArrayListWithCapacity(finalSplits.size());
+    // Re-serialize the splits after grouping.
+    int count = 0;
+    for (InputSplit inputSplit : finalSplits) {
+      MRSplitProto serializedSplit = MRHelpers.createSplitProto(inputSplit);
+      RootInputDataInformationEvent diEvent = new RootInputDataInformationEvent(
+          count, serializedSplit.toByteArray());
+      diEvent.setTargetIndex(count);
+      count++;
+      taskEvents.add(diEvent);
+    }
+
+    // Replace the Edge Managers
+    context.setVertexParallelism(
+        taskCount,
+        new VertexLocationHint(createTaskLocationHintsFromSplits(finalSplits
+            .toArray(new InputSplit[finalSplits.size()]))), emMap);
+
+    // Set the actual events for the tasks.
+    context.addRootInputEvents(inputName, taskEvents);
+  }
+
+  private byte[] getBytePayload(Multimap<Integer, Integer> routingTable) throws IOException {
+    CustomEdgeConfiguration edgeConf = 
+        new CustomEdgeConfiguration(routingTable.keySet().size(), routingTable);
+    DataOutputBuffer dob = new DataOutputBuffer();
+    edgeConf.write(dob);
+    byte[] serialized = dob.getData();
+
+    return serialized;
+  }
+
+  private FileSplit getFileSplitFromEvent(RootInputDataInformationEvent event)
+      throws IOException {
+    InputSplit inputSplit = null;
+    if (event.getDeserializedUserPayload() != null) {
+      inputSplit = (InputSplit) event.getDeserializedUserPayload();
+    } else {
+      MRSplitProto splitProto = MRSplitProto.parseFrom(event.getUserPayload());
+      SerializationFactory serializationFactory = new SerializationFactory(
+          new Configuration());
+      inputSplit = MRHelpers.createOldFormatSplitFromUserPayload(splitProto,
+          serializationFactory);
+    }
+
+    if (!(inputSplit instanceof FileSplit)) {
+      throw new UnsupportedOperationException(
+          "Cannot handle splits other than FileSplit for the moment");
+    }
+    return (FileSplit) inputSplit;
+  }
+
+  /*
+   * This method generates the map of bucket to file splits.
+   */
+  private void setBucketNumForPath(Map<Path, List<FileSplit>> pathFileSplitsMap) {
+    int bucketNum = 0;
+    int fsCount = 0;
+    for (Map.Entry<Path, List<FileSplit>> entry : pathFileSplitsMap.entrySet()) {
+      int bucketId = bucketNum % numBuckets;
+      for (FileSplit fsplit : entry.getValue()) {
+        fsCount++;
+        bucketToInitialSplitMap.put(bucketId, fsplit);
+      }
+      bucketNum++;
+    }
+
+    LOG.info("Total number of splits counted: " + fsCount + " and total files encountered: " 
+        + pathFileSplitsMap.size());
+  }
+
+  private void groupSplits () throws IOException {
+    estimateBucketSizes();
+    bucketToGroupedSplitMap = 
+        ArrayListMultimap.<Integer, InputSplit>create(bucketToInitialSplitMap);
+    
+    Map<Integer, Collection<InputSplit>> bucketSplitMap = bucketToInitialSplitMap.asMap();
+    for (int bucketId : bucketSplitMap.keySet()) {
+      Collection<InputSplit>inputSplitCollection = bucketSplitMap.get(bucketId);
+      TezMapredSplitsGrouper grouper = new TezMapredSplitsGrouper();
+
+      InputSplit[] groupedSplits = grouper.getGroupedSplits(conf, 
+          inputSplitCollection.toArray(new InputSplit[0]), bucketToNumTaskMap.get(bucketId),
+          HiveInputFormat.class.getName());
+      LOG.info("Original split size is " + 
+          inputSplitCollection.toArray(new InputSplit[0]).length + 
+          " grouped split size is " + groupedSplits.length);
+      bucketToGroupedSplitMap.removeAll(bucketId);
+      for (InputSplit inSplit : groupedSplits) {
+        bucketToGroupedSplitMap.put(bucketId, inSplit);
+      }
+    }
+  }
+
+  private void estimateBucketSizes() {
+    Map<Integer, Long>bucketSizeMap = new HashMap<Integer, Long>();
+    Map<Integer, Collection<InputSplit>> bucketSplitMap = bucketToInitialSplitMap.asMap();
+    long totalSize = 0;
+    for (int bucketId : bucketSplitMap.keySet()) {
+      Long size = 0L;
+      Collection<InputSplit>inputSplitCollection = bucketSplitMap.get(bucketId);
+      Iterator<InputSplit> iter = inputSplitCollection.iterator();
+      while (iter.hasNext()) {
+        FileSplit fsplit = (FileSplit)iter.next();
+        size += fsplit.getLength();
+        totalSize += fsplit.getLength();
+      }
+      bucketSizeMap.put(bucketId, size);
+    }
+
+    int totalResource = context.getTotalAVailableResource().getMemory();
+    int taskResource = context.getVertexTaskResource().getMemory();
+    float waves = conf.getFloat(
+        TezConfiguration.TEZ_AM_GROUPING_SPLIT_WAVES,
+        TezConfiguration.TEZ_AM_GROUPING_SPLIT_WAVES_DEFAULT);
+
+    int numTasks = (int)((totalResource*waves)/taskResource);
+    LOG.info("Total resource: " + totalResource + " Task Resource: " + taskResource
+        + " waves: " + waves + " total size of splits: " + totalSize + 
+        " total number of tasks: " + numTasks);
+
+    for (int bucketId : bucketSizeMap.keySet()) {
+      int numEstimatedTasks = 0;
+      if (totalSize != 0) {
+        numEstimatedTasks = (int)(numTasks * bucketSizeMap.get(bucketId) / totalSize);
+      }
+      LOG.info("Estimated number of tasks: " + numEstimatedTasks + " for bucket " + bucketId);
+      if (numEstimatedTasks == 0) {
+        numEstimatedTasks = 1;
+      }
+      bucketToNumTaskMap.put(bucketId, numEstimatedTasks);
+    }
+  }
+
+  private static List<TaskLocationHint> createTaskLocationHintsFromSplits(
+      org.apache.hadoop.mapred.InputSplit[] oldFormatSplits) {
+    Iterable<TaskLocationHint> iterable = Iterables.transform(Arrays.asList(oldFormatSplits),
+        new Function<org.apache.hadoop.mapred.InputSplit, TaskLocationHint>() {
+      @Override
+      public TaskLocationHint apply(org.apache.hadoop.mapred.InputSplit input) {
+        try {
+          if (input.getLocations() != null) {
+            return new TaskLocationHint(new HashSet<String>(Arrays.asList(input.getLocations())),
+                null);
+          } else {
+            LOG.info("NULL Location: returning an empty location hint");
+            return new TaskLocationHint(null,null);
+          }
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    });
+
+    return Lists.newArrayList(iterable);
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index c247030440..78e965bc86 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -24,12 +24,14 @@
 import java.io.IOException;
 import java.net.URI;
 import java.net.URISyntaxException;
+import java.nio.ByteBuffer;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.Set;
 
 import javax.security.auth.login.LoginException;
@@ -59,13 +61,16 @@
 import org.apache.hadoop.hive.ql.plan.BaseWork;
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
+import org.apache.hadoop.hive.ql.plan.TezWork;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.shims.HadoopShimsSecure.NullOutputCommitter;
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.BytesWritable;
+import org.apache.hadoop.io.DataOutputBuffer;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.OutputFormat;
@@ -80,6 +85,7 @@
 import org.apache.hadoop.yarn.util.Records;
 import org.apache.tez.dag.api.DAG;
 import org.apache.tez.dag.api.Edge;
+import org.apache.tez.dag.api.EdgeManagerDescriptor;
 import org.apache.tez.dag.api.EdgeProperty;
 import org.apache.tez.dag.api.EdgeProperty.DataMovementType;
 import org.apache.tez.dag.api.EdgeProperty.DataSourceType;
@@ -89,6 +95,7 @@
 import org.apache.tez.dag.api.OutputDescriptor;
 import org.apache.tez.dag.api.ProcessorDescriptor;
 import org.apache.tez.dag.api.Vertex;
+import org.apache.tez.dag.api.VertexManagerPluginDescriptor;
 import org.apache.tez.dag.api.VertexLocationHint;
 import org.apache.tez.dag.api.TezException;
 import org.apache.tez.client.PreWarmContext;
@@ -129,7 +136,7 @@ public URI apply(String input) {
           return new Path(input).toUri();
         }
       });
-    
+
       Set<URI> uris = new HashSet<URI>();
       Iterators.addAll(uris, pathIterator);
 
@@ -201,31 +208,45 @@ private JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {
    * @param group The parent VertexGroup
    * @param wConf The job conf of the child vertex
    * @param w The child vertex
-   * @param edgeType the type of connection between the two
+   * @param edgeProp the edge property of connection between the two
    * endpoints.
    */
   public GroupInputEdge createEdge(VertexGroup group, JobConf wConf,
-      Vertex w, EdgeType edgeType)
-      throws IOException {
-    
+      Vertex w, TezEdgeProperty edgeProp)
+    throws IOException {
+
     Class mergeInputClass;
-    
+
     LOG.info("Creating Edge between " + group.getGroupName() + " and " + w.getVertexName());
     w.getProcessorDescriptor().setUserPayload(MRHelpers.createUserPayloadFromConf(wConf));
 
+    EdgeType edgeType = edgeProp.getEdgeType();
     switch (edgeType) {
-    case BROADCAST_EDGE:
-      mergeInputClass = ConcatenatedMergedKeyValueInput.class;
-      break;
-
-    case SIMPLE_EDGE:
-    default:
-      mergeInputClass = TezMergedLogicalInput.class;
-      break;
-    }
-
-    return new GroupInputEdge(group, w, createEdgeProperty(edgeType),
-         new InputDescriptor(mergeInputClass.getName()));
+      case BROADCAST_EDGE:
+        mergeInputClass = ConcatenatedMergedKeyValueInput.class;
+        break;
+      case CUSTOM_EDGE:
+        mergeInputClass = ConcatenatedMergedKeyValueInput.class;
+        int numBuckets = edgeProp.getNumBuckets();
+        VertexManagerPluginDescriptor desc = new VertexManagerPluginDescriptor(
+            CustomPartitionVertex.class.getName());
+        byte[] userPayload = ByteBuffer.allocate(4).putInt(numBuckets).array();
+        desc.setUserPayload(userPayload);
+        w.setVertexManagerPlugin(desc);
+        break;
+
+      case CUSTOM_SIMPLE_EDGE:
+        mergeInputClass = ConcatenatedMergedKeyValueInput.class;
+        break;
+
+      case SIMPLE_EDGE:
+      default:
+        mergeInputClass = TezMergedLogicalInput.class;
+        break;
+    }
+
+    return new GroupInputEdge(group, w, createEdgeProperty(edgeProp),
+        new InputDescriptor(mergeInputClass.getName()));
   }
 
   /**
@@ -253,43 +274,83 @@ public void updateConfigurationForEdge(JobConf vConf, Vertex v, JobConf wConf, V
    * @return
    */
   public Edge createEdge(JobConf vConf, Vertex v, JobConf wConf, Vertex w,
-      EdgeType edgeType)
-      throws IOException {
+      TezEdgeProperty edgeProp)
+    throws IOException {
 
     updateConfigurationForEdge(vConf, v, wConf, w);
 
-    return new Edge(v, w, createEdgeProperty(edgeType));
+    if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {
+      int numBuckets = edgeProp.getNumBuckets();
+      byte[] userPayload = ByteBuffer.allocate(4).putInt(numBuckets).array();
+      VertexManagerPluginDescriptor desc = new VertexManagerPluginDescriptor(
+          CustomPartitionVertex.class.getName());
+      desc.setUserPayload(userPayload);
+      w.setVertexManagerPlugin(desc);
+    }
+
+    return new Edge(v, w, createEdgeProperty(edgeProp));
   }
 
   /*
    * Helper function to create an edge property from an edge type.
    */
-  private EdgeProperty createEdgeProperty(EdgeType edgeType) {
+  private EdgeProperty createEdgeProperty(TezEdgeProperty edgeProp) throws IOException {
     DataMovementType dataMovementType;
     Class logicalInputClass;
     Class logicalOutputClass;
 
+    EdgeProperty edgeProperty = null;
+    EdgeType edgeType = edgeProp.getEdgeType();
     switch (edgeType) {
-    case BROADCAST_EDGE:
-      dataMovementType = DataMovementType.BROADCAST;
-      logicalOutputClass = OnFileUnorderedKVOutput.class;
-      logicalInputClass = ShuffledUnorderedKVInput.class;
-      break;
-
-    case SIMPLE_EDGE:
-    default:
-      dataMovementType = DataMovementType.SCATTER_GATHER;
-      logicalOutputClass = OnFileSortedOutput.class;
-      logicalInputClass = ShuffledMergedInputLegacy.class;
-      break;
-    }
-
-    EdgeProperty edgeProperty =
+      case BROADCAST_EDGE:
+        dataMovementType = DataMovementType.BROADCAST;
+        logicalOutputClass = OnFileUnorderedKVOutput.class;
+        logicalInputClass = ShuffledUnorderedKVInput.class;
+        break;
+
+      case CUSTOM_EDGE:
+        
+        dataMovementType = DataMovementType.CUSTOM;
+        logicalOutputClass = OnFileSortedOutput.class;
+        logicalInputClass = ShuffledUnorderedKVInput.class;
+        EdgeManagerDescriptor edgeDesc = new EdgeManagerDescriptor(
+            CustomPartitionEdge.class.getName());
+        CustomEdgeConfiguration edgeConf = 
+            new CustomEdgeConfiguration(edgeProp.getNumBuckets(), null);
+          DataOutputBuffer dob = new DataOutputBuffer();
+          edgeConf.write(dob);
+          byte[] userPayload = dob.getData();
+        edgeDesc.setUserPayload(userPayload);
+        edgeProperty =
+          new EdgeProperty(edgeDesc,
+              DataSourceType.PERSISTED,
+              SchedulingType.SEQUENTIAL,
+              new OutputDescriptor(logicalOutputClass.getName()),
+              new InputDescriptor(logicalInputClass.getName()));
+        break;
+
+      case CUSTOM_SIMPLE_EDGE:
+        dataMovementType = DataMovementType.SCATTER_GATHER;
+        logicalOutputClass = OnFileSortedOutput.class;
+        logicalInputClass = ShuffledUnorderedKVInput.class;
+        break;
+
+      case SIMPLE_EDGE:
+      default:
+        dataMovementType = DataMovementType.SCATTER_GATHER;
+        logicalOutputClass = OnFileSortedOutput.class;
+        logicalInputClass = ShuffledMergedInputLegacy.class;
+        break;
+    }
+
+    if (edgeProperty == null) {
+      edgeProperty =
         new EdgeProperty(dataMovementType,
             DataSourceType.PERSISTED,
             SchedulingType.SEQUENTIAL,
             new OutputDescriptor(logicalOutputClass.getName()),
             new InputDescriptor(logicalInputClass.getName()));
+    }
 
     return edgeProperty;
   }
@@ -305,7 +366,7 @@ private Resource getContainerResource(Configuration conf) {
       HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVETEZCONTAINERSIZE) :
       conf.getInt(MRJobConfig.MAP_MEMORY_MB, MRJobConfig.DEFAULT_MAP_MEMORY_MB);
     int cpus = conf.getInt(MRJobConfig.MAP_CPU_VCORES,
-                           MRJobConfig.DEFAULT_MAP_CPU_VCORES);
+        MRJobConfig.DEFAULT_MAP_CPU_VCORES);
     return Resource.newInstance(memory, cpus);
   }
 
@@ -328,7 +389,7 @@ private String getContainerJavaOpts(Configuration conf) {
    */
   private Vertex createVertex(JobConf conf, MapWork mapWork,
       LocalResource appJarLr, List<LocalResource> additionalLr, FileSystem fs,
-      Path mrScratchDir, Context ctx) throws Exception {
+      Path mrScratchDir, Context ctx, TezWork tezWork) throws Exception {
 
     Path tezDir = getTezDir(mrScratchDir);
 
@@ -353,11 +414,30 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,
     Class inputFormatClass = conf.getClass("mapred.input.format.class",
         InputFormat.class);
 
-    // we'll set up tez to combine spits for us iff the input format
-    // is HiveInputFormat
-    if (inputFormatClass == HiveInputFormat.class) {
-      useTezGroupedSplits = true;
-      conf.setClass("mapred.input.format.class", TezGroupedSplitsInputFormat.class, InputFormat.class);
+    boolean vertexHasCustomInput = false;
+    if (tezWork != null) {
+      for (BaseWork baseWork : tezWork.getParents(mapWork)) {
+        if (tezWork.getEdgeType(baseWork, mapWork) == EdgeType.CUSTOM_EDGE) {
+          vertexHasCustomInput = true;
+        }
+      }
+    }
+    if (vertexHasCustomInput) {
+      useTezGroupedSplits = false;
+      // grouping happens in execution phase. Setting the class to TezGroupedSplitsInputFormat 
+      // here would cause pre-mature grouping which would be incorrect.
+      inputFormatClass = HiveInputFormat.class;
+      conf.setClass("mapred.input.format.class", HiveInputFormat.class, InputFormat.class);
+      // mapreduce.tez.input.initializer.serialize.event.payload should be set to false when using
+      // this plug-in to avoid getting a serialized event at run-time.
+      conf.setBoolean("mapreduce.tez.input.initializer.serialize.event.payload", false);
+    } else {
+      // we'll set up tez to combine spits for us iff the input format
+      // is HiveInputFormat
+      if (inputFormatClass == HiveInputFormat.class) {
+        useTezGroupedSplits = true;
+        conf.setClass("mapred.input.format.class", TezGroupedSplitsInputFormat.class, InputFormat.class);
+      }
     }
 
     if (HiveConf.getBoolVar(conf, ConfVars.HIVE_AM_SPLIT_GENERATION)) {
@@ -374,7 +454,7 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,
     byte[] serializedConf = MRHelpers.createUserPayloadFromConf(conf);
     map = new Vertex(mapWork.getName(),
         new ProcessorDescriptor(MapTezProcessor.class.getName()).
-             setUserPayload(serializedConf), numTasks, getContainerResource(conf));
+        setUserPayload(serializedConf), numTasks, getContainerResource(conf));
     Map<String, String> environment = new HashMap<String, String>();
     MRHelpers.updateEnvironmentForMRTasks(conf, environment, true);
     map.setTaskEnvironment(environment);
@@ -393,7 +473,7 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,
     }
     map.addInput(alias,
         new InputDescriptor(MRInputLegacy.class.getName()).
-               setUserPayload(mrInput), amSplitGeneratorClass);
+        setUserPayload(mrInput), amSplitGeneratorClass);
 
     Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();
     localResources.put(getBaseName(appJarLr), appJarLr);
@@ -447,7 +527,7 @@ private Vertex createVertex(JobConf conf, ReduceWork reduceWork,
     // create the vertex
     Vertex reducer = new Vertex(reduceWork.getName(),
         new ProcessorDescriptor(ReduceTezProcessor.class.getName()).
-             setUserPayload(MRHelpers.createUserPayloadFromConf(conf)),
+        setUserPayload(MRHelpers.createUserPayloadFromConf(conf)),
         reduceWork.getNumReduceTasks(), getContainerResource(conf));
 
     Map<String, String> environment = new HashMap<String, String>();
@@ -501,7 +581,7 @@ private LocalResource createLocalResource(FileSystem remoteFs, Path file,
    * @return prewarm context object
    */
   public PreWarmContext createPreWarmContext(TezSessionConfiguration sessionConfig, int numContainers,
-               Map<String, LocalResource> localResources) throws IOException, TezException {
+      Map<String, LocalResource> localResources) throws IOException, TezException {
 
     Configuration conf = sessionConfig.getTezConfiguration();
 
@@ -524,7 +604,7 @@ public PreWarmContext createPreWarmContext(TezSessionConfiguration sessionConfig
     }
 
     if(localResources != null) {
-       combinedResources.putAll(localResources);
+      combinedResources.putAll(localResources);
     }
 
     context.setLocalResources(combinedResources);
@@ -641,7 +721,7 @@ public String getHiveJarDirectory(Configuration conf) throws IOException, LoginE
 
   // the api that finds the jar being used by this class on disk
   public String getExecJarPathLocal () throws URISyntaxException {
-      // returns the location on disc of the jar of this class.
+    // returns the location on disc of the jar of this class.
     return DagUtils.class.getProtectionDomain().getCodeSource().getLocation().toURI().toString();
   }
 
@@ -670,7 +750,7 @@ public String getResourceBaseName(String pathStr) {
    * @throws IOException when any file system related call fails
    */
   private boolean checkPreExisting(Path src, Path dest, Configuration conf)
-      throws IOException {
+    throws IOException {
     FileSystem destFS = dest.getFileSystem(conf);
 
     if (!destFS.exists(dest)) {
@@ -699,7 +779,7 @@ private boolean checkPreExisting(Path src, Path dest, Configuration conf)
    * @throws IOException when any file system related calls fails.
    */
   public LocalResource localizeResource(Path src, Path dest, Configuration conf)
-      throws IOException {
+    throws IOException {
     FileSystem destFS = dest.getFileSystem(conf);
     if (!(destFS instanceof DistributedFileSystem)) {
       throw new IOException(ErrorMsg.INVALID_HDFS_URI.format(dest.toString()));
@@ -776,6 +856,7 @@ public JobConf initializeVertexConf(JobConf conf, BaseWork work) {
    * @param work The instance of BaseWork representing the actual work to be performed
    * by this vertex.
    * @param scratchDir HDFS scratch dir for this execution unit.
+   * @param list 
    * @param appJarLr Local resource for hive-exec.
    * @param additionalLr
    * @param fileSystem FS corresponding to scratchDir and LocalResources
@@ -783,15 +864,16 @@ public JobConf initializeVertexConf(JobConf conf, BaseWork work) {
    * @return Vertex
    */
   public Vertex createVertex(JobConf conf, BaseWork work,
-      Path scratchDir, LocalResource appJarLr, List<LocalResource> additionalLr,
-      FileSystem fileSystem, Context ctx, boolean hasChildren) throws Exception {
+      Path scratchDir, LocalResource appJarLr, 
+      List<LocalResource> additionalLr,
+      FileSystem fileSystem, Context ctx, boolean hasChildren, TezWork tezWork) throws Exception {
 
     Vertex v = null;
     // simply dispatch the call to the right method for the actual (sub-) type of
     // BaseWork.
     if (work instanceof MapWork) {
       v = createVertex(conf, (MapWork) work, appJarLr,
-          additionalLr, fileSystem, scratchDir, ctx);
+          additionalLr, fileSystem, scratchDir, ctx, tezWork);
     } else if (work instanceof ReduceWork) {
       v = createVertex(conf, (ReduceWork) work, appJarLr,
           additionalLr, fileSystem, scratchDir, ctx);
@@ -820,7 +902,7 @@ public Vertex createVertex(JobConf conf, BaseWork work,
     if (!hasChildren) {
       v.addOutput("out_"+work.getName(),
           new OutputDescriptor(MROutput.class.getName())
-               .setUserPayload(MRHelpers.createUserPayloadFromConf(conf)));
+          .setUserPayload(MRHelpers.createUserPayloadFromConf(conf)));
     }
 
     return v;
@@ -842,7 +924,7 @@ public void addCredentials(BaseWork work, DAG dag) {
    * be used with Tez. Assumes scratchDir exists.
    */
   public Path createTezDir(Path scratchDir, Configuration conf)
-      throws IOException {
+    throws IOException {
     Path tezDir = getTezDir(scratchDir);
     FileSystem fs = tezDir.getFileSystem(conf);
     fs.mkdirs(tezDir);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java
index bf94930872..e3925922ea 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HashTableLoader.java
@@ -49,7 +49,7 @@
  */
 public class HashTableLoader implements org.apache.hadoop.hive.ql.exec.HashTableLoader {
 
-  private static final Log LOG = LogFactory.getLog(MapJoinOperator.class.getName());
+  private static final Log LOG = LogFactory.getLog(HashTableLoader.class.getName());
 
   private ExecMapperContext context;
   private Configuration hconf;
@@ -122,8 +122,13 @@ public void load(
         throw new HiveException(e);
       }
       // Register that the Input has been cached.
-      tezCacheAccess.registerCachedInput(inputName);
-      LOG.info("Setting Input: " + inputName + " as cached");
+      LOG.info("Is this a bucket map join: " + desc.isBucketMapJoin());
+      // cache is disabled for bucket map join because of the same reason
+      // given in loadHashTable in MapJoinOperator.
+      if (!desc.isBucketMapJoin()) {
+        tezCacheAccess.registerCachedInput(inputName);
+        LOG.info("Setting Input: " + inputName + " as cached");
+      }
     }
     if (lastKey == null) {
       lastKey = new MapJoinKeyObject(); // No rows in tables, the key type doesn't matter.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
index 803903256b..a8b9638dc1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java
@@ -149,6 +149,7 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out
       // Start the actual Inputs. After MRInput initialization.
       for (Entry<String, LogicalInput> inputEntry : inputs.entrySet()) {
         if (!cacheAccess.isInputCached(inputEntry.getKey())) {
+          LOG.info("Input: " + inputEntry.getKey() + " is not cached");
           inputEntry.getValue().start();
         } else {
           LOG.info("Input: " + inputEntry.getKey() + " is already cached. Skipping start");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
index 5dd8f986ef..385e22ee71 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
@@ -42,9 +42,10 @@
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.BaseWork;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 import org.apache.hadoop.hive.ql.plan.TezWork;
 import org.apache.hadoop.hive.ql.plan.UnionWork;
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.mapred.JobConf;
@@ -222,7 +223,7 @@ DAG build(JobConf conf, TezWork work, Path scratchDir,
         // split the children into vertices that make up the union and vertices that are
         // proper children of the union
         for (BaseWork v: work.getChildren(w)) {
-          EdgeType type = work.getEdgeProperty(w, v);
+          EdgeType type = work.getEdgeProperty(w, v).getEdgeType();
           if (type == EdgeType.CONTAINS) {
             unionWorkItems.add(v);
           } else {
@@ -257,7 +258,7 @@ DAG build(JobConf conf, TezWork work, Path scratchDir,
         // Regular vertices
         JobConf wxConf = utils.initializeVertexConf(conf, w);
         Vertex wx = utils.createVertex(wxConf, w, tezDir, appJarLr, 
-          additionalLr, fs, ctx, !isFinal);
+          additionalLr, fs, ctx, !isFinal, work);
         dag.addVertex(wx);
         utils.addCredentials(w, dag);
         perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_CREATE_VERTEX + w.getName());
@@ -269,9 +270,9 @@ DAG build(JobConf conf, TezWork work, Path scratchDir,
           assert workToVertex.containsKey(v);
           Edge e = null;
 
-          EdgeType edgeType = work.getEdgeProperty(w, v);
-          
-          e = utils.createEdge(wxConf, wx, workToConf.get(v), workToVertex.get(v), edgeType);
+          TezEdgeProperty edgeProp = work.getEdgeProperty(w, v);
+
+          e = utils.createEdge(wxConf, wx, workToConf.get(v), workToVertex.get(v), edgeProp);
           dag.addEdge(e);
         }
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/DefaultHivePartitioner.java b/ql/src/java/org/apache/hadoop/hive/ql/io/DefaultHivePartitioner.java
index 61f69a4c82..0f09cb219e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/DefaultHivePartitioner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/DefaultHivePartitioner.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.io;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.mapred.lib.HashPartitioner;
 
 /** Partition keys by their {@link Object#hashCode()}. */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/lib/ForwardWalker.java b/ql/src/java/org/apache/hadoop/hive/ql/lib/ForwardWalker.java
new file mode 100644
index 0000000000..aac31fa703
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/lib/ForwardWalker.java
@@ -0,0 +1,70 @@
+package org.apache.hadoop.hive.ql.lib;
+
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
+import org.apache.hadoop.hive.ql.lib.Dispatcher;
+import org.apache.hadoop.hive.ql.lib.Node;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+
+public class ForwardWalker extends DefaultGraphWalker {
+
+  /**
+* Constructor.
+*
+* @param disp
+* dispatcher to call for each op encountered
+*/
+  public ForwardWalker(Dispatcher disp) {
+    super(disp);
+  }
+
+  @SuppressWarnings("unchecked")
+  protected boolean allParentsDispatched(Node nd) {
+    Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>) nd;
+    if (op.getParentOperators() == null) {
+      return true;
+    }
+    for (Node pNode : op.getParentOperators()) {
+      if (!getDispatchedList().contains(pNode)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @SuppressWarnings("unchecked")
+  protected void addAllParents(Node nd) {
+    Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>) nd;
+    if (op.getParentOperators() == null) {
+      return;
+    }
+    getToWalk().removeAll(op.getParentOperators());
+    getToWalk().addAll(0, op.getParentOperators());
+  }
+
+  /**
+* walk the current operator and its descendants.
+*
+* @param nd
+* current operator in the graph
+* @throws SemanticException
+*/
+  public void walk(Node nd) throws SemanticException {
+    if (opStack.empty() || nd != opStack.peek()) {
+      opStack.push(nd);
+    }
+    if (allParentsDispatched(nd)) {
+      // all children are done or no need to walk the children
+      if (!getDispatchedList().contains(nd)) {
+        getToWalk().addAll(nd.getChildren());
+        dispatch(nd, opStack);
+      }
+      opStack.pop();
+      return;
+    }
+    // add children, self to the front of the queue in that order
+    getToWalk().add(0, nd);
+    addAllParents(nd);
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
index 2850c7f819..60424701cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java
@@ -28,7 +28,6 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Stack;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
@@ -75,7 +74,7 @@ public AbstractBucketJoinProc() {
   abstract public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       Object... nodeOutputs) throws SemanticException;
 
-  private static List<String> getBucketFilePathsOfPartition(
+  public static List<String> getBucketFilePathsOfPartition(
       Path location, ParseContext pGraphContext) throws SemanticException {
     List<String> fileNames = new ArrayList<String>();
     try {
@@ -134,7 +133,7 @@ protected boolean canConvertMapJoinToBucketMapJoin(
       ParseContext pGraphContext,
       BucketJoinProcCtx context) throws SemanticException {
 
-    QBJoinTree joinCtx = this.pGraphContext.getMapJoinContext().get(mapJoinOp);
+    QBJoinTree joinCtx = pGraphContext.getMapJoinContext().get(mapJoinOp);
     if (joinCtx == null) {
       return false;
     }
@@ -454,7 +453,7 @@ private static Map<String, List<String>> convert(Map<Partition, List<String>> ma
     return converted;
   }
 
-  public List<String> toColumns(List<ExprNodeDesc> keys) {
+  public static List<String> toColumns(List<ExprNodeDesc> keys) {
     List<String> columns = new ArrayList<String>();
     for (ExprNodeDesc key : keys) {
       if (!(key instanceof ExprNodeColumnDesc)) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
index 384342a53f..3077d75f9f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java
@@ -18,23 +18,31 @@
 
 package org.apache.hadoop.hive.ql.optimizer;
 
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 import java.util.Set;
 import java.util.Stack;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.GroupByOperator;
 import org.apache.hadoop.hive.ql.exec.JoinOperator;
 import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
+import org.apache.hadoop.hive.ql.exec.MuxOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
 import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
+import org.apache.hadoop.hive.ql.plan.OpTraits;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.Statistics;
 
@@ -50,15 +58,15 @@ public class ConvertJoinMapJoin implements NodeProcessor {
   static final private Log LOG = LogFactory.getLog(ConvertJoinMapJoin.class.getName());
 
   @Override
-  /*
-   * (non-Javadoc)
-   * we should ideally not modify the tree we traverse.
-   * However, since we need to walk the tree at any time when we modify the
-   * operator, we might as well do it here.
-   */
-  public Object process(Node nd, Stack<Node> stack,
-      NodeProcessorCtx procCtx, Object... nodeOutputs)
-      throws SemanticException {
+    /*
+     * (non-Javadoc)
+     * we should ideally not modify the tree we traverse.
+     * However, since we need to walk the tree at any time when we modify the
+     * operator, we might as well do it here.
+     */
+    public Object process(Node nd, Stack<Node> stack,
+        NodeProcessorCtx procCtx, Object... nodeOutputs)
+    throws SemanticException {
 
     OptimizeTezProcContext context = (OptimizeTezProcContext) procCtx;
 
@@ -67,12 +75,178 @@ public Object process(Node nd, Stack<Node> stack,
     }
 
     JoinOperator joinOp = (JoinOperator) nd;
+    // if we have traits, and table info is present in the traits, we know the 
+    // exact number of buckets. Else choose the largest number of estimated
+    // reducers from the parent operators.
+    int numBuckets = -1;
+    int estimatedBuckets = -1;
+    for (Operator<? extends OperatorDesc>parentOp : joinOp.getParentOperators()) {
+      if (parentOp.getOpTraits().getNumBuckets() > 0) {
+        numBuckets = (numBuckets < parentOp.getOpTraits().getNumBuckets()) ? 
+            parentOp.getOpTraits().getNumBuckets() : numBuckets; 
+      }
+      ReduceSinkOperator rs = (ReduceSinkOperator)parentOp;
+      estimatedBuckets = (estimatedBuckets < rs.getConf().getNumReducers()) ? 
+          rs.getConf().getNumReducers() : estimatedBuckets;
+    }
+
+    if (numBuckets <= 0) {
+      numBuckets = estimatedBuckets;
+      if (numBuckets <= 0) {
+        numBuckets = 1;
+      }
+    }
+    LOG.info("Estimated number of buckets " + numBuckets);
+    int mapJoinConversionPos = mapJoinConversionPos(joinOp, context, numBuckets);
+    if (mapJoinConversionPos < 0) {
+      return null;
+    }
+
+    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_CONVERT_JOIN_BUCKET_MAPJOIN_TEZ)) {
+      if (convertJoinBucketMapJoin(joinOp, context, mapJoinConversionPos)) {
+        return null;
+      }
+    }
+
+    LOG.info("Convert to non-bucketed map join");
+    MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, mapJoinConversionPos);
+    // map join operator by default has no bucket cols
+    mapJoinOp.setOpTraits(new OpTraits(null, -1));
+    // propagate this change till the next RS
+    for (Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {
+      setAllChildrenTraitsToNull(childOp);
+    }
+
+    return null;
+  }
+
+  private void setAllChildrenTraitsToNull(Operator<? extends OperatorDesc> currentOp) {
+    if (currentOp instanceof ReduceSinkOperator) {
+      return;
+    }
+    currentOp.setOpTraits(new OpTraits(null, -1));
+    for (Operator<? extends OperatorDesc> childOp : currentOp.getChildOperators()) {
+      if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof GroupByOperator)) {
+        break;
+      }
+      setAllChildrenTraitsToNull(childOp);
+    }
+  }
+
+  private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context, 
+      int bigTablePosition) throws SemanticException {
+
+    TezBucketJoinProcCtx tezBucketJoinProcCtx = new TezBucketJoinProcCtx(context.conf);
+
+    if (!checkConvertJoinBucketMapJoin(joinOp, context, bigTablePosition, tezBucketJoinProcCtx)) {
+      LOG.info("Check conversion to bucket map join failed.");
+      return false;
+    }
+
+    MapJoinOperator mapJoinOp = 
+      convertJoinMapJoin(joinOp, context, bigTablePosition);
+    MapJoinDesc joinDesc = mapJoinOp.getConf();
+    joinDesc.setBucketMapJoin(true);
+
+    // we can set the traits for this join operator
+    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),
+        tezBucketJoinProcCtx.getNumBuckets());
+    mapJoinOp.setOpTraits(opTraits);
+    setNumberOfBucketsOnChildren(mapJoinOp);
+
+    // Once the conversion is done, we can set the partitioner to bucket cols on the small table    
+    Map<String, Integer> bigTableBucketNumMapping = new HashMap<String, Integer>();
+    bigTableBucketNumMapping.put(joinDesc.getBigTableAlias(), tezBucketJoinProcCtx.getNumBuckets());
+    joinDesc.setBigTableBucketNumMapping(bigTableBucketNumMapping);
+    LOG.info("Setting legacy map join to " + (!tezBucketJoinProcCtx.isSubQuery()));
+    joinDesc.setCustomBucketMapJoin(!tezBucketJoinProcCtx.isSubQuery());
+
+    return true;
+  }
 
+  private void setNumberOfBucketsOnChildren(Operator<? extends OperatorDesc> currentOp) {
+    int numBuckets = currentOp.getOpTraits().getNumBuckets();
+    for (Operator<? extends OperatorDesc>op : currentOp.getChildOperators()) {
+      if (!(op instanceof ReduceSinkOperator) && !(op instanceof GroupByOperator)) {
+        op.getOpTraits().setNumBuckets(numBuckets);
+        setNumberOfBucketsOnChildren(op);
+      }
+    }
+  }
+
+  /*
+   *  We perform the following checks to see if we can convert to a bucket map join
+   *  1. If the parent reduce sink of the big table side has the same emit key cols as 
+   *  its parent, we can create a bucket map join eliminating the reduce sink.
+   *  2. If we have the table information, we can check the same way as in Mapreduce to 
+   *  determine if we can perform a Bucket Map Join.
+   */
+  private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp, 
+      OptimizeTezProcContext context, int bigTablePosition, 
+      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {
+    // bail on mux-operator because mux operator masks the emit keys of the
+    // constituent reduce sinks
+    if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {
+      LOG.info("Operator is " + joinOp.getParentOperators().get(0).getName() +
+          ". Cannot convert to bucket map join");
+      return false;
+    }
+
+    ReduceSinkOperator rs = (ReduceSinkOperator) joinOp.getParentOperators().get(bigTablePosition);
+    /*
+     * this is the case when the big table is a sub-query and is probably
+     * already bucketed by the join column in say a group by operation 
+     */
+    List<List<String>> colNames = rs.getParentOperators().get(0).getOpTraits().getBucketColNames();
+    if ((colNames != null) && (colNames.isEmpty() == false)) {
+      Operator<? extends OperatorDesc>parentOfParent = rs.getParentOperators().get(0);
+      for (List<String>listBucketCols : parentOfParent.getOpTraits().getBucketColNames()) {
+        // can happen if this operator does not carry forward the previous bucketing columns
+        // for e.g. another join operator which does not carry one of the sides' key columns
+        if (listBucketCols.isEmpty()) {
+          continue;
+        }
+        int colCount = 0;
+        // parent op is guaranteed to have a single list because it is a reduce sink
+        for (String colName : rs.getOpTraits().getBucketColNames().get(0)) {
+          // all columns need to be at least a subset of the parentOfParent's bucket cols
+          ExprNodeDesc exprNodeDesc = rs.getColumnExprMap().get(colName);
+          if (exprNodeDesc instanceof ExprNodeColumnDesc) {
+            if (((ExprNodeColumnDesc)exprNodeDesc).getColumn().equals(listBucketCols.get(colCount))) {
+              colCount++;
+            } else {
+              break;
+            }
+          }
+          
+          if (colCount == rs.getOpTraits().getBucketColNames().get(0).size()) {
+            // all keys matched.
+            int numBuckets = parentOfParent.getOpTraits().getNumBuckets();
+            boolean isSubQuery = false;
+            if (numBuckets < 0) {
+              isSubQuery = true;
+              numBuckets = rs.getConf().getNumReducers();
+            }
+            tezBucketJoinProcCtx.setNumBuckets(numBuckets);
+            tezBucketJoinProcCtx.setIsSubQuery(isSubQuery);
+            return true;
+          }
+        }
+      }
+      return false;
+    }
+
+    LOG.info("No info available to check for bucket map join. Cannot convert");
+    return false;
+  }
+
+  public int mapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext context, 
+      int buckets) {
     Set<Integer> bigTableCandidateSet = MapJoinProcessor.
       getBigTableCandidates(joinOp.getConf().getConds());
 
     long maxSize = context.conf.getLongVar(
-      HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);
+        HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);
 
     int bigTablePosition = -1;
 
@@ -89,7 +263,7 @@ public Object process(Node nd, Stack<Node> stack,
       Statistics currInputStat = parentOp.getStatistics();
       if (currInputStat == null) {
         LOG.warn("Couldn't get statistics from: "+parentOp);
-        return null;
+        return -1;
       }
 
       long inputSize = currInputStat.getDataSize();
@@ -100,14 +274,14 @@ public Object process(Node nd, Stack<Node> stack,
         if (bigTableFound) {
           // cannot convert to map join; we've already chosen a big table
           // on size and there's another one that's bigger.
-          return null;
+          return -1;
         }
 
-        if (inputSize > maxSize) {
+        if (inputSize/buckets > maxSize) {
           if (!bigTableCandidateSet.contains(pos)) {
             // can't use the current table as the big table, but it's too
             // big for the map side.
-            return null;
+            return -1;
           }
 
           bigTableFound = true;
@@ -119,10 +293,10 @@ public Object process(Node nd, Stack<Node> stack,
           totalSize += bigInputStat.getDataSize();
         }
 
-        if (totalSize > maxSize) {
+        if (totalSize/buckets > maxSize) {
           // sum of small tables size in this join exceeds configured limit
           // hence cannot convert.
-          return null;
+          return -1;
         }
 
         if (bigTableCandidateSet.contains(pos)) {
@@ -131,37 +305,45 @@ public Object process(Node nd, Stack<Node> stack,
         }
       } else {
         totalSize += currInputStat.getDataSize();
-        if (totalSize > maxSize) {
+        if (totalSize/buckets > maxSize) {
           // cannot hold all map tables in memory. Cannot convert.
-          return null;
+          return -1;
         }
       }
       pos++;
     }
 
-    if (bigTablePosition == -1) {
-      // all tables have size 0. We let the shuffle join handle this case.
-      return null;
-    }
+    return bigTablePosition;
+  }
 
-    /*
-     * Once we have decided on the map join, the tree would transform from
-     *
-     *        |                   |
-     *       Join               MapJoin
-     *       / \                /   \
-     *      RS RS   --->      RS    TS (big table)
-     *      /   \            /
-     *    TS     TS         TS (small table)
-     *
-     * for tez.
-     */
+  /*
+   * Once we have decided on the map join, the tree would transform from
+   *
+   *        |                   |
+   *       Join               MapJoin
+   *       / \                /   \
+   *     RS   RS   --->     RS    TS (big table)
+   *    /      \           /
+   *   TS       TS        TS (small table)
+   *
+   * for tez.
+   */
+
+  public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcContext context, 
+      int bigTablePosition) throws SemanticException {
+    // bail on mux operator because currently the mux operator masks the emit keys 
+    // of the constituent reduce sinks.
+    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {
+      if (parentOp instanceof MuxOperator) {
+        return null;
+      }
+    }
 
-    // convert to a map join operator with this information
+    //can safely convert the join to a map join.
     ParseContext parseContext = context.parseContext;
     MapJoinOperator mapJoinOp = MapJoinProcessor.
       convertJoinOpMapJoinOp(context.conf, parseContext.getOpParseCtx(),
-      joinOp, parseContext.getJoinContext().get(joinOp), bigTablePosition, true);
+          joinOp, parseContext.getJoinContext().get(joinOp), bigTablePosition, true);
 
     Operator<? extends OperatorDesc> parentBigTableOp
       = mapJoinOp.getParentOperators().get(bigTablePosition);
@@ -169,10 +351,10 @@ public Object process(Node nd, Stack<Node> stack,
     if (parentBigTableOp instanceof ReduceSinkOperator) {
       mapJoinOp.getParentOperators().remove(bigTablePosition);
       if (!(mapJoinOp.getParentOperators().contains(
-          parentBigTableOp.getParentOperators().get(0)))) {
+              parentBigTableOp.getParentOperators().get(0)))) {
         mapJoinOp.getParentOperators().add(bigTablePosition,
-          parentBigTableOp.getParentOperators().get(0));
-      }
+            parentBigTableOp.getParentOperators().get(0));
+              }
       parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);
       for (Operator<? extends OperatorDesc> op : mapJoinOp.getParentOperators()) {
         if (!(op.getChildOperators().contains(mapJoinOp))) {
@@ -182,6 +364,6 @@ public Object process(Node nd, Stack<Node> stack,
       }
     }
 
-    return null;
+    return mapJoinOp;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
index 2575cfb6e5..3f16dc24dc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.ql.optimizer.index.RewriteGBUsingIndex;
 import org.apache.hadoop.hive.ql.optimizer.lineage.Generator;
 import org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.ListBucketingPruner;
+import org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits;
 import org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover;
 import org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner;
 import org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics;
@@ -124,7 +125,9 @@ public void initialize(HiveConf hiveConf) {
     if (pctx.getContext().getExplain() ||
         HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez")) {
       transformations.add(new AnnotateWithStatistics());
+      transformations.add(new AnnotateWithOpTraits());
     }
+
     transformations.add(new SimpleFetchOptimizer());  // must be called last
 
     if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEFETCHTASKAGGR)) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
index 135bb4df52..f7b499beb0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.optimizer;
 
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Stack;
@@ -44,8 +45,9 @@
 import org.apache.hadoop.hive.ql.plan.PlanUtils;
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 import org.apache.hadoop.hive.ql.plan.TezWork;
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
 
 public class ReduceSinkMapJoinProc implements NodeProcessor {
 
@@ -110,12 +112,24 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,
     LOG.debug("Mapjoin "+mapJoinOp+", pos: "+pos+" --> "+parentWork.getName());
     mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());
 
+    int numBuckets = -1;
+    EdgeType edgeType = EdgeType.BROADCAST_EDGE;
+    if (mapJoinOp.getConf().isBucketMapJoin()) {
+      numBuckets = (Integer) mapJoinOp.getConf().getBigTableBucketNumMapping().values().toArray()[0];
+      if (mapJoinOp.getConf().getCustomBucketMapJoin()) {
+        edgeType = EdgeType.CUSTOM_EDGE;
+      } else {
+        edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;
+      }
+    }
+    TezEdgeProperty edgeProp = new TezEdgeProperty(null, edgeType, numBuckets);
+
     if (mapJoinWork != null) {
       for (BaseWork myWork: mapJoinWork) {
         // link the work with the work associated with the reduce sink that triggered this rule
         TezWork tezWork = context.currentTask.getWork();
         LOG.debug("connecting "+parentWork.getName()+" with "+myWork.getName());
-        tezWork.connect(parentWork, myWork, EdgeType.BROADCAST_EDGE);
+        tezWork.connect(parentWork, myWork, edgeProp);
         
         ReduceSinkOperator r = null;
         if (parentRS.getConf().getOutputName() != null) {
@@ -134,12 +148,14 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,
     }
 
     // remember in case we need to connect additional work later
-    List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(mapJoinOp);
-    if (linkWorkList == null) {
-      linkWorkList = new ArrayList<BaseWork>();
+    Map<BaseWork, TezEdgeProperty> linkWorkMap = null;
+    if (context.linkOpWithWorkMap.containsKey(mapJoinOp)) {
+      linkWorkMap = context.linkOpWithWorkMap.get(mapJoinOp);
+    } else {
+      linkWorkMap = new HashMap<BaseWork, TezEdgeProperty>();
     }
-    linkWorkList.add(parentWork);
-    context.linkOpWithWorkMap.put(mapJoinOp, linkWorkList);
+    linkWorkMap.put(parentWork, edgeProp);
+    context.linkOpWithWorkMap.put(mapJoinOp, linkWorkMap);
     
     List<ReduceSinkOperator> reduceSinks 
       = context.linkWorkWithReduceSinkMap.get(parentWork);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/TezBucketJoinProcCtx.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/TezBucketJoinProcCtx.java
new file mode 100644
index 0000000000..821f60c598
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/TezBucketJoinProcCtx.java
@@ -0,0 +1,50 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer;
+
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+
+public class TezBucketJoinProcCtx extends BucketJoinProcCtx {
+  // determines if we need to use custom edge or one-to-one edge
+  boolean isSubQuery = false;
+  int numBuckets = -1;
+
+  public TezBucketJoinProcCtx(HiveConf conf) {
+    super(conf);
+  }
+
+  public void setIsSubQuery (boolean isSubQuery) {
+    this.isSubQuery = isSubQuery;
+  }
+
+  public boolean isSubQuery () {
+    return isSubQuery;
+  }
+
+  public void setNumBuckets(int numBuckets) {
+    this.numBuckets = numBuckets;
+  }
+
+  public Integer getNumBuckets() {
+    return numBuckets;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/AnnotateOpTraitsProcCtx.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/AnnotateOpTraitsProcCtx.java
new file mode 100644
index 0000000000..4bb2f634a1
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/AnnotateOpTraitsProcCtx.java
@@ -0,0 +1,55 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer.metainfo.annotation;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+
+public class AnnotateOpTraitsProcCtx implements NodeProcessorCtx {
+
+  ParseContext parseContext;
+  HiveConf conf;
+  
+  public AnnotateOpTraitsProcCtx(ParseContext parseContext) {
+    this.setParseContext(parseContext);
+    if(parseContext != null) {
+      this.setConf(parseContext.getConf());
+    } else {
+      this.setConf(null);
+    }
+  }
+
+  public HiveConf getConf() {
+    return conf;
+  }
+
+  public void setConf(HiveConf conf) {
+    this.conf = conf;
+  }
+
+  public ParseContext getParseContext() {
+    return parseContext;
+  }
+
+  public void setParseContext(ParseContext parseContext) {
+    this.parseContext = parseContext;
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/AnnotateWithOpTraits.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/AnnotateWithOpTraits.java
new file mode 100644
index 0000000000..da91d38b53
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/AnnotateWithOpTraits.java
@@ -0,0 +1,78 @@
+package org.apache.hadoop.hive.ql.optimizer.metainfo.annotation;
+
+import java.util.ArrayList;
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+import org.apache.hadoop.hive.ql.exec.DemuxOperator;
+import org.apache.hadoop.hive.ql.exec.GroupByOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.MapJoinOperator;
+import org.apache.hadoop.hive.ql.exec.MuxOperator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator;
+import org.apache.hadoop.hive.ql.exec.SelectOperator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.exec.UnionOperator;
+import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
+import org.apache.hadoop.hive.ql.lib.Dispatcher;
+import org.apache.hadoop.hive.ql.lib.GraphWalker;
+import org.apache.hadoop.hive.ql.lib.Node;
+import org.apache.hadoop.hive.ql.lib.NodeProcessor;
+import org.apache.hadoop.hive.ql.lib.PreOrderWalker;
+import org.apache.hadoop.hive.ql.lib.Rule;
+import org.apache.hadoop.hive.ql.lib.RuleRegExp;
+import org.apache.hadoop.hive.ql.optimizer.Transform;
+import org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+
+/*
+ * This class annotates each operator with its traits. The OpTraits class
+ * specifies the traits that are populated for each operator.
+ */
+public class AnnotateWithOpTraits implements Transform {
+
+  @Override
+  public ParseContext transform(ParseContext pctx) throws SemanticException {
+    AnnotateOpTraitsProcCtx annotateCtx = new AnnotateOpTraitsProcCtx(pctx);
+
+    // create a walker which walks the tree in a DFS manner while maintaining the
+    // operator stack. The dispatcher generates the plan from the operator tree
+    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();
+    opRules.put(new RuleRegExp("TS", TableScanOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getTableScanRule());
+    opRules.put(new RuleRegExp("RS", ReduceSinkOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getReduceSinkRule());
+    opRules.put(new RuleRegExp("JOIN", JoinOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getJoinRule());
+    opRules.put(new RuleRegExp("MAPJOIN", MapJoinOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getMultiParentRule());
+    opRules.put(new RuleRegExp("SMB", SMBMapJoinOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getMultiParentRule());
+    opRules.put(new RuleRegExp("MUX", MuxOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getMultiParentRule());
+    opRules.put(new RuleRegExp("DEMUX", DemuxOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getMultiParentRule());
+    opRules.put(new RuleRegExp("UNION", UnionOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getMultiParentRule());
+    opRules.put(new RuleRegExp("GBY", GroupByOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getGroupByRule());
+    opRules.put(new RuleRegExp("SEL", SelectOperator.getOperatorName() + "%"),
+        OpTraitsRulesProcFactory.getSelectRule());
+
+    // The dispatcher fires the processor corresponding to the closest matching
+    // rule and passes the context along
+    Dispatcher disp = new DefaultRuleDispatcher(OpTraitsRulesProcFactory.getDefaultRule(), opRules,
+        annotateCtx);
+    GraphWalker ogw = new PreOrderWalker(disp);
+
+    // Create a list of topop nodes
+    ArrayList<Node> topNodes = new ArrayList<Node>();
+    topNodes.addAll(pctx.getTopOps().values());
+    ogw.startWalking(topNodes, null);
+
+    return pctx;
+  }
+
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java
new file mode 100644
index 0000000000..1c959e3ffe
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java
@@ -0,0 +1,355 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.optimizer.metainfo.annotation;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map.Entry;
+import java.util.Stack;
+
+import org.apache.hadoop.hive.ql.exec.GroupByOperator;
+import org.apache.hadoop.hive.ql.exec.JoinOperator;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
+import org.apache.hadoop.hive.ql.exec.SelectOperator;
+import org.apache.hadoop.hive.ql.exec.TableScanOperator;
+import org.apache.hadoop.hive.ql.lib.Node;
+import org.apache.hadoop.hive.ql.lib.NodeProcessor;
+import org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc;
+import org.apache.hadoop.hive.ql.parse.ParseContext;
+import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.OpTraits;
+import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+
+/*
+ * This class populates the following operator traits for the entire operator tree:
+ * 1. Bucketing columns.
+ * 2. Table
+ * 3. Pruned partitions
+ * 
+ * Bucketing columns refer to not to the bucketing columns from the table object but instead
+ * to the dynamic 'bucketing' done by operators such as reduce sinks and group-bys.
+ * All the operators have a translation from their input names to the output names corresponding
+ * to the bucketing column. The colExprMap that is a part of every operator is used in this
+ * transformation.
+ * 
+ * The table object is used for the base-case in map-reduce when deciding to perform a bucket
+ * map join. This object is used in the BucketMapJoinProc to find if number of files for the
+ * table correspond to the number of buckets specified in the meta data.
+ * 
+ * The pruned partition information has the same purpose as the table object at the moment.
+ * 
+ * The traits of sorted-ness etc. can be populated as well for future optimizations to make use of.
+ */
+
+public class OpTraitsRulesProcFactory {
+
+  public static class DefaultRule implements NodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+      @SuppressWarnings("unchecked")
+      Operator<? extends OperatorDesc> op = (Operator<? extends OperatorDesc>)nd;
+      op.setOpTraits(op.getParentOperators().get(0).getOpTraits());
+      return null;
+    }
+
+  }
+
+  /*
+   * Reduce sink operator is the de-facto operator 
+   * for determining keyCols (emit keys of a map phase)
+   */
+  public static class ReduceSinkRule implements NodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+
+      ReduceSinkOperator rs = (ReduceSinkOperator)nd;
+      List<String> bucketCols = new ArrayList<String>();
+      if (rs.getColumnExprMap() != null) {
+        for (ExprNodeDesc exprDesc : rs.getConf().getKeyCols()) {
+          for (Entry<String, ExprNodeDesc> entry : rs.getColumnExprMap().entrySet()) {
+            if (exprDesc.isSame(entry.getValue())) {
+              bucketCols.add(entry.getKey());
+            }
+          }
+        }
+      }
+
+      List<List<String>> listBucketCols = new ArrayList<List<String>>();
+      listBucketCols.add(bucketCols);
+      OpTraits opTraits = new OpTraits(listBucketCols, -1);
+      rs.setOpTraits(opTraits);
+      return null;
+    }
+  }
+
+  /*
+   * Table scan has the table object and pruned partitions that has information such as
+   * bucketing, sorting, etc. that is used later for optimization.
+   */
+  public static class TableScanRule implements NodeProcessor {
+
+    public boolean checkBucketedTable(Table tbl, 
+        ParseContext pGraphContext,
+        PrunedPartitionList prunedParts) throws SemanticException {
+
+      if (tbl.isPartitioned()) {
+        List<Partition> partitions = prunedParts.getNotDeniedPartns();
+        // construct a mapping of (Partition->bucket file names) and (Partition -> bucket number)
+        if (!partitions.isEmpty()) {
+          for (Partition p : partitions) {
+            List<String> fileNames =
+                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), pGraphContext);
+            // The number of files for the table should be same as number of buckets.
+            int bucketCount = p.getBucketCount();
+
+            if (fileNames.size() != 0 && fileNames.size() != bucketCount) {
+              return false;
+            }
+          }
+        }
+      } else {
+
+        List<String> fileNames =
+            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), pGraphContext);
+        Integer num = new Integer(tbl.getNumBuckets());
+
+        // The number of files for the table should be same as number of buckets.
+        if (fileNames.size() != 0 && fileNames.size() != num) {
+          return false;
+        }
+      }
+
+      return true;
+    }
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+      TableScanOperator ts = (TableScanOperator)nd;
+      AnnotateOpTraitsProcCtx opTraitsCtx = (AnnotateOpTraitsProcCtx)procCtx;
+      Table table = opTraitsCtx.getParseContext().getTopToTable().get(ts);
+      PrunedPartitionList prunedPartList = null;
+      try {
+        prunedPartList =
+            opTraitsCtx.getParseContext().getPrunedPartitions(ts.getConf().getAlias(), ts);
+      } catch (HiveException e) {
+        prunedPartList = null;
+      }
+      boolean bucketMapJoinConvertible = checkBucketedTable(table, 
+          opTraitsCtx.getParseContext(), prunedPartList);
+      List<List<String>>bucketCols = new ArrayList<List<String>>();
+      int numBuckets = -1;
+      if (bucketMapJoinConvertible) {
+        bucketCols.add(table.getBucketCols());
+        numBuckets = table.getNumBuckets();
+      }
+      OpTraits opTraits = new OpTraits(bucketCols, numBuckets);
+      ts.setOpTraits(opTraits);
+      return null;
+    }
+  }
+
+  /*
+   * Group-by re-orders the keys emitted hence, the keyCols would change.
+   */
+  public static class GroupByRule implements NodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+      GroupByOperator gbyOp = (GroupByOperator)nd;
+      List<String> gbyKeys = new ArrayList<String>();
+      for (ExprNodeDesc exprDesc : gbyOp.getConf().getKeys()) {
+        for (Entry<String, ExprNodeDesc> entry : gbyOp.getColumnExprMap().entrySet()) {
+          if (exprDesc.isSame(entry.getValue())) {
+            gbyKeys.add(entry.getKey());
+          }
+        }
+      }
+
+      List<List<String>> listBucketCols = new ArrayList<List<String>>();
+      listBucketCols.add(gbyKeys);
+      OpTraits opTraits = new OpTraits(listBucketCols, -1);
+      gbyOp.setOpTraits(opTraits);
+      return null;
+    }
+  }
+
+  public static class SelectRule implements NodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+      SelectOperator selOp = (SelectOperator)nd;
+      List<List<String>> parentBucketColNames = 
+          selOp.getParentOperators().get(0).getOpTraits().getBucketColNames();
+
+      List<List<String>> listBucketCols = new ArrayList<List<String>>();
+      if (selOp.getColumnExprMap() != null) {
+        if (parentBucketColNames != null) {
+          for (List<String> colNames : parentBucketColNames) {
+            List<String> bucketColNames = new ArrayList<String>();
+            for (String colName : colNames) {
+              for (Entry<String, ExprNodeDesc> entry : selOp.getColumnExprMap().entrySet()) {
+                if (entry.getValue() instanceof ExprNodeColumnDesc) {
+                  if(((ExprNodeColumnDesc)(entry.getValue())).getColumn().equals(colName)) {
+                    bucketColNames.add(entry.getKey());
+                  }
+                }
+              }
+            }
+            listBucketCols.add(bucketColNames);
+          }
+        }
+      }
+
+      int numBuckets = -1;
+      if (selOp.getParentOperators().get(0).getOpTraits() != null) {
+        numBuckets = selOp.getParentOperators().get(0).getOpTraits().getNumBuckets();
+      }
+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets);
+      selOp.setOpTraits(opTraits);
+      return null;
+    }
+  }
+
+  public static class JoinRule implements NodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+      JoinOperator joinOp = (JoinOperator)nd;
+      List<List<String>> bucketColsList = new ArrayList<List<String>>();
+      byte pos = 0;
+      for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {
+        if (!(parentOp instanceof ReduceSinkOperator)) {
+          // can be mux operator
+          break;
+        }
+        ReduceSinkOperator rsOp = (ReduceSinkOperator)parentOp;
+        if (rsOp.getOpTraits() == null) {
+          ReduceSinkRule rsRule = new ReduceSinkRule();
+          rsRule.process(rsOp, stack, procCtx, nodeOutputs);
+        }
+        bucketColsList.add(getOutputColNames(joinOp, rsOp, pos));
+        pos++;
+      }
+
+      joinOp.setOpTraits(new OpTraits(bucketColsList, -1));
+      return null;
+    }
+
+    private List<String> getOutputColNames(JoinOperator joinOp,
+        ReduceSinkOperator rs, byte pos) {
+      List<List<String>> parentBucketColNames =
+          rs.getOpTraits().getBucketColNames();
+
+      if (parentBucketColNames != null) {
+        List<String> bucketColNames = new ArrayList<String>();
+
+        // guaranteed that there is only 1 list within this list because
+        // a reduce sink always brings down the bucketing cols to a single list.
+        // may not be true with correlation operators (mux-demux)
+        List<String> colNames = parentBucketColNames.get(0);
+        for (String colName : colNames) {
+          for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {
+            if (exprNode instanceof ExprNodeColumnDesc) {
+              if(((ExprNodeColumnDesc)(exprNode)).getColumn().equals(colName)) {
+                for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {
+                  if (entry.getValue().isSame(exprNode)) {
+                    bucketColNames.add(entry.getKey());
+                    // we have found the colName
+                    break;
+                  }
+                }
+              } else {
+                // continue on to the next exprNode to find a match
+                continue;
+              }
+              // we have found the colName. No need to search more exprNodes.
+              break;
+            }
+          }
+        }
+
+        return bucketColNames;
+      }
+
+      // no col names in parent
+      return null;
+    }
+  }
+
+  /*
+   *  When we have operators that have multiple parents, it is not
+   *  clear which parent's traits we need to propagate forward.
+   */
+  public static class MultiParentRule implements NodeProcessor {
+
+    @Override
+    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
+        Object... nodeOutputs) throws SemanticException {
+      OpTraits opTraits = new OpTraits(null, -1);
+      @SuppressWarnings("unchecked")
+      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>)nd;
+      operator.setOpTraits(opTraits);
+      return null;
+    } 
+  }
+
+  public static NodeProcessor getTableScanRule() {
+    return new TableScanRule();
+  }
+
+  public static NodeProcessor getReduceSinkRule() {
+    return new ReduceSinkRule();
+  }
+  
+  public static NodeProcessor getSelectRule() {
+    return new SelectRule();
+  }
+
+  public static NodeProcessor getDefaultRule() {
+    return new DefaultRule();
+  }
+
+  public static NodeProcessor getMultiParentRule() {
+    return new MultiParentRule();
+  }
+
+  public static NodeProcessor getGroupByRule() {
+    return new GroupByRule();
+  }
+
+  public static NodeProcessor getJoinRule() {
+    return new JoinRule();
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
index ec21aa8e07..6fe0ae3dec 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java
@@ -26,6 +26,8 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.commons.lang3.tuple.ImmutablePair;
+import org.apache.commons.lang3.tuple.Pair;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.DependencyCollectionTask;
@@ -45,6 +47,8 @@
 import org.apache.hadoop.hive.ql.plan.MoveWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 import org.apache.hadoop.hive.ql.plan.TezWork;
 
 /**
@@ -89,7 +93,7 @@ public class GenTezProcContext implements NodeProcessorCtx{
 
   // a map that keeps track of work that need to be linked while
   // traversing an operator tree
-  public final Map<Operator<?>, List<BaseWork>> linkOpWithWorkMap;
+  public final Map<Operator<?>, Map<BaseWork,TezEdgeProperty>> linkOpWithWorkMap;
 
   // a map to keep track of what reduce sinks have to be hooked up to
   // map join work
@@ -144,7 +148,7 @@ public GenTezProcContext(HiveConf conf, ParseContext parseContext,
     this.currentTask = (TezTask) TaskFactory.get(
          new TezWork(conf.getVar(HiveConf.ConfVars.HIVEQUERYID)), conf);
     this.leafOperatorToFollowingWork = new HashMap<Operator<?>, BaseWork>();
-    this.linkOpWithWorkMap = new HashMap<Operator<?>, List<BaseWork>>();
+    this.linkOpWithWorkMap = new HashMap<Operator<?>, Map<BaseWork, TezEdgeProperty>>();
     this.linkWorkWithReduceSinkMap = new HashMap<BaseWork, List<ReduceSinkOperator>>();
     this.mapJoinWorkMap = new HashMap<MapJoinOperator, List<BaseWork>>();
     this.rootToWorkMap = new HashMap<Operator<?>, BaseWork>();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
index 62fb6d48e1..3f70092269 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java
@@ -46,9 +46,10 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
 import org.apache.hadoop.hive.ql.plan.TezWork;
 import org.apache.hadoop.hive.ql.plan.UnionWork;
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 
 /**
  * GenTezUtils is a collection of shared helper methods to produce
@@ -104,9 +105,10 @@ public ReduceWork createReduceWork(GenTezProcContext context, Operator<?> root,
     setupReduceSink(context, reduceWork, reduceSink);
 
     tezWork.add(reduceWork);
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
     tezWork.connect(
         context.preceedingWork,
-        reduceWork, EdgeType.SIMPLE_EDGE);
+        reduceWork, edgeProp);
     context.connectedReduceSinks.add(reduceSink);
 
     return reduceWork;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java
index f0b3fdf612..41aeb4c01a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java
@@ -23,6 +23,7 @@
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Map.Entry;
 import java.util.Stack;
 
 import org.apache.commons.logging.Log;
@@ -45,9 +46,10 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
 import org.apache.hadoop.hive.ql.plan.TezWork;
 import org.apache.hadoop.hive.ql.plan.UnionWork;
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 
 /**
  * GenTezWork separates the operator tree into tez tasks.
@@ -160,30 +162,34 @@ public Object process(Node nd, Stack<Node> stack,
          * RS following the TS, we have already generated work for the TS-RS.
          * We need to hook the current work to this generated work.
          */
-        List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(mj);
-        if (linkWorkList != null) {
-          if (context.linkChildOpWithDummyOp.containsKey(mj)) {
-            for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {
-              work.addDummyOp((HashTableDummyOperator) dummy);
+        if (context.linkOpWithWorkMap.containsKey(mj)) {
+          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);
+          if (linkWorkMap != null) {
+            if (context.linkChildOpWithDummyOp.containsKey(mj)) {
+              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {
+                work.addDummyOp((HashTableDummyOperator) dummy);
+              }
             }
-          }
-          for (BaseWork parentWork : linkWorkList) {
-            LOG.debug("connecting "+parentWork.getName()+" with "+work.getName());
-            tezWork.connect(parentWork, work, EdgeType.BROADCAST_EDGE);
-
-            // need to set up output name for reduce sink now that we know the name
-            // of the downstream work
-            for (ReduceSinkOperator r:
-                   context.linkWorkWithReduceSinkMap.get(parentWork)) {
-              if (r.getConf().getOutputName() != null) {
-                LOG.debug("Cloning reduce sink for multi-child broadcast edge");
-                // we've already set this one up. Need to clone for the next work.
-                r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(
-                    (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());
-                context.clonedReduceSinks.add(r);
+            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {
+              BaseWork parentWork = parentWorkMap.getKey();
+              LOG.debug("connecting "+parentWork.getName()+" with "+work.getName());
+              TezEdgeProperty edgeProp = parentWorkMap.getValue();
+              tezWork.connect(parentWork, work, edgeProp);
+              
+              // need to set up output name for reduce sink now that we know the name
+              // of the downstream work
+              for (ReduceSinkOperator r:
+                     context.linkWorkWithReduceSinkMap.get(parentWork)) {
+                if (r.getConf().getOutputName() != null) {
+                  LOG.debug("Cloning reduce sink for multi-child broadcast edge");
+                  // we've already set this one up. Need to clone for the next work.
+                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(
+                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());
+                  context.clonedReduceSinks.add(r);
+                }
+                r.getConf().setOutputName(work.getName());
+                context.connectedReduceSinks.add(r);
               }
-              r.getConf().setOutputName(work.getName());
-              context.connectedReduceSinks.add(r);
             }
           }
         }
@@ -221,7 +227,8 @@ public Object process(Node nd, Stack<Node> stack,
 
       // finally hook everything up
       LOG.debug("Connecting union work ("+unionWork+") with work ("+work+")");
-      tezWork.connect(unionWork, work, EdgeType.CONTAINS);
+      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);
+      tezWork.connect(unionWork, work, edgeProp);
       unionWork.addUnionOperators(context.currentUnionOperators);
       context.currentUnionOperators.clear();
       context.workWithUnionOperators.add(work);
@@ -261,7 +268,8 @@ public Object process(Node nd, Stack<Node> stack,
 
       if (!context.connectedReduceSinks.contains(rs)) {
         // add dependency between the two work items
-        tezWork.connect(work, rWork, EdgeType.SIMPLE_EDGE);
+        TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
+        tezWork.connect(work, rWork, edgeProp);
         context.connectedReduceSinks.add(rs);
       }
     } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
index e1ce79a41b..e2ea0d4947 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
@@ -48,6 +48,7 @@
 import org.apache.hadoop.hive.ql.lib.CompositeProcessor;
 import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
 import org.apache.hadoop.hive.ql.lib.Dispatcher;
+import org.apache.hadoop.hive.ql.lib.ForwardWalker;
 import org.apache.hadoop.hive.ql.lib.GraphWalker;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.ql.lib.NodeProcessor;
@@ -117,7 +118,7 @@ protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,
     Dispatcher disp = new DefaultRuleDispatcher(null, opRules, procCtx);
     List<Node> topNodes = new ArrayList<Node>();
     topNodes.addAll(pCtx.getTopOps().values());
-    GraphWalker ogw = new TezWalker(disp);
+    GraphWalker ogw = new ForwardWalker(disp);
     ogw.startWalking(topNodes, null);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
index 0c10c309d7..8410664bc7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
@@ -22,6 +22,7 @@ public class AbstractOperatorDesc implements OperatorDesc {
 
   protected boolean vectorMode = false;
   protected transient Statistics statistics;
+  protected transient OpTraits opTraits;
 
   @Override
   @Explain(skipHeader = true, displayName = "Statistics")
@@ -42,4 +43,12 @@ public Object clone() throws CloneNotSupportedException {
   public void setVectorMode(boolean vm) {
     this.vectorMode = vm;
   }
+  
+  public OpTraits getOpTraits() {
+    return opTraits;
+  }
+  
+  public void setOpTraits(OpTraits opTraits) {
+    this.opTraits = opTraits;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
index 94d4ca6c65..63579faad8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapJoinDesc.java
@@ -49,6 +49,9 @@ public class MapJoinDesc extends JoinDesc implements Serializable {
 
   // for tez. used to remember which position maps to which logical input
   private Map<Integer, String> parentToInput = new HashMap<Integer, String>();
+  
+  // for tez. used to remember which type of a Bucket Map Join this is.
+  private boolean customBucketMapJoin;
 
   // table alias (small) --> input file name (big) --> target file names (small)
   private Map<String, Map<String, List<String>>> aliasBucketFileNameMapping;
@@ -81,6 +84,7 @@ public MapJoinDesc(MapJoinDesc clone) {
     this.bigTablePartSpecToFileMapping = clone.bigTablePartSpecToFileMapping;
     this.dumpFilePrefix = clone.dumpFilePrefix;
     this.parentToInput = clone.parentToInput;
+    this.customBucketMapJoin = clone.customBucketMapJoin;
   }
 
   public MapJoinDesc(final Map<Byte, List<ExprNodeDesc>> keys,
@@ -280,4 +284,12 @@ public void setHashTableMemoryUsage(float hashtableMemoryUsage) {
   public float getHashTableMemoryUsage() {
     return hashtableMemoryUsage;
   }
+  
+  public void setCustomBucketMapJoin(boolean customBucketMapJoin) {
+    this.customBucketMapJoin = customBucketMapJoin;
+  }
+  
+  public boolean getCustomBucketMapJoin() {
+    return this.customBucketMapJoin;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java
new file mode 100644
index 0000000000..125ad21f6f
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.plan;
+
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.metadata.Table;
+import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
+
+public class OpTraits {
+  
+  List<List<String>> bucketColNames;
+  int numBuckets;
+  
+  public OpTraits(List<List<String>> bucketColNames, int numBuckets) {
+    this.bucketColNames = bucketColNames;
+    this.numBuckets = numBuckets;
+  }
+
+  public List<List<String>> getBucketColNames() {
+    return bucketColNames;
+  }
+
+  public int getNumBuckets() {
+    return numBuckets;
+  }
+
+  public void setBucketColNames(List<List<String>> bucketColNames) {
+    this.bucketColNames = bucketColNames;    
+  }
+
+  public void setNumBuckets(int numBuckets) {
+    this.numBuckets = numBuckets;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
index 6c2efaf93e..c8c9570b52 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
@@ -24,4 +24,6 @@ public interface OperatorDesc extends Serializable, Cloneable {
   public Object clone() throws CloneNotSupportedException;
   public Statistics getStatistics();
   public void setStatistics(Statistics statistics);
+  public OpTraits getOpTraits();
+  public void setOpTraits(OpTraits opTraits);
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/TezEdgeProperty.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/TezEdgeProperty.java
new file mode 100644
index 0000000000..96adb8484f
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/TezEdgeProperty.java
@@ -0,0 +1,45 @@
+package org.apache.hadoop.hive.ql.plan;
+
+import java.util.List;
+import java.util.Map;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
+
+public class TezEdgeProperty {
+  
+  public enum EdgeType {
+    SIMPLE_EDGE,
+    BROADCAST_EDGE, 
+    CONTAINS,
+    CUSTOM_EDGE,
+    CUSTOM_SIMPLE_EDGE,
+  }
+
+  private HiveConf hiveConf;
+  private EdgeType edgeType;
+  private int numBuckets;
+
+  public TezEdgeProperty(HiveConf hiveConf, EdgeType edgeType, 
+      int buckets) {
+    this.hiveConf = hiveConf;
+    this.edgeType = edgeType;
+    this.numBuckets = buckets;
+  }
+
+  public TezEdgeProperty(EdgeType edgeType) {
+    this(null, edgeType, -1);
+  }
+
+  public EdgeType getEdgeType() {
+    return edgeType;
+  }
+
+  public HiveConf getHiveConf () {
+    return hiveConf;
+  }
+
+  public int getNumBuckets() {
+    return numBuckets;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java
index f974c5773a..996268e355 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java
@@ -32,6 +32,8 @@
 import org.apache.commons.lang3.tuple.Pair;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
+import org.apache.tez.dag.api.EdgeProperty;
 
 /**
  * TezWork. This class encapsulates all the work objects that can be executed
@@ -43,12 +45,6 @@
 @Explain(displayName = "Tez")
 public class TezWork extends AbstractOperatorDesc {
 
-  public enum EdgeType {
-    SIMPLE_EDGE,
-    BROADCAST_EDGE,
-    CONTAINS
-  }
-
   private static transient final Log LOG = LogFactory.getLog(TezWork.class);
 
   private static int counter;
@@ -57,8 +53,8 @@ public enum EdgeType {
   private final Set<BaseWork> leaves = new HashSet<BaseWork>();
   private final Map<BaseWork, List<BaseWork>> workGraph = new HashMap<BaseWork, List<BaseWork>>();
   private final Map<BaseWork, List<BaseWork>> invertedWorkGraph = new HashMap<BaseWork, List<BaseWork>>();
-  private final Map<Pair<BaseWork, BaseWork>, EdgeType> edgeProperties =
-      new HashMap<Pair<BaseWork, BaseWork>, EdgeType>();
+  private final Map<Pair<BaseWork, BaseWork>, TezEdgeProperty> edgeProperties =
+      new HashMap<Pair<BaseWork, BaseWork>, TezEdgeProperty>();
 
   public TezWork(String name) {
     this.name = name + ":" + (++counter);
@@ -146,19 +142,6 @@ public void add(BaseWork w) {
     leaves.add(w);
   }
 
-  /**
-   * connect adds an edge between a and b. Both nodes have
-   * to be added prior to calling connect.
-   */
-  public void connect(BaseWork a, BaseWork b, EdgeType edgeType) {
-    workGraph.get(a).add(b);
-    invertedWorkGraph.get(b).add(a);
-    roots.remove(b);
-    leaves.remove(a);
-    ImmutablePair workPair = new ImmutablePair(a, b);
-    edgeProperties.put(workPair, edgeType);
-  }
-
   /**
    * disconnect removes an edge between a and b. Both a and
    * b have to be in the graph. If there is no matching edge
@@ -242,10 +225,14 @@ public void remove(BaseWork work) {
     invertedWorkGraph.remove(work);
   }
 
+  public EdgeType getEdgeType(BaseWork a, BaseWork b) {
+    return edgeProperties.get(new ImmutablePair(a,b)).getEdgeType();
+  }
+
   /**
    * returns the edge type connecting work a and b
    */
-  public EdgeType getEdgeProperty(BaseWork a, BaseWork b) {
+  public TezEdgeProperty getEdgeProperty(BaseWork a, BaseWork b) {
     return edgeProperties.get(new ImmutablePair(a,b));
   }
 
@@ -275,7 +262,7 @@ public Map<String, List<Dependency>> getDependencyMap() {
       for (BaseWork d: entry.getValue()) {
         Dependency dependency = new Dependency();
         dependency.w = d;
-        dependency.type = getEdgeProperty(d, entry.getKey());
+        dependency.type = getEdgeType(d, entry.getKey());
         dependencies.add(dependency);
       }
       if (!dependencies.isEmpty()) {
@@ -284,4 +271,19 @@ public Map<String, List<Dependency>> getDependencyMap() {
     }
     return result;
   }
+
+  /**
+   * connect adds an edge between a and b. Both nodes have
+   * to be added prior to calling connect.
+   * @param  
+   */
+  public void connect(BaseWork a, BaseWork b,
+      TezEdgeProperty edgeProp) {
+    workGraph.get(a).add(b);
+    invertedWorkGraph.get(b).add(a);
+    roots.remove(b);
+    leaves.remove(a);
+    ImmutablePair workPair = new ImmutablePair(a, b);
+    edgeProperties.put(workPair, edgeProp);
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java
index 859b5ad0e1..4810fd0f0e 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java
@@ -47,8 +47,9 @@
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 import org.apache.hadoop.hive.ql.plan.TezWork;
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.yarn.api.records.LocalResource;
@@ -92,7 +93,7 @@ public void setUp() throws Exception {
     when(path.getFileSystem(any(Configuration.class))).thenReturn(fs);
     when(utils.getTezDir(any(Path.class))).thenReturn(path);
     when(utils.createVertex(any(JobConf.class), any(BaseWork.class), any(Path.class), any(LocalResource.class),
-        any(List.class), any(FileSystem.class), any(Context.class), anyBoolean())).thenAnswer(new Answer<Vertex>() {
+        any(List.class), any(FileSystem.class), any(Context.class), anyBoolean(), any(TezWork.class))).thenAnswer(new Answer<Vertex>() {
 
           @Override
           public Vertex answer(InvocationOnMock invocation) throws Throwable {
@@ -103,7 +104,7 @@ public Vertex answer(InvocationOnMock invocation) throws Throwable {
         });
 
     when(utils.createEdge(any(JobConf.class), any(Vertex.class), any(JobConf.class),
-        any(Vertex.class), any(EdgeType.class))).thenAnswer(new Answer<Edge>() {
+        any(Vertex.class), any(TezEdgeProperty.class))).thenAnswer(new Answer<Edge>() {
 
           @Override
           public Edge answer(InvocationOnMock invocation) throws Throwable {
@@ -145,9 +146,10 @@ public Edge answer(InvocationOnMock invocation) throws Throwable {
     rws[0].setReducer(op);
     rws[1].setReducer(op);
 
-    work.connect(mws[0], rws[0], EdgeType.SIMPLE_EDGE);
-    work.connect(mws[1], rws[0], EdgeType.SIMPLE_EDGE);
-    work.connect(rws[0], rws[1], EdgeType.SIMPLE_EDGE);
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
+    work.connect(mws[0], rws[0], edgeProp);
+    work.connect(mws[1], rws[0], edgeProp);
+    work.connect(rws[0], rws[1], edgeProp);
 
     task = new TezTask(utils);
     task.setWork(work);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/plan/TestTezWork.java b/ql/src/test/org/apache/hadoop/hive/ql/plan/TestTezWork.java
index d57a64cd91..26001493d7 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/plan/TestTezWork.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/plan/TestTezWork.java
@@ -22,7 +22,7 @@
 
 import junit.framework.Assert;
 
-import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;
+import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 import org.junit.Before;
 import org.junit.Test;
 
@@ -62,7 +62,8 @@ public void testConnect() throws Exception {
     BaseWork parent = nodes.get(0);
     BaseWork child = nodes.get(1);
 
-    work.connect(parent, child, EdgeType.SIMPLE_EDGE);
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
+    work.connect(parent, child, edgeProp);
 
     Assert.assertEquals(work.getParents(child).size(), 1);
     Assert.assertEquals(work.getChildren(parent).size(), 1);
@@ -78,7 +79,7 @@ public void testConnect() throws Exception {
       Assert.assertEquals(work.getChildren(w).size(), 0);
     }
 
-    Assert.assertEquals(work.getEdgeProperty(parent, child), EdgeType.SIMPLE_EDGE);
+    Assert.assertEquals(work.getEdgeProperty(parent, child).getEdgeType(), EdgeType.SIMPLE_EDGE);
   }
 
   @Test
@@ -86,7 +87,8 @@ public void testBroadcastConnect() throws Exception {
     BaseWork parent = nodes.get(0);
     BaseWork child = nodes.get(1);
 
-    work.connect(parent, child, EdgeType.BROADCAST_EDGE);
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.BROADCAST_EDGE);
+    work.connect(parent, child, edgeProp);
 
     Assert.assertEquals(work.getParents(child).size(), 1);
     Assert.assertEquals(work.getChildren(parent).size(), 1);
@@ -102,7 +104,7 @@ public void testBroadcastConnect() throws Exception {
       Assert.assertEquals(work.getChildren(w).size(), 0);
     }
 
-    Assert.assertEquals(work.getEdgeProperty(parent, child), EdgeType.BROADCAST_EDGE);
+    Assert.assertEquals(work.getEdgeProperty(parent, child).getEdgeType(), EdgeType.BROADCAST_EDGE);
   }
 
   @Test
@@ -110,8 +112,9 @@ public void testDisconnect() throws Exception {
     BaseWork parent = nodes.get(0);
     BaseWork children[] = {nodes.get(1), nodes.get(2)};
 
-    work.connect(parent, children[0], EdgeType.SIMPLE_EDGE);
-    work.connect(parent, children[1], EdgeType.SIMPLE_EDGE);
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
+    work.connect(parent, children[0], edgeProp);
+    work.connect(parent, children[1], edgeProp);
 
     work.disconnect(parent, children[0]);
 
@@ -128,8 +131,9 @@ public void testRemove() throws Exception {
     BaseWork parent = nodes.get(0);
     BaseWork children[] = {nodes.get(1), nodes.get(2)};
 
-    work.connect(parent, children[0], EdgeType.SIMPLE_EDGE);
-    work.connect(parent, children[1], EdgeType.SIMPLE_EDGE);
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
+    work.connect(parent, children[0], edgeProp);
+    work.connect(parent, children[1], edgeProp);
 
     work.remove(parent);
 
@@ -142,8 +146,9 @@ public void testRemove() throws Exception {
 
   @Test
   public void testGetAllWork() throws Exception {
+    TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);
     for (int i = 4; i > 0; --i) {
-      work.connect(nodes.get(i), nodes.get(i-1), EdgeType.SIMPLE_EDGE);
+      work.connect(nodes.get(i), nodes.get(i-1), edgeProp);
     }
 
     List<BaseWork> sorted = work.getAllWork();
diff --git a/ql/src/test/queries/clientpositive/bucket_map_join_tez1.q b/ql/src/test/queries/clientpositive/bucket_map_join_tez1.q
new file mode 100644
index 0000000000..c9266a59c3
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/bucket_map_join_tez1.q
@@ -0,0 +1,85 @@
+set hive.auto.convert.join=true;
+set hive.auto.convert.join.noconditionaltask=true;
+set hive.auto.convert.join.noconditionaltask.size=10000;
+
+CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
+CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
+CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
+
+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
+
+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+
+set hive.enforce.bucketing=true;
+set hive.enforce.sorting = true;
+set hive.optimize.bucketingsorting=false;
+insert overwrite table tab_part partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin_part;
+
+CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
+insert overwrite table tab partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin;
+
+set hive.convert.join.bucket.mapjoin.tez = true;
+explain
+select a.key, a.value, b.value
+from tab a join tab_part b on a.key = b.key;
+
+-- one side is really bucketed. srcbucket_mapjoin is not really a bucketed table.
+-- In this case the sub-query is chosen as the big table.
+explain
+select a.k1, a.v1, b.value
+from (select sum(substr(srcbucket_mapjoin.value,5)) as v1, key as k1 from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab b on a.k1 = b.key;
+
+explain
+select a.k1, a.v1, b.value
+from (select sum(substr(tab.value,5)) as v1, key as k1 from tab_part join tab on tab_part.key = tab.key GROUP BY tab.key) a
+join tab b on a.k1 = b.key;
+
+explain
+select a.k1, a.v1, b.value
+from (select sum(substr(x.value,5)) as v1, x.key as k1 from tab x join tab y on x.key = y.key GROUP BY x.key) a
+join tab_part b on a.k1 = b.key;
+
+-- multi-way join
+explain
+select a.key, a.value, b.value
+from tab_part a join tab b on a.key = b.key join tab c on a.key = c.key;
+
+explain
+select a.key, a.value, c.value
+from (select x.key, x.value from tab_part x join tab y on x.key = y.key) a join tab c on a.key = c.key;
+
+-- in this case sub-query is the small table
+explain
+select a.key, a.value, b.value
+from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab_part b on a.key = b.key;
+
+set hive.map.aggr=false;
+explain
+select a.key, a.value, b.value
+from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab_part b on a.key = b.key;
+
+-- join on non-bucketed column results in broadcast join.
+explain
+select a.key, a.value, b.value
+from tab a join tab_part b on a.value = b.value;
+
+CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
+insert overwrite table tab1
+select key,value from srcbucket_mapjoin;
+
+explain
+select a.key, a.value, b.value
+from tab1 a join tab_part b on a.key = b.key;
+
+explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value;
+
+
diff --git a/ql/src/test/queries/clientpositive/bucket_map_join_tez2.q b/ql/src/test/queries/clientpositive/bucket_map_join_tez2.q
new file mode 100644
index 0000000000..a3588ec94c
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/bucket_map_join_tez2.q
@@ -0,0 +1,50 @@
+set hive.auto.convert.join=true;
+set hive.auto.convert.join.noconditionaltask=true;
+set hive.auto.convert.join.noconditionaltask.size=10000;
+
+CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
+CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
+CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
+
+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
+
+load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
+
+set hive.enforce.bucketing=true;
+set hive.enforce.sorting = true;
+set hive.optimize.bucketingsorting=false;
+insert overwrite table tab_part partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin_part;
+
+CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
+insert overwrite table tab partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin;
+
+set hive.convert.join.bucket.mapjoin.tez = true;
+
+explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value;
+
+CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
+insert overwrite table tab1
+select key,value from srcbucket_mapjoin;
+
+explain
+select a.key, a.value, b.value
+from tab1 a join src b on a.key = b.key;
+
+explain
+select a.key, b.key from (select key from tab_part where key > 1) a join (select key from tab_part where key > 2) b on a.key = b.key;
+
+explain
+select a.key, b.key from (select key from tab_part where key > 1) a left outer join (select key from tab_part where key > 2) b on a.key = b.key;
+
+explain
+select a.key, b.key from (select key from tab_part where key > 1) a right outer join (select key from tab_part where key > 2) b on a.key = b.key;
+
+explain select a.key, b.key from (select distinct key from tab) a join tab b on b.key = a.key;
+
+explain select a.value, b.value from (select distinct value from tab) a join tab b on b.key = a.value;
diff --git a/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out b/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out
new file mode 100644
index 0000000000..661f8c9d0f
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out
@@ -0,0 +1,1127 @@
+PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@srcbucket_mapjoin
+PREHOOK: query: CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tab_part
+PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@srcbucket_mapjoin_part
+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin
+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: insert overwrite table tab_part partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin_part
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcbucket_mapjoin_part
+PREHOOK: Input: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: Output: default@tab_part@ds=2008-04-08
+POSTHOOK: query: insert overwrite table tab_part partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin_part
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcbucket_mapjoin_part
+POSTHOOK: Input: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: Output: default@tab_part@ds=2008-04-08
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tab
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: insert overwrite table tab partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcbucket_mapjoin
+PREHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: Output: default@tab@ds=2008-04-08
+POSTHOOK: query: insert overwrite table tab partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcbucket_mapjoin
+POSTHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+POSTHOOK: Output: default@tab@ds=2008-04-08
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: explain
+select a.key, a.value, b.value
+from tab a join tab_part b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, a.value, b.value
+from tab a join tab_part b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Map 2 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 {value}
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col6
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string), _col6 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int), value (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: -- one side is really bucketed. srcbucket_mapjoin is not really a bucketed table.
+-- In this case the sub-query is chosen as the big table.
+explain
+select a.k1, a.v1, b.value
+from (select sum(substr(srcbucket_mapjoin.value,5)) as v1, key as k1 from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab b on a.k1 = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- one side is really bucketed. srcbucket_mapjoin is not really a bucketed table.
+-- In this case the sub-query is chosen as the big table.
+explain
+select a.k1, a.v1, b.value
+from (select sum(substr(srcbucket_mapjoin.value,5)) as v1, key as k1 from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab b on a.k1 = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Reducer 3 (CUSTOM_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0} {_col1}
+                      1 {value}
+                    keys:
+                      0 _col1 (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col3
+                    Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col1 (type: int), _col0 (type: double), _col3 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: srcbucket_mapjoin
+                  Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: int), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: sum(substr(value, 5))
+                      keys: key (type: int)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col1 (type: double)
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: double), _col0 (type: int)
+                  outputColumnNames: _col0, _col1
+                  Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col1 (type: int)
+                    sort order: +
+                    Map-reduce partition columns: _col1 (type: int)
+                    Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: double), _col1 (type: int)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.k1, a.v1, b.value
+from (select sum(substr(tab.value,5)) as v1, key as k1 from tab_part join tab on tab_part.key = tab.key GROUP BY tab.key) a
+join tab b on a.k1 = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.k1, a.v1, b.value
+from (select sum(substr(tab.value,5)) as v1, key as k1 from tab_part join tab on tab_part.key = tab.key GROUP BY tab.key) a
+join tab b on a.k1 = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 2 <- Map 4 (CUSTOM_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE), Map 1 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: value (type: string)
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 
+                      1 {key} {value}
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col5, _col6
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col5 (type: int), _col6 (type: string)
+                      outputColumnNames: _col5, _col6
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: sum(substr(_col6, 5))
+                        keys: _col5 (type: int)
+                        mode: hash
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int)
+                          sort order: +
+                          Map-reduce partition columns: _col0 (type: int)
+                          Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col1 (type: double)
+        Map 4 
+            Map Operator Tree:
+                TableScan
+                  alias: tab
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int), value (type: string)
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: double), _col0 (type: int)
+                  outputColumnNames: _col0, _col1
+                  Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0} {_col1}
+                      1 {value}
+                    keys:
+                      0 _col1 (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col3
+                    Statistics: Num rows: 302 Data size: 3213 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col1 (type: int), _col0 (type: double), _col3 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 302 Data size: 3213 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 302 Data size: 3213 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.k1, a.v1, b.value
+from (select sum(substr(x.value,5)) as v1, x.key as k1 from tab x join tab y on x.key = y.key GROUP BY x.key) a
+join tab_part b on a.k1 = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.k1, a.v1, b.value
+from (select sum(substr(x.value,5)) as v1, x.key as k1 from tab x join tab y on x.key = y.key GROUP BY x.key) a
+join tab_part b on a.k1 = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Reducer 3 (CUSTOM_EDGE)
+        Map 2 <- Map 4 (CUSTOM_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0} {_col1}
+                      1 {value}
+                    keys:
+                      0 _col1 (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col3
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col1 (type: int), _col0 (type: double), _col3 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: x
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        aggregations: sum(substr(_col1, 5))
+                        keys: _col0 (type: int)
+                        mode: hash
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int)
+                          sort order: +
+                          Map-reduce partition columns: _col0 (type: int)
+                          Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                          value expressions: _col1 (type: double)
+        Map 4 
+            Map Operator Tree:
+                TableScan
+                  alias: y
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 133 Data size: 1411 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: double), _col0 (type: int)
+                  outputColumnNames: _col0, _col1
+                  Statistics: Num rows: 133 Data size: 1411 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col1 (type: int)
+                    sort order: +
+                    Map-reduce partition columns: _col1 (type: int)
+                    Statistics: Num rows: 133 Data size: 1411 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: double), _col1 (type: int)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: -- multi-way join
+explain
+select a.key, a.value, b.value
+from tab_part a join tab b on a.key = b.key join tab c on a.key = c.key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- multi-way join
+explain
+select a.key, a.value, b.value
+from tab_part a join tab b on a.key = b.key join tab c on a.key = c.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 3 <- Map 1 (CUSTOM_EDGE), Map 2 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: value (type: string)
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: c
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                         Inner Join 0 to 2
+                    condition expressions:
+                      0 {key} {value}
+                      1 {value}
+                      2 
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                      2 key (type: int)
+                    outputColumnNames: _col0, _col1, _col6
+                    Statistics: Num rows: 1100 Data size: 11686 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string), _col6 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 1100 Data size: 11686 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 1100 Data size: 11686 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.key, a.value, c.value
+from (select x.key, x.value from tab_part x join tab y on x.key = y.key) a join tab c on a.key = c.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, a.value, c.value
+from (select x.key, x.value from tab_part x join tab y on x.key = y.key) a join tab c on a.key = c.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 2 <- Map 1 (CUSTOM_EDGE), Map 3 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: c
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: value (type: string)
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: x
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      Map Join Operator
+                        condition map:
+                             Inner Join 0 to 1
+                        condition expressions:
+                          0 {_col0} {_col1}
+                          1 {value}
+                        keys:
+                          0 _col0 (type: int)
+                          1 key (type: int)
+                        outputColumnNames: _col0, _col1, _col3
+                        Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                        Select Operator
+                          expressions: _col0 (type: int), _col1 (type: string), _col3 (type: string)
+                          outputColumnNames: _col0, _col1, _col2
+                          Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                          File Output Operator
+                            compressed: false
+                            Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                            table:
+                                input format: org.apache.hadoop.mapred.TextInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: y
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: -- in this case sub-query is the small table
+explain
+select a.key, a.value, b.value
+from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab_part b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: -- in this case sub-query is the small table
+explain
+select a.key, a.value, b.value
+from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab_part b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Reducer 3 (CUSTOM_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0} {_col1}
+                      1 {value}
+                    keys:
+                      0 _col0 (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col3
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: double), _col3 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: srcbucket_mapjoin
+                  Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: int), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: sum(substr(value, 5))
+                      keys: key (type: int)
+                      mode: hash
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col1 (type: double)
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: double)
+                  outputColumnNames: _col0, _col1
+                  Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int)
+                    sort order: +
+                    Map-reduce partition columns: _col0 (type: int)
+                    Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: int), _col1 (type: double)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.key, a.value, b.value
+from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab_part b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, a.value, b.value
+from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
+join tab_part b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Reducer 3 (CUSTOM_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0} {_col1}
+                      1 {value}
+                    keys:
+                      0 _col0 (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col3
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: double), _col3 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: srcbucket_mapjoin
+                  Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: int), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: key (type: int)
+                      sort order: +
+                      Map-reduce partition columns: key (type: int)
+                      Statistics: Num rows: 27 Data size: 2808 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: substr(value, 5) (type: string)
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: sum(VALUE._col0)
+                keys: KEY._col0 (type: int)
+                mode: complete
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: double)
+                  outputColumnNames: _col0, _col1
+                  Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int)
+                    sort order: +
+                    Map-reduce partition columns: _col0 (type: int)
+                    Statistics: Num rows: 13 Data size: 1352 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: int), _col1 (type: double)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: -- join on non-bucketed column results in broadcast join.
+explain
+select a.key, a.value, b.value
+from tab a join tab_part b on a.value = b.value
+PREHOOK: type: QUERY
+POSTHOOK: query: -- join on non-bucketed column results in broadcast join.
+explain
+select a.key, a.value, b.value
+from tab a join tab_part b on a.value = b.value
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Map 2 (BROADCAST_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 {value}
+                    keys:
+                      0 value (type: string)
+                      1 value (type: string)
+                    outputColumnNames: _col0, _col1, _col6
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string), _col6 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: value (type: string)
+                    sort order: +
+                    Map-reduce partition columns: value (type: string)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int), value (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tab1
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: insert overwrite table tab1
+select key,value from srcbucket_mapjoin
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcbucket_mapjoin
+PREHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: Output: default@tab1
+POSTHOOK: query: insert overwrite table tab1
+select key,value from srcbucket_mapjoin
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcbucket_mapjoin
+POSTHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+POSTHOOK: Output: default@tab1
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: explain
+select a.key, a.value, b.value
+from tab1 a join tab_part b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, a.value, b.value
+from tab1 a join tab_part b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Map 2 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 {value}
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1, _col5
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string), _col5 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int), value (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 3 <- Map 2 (CUSTOM_EDGE), Map 1 (BROADCAST_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: value (type: string)
+                    sort order: +
+                    Map-reduce partition columns: value (type: string)
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int)
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: c
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Map Join Operator
+                      condition map:
+                           Inner Join 0 to 1
+                      condition expressions:
+                        0 {_col0}
+                        1 {key}
+                      keys:
+                        0 _col1 (type: string)
+                        1 value (type: string)
+                      outputColumnNames: _col5, _col10
+                      Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                      Select Operator
+                        expressions: _col5 (type: int), _col10 (type: int)
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                        File Output Operator
+                          compressed: false
+                          Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                          table:
+                              input format: org.apache.hadoop.mapred.TextInputFormat
+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
diff --git a/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out b/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out
new file mode 100644
index 0000000000..32cc3e7b09
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out
@@ -0,0 +1,710 @@
+PREHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@srcbucket_mapjoin
+PREHOOK: query: CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tab_part
+PREHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@srcbucket_mapjoin_part
+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin
+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: query: load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: query: insert overwrite table tab_part partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin_part
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcbucket_mapjoin_part
+PREHOOK: Input: default@srcbucket_mapjoin_part@ds=2008-04-08
+PREHOOK: Output: default@tab_part@ds=2008-04-08
+POSTHOOK: query: insert overwrite table tab_part partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin_part
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcbucket_mapjoin_part
+POSTHOOK: Input: default@srcbucket_mapjoin_part@ds=2008-04-08
+POSTHOOK: Output: default@tab_part@ds=2008-04-08
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tab
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: insert overwrite table tab partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcbucket_mapjoin
+PREHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: Output: default@tab@ds=2008-04-08
+POSTHOOK: query: insert overwrite table tab partition (ds='2008-04-08')
+select key,value from srcbucket_mapjoin
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcbucket_mapjoin
+POSTHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+POSTHOOK: Output: default@tab@ds=2008-04-08
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 3 <- Map 2 (CUSTOM_EDGE), Map 1 (BROADCAST_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: value (type: string)
+                    sort order: +
+                    Map-reduce partition columns: value (type: string)
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int)
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: c
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: key (type: int)
+                    sort order: +
+                    Map-reduce partition columns: key (type: int)
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 
+                    keys:
+                      0 key (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE
+                    Map Join Operator
+                      condition map:
+                           Inner Join 0 to 1
+                      condition expressions:
+                        0 {_col0}
+                        1 {key}
+                      keys:
+                        0 _col1 (type: string)
+                        1 value (type: string)
+                      outputColumnNames: _col5, _col10
+                      Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                      Select Operator
+                        expressions: _col5 (type: int), _col10 (type: int)
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                        File Output Operator
+                          compressed: false
+                          Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE
+                          table:
+                              input format: org.apache.hadoop.mapred.TextInputFormat
+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+POSTHOOK: query: CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tab1
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: insert overwrite table tab1
+select key,value from srcbucket_mapjoin
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcbucket_mapjoin
+PREHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+PREHOOK: Output: default@tab1
+POSTHOOK: query: insert overwrite table tab1
+select key,value from srcbucket_mapjoin
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcbucket_mapjoin
+POSTHOOK: Input: default@srcbucket_mapjoin@ds=2008-04-08
+POSTHOOK: Output: default@tab1
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: explain
+select a.key, a.value, b.value
+from tab1 a join src b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, a.value, b.value
+from tab1 a join src b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Map 2 (BROADCAST_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {key} {value}
+                      1 {value}
+                    keys:
+                      0 UDFToDouble(key) (type: double)
+                      1 UDFToDouble(key) (type: double)
+                    outputColumnNames: _col0, _col1, _col5
+                    Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: string), _col5 (type: string)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: a
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: UDFToDouble(key) (type: double)
+                    sort order: +
+                    Map-reduce partition columns: UDFToDouble(key) (type: double)
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: key (type: int), value (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.key, b.key from (select key from tab_part where key > 1) a join (select key from tab_part where key > 2) b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, b.key from (select key from tab_part where key > 1) a join (select key from tab_part where key > 2) b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Map 2 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: (key > 1) (type: boolean)
+                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                      Map Join Operator
+                        condition map:
+                             Inner Join 0 to 1
+                        condition expressions:
+                          0 {_col0}
+                          1 {_col0}
+                        keys:
+                          0 _col0 (type: int)
+                          1 _col0 (type: int)
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                        Select Operator
+                          expressions: _col0 (type: int), _col1 (type: int)
+                          outputColumnNames: _col0, _col1
+                          Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                          File Output Operator
+                            compressed: false
+                            Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                            table:
+                                input format: org.apache.hadoop.mapred.TextInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: (key > 2) (type: boolean)
+                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: int)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.key, b.key from (select key from tab_part where key > 1) a left outer join (select key from tab_part where key > 2) b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, b.key from (select key from tab_part where key > 1) a left outer join (select key from tab_part where key > 2) b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Map 2 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: (key > 1) (type: boolean)
+                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                      Map Join Operator
+                        condition map:
+                             Left Outer Join0 to 1
+                        condition expressions:
+                          0 {_col0}
+                          1 {_col0}
+                        keys:
+                          0 _col0 (type: int)
+                          1 _col0 (type: int)
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                        Select Operator
+                          expressions: _col0 (type: int), _col1 (type: int)
+                          outputColumnNames: _col0, _col1
+                          Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                          File Output Operator
+                            compressed: false
+                            Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                            table:
+                                input format: org.apache.hadoop.mapred.TextInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: (key > 2) (type: boolean)
+                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: int)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain
+select a.key, b.key from (select key from tab_part where key > 1) a right outer join (select key from tab_part where key > 2) b on a.key = b.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select a.key, b.key from (select key from tab_part where key > 1) a right outer join (select key from tab_part where key > 2) b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 2 <- Map 1 (CUSTOM_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: (key > 1) (type: boolean)
+                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: int)
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: tab_part
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: (key > 2) (type: boolean)
+                    Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: key (type: int)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 166 Data size: 1763 Basic stats: COMPLETE Column stats: NONE
+                      Map Join Operator
+                        condition map:
+                             Right Outer Join0 to 1
+                        condition expressions:
+                          0 {_col0}
+                          1 {_col0}
+                        keys:
+                          0 _col0 (type: int)
+                          1 _col0 (type: int)
+                        outputColumnNames: _col0, _col1
+                        Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                        Select Operator
+                          expressions: _col0 (type: int), _col1 (type: int)
+                          outputColumnNames: _col0, _col1
+                          Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                          File Output Operator
+                            compressed: false
+                            Statistics: Num rows: 182 Data size: 1939 Basic stats: COMPLETE Column stats: NONE
+                            table:
+                                input format: org.apache.hadoop.mapred.TextInputFormat
+                                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain select a.key, b.key from (select distinct key from tab) a join tab b on b.key = a.key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select a.key, b.key from (select distinct key from tab) a join tab b on b.key = a.key
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Reducer 3 (CUSTOM_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0}
+                      1 {key}
+                    keys:
+                      0 _col0 (type: int)
+                      1 key (type: int)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: int), _col1 (type: int)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: tab
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: int)
+                    outputColumnNames: key
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      keys: key (type: int)
+                      mode: hash
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
+                        Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: int)
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 121 Data size: 1283 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: int)
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 121 Data size: 1283 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int)
+                    sort order: +
+                    Map-reduce partition columns: _col0 (type: int)
+                    Statistics: Num rows: 121 Data size: 1283 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: int)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+PREHOOK: query: explain select a.value, b.value from (select distinct value from tab) a join tab b on b.key = a.value
+PREHOOK: type: QUERY
+POSTHOOK: query: explain select a.value, b.value from (select distinct value from tab) a join tab b on b.key = a.value
+POSTHOOK: type: QUERY
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab1.key SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab1.value SIMPLE [(srcbucket_mapjoin)srcbucket_mapjoin.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).key SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: tab_part PARTITION(ds=2008-04-08).value SIMPLE [(srcbucket_mapjoin_part)srcbucket_mapjoin_part.FieldSchema(name:value, type:string, comment:null), ]
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Map 1 <- Reducer 3 (BROADCAST_EDGE)
+        Reducer 3 <- Map 2 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: b
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Map Join Operator
+                    condition map:
+                         Inner Join 0 to 1
+                    condition expressions:
+                      0 {_col0}
+                      1 {value}
+                    keys:
+                      0 UDFToDouble(_col0) (type: double)
+                      1 UDFToDouble(key) (type: double)
+                    outputColumnNames: _col0, _col2
+                    Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: _col0 (type: string), _col2 (type: string)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 266 Data size: 2822 Basic stats: COMPLETE Column stats: NONE
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Map 2 
+            Map Operator Tree:
+                TableScan
+                  alias: tab
+                  Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: value (type: string)
+                    outputColumnNames: value
+                    Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      keys: value (type: string)
+                      mode: hash
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: string)
+                        Statistics: Num rows: 242 Data size: 2566 Basic stats: COMPLETE Column stats: NONE
+        Reducer 3 
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0
+                Statistics: Num rows: 121 Data size: 1283 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: string)
+                  outputColumnNames: _col0
+                  Statistics: Num rows: 121 Data size: 1283 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: UDFToDouble(_col0) (type: double)
+                    sort order: +
+                    Map-reduce partition columns: UDFToDouble(_col0) (type: double)
+                    Statistics: Num rows: 121 Data size: 1283 Basic stats: COMPLETE Column stats: NONE
+                    value expressions: _col0 (type: string)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
