diff --git a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
index b15928c7cb..ee61350ab1 100644
--- a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
+++ b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
@@ -39,6 +39,8 @@
 import org.apache.hadoop.fs.permission.FsAction;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.shims.HadoopShims;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.Shell;
 
@@ -48,6 +50,19 @@
  */
 public final class FileUtils {
   private static final Log LOG = LogFactory.getLog(FileUtils.class.getName());
+
+  /**
+   * Accept all paths.
+   */
+  private static class AcceptAllPathFilter implements PathFilter {
+    @Override
+    public boolean accept(Path path) {
+      return true;
+    }
+  }
+
+  private static final PathFilter allPathFilter = new AcceptAllPathFilter();
+
   /**
    * Variant of Path.makeQualified that qualifies the input path against the default file system
    * indicated by the configuration
@@ -524,4 +539,53 @@ public static boolean copy(FileSystem srcFS, Path src,
     }
     return copied;
   }
+
+  /**
+   * Deletes all files under a directory, sending them to the trash.  Leaves the directory as is.
+   * @param fs FileSystem to use
+   * @param f path of directory
+   * @param conf hive configuration
+   * @return true if deletion successful
+   * @throws FileNotFoundException
+   * @throws IOException
+   */
+  public static boolean trashFilesUnderDir(FileSystem fs, Path f, Configuration conf) throws FileNotFoundException, IOException {
+    FileStatus[] statuses = fs.listStatus(f, allPathFilter);
+    boolean result = true;
+    for (FileStatus status : statuses) {
+      result = result & moveToTrash(fs, status.getPath(), conf);
+    }
+    return result;
+  }
+
+  /**
+   * Move a particular file or directory to the trash.
+   * @param fs FileSystem to use
+   * @param f path of file or directory to move to trash.
+   * @param conf
+   * @return true if move successful
+   * @throws IOException
+   */
+  public static boolean moveToTrash(FileSystem fs, Path f, Configuration conf) throws IOException {
+    LOG.info("deleting  " + f);
+    HadoopShims hadoopShim = ShimLoader.getHadoopShims();
+
+    boolean skipTrash = HiveConf.getBoolVar(conf,
+        HiveConf.ConfVars.HIVE_WAREHOUSE_DATA_SKIPTRASH);
+
+    if (skipTrash) {
+      LOG.info("Not moving "+ f +" to trash due to configuration " +
+        HiveConf.ConfVars.HIVE_WAREHOUSE_DATA_SKIPTRASH + " is set to true.");
+    } else if (hadoopShim.moveToAppropriateTrash(fs, f, conf)) {
+      LOG.info("Moved to trash: " + f);
+      return true;
+    }
+
+    boolean result = fs.delete(f, true);
+    if (!result) {
+      LOG.error("Failed to delete " + f);
+    }
+    return result;
+  }
+
 }
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java
index a3ea6821b5..4f566d2a29 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestFolderPermissions.java
@@ -223,6 +223,48 @@ public void testExternalTable() throws Exception {
     }
   }
 
+  @Test
+  public void testInsertOverwrite() throws Exception {
+    //case 1 is non-partitioned table.
+    String tableName = "insertoverwrite";
+
+    CommandProcessorResponse ret = driver.run("CREATE TABLE " + tableName + " (key string, value string)");
+    Assert.assertEquals(0,ret.getResponseCode());
+
+    String tableLoc = testDir + "/" + tableName;
+    assertExistence(testDir + "/" + tableName);
+    setPermissions(testDir + "/" + tableName, FsPermission.createImmutable((short) 0777));
+
+    ret = driver.run("insert overwrite table " + tableName + " select key,value from mysrc");
+    Assert.assertEquals(0,ret.getResponseCode());
+
+    Assert.assertTrue(listChildrenPerms(tableLoc).size() > 0);
+    for (FsPermission perm : listChildrenPerms(tableLoc)) {
+      Assert.assertEquals("rwxrwxrwx", perm.toString());
+    }
+
+    //case 2 is partitioned table.
+    tableName = "insertoverwritepartition";
+
+    ret = driver.run("CREATE TABLE " + tableName + " (key string, value string) partitioned by (part1 int, part2 int)");
+    Assert.assertEquals(0,ret.getResponseCode());
+
+    ret = driver.run("insert overwrite table " + tableName + " partition(part1='1',part2='1') select key,value from mysrc");
+    Assert.assertEquals(0,ret.getResponseCode());
+
+    String partLoc = testDir + "/" + tableName + "/part1=1/part2=1";
+    assertExistence(partLoc);
+    setPermissions(partLoc, FsPermission.createImmutable((short) 0777));
+
+    ret = driver.run("insert overwrite table " + tableName + " partition(part1='1',part2='1') select key,value from mysrc");
+    Assert.assertEquals(0,ret.getResponseCode());
+
+    Assert.assertTrue(listChildrenPerms(tableLoc).size() > 0);
+    for (FsPermission perm : listChildrenPerms(tableLoc)) {
+      Assert.assertEquals("rwxrwxrwx", perm.toString());
+    }
+  }
+
   @Test
   public void testEximPermissionInheritance() throws Exception {
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index 104423e107..341747456d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -2392,16 +2392,11 @@ static protected void replaceFiles(Path srcf, Path destf, Path oldPath,
         try {
           FileSystem fs2 = oldPath.getFileSystem(conf);
           if (fs2.exists(oldPath)) {
-            // use FsShell to move data to .Trash first rather than delete permanently
-            FsShell fshell = new FsShell();
-            fshell.setConf(conf);
-            String[] rmr = isHadoop1() ? new String[]{"-rmr", oldPath.toString()} :
-                new String[]{"-rm", "-r", oldPath.toString()};
-            fshell.run(rmr);
+            FileUtils.trashFilesUnderDir(fs2, oldPath, conf);
           }
         } catch (Exception e) {
           //swallow the exception
-          LOG.warn("Directory " + oldPath.toString() + " canot be removed.");
+          LOG.warn("Directory " + oldPath.toString() + " canot be removed:" + StringUtils.stringifyException(e));
         }
       }
 
