diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
index 6453069e28..f98327fb7c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
@@ -137,8 +137,10 @@ private static class Field {
 
     private ObjectInspector objectInspector;
 
+    private VectorBatchDeserializer deserializer;
+
     public Field(PrimitiveCategory primitiveCategory, DataTypePhysicalVariation dataTypePhysicalVariation,
-        int maxLength) {
+        int maxLength, VectorBatchDeserializer deserializer) {
       isPrimitive = true;
       this.category = Category.PRIMITIVE;
       this.primitiveCategory = primitiveCategory;
@@ -149,9 +151,11 @@ public Field(PrimitiveCategory primitiveCategory, DataTypePhysicalVariation data
       this.complexTypeHelper = null;
       this.objectInspector = PrimitiveObjectInspectorFactory.
           getPrimitiveWritableObjectInspector(primitiveCategory);
+      this.deserializer = deserializer;
     }
 
-    public Field(Category category, ComplexTypeHelper complexTypeHelper, TypeInfo typeInfo) {
+    public Field(Category category, ComplexTypeHelper complexTypeHelper, TypeInfo typeInfo,
+                 VectorBatchDeserializer deserializer) {
       isPrimitive = false;
       this.category = category;
       this.objectInspector = TypeInfoUtils.getStandardWritableObjectInspectorFromTypeInfo(typeInfo);
@@ -161,6 +165,7 @@ public Field(Category category, ComplexTypeHelper complexTypeHelper, TypeInfo ty
       this.isConvert = false;
       this.conversionWritable = null;
       this.complexTypeHelper = complexTypeHelper;
+      this.deserializer = deserializer;
     }
 
     public boolean getIsPrimitive() {
@@ -210,6 +215,10 @@ public ComplexTypeHelper getComplexHelper() {
     public ObjectInspector getObjectInspector() {
       return objectInspector;
     }
+
+    public VectorBatchDeserializer getDeserializer() {
+      return deserializer;
+    }
   }
 
   /*
@@ -248,20 +257,68 @@ private Field allocatePrimitiveField(TypeInfo sourceTypeInfo,
       DataTypePhysicalVariation dataTypePhysicalVariation) {
     final PrimitiveTypeInfo sourcePrimitiveTypeInfo = (PrimitiveTypeInfo) sourceTypeInfo;
     final PrimitiveCategory sourcePrimitiveCategory = sourcePrimitiveTypeInfo.getPrimitiveCategory();
-    final int maxLength;
+    int maxLength = 0;
+    VectorBatchDeserializer deserializer;
+
     switch (sourcePrimitiveCategory) {
-    case CHAR:
-      maxLength = ((CharTypeInfo) sourcePrimitiveTypeInfo).getLength();
-      break;
-    case VARCHAR:
-      maxLength = ((VarcharTypeInfo) sourcePrimitiveTypeInfo).getLength();
-      break;
-    default:
-      // No additional data type specific setting.
-      maxLength = 0;
-      break;
+      case VOID:
+        deserializer = new VectorVoidDeserializer();
+        break;
+      case BOOLEAN:
+        deserializer = new VectorBooleanDeserializer();
+        break;
+      case BYTE:
+        deserializer = new VectorByteDeserializer();
+        break;
+      case SHORT:
+        deserializer = new VectorShortDeserializer();
+        break;
+      case INT:
+        deserializer = new VectorIntDeserializer();
+        break;
+      case LONG:
+        deserializer = new VectorLongDeserializer();
+        break;
+      case TIMESTAMP:
+        deserializer = new VectorTimestampDeserializer();
+        break;
+      case DATE:
+        deserializer = new VectorDateDeserializer();
+        break;
+      case FLOAT:
+        deserializer = new VectorFloatDeserializer();
+        break;
+      case DOUBLE:
+        deserializer = new VectorDoubleDeserializer();
+        break;
+      case BINARY:
+        deserializer = new VectorBinaryDeserializer();
+        break;
+      case STRING:
+        deserializer = new VectorStringDeserializer();
+        break;
+      case VARCHAR:
+        maxLength = ((VarcharTypeInfo) sourcePrimitiveTypeInfo).getLength();
+        deserializer = new VectorVarcharDeserializer();
+        break;
+      case CHAR:
+        maxLength = ((CharTypeInfo) sourcePrimitiveTypeInfo).getLength();
+        deserializer = new VectorCharDeserializer();
+        break;
+      case DECIMAL:
+        deserializer = new VectorDecimalDeserializer();
+        break;
+      case INTERVAL_YEAR_MONTH:
+        deserializer = new VectorIntervalYearMonthDeserializer();
+        break;
+      case INTERVAL_DAY_TIME:
+        deserializer = new VectorIntervalDayTimeDeserializer();
+        break;
+      default:
+        throw new RuntimeException("Primitive category " + sourcePrimitiveCategory +
+                " not supported");
     }
-    return new Field(sourcePrimitiveCategory, dataTypePhysicalVariation, maxLength);
+    return new Field(sourcePrimitiveCategory, dataTypePhysicalVariation, maxLength, deserializer);
   }
 
   private Field allocateComplexField(TypeInfo sourceTypeInfo) {
@@ -273,7 +330,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         final ListComplexTypeHelper listHelper =
             new ListComplexTypeHelper(
                 allocateField(listTypeInfo.getListElementTypeInfo(), DataTypePhysicalVariation.NONE));
-        return new Field(category, listHelper, sourceTypeInfo);
+        return new Field(category, listHelper, sourceTypeInfo, new VectorListDeserializer());
       }
     case MAP:
       {
@@ -282,7 +339,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
             new MapComplexTypeHelper(
                 allocateField(mapTypeInfo.getMapKeyTypeInfo(), DataTypePhysicalVariation.NONE),
                 allocateField(mapTypeInfo.getMapValueTypeInfo(), DataTypePhysicalVariation.NONE));
-        return new Field(category, mapHelper, sourceTypeInfo);
+        return new Field(category, mapHelper, sourceTypeInfo, new VectorMapDeserializer());
       }
     case STRUCT:
       {
@@ -295,7 +352,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         }
         final StructComplexTypeHelper structHelper =
             new StructComplexTypeHelper(fields);
-        return new Field(category, structHelper, sourceTypeInfo);
+        return new Field(category, structHelper, sourceTypeInfo, new VectorStructDeserializer());
       }
     case UNION:
       {
@@ -308,7 +365,7 @@ private Field allocateComplexField(TypeInfo sourceTypeInfo) {
         }
         final UnionComplexTypeHelper unionHelper =
             new UnionComplexTypeHelper(fields);
-        return new Field(category, unionHelper, sourceTypeInfo);
+        return new Field(category, unionHelper, sourceTypeInfo, new VectorUnionDeserializer());
       }
     default:
       throw new RuntimeException("Category " + category + " not supported");
@@ -564,173 +621,515 @@ public void init() throws HiveException {
     init(0);
   }
 
-  private void storePrimitiveRowColumn(ColumnVector colVector, Field field,
-      int batchIndex, boolean canRetainByteRef) throws IOException {
-
-    switch (field.getPrimitiveCategory()) {
-    case VOID:
+  class VectorVoidDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       VectorizedBatchUtil.setNullColIsNullValue(colVector, batchIndex);
-      return;
-    case BOOLEAN:
-      ((LongColumnVector) colVector).vector[batchIndex] = (deserializeRead.currentBoolean ? 1 : 0);
-      break;
-    case BYTE:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return null;
+    }
+  }
+
+  class VectorBooleanDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      ((LongColumnVector) colVector).vector[batchIndex] =
+              (deserializeRead.currentBoolean ? 1 : 0);
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertBoolean(field.getConversionWritable());
+    }
+
+    private Object convertBoolean(Object writable) {
+      if (writable == null) {
+        writable = new BooleanWritable();
+      }
+      ((BooleanWritable) writable).set(deserializeRead.currentBoolean);
+      return writable;
+    }
+  }
+
+  class VectorByteDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((LongColumnVector) colVector).vector[batchIndex] = deserializeRead.currentByte;
-      break;
-    case SHORT:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertByte(field.getConversionWritable());
+    }
+
+    private Object convertByte(Object writable) {
+      if (writable == null) {
+        writable = new ByteWritable();
+      }
+      ((ByteWritable) writable).set(deserializeRead.currentByte);
+      return writable;
+    }
+  }
+
+  class VectorShortDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((LongColumnVector) colVector).vector[batchIndex] = deserializeRead.currentShort;
-      break;
-    case INT:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertShort(field.getConversionWritable());
+    }
+
+    private Object convertShort(Object writable) {
+      if (writable == null) {
+        writable = new ShortWritable();
+      }
+      ((ShortWritable) writable).set(deserializeRead.currentShort);
+      return writable;
+    }
+  }
+
+  class VectorIntDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((LongColumnVector) colVector).vector[batchIndex] = deserializeRead.currentInt;
-      break;
-    case LONG:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertInt(field.getConversionWritable());
+    }
+
+    private Object convertInt(Object writable) {
+      if (writable == null) {
+        writable = new IntWritable();
+      }
+      ((IntWritable) writable).set(deserializeRead.currentInt);
+      return writable;
+    }
+  }
+
+  class VectorLongDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((LongColumnVector) colVector).vector[batchIndex] = deserializeRead.currentLong;
-      break;
-    case TIMESTAMP:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertLong(field.getConversionWritable());
+    }
+
+    private Object convertLong(Object writable) {
+      if (writable == null) {
+        writable = new LongWritable();
+      }
+      ((LongWritable) writable).set(deserializeRead.currentLong);
+      return writable;
+    }
+  }
+
+  class VectorTimestampDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((TimestampColumnVector) colVector).set(
-          batchIndex, deserializeRead.currentTimestampWritable.getTimestamp().toSqlTimestamp());
-      break;
-    case DATE:
-      ((LongColumnVector) colVector).vector[batchIndex] = deserializeRead.currentDateWritable.getDays();
-      break;
-    case FLOAT:
+              batchIndex, deserializeRead.currentTimestampWritable.getTimestamp().toSqlTimestamp());
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertTimestamp(field.getConversionWritable());
+    }
+
+    private Object convertTimestamp(Object writable) {
+      if (writable == null) {
+        writable = new TimestampWritableV2();
+      }
+      ((TimestampWritableV2) writable).set(deserializeRead.currentTimestampWritable);
+      return writable;
+    }
+  }
+
+  class VectorDateDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      ((LongColumnVector) colVector).vector[batchIndex] =
+              deserializeRead.currentDateWritable.getDays();
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertDate(field.getConversionWritable());
+    }
+
+    private Object convertDate(Object writable) {
+      if (writable == null) {
+        writable = new DateWritableV2();
+      }
+      ((DateWritableV2) writable).set(deserializeRead.currentDateWritable);
+      return writable;
+    }
+
+  }
+
+  class VectorFloatDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((DoubleColumnVector) colVector).vector[batchIndex] = deserializeRead.currentFloat;
-      break;
-    case DOUBLE:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertFloat(field.getConversionWritable());
+    }
+
+    private Object convertFloat(Object writable) {
+      if (writable == null) {
+        writable = new FloatWritable();
+      }
+      ((FloatWritable) writable).set(deserializeRead.currentFloat);
+      return writable;
+    }
+  }
+
+  class VectorDoubleDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((DoubleColumnVector) colVector).vector[batchIndex] = deserializeRead.currentDouble;
-      break;
-    case BINARY:
-    case STRING:
-      {
-        final BytesColumnVector bytesColVec = ((BytesColumnVector) colVector);
-        if (deserializeRead.currentExternalBufferNeeded) {
-          bytesColVec.ensureValPreallocated(deserializeRead.currentExternalBufferNeededLen);
-          deserializeRead.copyToExternalBuffer(
-              bytesColVec.getValPreallocatedBytes(), bytesColVec.getValPreallocatedStart());
-          bytesColVec.setValPreallocated(
-              batchIndex,
-              deserializeRead.currentExternalBufferNeededLen);
-        } else if (canRetainByteRef && inputBytes == deserializeRead.currentBytes) {
-          bytesColVec.setRef(
-              batchIndex,
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertDouble(field.getConversionWritable());
+    }
+
+    private Object convertDouble(Object writable) {
+      if (writable == null) {
+        writable = new DoubleWritable();
+      }
+      ((DoubleWritable) writable).set(deserializeRead.currentDouble);
+      return writable;
+    }
+  }
+
+  class VectorBinaryDeserializer extends VectorStringDeserializer {
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertBinary(field.getConversionWritable(), batchIndex);
+    }
+
+    private Object convertBinary(Object writable, int batchIndex) {
+      if (writable == null) {
+        writable = new BytesWritable();
+      }
+      if (deserializeRead.currentBytes == null) {
+        LOG.info("null binary entry: batchIndex " + batchIndex);
+      }
+
+      ((BytesWritable) writable).set(
               deserializeRead.currentBytes,
               deserializeRead.currentBytesStart,
               deserializeRead.currentBytesLength);
-        } else {
-          bytesColVec.setVal(
-              batchIndex,
+      return writable;
+    }
+  }
+
+  class VectorStringDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      storeString(colVector, field, batchIndex, canRetainByteRef);
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertString(field.getConversionWritable(), batchIndex);
+    }
+
+    private void storeString(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      final BytesColumnVector bytesColVec = ((BytesColumnVector) colVector);
+      if (deserializeRead.currentExternalBufferNeeded) {
+        bytesColVec.ensureValPreallocated(deserializeRead.currentExternalBufferNeededLen);
+        deserializeRead.copyToExternalBuffer(
+                bytesColVec.getValPreallocatedBytes(), bytesColVec.getValPreallocatedStart());
+        bytesColVec.setValPreallocated(
+                batchIndex,
+                deserializeRead.currentExternalBufferNeededLen);
+      } else if (canRetainByteRef && inputBytes == deserializeRead.currentBytes) {
+        bytesColVec.setRef(
+                batchIndex,
+                deserializeRead.currentBytes,
+                deserializeRead.currentBytesStart,
+                deserializeRead.currentBytesLength);
+      } else {
+        bytesColVec.setVal(
+                batchIndex,
+                deserializeRead.currentBytes,
+                deserializeRead.currentBytesStart,
+                deserializeRead.currentBytesLength);
+      }
+    }
+
+    private Object convertString(Object writable, int batchIndex) {
+      if (writable == null) {
+        writable = new Text();
+      }
+      if (deserializeRead.currentBytes == null) {
+        throw new RuntimeException("null string entry: batchIndex " + batchIndex);
+      }
+
+      // Use org.apache.hadoop.io.Text as our helper to go from byte[] to String.
+      ((Text) writable).set(
               deserializeRead.currentBytes,
               deserializeRead.currentBytesStart,
               deserializeRead.currentBytesLength);
-        }
+      return writable;
+    }
+  }
+
+  class VectorVarcharDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+      // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
+      // that does not use Java String objects.
+      final BytesColumnVector bytesColVec = ((BytesColumnVector) colVector);
+      if (deserializeRead.currentExternalBufferNeeded) {
+        // Write directly into our BytesColumnVector value buffer.
+        bytesColVec.ensureValPreallocated(deserializeRead.currentExternalBufferNeededLen);
+        final byte[] convertBuffer = bytesColVec.getValPreallocatedBytes();
+        final int convertBufferStart = bytesColVec.getValPreallocatedStart();
+        deserializeRead.copyToExternalBuffer(
+                convertBuffer,
+                convertBufferStart);
+        bytesColVec.setValPreallocated(
+                batchIndex,
+                StringExpr.truncate(
+                        convertBuffer,
+                        convertBufferStart,
+                        deserializeRead.currentExternalBufferNeededLen,
+                        field.getMaxLength()));
+      } else if (canRetainByteRef && inputBytes == deserializeRead.currentBytes) {
+        bytesColVec.setRef(
+                batchIndex,
+                deserializeRead.currentBytes,
+                deserializeRead.currentBytesStart,
+                StringExpr.truncate(
+                        deserializeRead.currentBytes,
+                        deserializeRead.currentBytesStart,
+                        deserializeRead.currentBytesLength,
+                        field.getMaxLength()));
+      } else {
+        bytesColVec.setVal(
+                batchIndex,
+                deserializeRead.currentBytes,
+                deserializeRead.currentBytesStart,
+                StringExpr.truncate(
+                        deserializeRead.currentBytes,
+                        deserializeRead.currentBytesStart,
+                        deserializeRead.currentBytesLength,
+                        field.getMaxLength()));
       }
-      break;
-    case VARCHAR:
-      {
-        // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
-        // that does not use Java String objects.
-        final BytesColumnVector bytesColVec = ((BytesColumnVector) colVector);
-        if (deserializeRead.currentExternalBufferNeeded) {
-          // Write directly into our BytesColumnVector value buffer.
-          bytesColVec.ensureValPreallocated(deserializeRead.currentExternalBufferNeededLen);
-          final byte[] convertBuffer = bytesColVec.getValPreallocatedBytes();
-          final int convertBufferStart = bytesColVec.getValPreallocatedStart();
-          deserializeRead.copyToExternalBuffer(
-              convertBuffer,
-              convertBufferStart);
-          bytesColVec.setValPreallocated(
-              batchIndex,
-              StringExpr.truncate(
-                  convertBuffer,
-                  convertBufferStart,
-                  deserializeRead.currentExternalBufferNeededLen,
-                  field.getMaxLength()));
-        } else if (canRetainByteRef && inputBytes == deserializeRead.currentBytes) {
-          bytesColVec.setRef(
-              batchIndex,
-              deserializeRead.currentBytes,
-              deserializeRead.currentBytesStart,
-              StringExpr.truncate(
-                  deserializeRead.currentBytes,
-                  deserializeRead.currentBytesStart,
-                  deserializeRead.currentBytesLength,
-                  field.getMaxLength()));
-        } else {
-          bytesColVec.setVal(
-              batchIndex,
-              deserializeRead.currentBytes,
-              deserializeRead.currentBytesStart,
-              StringExpr.truncate(
-                  deserializeRead.currentBytes,
-                  deserializeRead.currentBytesStart,
-                  deserializeRead.currentBytesLength,
-                  field.getMaxLength()));
-        }
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex,
+                   Field field) throws IOException {
+      return convertVarchar(field.getConversionWritable(), batchIndex, field);
+    }
+
+    private Object convertVarchar(Object writable, int batchIndex, Field field) {
+      if (writable == null) {
+        writable = new HiveVarcharWritable();
       }
-      break;
-    case CHAR:
-      {
-        // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
-        // that does not use Java String objects.
-        final BytesColumnVector bytesColVec = ((BytesColumnVector) colVector);
-        if (deserializeRead.currentExternalBufferNeeded) {
-          // Write directly into our BytesColumnVector value buffer.
-          bytesColVec.ensureValPreallocated(deserializeRead.currentExternalBufferNeededLen);
-          final byte[] convertBuffer = bytesColVec.getValPreallocatedBytes();
-          final int convertBufferStart = bytesColVec.getValPreallocatedStart();
-          deserializeRead.copyToExternalBuffer(
-              convertBuffer,
-              convertBufferStart);
-          bytesColVec.setValPreallocated(
-              batchIndex,
-              StringExpr.rightTrimAndTruncate(
-                  convertBuffer,
-                  convertBufferStart,
-                  deserializeRead.currentExternalBufferNeededLen,
-                  field.getMaxLength()));
-        } else if (canRetainByteRef && inputBytes == deserializeRead.currentBytes) {
-          bytesColVec.setRef(
-              batchIndex,
+      // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
+      // that does not use Java String objects.
+      if (deserializeRead.currentBytes == null) {
+        throw new RuntimeException(
+                "null varchar entry: batchIndex " + batchIndex);
+      }
+
+      int adjustedLength = StringExpr.truncate(
               deserializeRead.currentBytes,
               deserializeRead.currentBytesStart,
-              StringExpr.rightTrimAndTruncate(
-                  deserializeRead.currentBytes,
-                  deserializeRead.currentBytesStart,
-                  deserializeRead.currentBytesLength,
-                  field.getMaxLength()));
-        } else {
-          bytesColVec.setVal(
-              batchIndex,
+              deserializeRead.currentBytesLength,
+              field.getMaxLength());
+
+      ((HiveVarcharWritable) writable).set(
+              new String(
+                      deserializeRead.currentBytes,
+                      deserializeRead.currentBytesStart,
+                      adjustedLength,
+                      Charsets.UTF_8),
+              -1);
+      return writable;
+    }
+  }
+
+  class VectorCharDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+      // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
+      // that does not use Java String objects.
+      final BytesColumnVector bytesColVec = ((BytesColumnVector) colVector);
+      if (deserializeRead.currentExternalBufferNeeded) {
+        // Write directly into our BytesColumnVector value buffer.
+        bytesColVec.ensureValPreallocated(deserializeRead.currentExternalBufferNeededLen);
+        final byte[] convertBuffer = bytesColVec.getValPreallocatedBytes();
+        final int convertBufferStart = bytesColVec.getValPreallocatedStart();
+        deserializeRead.copyToExternalBuffer(
+                convertBuffer,
+                convertBufferStart);
+        bytesColVec.setValPreallocated(
+                batchIndex,
+                StringExpr.rightTrimAndTruncate(
+                        convertBuffer,
+                        convertBufferStart,
+                        deserializeRead.currentExternalBufferNeededLen,
+                        field.getMaxLength()));
+      } else if (canRetainByteRef && inputBytes == deserializeRead.currentBytes) {
+        bytesColVec.setRef(
+                batchIndex,
+                deserializeRead.currentBytes,
+                deserializeRead.currentBytesStart,
+                StringExpr.rightTrimAndTruncate(
+                        deserializeRead.currentBytes,
+                        deserializeRead.currentBytesStart,
+                        deserializeRead.currentBytesLength,
+                        field.getMaxLength()));
+      } else {
+        bytesColVec.setVal(
+                batchIndex,
+                deserializeRead.currentBytes,
+                deserializeRead.currentBytesStart,
+                StringExpr.rightTrimAndTruncate(
+                        deserializeRead.currentBytes,
+                        deserializeRead.currentBytesStart,
+                        deserializeRead.currentBytesLength,
+                        field.getMaxLength()));
+      }
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertChar(field.getConversionWritable(), batchIndex, field);
+    }
+
+    private Object convertChar(Object writable, int batchIndex, Field field) {
+      if (writable == null) {
+        writable = new HiveCharWritable();
+      }
+      // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
+      // that does not use Java String objects.
+      if (deserializeRead.currentBytes == null) {
+        throw new RuntimeException(
+                "null char entry: batchIndex " + batchIndex);
+      }
+
+      int adjustedLength = StringExpr.rightTrimAndTruncate(
               deserializeRead.currentBytes,
               deserializeRead.currentBytesStart,
-              StringExpr.rightTrimAndTruncate(
-                  deserializeRead.currentBytes,
-                  deserializeRead.currentBytesStart,
-                  deserializeRead.currentBytesLength,
-                  field.getMaxLength()));
-        }
-      }
-      break;
-    case DECIMAL:
+              deserializeRead.currentBytesLength,
+              field.getMaxLength());
+
+      ((HiveCharWritable) writable).set(
+              new String(
+                      deserializeRead.currentBytes,
+                      deserializeRead.currentBytesStart,
+                      adjustedLength, Charsets.UTF_8),
+              -1);
+      return writable;
+    }
+  }
+
+  class VectorDecimalDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       if (field.getDataTypePhysicalVariation() == DataTypePhysicalVariation.DECIMAL_64) {
         ((Decimal64ColumnVector) colVector).vector[batchIndex] = deserializeRead.currentDecimal64;
       } else {
         // The DecimalColumnVector set method will quickly copy the deserialized decimal writable fields.
         ((DecimalColumnVector) colVector).set(
-            batchIndex, deserializeRead.currentHiveDecimalWritable);
+                batchIndex, deserializeRead.currentHiveDecimalWritable);
       }
-      break;
-    case INTERVAL_YEAR_MONTH:
-      ((LongColumnVector) colVector).vector[batchIndex] =
-          deserializeRead.currentHiveIntervalYearMonthWritable.getHiveIntervalYearMonth().getTotalMonths();
-      break;
-    case INTERVAL_DAY_TIME:
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex,
+                   Field field) throws IOException {
+      return convertDecimal(field.getConversionWritable());
+    }
+
+    private Object convertDecimal(Object writable) {
+      if (writable == null) {
+        writable = new HiveDecimalWritable();
+      }
+      ((HiveDecimalWritable) writable).set(
+              deserializeRead.currentHiveDecimalWritable);
+      return writable;
+    }
+  }
+
+  class VectorIntervalYearMonthDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      ((LongColumnVector) colVector).vector[batchIndex] = deserializeRead.
+              currentHiveIntervalYearMonthWritable.getHiveIntervalYearMonth().getTotalMonths();
+
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertIntervalYearMonth(field.getConversionWritable());
+    }
+
+    private Object convertIntervalYearMonth(Object writable) {
+      if (writable == null) {
+        writable = new HiveIntervalYearMonthWritable();
+      }
+      ((HiveIntervalYearMonthWritable) writable).set(
+              deserializeRead.currentHiveIntervalYearMonthWritable);
+      return writable;
+    }
+  }
+
+  class VectorIntervalDayTimeDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
       ((IntervalDayTimeColumnVector) colVector).set(
-          batchIndex, deserializeRead.currentHiveIntervalDayTimeWritable.getHiveIntervalDayTime());
-      break;
-    default:
-      throw new RuntimeException("Primitive category " + field.getPrimitiveCategory() +
-          " not supported");
+              batchIndex, deserializeRead.currentHiveIntervalDayTimeWritable.getHiveIntervalDayTime());
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertIntervalDayTime(field.getConversionWritable());
+    }
+
+    private Object convertIntervalDayTime(Object writable) {
+      if (writable == null) {
+        writable = new HiveIntervalDayTimeWritable();
+      }
+      ((HiveIntervalDayTimeWritable) writable).set(
+              deserializeRead.currentHiveIntervalDayTimeWritable);
+      return writable;
     }
   }
 
@@ -806,139 +1205,255 @@ private void storeComplexFieldRowColumn(ColumnVector fieldColVector,
       return;
     }
 
-    if (field.getIsPrimitive()) {
-      storePrimitiveRowColumn(fieldColVector, field, batchIndex, canRetainByteRef);
-    } else {
-      switch (field.getCategory()) {
-      case LIST:
-        storeListRowColumn(fieldColVector, field, batchIndex, canRetainByteRef);
-        break;
-      case MAP:
-        storeMapRowColumn(fieldColVector, field, batchIndex, canRetainByteRef);
-        break;
-      case STRUCT:
-        storeStructRowColumn(fieldColVector, field, batchIndex, canRetainByteRef);
-        break;
-      case UNION:
-        storeUnionRowColumn(fieldColVector, field, batchIndex, canRetainByteRef);
-        break;
-      default:
-        throw new RuntimeException("Category " + field.getCategory() + " not supported");
-      }
-    }
-
+    field.deserializer.store(fieldColVector, field, batchIndex, canRetainByteRef);
     fieldColVector.isNull[batchIndex] = false;
   }
 
-  private void storeListRowColumn(ColumnVector colVector,
-      Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+  class VectorListDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      storeListRowColumn(colVector, field, batchIndex, canRetainByteRef);
+    }
 
-    final ListColumnVector listColVector = (ListColumnVector) colVector;
-    final ColumnVector elementColVector = listColVector.child;
-    int offset = listColVector.childCount;
-    listColVector.isNull[batchIndex] = false;
-    listColVector.offsets[batchIndex] = offset;
+    @Override
+    Object convert(ColumnVector batch, int batchIndex,
+                   Field field) throws IOException {
+      return convertListRowColumn(batch, batchIndex, field);
+    }
 
-    final ListComplexTypeHelper listHelper = (ListComplexTypeHelper) field.getComplexHelper();
+    private Object convertListRowColumn(
+            ColumnVector colVector, int batchIndex, Field field) throws IOException {
 
-    int listLength = 0;
-    while (deserializeRead.isNextComplexMultiValue()) {
+      final SettableListObjectInspector listOI = (SettableListObjectInspector) field.objectInspector;
+      final ListComplexTypeHelper listHelper = (ListComplexTypeHelper) field.getComplexHelper();
+      final Field elementField = listHelper.getElementField();
+      final List<Object> tempList = new ArrayList<>();
+      final ListColumnVector listColumnVector = (ListColumnVector) colVector;
 
-      // Ensure child size.
-      final int childCapacity = listColVector.child.isNull.length;
-      if (childCapacity < offset / 0.75) {
-        listColVector.child.ensureSize(childCapacity * 2, true);
+      while (deserializeRead.isNextComplexMultiValue()) {
+        tempList.add(
+                convertComplexFieldRowColumn(listColumnVector.child, batchIndex, elementField));
       }
 
-      storeComplexFieldRowColumn(
-          elementColVector, listHelper.getElementField(), offset, canRetainByteRef);
-      offset++;
-      listLength++;
+      final int size = tempList.size();
+      final Object list = listOI.create(size);
+      for (int i = 0; i < size; i++) {
+        listOI.set(list, i, tempList.get(i));
+      }
+      return list;
     }
 
-    listColVector.childCount += listLength;
-    listColVector.lengths[batchIndex] = listLength;
-  }
+    private void storeListRowColumn(ColumnVector colVector,
+                                    Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
 
-  private void storeMapRowColumn(ColumnVector colVector,
-      Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+      final ListColumnVector listColVector = (ListColumnVector) colVector;
+      final ColumnVector elementColVector = listColVector.child;
+      int offset = listColVector.childCount;
+      listColVector.isNull[batchIndex] = false;
+      listColVector.offsets[batchIndex] = offset;
+
+      final ListComplexTypeHelper listHelper = (ListComplexTypeHelper) field.getComplexHelper();
+
+      int listLength = 0;
+      while (deserializeRead.isNextComplexMultiValue()) {
 
-    final MapColumnVector mapColVector = (MapColumnVector) colVector;
-    final MapComplexTypeHelper mapHelper = (MapComplexTypeHelper) field.getComplexHelper();
-    final ColumnVector keysColVector = mapColVector.keys;
-    final ColumnVector valuesColVector = mapColVector.values;
-    int offset = mapColVector.childCount;
-    mapColVector.offsets[batchIndex] = offset;
-    mapColVector.isNull[batchIndex] = false;
-
-    int keyValueCount = 0;
-    while (deserializeRead.isNextComplexMultiValue()) {
-
-      // Ensure child size.
-      final int childCapacity = mapColVector.keys.isNull.length;
-      if (childCapacity < offset / 0.75) {
-        mapColVector.keys.ensureSize(childCapacity * 2, true);
-        mapColVector.values.ensureSize(childCapacity * 2, true);
+        // Ensure child size.
+        final int childCapacity = listColVector.child.isNull.length;
+        if (childCapacity < offset / 0.75) {
+          listColVector.child.ensureSize(childCapacity * 2, true);
+        }
+
+        storeComplexFieldRowColumn(
+                elementColVector, listHelper.getElementField(), offset, canRetainByteRef);
+        offset++;
+        listLength++;
       }
 
-      // Key.
-      storeComplexFieldRowColumn(
-          keysColVector, mapHelper.getKeyField(), offset, canRetainByteRef);
+      listColVector.childCount += listLength;
+      listColVector.lengths[batchIndex] = listLength;
+    }
+  }
 
-      // Value.
-      storeComplexFieldRowColumn(
-          valuesColVector, mapHelper.getValueField(), offset, canRetainByteRef);
+  class VectorMapDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      storeMapRowColumn((ColumnVector)colVector, field, batchIndex, canRetainByteRef);
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException {
+      return convertMapRowColumn(batch, batchIndex, field);
+    }
 
-      offset++;
-      keyValueCount++;
+    private Object convertMapRowColumn(
+            ColumnVector colVector, int batchIndex, Field field) throws IOException {
+
+      final SettableMapObjectInspector mapOI = (SettableMapObjectInspector) field.objectInspector;
+      final MapComplexTypeHelper mapHelper = (MapComplexTypeHelper) field.getComplexHelper();
+      final Field keyField = mapHelper.getKeyField();
+      final Field valueField = mapHelper.getValueField();
+      final MapColumnVector mapColumnVector = (MapColumnVector) colVector;
+
+      final Object map = mapOI.create();
+      while (deserializeRead.isNextComplexMultiValue()) {
+        final Object key = convertComplexFieldRowColumn(mapColumnVector.keys, batchIndex, keyField);
+        final Object value = convertComplexFieldRowColumn(mapColumnVector.values, batchIndex, valueField);
+        mapOI.put(map, key, value);
+      }
+      return map;
     }
 
-    mapColVector.childCount += keyValueCount;
-    mapColVector.lengths[batchIndex] = keyValueCount;
+    private void storeMapRowColumn(ColumnVector colVector,
+                                   Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+
+      final MapColumnVector mapColVector = (MapColumnVector) colVector;
+      final MapComplexTypeHelper mapHelper = (MapComplexTypeHelper) field.getComplexHelper();
+      final ColumnVector keysColVector = mapColVector.keys;
+      final ColumnVector valuesColVector = mapColVector.values;
+      int offset = mapColVector.childCount;
+      mapColVector.offsets[batchIndex] = offset;
+      mapColVector.isNull[batchIndex] = false;
+
+      int keyValueCount = 0;
+      while (deserializeRead.isNextComplexMultiValue()) {
+
+        // Ensure child size.
+        final int childCapacity = mapColVector.keys.isNull.length;
+        if (childCapacity < offset / 0.75) {
+          mapColVector.keys.ensureSize(childCapacity * 2, true);
+          mapColVector.values.ensureSize(childCapacity * 2, true);
+        }
+
+        // Key.
+        storeComplexFieldRowColumn(
+                keysColVector, mapHelper.getKeyField(), offset, canRetainByteRef);
+
+        // Value.
+        storeComplexFieldRowColumn(
+                valuesColVector, mapHelper.getValueField(), offset, canRetainByteRef);
+
+        offset++;
+        keyValueCount++;
+      }
+
+      mapColVector.childCount += keyValueCount;
+      mapColVector.lengths[batchIndex] = keyValueCount;
+    }
   }
 
-  private void storeStructRowColumn(ColumnVector colVector,
-      Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+  class VectorStructDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef)
+            throws IOException {
+      storeStructRowColumn(colVector, field, batchIndex, canRetainByteRef);
+    }
 
-    final StructColumnVector structColVector = (StructColumnVector) colVector;
-    final ColumnVector[] colVectorFields = structColVector.fields;
-    final StructComplexTypeHelper structHelper = (StructComplexTypeHelper) field.getComplexHelper();
-    final Field[] fields = structHelper.getFields();
-    structColVector.isNull[batchIndex] = false;
+    @Override
+    Object convert(ColumnVector batch, int batchIndex,
+                   Field field) throws IOException {
+      return convertStructRowColumn(batch, batchIndex, field);
+    }
 
-    int i = 0;
-    for (ColumnVector colVectorField : colVectorFields) {
-      storeComplexFieldRowColumn(
-          colVectorField,
-          fields[i],
-          batchIndex,
-          canRetainByteRef);
-      i++;
+    private Object convertStructRowColumn(
+            ColumnVector colVector, int batchIndex, Field field) throws IOException {
+
+      final SettableStructObjectInspector structOI = (SettableStructObjectInspector) field.objectInspector;
+      final List<? extends StructField> structFields = structOI.getAllStructFieldRefs();
+      final StructComplexTypeHelper structHelper = (StructComplexTypeHelper) field.getComplexHelper();
+      final Field[] fields = structHelper.getFields();
+      final StructColumnVector structColumnVector = (StructColumnVector) colVector;
+
+      final Object struct = structOI.create();
+      for (int i = 0; i < fields.length; i++) {
+        final Object fieldObject =
+                convertComplexFieldRowColumn(structColumnVector.fields[i], batchIndex, fields[i]);
+        structOI.setStructFieldData(struct, structFields.get(i), fieldObject);
+      }
+      deserializeRead.finishComplexVariableFieldsType();
+      return struct;
+    }
+
+    private void storeStructRowColumn(ColumnVector colVector,
+                                      Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+
+      final StructColumnVector structColVector = (StructColumnVector) colVector;
+      final ColumnVector[] colVectorFields = structColVector.fields;
+      final StructComplexTypeHelper structHelper = (StructComplexTypeHelper) field.getComplexHelper();
+      final Field[] fields = structHelper.getFields();
+      structColVector.isNull[batchIndex] = false;
+
+      int i = 0;
+      for (ColumnVector colVectorField : colVectorFields) {
+        storeComplexFieldRowColumn(
+                colVectorField,
+                fields[i],
+                batchIndex,
+                canRetainByteRef);
+        i++;
+      }
+      deserializeRead.finishComplexVariableFieldsType();
     }
-    deserializeRead.finishComplexVariableFieldsType();
   }
 
-  private void storeUnionRowColumn(ColumnVector colVector,
-      Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+  class VectorUnionDeserializer extends VectorBatchDeserializer {
+    @Override
+    void store(ColumnVector colVector, Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+      storeUnionRowColumn(colVector, field, batchIndex, canRetainByteRef);
+    }
+
+    @Override
+    Object convert(ColumnVector batch, int batchIndex,
+                   Field field) throws IOException {
+      return convertUnionRowColumn(batch, batchIndex, field);
+    }
 
-    deserializeRead.readComplexField();
+    private Object convertUnionRowColumn(
+            ColumnVector colVector, int batchIndex, Field field) throws IOException {
 
-    // The read field of the Union gives us its tag.
-    final int tag = deserializeRead.currentInt;
+      final SettableUnionObjectInspector unionOI = (SettableUnionObjectInspector) field.objectInspector;
+      final UnionComplexTypeHelper unionHelper = (UnionComplexTypeHelper) field.getComplexHelper();
+      final Field[] fields = unionHelper.getFields();
+      final UnionColumnVector unionColumnVector = (UnionColumnVector) colVector;
 
-    final UnionColumnVector unionColVector = (UnionColumnVector) colVector;
-    final ColumnVector[] colVectorFields = unionColVector.fields;
-    final UnionComplexTypeHelper unionHelper = (UnionComplexTypeHelper) field.getComplexHelper();
+      final Object union = unionOI.create();
+      final int tag = deserializeRead.currentInt;
+      unionOI.setFieldAndTag(union, new StandardUnion((byte) tag,
+              convertComplexFieldRowColumn(unionColumnVector.fields[tag], batchIndex, fields[tag])), (byte) tag);
+      deserializeRead.finishComplexVariableFieldsType();
+      return union;
+    }
+
+    private void storeUnionRowColumn(ColumnVector colVector,
+                                     Field field, int batchIndex, boolean canRetainByteRef) throws IOException {
+
+      deserializeRead.readComplexField();
+
+      // The read field of the Union gives us its tag.
+      final int tag = deserializeRead.currentInt;
+
+      final UnionColumnVector unionColVector = (UnionColumnVector) colVector;
+      final ColumnVector[] colVectorFields = unionColVector.fields;
+      final UnionComplexTypeHelper unionHelper = (UnionComplexTypeHelper) field.getComplexHelper();
+
+      unionColVector.isNull[batchIndex] = false;
+      unionColVector.tags[batchIndex] = tag;
 
-    unionColVector.isNull[batchIndex] = false;
-    unionColVector.tags[batchIndex] = tag;
+      storeComplexFieldRowColumn(
+              colVectorFields[tag],
+              unionHelper.getFields()[tag],
+              batchIndex,
+              canRetainByteRef);
+      deserializeRead.finishComplexVariableFieldsType();
+    }
+  }
+
+  abstract static class VectorBatchDeserializer {
+    abstract void store(ColumnVector colVector, Field field, int batchIndex,
+                        boolean canRetainByteRef) throws IOException;
 
-    storeComplexFieldRowColumn(
-        colVectorFields[tag],
-        unionHelper.getFields()[tag],
-        batchIndex,
-        canRetainByteRef);
-    deserializeRead.finishComplexVariableFieldsType();
+    abstract Object convert(ColumnVector batch, int batchIndex, Field field) throws IOException;
   }
 
   /**
@@ -959,27 +1474,7 @@ private void storeRowColumn(VectorizedRowBatch batch, int batchIndex,
 
     final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];
     final ColumnVector colVector = batch.cols[projectionColumnNum];
-
-    if (field.getIsPrimitive()) {
-      storePrimitiveRowColumn(colVector, field, batchIndex, canRetainByteRef);
-    } else {
-      switch (field.getCategory()) {
-      case LIST:
-        storeListRowColumn(colVector, field, batchIndex, canRetainByteRef);
-        break;
-      case MAP:
-        storeMapRowColumn(colVector, field, batchIndex, canRetainByteRef);
-        break;
-      case STRUCT:
-        storeStructRowColumn(colVector, field, batchIndex, canRetainByteRef);
-        break;
-      case UNION:
-        storeUnionRowColumn(colVector, field, batchIndex, canRetainByteRef);
-        break;
-      default:
-        throw new RuntimeException("Category " + field.getCategory() + " not supported");
-      }
-    }
+    field.deserializer.store(colVector, field, batchIndex, canRetainByteRef);
 
     // We always set the null flag to false when there is a value.
     colVector.isNull[batchIndex] = false;
@@ -1001,28 +1496,8 @@ private void convertRowColumn(VectorizedRowBatch batch, int batchIndex,
 
     final int projectionColumnIndex = projectionColumnNums[logicalColumnIndex];
     final ColumnVector colVector = batch.cols[projectionColumnIndex];
-
-    final Object convertSourceWritable;
-    if (field.getIsPrimitive()) {
-      convertSourceWritable = convertPrimitiveRowColumn(batchIndex, field);
-    } else {
-      switch (field.getCategory()) {
-      case LIST:
-        convertSourceWritable = convertListRowColumn(colVector, batchIndex, field);
-        break;
-      case MAP:
-        convertSourceWritable = convertMapRowColumn(colVector, batchIndex, field);
-        break;
-      case STRUCT:
-        convertSourceWritable = convertStructRowColumn(colVector, batchIndex, field);
-        break;
-      case UNION:
-        convertSourceWritable = convertUnionRowColumn(colVector, batchIndex, field);
-        break;
-      default:
-        throw new RuntimeException();
-      }
-    }
+    Object convertSourceWritable =
+            field.deserializer.convert(colVector, batchIndex, field);
 
     /*
      * Convert our source object we just read into the target object and store that in the
@@ -1041,296 +1516,7 @@ private Object convertComplexFieldRowColumn(ColumnVector colVector, int batchInd
     }
 
     colVector.isNull[batchIndex] = false;
-    if (field.getIsPrimitive()) {
-      return convertPrimitiveRowColumn(batchIndex, field);
-    }
-
-    switch (field.getCategory()) {
-    case LIST:
-      return convertListRowColumn(colVector, batchIndex, field);
-    case MAP:
-      return convertMapRowColumn(colVector, batchIndex, field);
-    case STRUCT:
-      return convertStructRowColumn(colVector, batchIndex, field);
-    case UNION:
-      return convertUnionRowColumn(colVector, batchIndex, field);
-    default:
-      throw new RuntimeException();
-    }
-  }
-
-  private Object convertPrimitiveRowColumn(int batchIndex, Field field) throws IOException {
-
-    Object writable = field.getConversionWritable();
-    switch (field.getPrimitiveCategory()) {
-    case VOID:
-      writable = null;
-      break;
-    case BOOLEAN:
-      {
-        if (writable == null) {
-          writable = new BooleanWritable();
-        }
-        ((BooleanWritable) writable).set(deserializeRead.currentBoolean);
-      }
-      break;
-    case BYTE:
-      {
-        if (writable == null) {
-          writable = new ByteWritable();
-        }
-        ((ByteWritable) writable).set(deserializeRead.currentByte);
-      }
-      break;
-    case SHORT:
-      {
-        if (writable == null) {
-          writable = new ShortWritable();
-        }
-        ((ShortWritable) writable).set(deserializeRead.currentShort);
-      }
-      break;
-    case INT:
-      {
-        if (writable == null) {
-          writable = new IntWritable();
-        }
-        ((IntWritable) writable).set(deserializeRead.currentInt);
-      }
-      break;
-    case LONG:
-      {
-        if (writable == null) {
-          writable = new LongWritable();
-        }
-        ((LongWritable) writable).set(deserializeRead.currentLong);
-      }
-      break;
-    case TIMESTAMP:
-      {
-        if (writable == null) {
-          writable = new TimestampWritableV2();
-        }
-        ((TimestampWritableV2) writable).set(deserializeRead.currentTimestampWritable);
-      }
-      break;
-    case DATE:
-      {
-        if (writable == null) {
-          writable = new DateWritableV2();
-        }
-        ((DateWritableV2) writable).set(deserializeRead.currentDateWritable);
-      }
-      break;
-    case FLOAT:
-      {
-        if (writable == null) {
-          writable = new FloatWritable();
-        }
-        ((FloatWritable) writable).set(deserializeRead.currentFloat);
-      }
-      break;
-    case DOUBLE:
-      {
-        if (writable == null) {
-          writable = new DoubleWritable();
-        }
-        ((DoubleWritable) writable).set(deserializeRead.currentDouble);
-      }
-      break;
-    case BINARY:
-      {
-        if (writable == null) {
-          writable = new BytesWritable();
-        }
-        if (deserializeRead.currentBytes == null) {
-          LOG.info(
-              "null binary entry: batchIndex " + batchIndex);
-        }
-
-        ((BytesWritable) writable).set(
-            deserializeRead.currentBytes,
-            deserializeRead.currentBytesStart,
-            deserializeRead.currentBytesLength);
-        break;
-      }
-    case STRING:
-      {
-        if (writable == null) {
-          writable = new Text();
-        }
-        if (deserializeRead.currentBytes == null) {
-          throw new RuntimeException(
-              "null string entry: batchIndex " + batchIndex);
-        }
-
-        // Use org.apache.hadoop.io.Text as our helper to go from byte[] to String.
-        ((Text) writable).set(
-            deserializeRead.currentBytes,
-            deserializeRead.currentBytesStart,
-            deserializeRead.currentBytesLength);
-      }
-      break;
-    case VARCHAR:
-      {
-        if (writable == null) {
-          writable = new HiveVarcharWritable();
-        }
-        // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
-        // that does not use Java String objects.
-        if (deserializeRead.currentBytes == null) {
-          throw new RuntimeException(
-              "null varchar entry: batchIndex " + batchIndex);
-        }
-
-        int adjustedLength = StringExpr.truncate(
-            deserializeRead.currentBytes,
-            deserializeRead.currentBytesStart,
-            deserializeRead.currentBytesLength,
-            field.getMaxLength());
-
-        ((HiveVarcharWritable) writable).set(
-            new String(
-              deserializeRead.currentBytes,
-              deserializeRead.currentBytesStart,
-              adjustedLength,
-              Charsets.UTF_8),
-            -1);
-      }
-      break;
-    case CHAR:
-      {
-        if (writable == null) {
-          writable = new HiveCharWritable();
-        }
-        // Use the basic STRING bytes read to get access, then use our optimal truncate/trim method
-        // that does not use Java String objects.
-        if (deserializeRead.currentBytes == null) {
-          throw new RuntimeException(
-              "null char entry: batchIndex " + batchIndex);
-        }
-
-        int adjustedLength = StringExpr.rightTrimAndTruncate(
-            deserializeRead.currentBytes,
-            deserializeRead.currentBytesStart,
-            deserializeRead.currentBytesLength,
-            field.getMaxLength());
-
-        ((HiveCharWritable) writable).set(
-            new String(
-              deserializeRead.currentBytes,
-              deserializeRead.currentBytesStart,
-              adjustedLength, Charsets.UTF_8),
-            -1);
-      }
-      break;
-    case DECIMAL:
-      {
-        if (writable == null) {
-          writable = new HiveDecimalWritable();
-        }
-        ((HiveDecimalWritable) writable).set(
-            deserializeRead.currentHiveDecimalWritable);
-      }
-      break;
-    case INTERVAL_YEAR_MONTH:
-      {
-        if (writable == null) {
-          writable = new HiveIntervalYearMonthWritable();
-        }
-        ((HiveIntervalYearMonthWritable) writable).set(
-            deserializeRead.currentHiveIntervalYearMonthWritable);
-      }
-      break;
-    case INTERVAL_DAY_TIME:
-      {
-        if (writable == null) {
-          writable = new HiveIntervalDayTimeWritable();
-        }
-        ((HiveIntervalDayTimeWritable) writable).set(
-            deserializeRead.currentHiveIntervalDayTimeWritable);
-      }
-      break;
-    default:
-      throw new RuntimeException("Primitive category " + field.getPrimitiveCategory() +
-          " not supported");
-    }
-    return writable;
-  }
-
-  private Object convertListRowColumn(
-      ColumnVector colVector, int batchIndex, Field field) throws IOException {
-
-    final SettableListObjectInspector listOI = (SettableListObjectInspector) field.objectInspector;
-    final ListComplexTypeHelper listHelper = (ListComplexTypeHelper) field.getComplexHelper();
-    final Field elementField = listHelper.getElementField();
-    final List<Object> tempList = new ArrayList<>();
-    final ListColumnVector listColumnVector = (ListColumnVector) colVector;
-
-    while (deserializeRead.isNextComplexMultiValue()) {
-      tempList.add(
-          convertComplexFieldRowColumn(listColumnVector.child, batchIndex, elementField));
-    }
-
-    final int size = tempList.size();
-    final Object list = listOI.create(size);
-    for (int i = 0; i < size; i++) {
-      listOI.set(list, i, tempList.get(i));
-    }
-    return list;
-  }
-
-  private Object convertMapRowColumn(
-      ColumnVector colVector, int batchIndex, Field field) throws IOException {
-
-    final SettableMapObjectInspector mapOI = (SettableMapObjectInspector) field.objectInspector;
-    final MapComplexTypeHelper mapHelper = (MapComplexTypeHelper) field.getComplexHelper();
-    final Field keyField = mapHelper.getKeyField();
-    final Field valueField = mapHelper.getValueField();
-    final MapColumnVector mapColumnVector = (MapColumnVector) colVector;
-
-    final Object map = mapOI.create();
-    while (deserializeRead.isNextComplexMultiValue()) {
-      final Object key = convertComplexFieldRowColumn(mapColumnVector.keys, batchIndex, keyField);
-      final Object value = convertComplexFieldRowColumn(mapColumnVector.values, batchIndex, valueField);
-      mapOI.put(map, key, value);
-    }
-    return map;
-  }
-
-  private Object convertStructRowColumn(
-      ColumnVector colVector, int batchIndex, Field field) throws IOException {
-
-    final SettableStructObjectInspector structOI = (SettableStructObjectInspector) field.objectInspector;
-    final List<? extends StructField> structFields = structOI.getAllStructFieldRefs();
-    final StructComplexTypeHelper structHelper = (StructComplexTypeHelper) field.getComplexHelper();
-    final Field[] fields = structHelper.getFields();
-    final StructColumnVector structColumnVector = (StructColumnVector) colVector;
-
-    final Object struct = structOI.create();
-    for (int i = 0; i < fields.length; i++) {
-      final Object fieldObject =
-          convertComplexFieldRowColumn(structColumnVector.fields[i], batchIndex, fields[i]);
-      structOI.setStructFieldData(struct, structFields.get(i), fieldObject);
-    }
-    deserializeRead.finishComplexVariableFieldsType();
-    return struct;
-  }
-
-  private Object convertUnionRowColumn(
-      ColumnVector colVector, int batchIndex, Field field) throws IOException {
-
-    final SettableUnionObjectInspector unionOI = (SettableUnionObjectInspector) field.objectInspector;
-    final UnionComplexTypeHelper unionHelper = (UnionComplexTypeHelper) field.getComplexHelper();
-    final Field[] fields = unionHelper.getFields();
-    final UnionColumnVector unionColumnVector = (UnionColumnVector) colVector;
-
-    final Object union = unionOI.create();
-    final int tag = deserializeRead.currentInt;
-    unionOI.setFieldAndTag(union, new StandardUnion((byte) tag,
-        convertComplexFieldRowColumn(unionColumnVector.fields[tag], batchIndex, fields[tag])), (byte) tag);
-    deserializeRead.finishComplexVariableFieldsType();
-    return union;
+    return field.deserializer.convert(colVector, batchIndex, field);
   }
 
   /**
@@ -1482,7 +1668,6 @@ public void deserializeByRef(VectorizedRowBatch batch, int batchIndex) throws IO
     }
   }
 
-
   public String getDetailedReadPositionString() {
     return deserializeRead.getDetailedReadPositionString();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java
index 66585af577..fd4fffe292 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java
@@ -61,28 +61,15 @@ public final class VectorSerializeRow<T extends SerializeWrite> {
   private Field root;
 
   private static class Field {
-    Field[] children;
-
-    boolean isPrimitive;
-    Category category;
-    PrimitiveCategory primitiveCategory;
-    TypeInfo typeInfo;
-
-    int count;
-
-    ObjectInspector objectInspector;
-    int outputColumnNum;
-
-    Field() {
-      children = null;
-      isPrimitive = false;
-      category = null;
-      primitiveCategory = null;
-      typeInfo = null;
-      count = 0;
-      objectInspector = null;
-      outputColumnNum = -1;
-    }
+    Field[] children = null;
+    boolean isPrimitive = false;
+    Category category = null;
+    PrimitiveCategory primitiveCategory = null;
+    TypeInfo typeInfo = null;
+    int count = 0;
+    ObjectInspector objectInspector = null;
+    int outputColumnNum = -1;
+    VectorSerializeWriter writer = null;
   }
 
   private VectorExtractRow vectorExtractRow;
@@ -113,6 +100,54 @@ private Field createField(TypeInfo typeInfo) {
     if (category == Category.PRIMITIVE) {
       field.isPrimitive = true;
       field.primitiveCategory = ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory();
+      switch (field.primitiveCategory) {
+        case BOOLEAN:
+          field.writer = new VectorSerializeBooleanWriter();
+          break;
+        case BYTE:
+          field.writer = new VectorSerializeByteWriter();
+          break;
+        case SHORT:
+          field.writer = new VectorSerializeShortWriter();
+          break;
+        case INT:
+          field.writer = new VectorSerializeIntWriter();
+          break;
+        case LONG:
+          field.writer = new VectorSerializeLongWriter();
+          break;
+        case DATE:
+          field.writer = new VectorSerializeDateWriter();
+          break;
+        case TIMESTAMP:
+          field.writer = new VectorSerializeTimestampWriter();
+          break;
+        case FLOAT:
+          field.writer = new VectorSerializeFloatWriter();
+          break;
+        case DOUBLE:
+          field.writer = new VectorSerializeDoubleWriter();
+          break;
+        case STRING:
+        case CHAR:
+        case VARCHAR:
+          field.writer = new VectorSerializeStringWriter();
+          break;
+        case BINARY:
+          field.writer = new VectorSerializeBinaryWriter();
+          break;
+        case DECIMAL:
+          field.writer = new VectorSerializeDecimalWriter();
+          break;
+        case INTERVAL_YEAR_MONTH:
+          field.writer = new VectorSerializeHiveIntervalYearMonthWriter();
+          break;
+        case INTERVAL_DAY_TIME:
+          field.writer = new VectorSerializeHiveIntervalDayTimeWriter();
+          break;
+        default:
+          throw new RuntimeException("Unexpected primitive category " + field.primitiveCategory);
+      }
     } else {
       field.isPrimitive = false;
       field.objectInspector =
@@ -121,21 +156,25 @@ private Field createField(TypeInfo typeInfo) {
       case LIST:
         field.children = new Field[1];
         field.children[0] = createField(((ListTypeInfo) typeInfo).getListElementTypeInfo());
+        field.writer = new VectorSerializeListWriter();
         break;
       case MAP:
         field.children = new Field[2];
         field.children[0] = createField(((MapTypeInfo) typeInfo).getMapKeyTypeInfo());
         field.children[1] = createField(((MapTypeInfo) typeInfo).getMapValueTypeInfo());
+        field.writer = new VectorSerializeMapWriter();
         break;
       case STRUCT:
         StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
         List<TypeInfo> fieldTypeInfos = structTypeInfo.getAllStructFieldTypeInfos();
         field.children = createFields(fieldTypeInfos.toArray(new TypeInfo[fieldTypeInfos.size()]));
+        field.writer = new VectorSerializeStructWriter();
         break;
       case UNION:
         UnionTypeInfo unionTypeInfo = (UnionTypeInfo) typeInfo;
         List<TypeInfo> objectTypeInfos = unionTypeInfo.getAllUnionObjectTypeInfos();
         field.children = createFields(objectTypeInfos.toArray(new TypeInfo[objectTypeInfos.size()]));
+        field.writer = new VectorSerializeUnionWriter();
         break;
       default:
         throw new RuntimeException();
@@ -274,216 +313,269 @@ private void serializeWrite(
       return;
     }
     isAllNulls = false;
+    field.writer.serialize(colVector, field, adjustedBatchIndex);
+  }
 
-    if (field.isPrimitive) {
-      serializePrimitiveWrite(colVector, field, adjustedBatchIndex);
-      return;
+  abstract static class VectorSerializeWriter {
+    abstract void serialize(Object colVector, Field field, int adjustedBatchIndex) throws IOException;
+  }
+
+  class VectorSerializeUnionWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeUnionWrite((UnionColumnVector)colInfo, field, adjustedBatchIndex);
     }
-    final Category category = field.category;
-    switch (category) {
-    case LIST:
-      serializeListWrite(
-          (ListColumnVector) colVector,
-          field,
-          adjustedBatchIndex);
-      break;
-    case MAP:
-      serializeMapWrite(
-          (MapColumnVector) colVector,
-          field,
-          adjustedBatchIndex);
-      break;
-    case STRUCT:
-      serializeStructWrite(
-          (StructColumnVector) colVector,
-          field,
-          adjustedBatchIndex);
-      break;
-    case UNION:
-      serializeUnionWrite(
-          (UnionColumnVector) colVector,
-          field,
-          adjustedBatchIndex);
-      break;
-    default:
-      throw new RuntimeException("Unexpected category " + category);
+
+    private void serializeUnionWrite(
+            UnionColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
+
+      UnionTypeInfo typeInfo = (UnionTypeInfo) field.typeInfo;
+      UnionObjectInspector objectInspector = (UnionObjectInspector) field.objectInspector;
+
+      final byte tag = (byte) colVector.tags[adjustedBatchIndex];
+      final ColumnVector fieldColumnVector = colVector.fields[tag];
+      final Field childField = field.children[tag];
+
+      serializeWrite.beginUnion(tag);
+      serializeWrite(
+              fieldColumnVector,
+              childField,
+              adjustedBatchIndex);
+      serializeWrite.finishUnion();
     }
   }
 
-  private void serializeUnionWrite(
-      UnionColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
+  class VectorSerializeStructWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeStructWrite((StructColumnVector)colInfo, field, adjustedBatchIndex);
+    }
+
+    private void serializeStructWrite(
+            StructColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
 
-    UnionTypeInfo typeInfo = (UnionTypeInfo) field.typeInfo;
-    UnionObjectInspector objectInspector = (UnionObjectInspector) field.objectInspector;
+      StructTypeInfo typeInfo = (StructTypeInfo) field.typeInfo;
+      StructObjectInspector objectInspector = (StructObjectInspector) field.objectInspector;
 
-    final byte tag = (byte) colVector.tags[adjustedBatchIndex];
-    final ColumnVector fieldColumnVector = colVector.fields[tag];
-    final Field childField = field.children[tag];
+      final ColumnVector[] fieldColumnVectors = colVector.fields;
+      final Field[] children = field.children;
+      final List<? extends StructField> structFields = objectInspector.getAllStructFieldRefs();
+      final int size = field.count;
 
-    serializeWrite.beginUnion(tag);
-    serializeWrite(
-        fieldColumnVector,
-        childField,
-        adjustedBatchIndex);
-    serializeWrite.finishUnion();
+      final List list = (List) vectorExtractRow.extractRowColumn(
+              colVector, typeInfo, objectInspector, adjustedBatchIndex);
+
+      serializeWrite.beginStruct(list);
+      for (int i = 0; i < size; i++) {
+        if (i > 0) {
+          serializeWrite.separateStruct();
+        }
+        serializeWrite(
+                fieldColumnVectors[i],
+                children[i],
+                adjustedBatchIndex);
+      }
+      serializeWrite.finishStruct();
+    }
   }
 
-  private void serializeStructWrite(
-      StructColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
+  class VectorSerializeMapWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeMapWrite((MapColumnVector)colInfo, field, adjustedBatchIndex);
+    }
 
-    StructTypeInfo typeInfo = (StructTypeInfo) field.typeInfo;
-    StructObjectInspector objectInspector = (StructObjectInspector) field.objectInspector;
+    private void serializeMapWrite(
+            MapColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
 
-    final ColumnVector[] fieldColumnVectors = colVector.fields;
-    final Field[] children = field.children;
-    final List<? extends StructField> structFields = objectInspector.getAllStructFieldRefs();
-    final int size = field.count;
+      MapTypeInfo typeInfo = (MapTypeInfo) field.typeInfo;
+      MapObjectInspector objectInspector = (MapObjectInspector) field.objectInspector;
 
-    final List list = (List) vectorExtractRow.extractRowColumn(
-        colVector, typeInfo, objectInspector, adjustedBatchIndex);
+      final ColumnVector keyColumnVector = colVector.keys;
+      final ColumnVector valueColumnVector = colVector.values;
+      final Field keyField = field.children[0];
+      final Field valueField = field.children[1];
+      final int offset = (int) colVector.offsets[adjustedBatchIndex];
+      final int size = (int) colVector.lengths[adjustedBatchIndex];
 
-    serializeWrite.beginStruct(list);
-    for (int i = 0; i < size; i++) {
-      if (i > 0) {
-        serializeWrite.separateStruct();
+      final Map map = (Map) vectorExtractRow.extractRowColumn(
+              colVector, typeInfo, objectInspector, adjustedBatchIndex);
+
+      serializeWrite.beginMap(map);
+      for (int i = 0; i < size; i++) {
+        if (i > 0) {
+          serializeWrite.separateKeyValuePair();
+        }
+        serializeWrite(keyColumnVector, keyField, offset + i);
+        serializeWrite.separateKey();
+        serializeWrite(valueColumnVector, valueField, offset + i);
       }
-      serializeWrite(
-          fieldColumnVectors[i],
-          children[i],
-          adjustedBatchIndex);
+      serializeWrite.finishMap();
     }
-    serializeWrite.finishStruct();
   }
 
-  private void serializeMapWrite(
-      MapColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
+  class VectorSerializeListWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeListWrite((ListColumnVector)colInfo, field, adjustedBatchIndex);
+    }
 
-    MapTypeInfo typeInfo = (MapTypeInfo) field.typeInfo;
-    MapObjectInspector objectInspector = (MapObjectInspector) field.objectInspector;
+    private void serializeListWrite(
+            ListColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
 
-    final ColumnVector keyColumnVector = colVector.keys;
-    final ColumnVector valueColumnVector = colVector.values;
-    final Field keyField = field.children[0];
-    final Field valueField = field.children[1];
-    final int offset = (int) colVector.offsets[adjustedBatchIndex];
-    final int size = (int) colVector.lengths[adjustedBatchIndex];
+      final ListTypeInfo typeInfo = (ListTypeInfo) field.typeInfo;
+      final ListObjectInspector objectInspector = (ListObjectInspector) field.objectInspector;
 
-    final Map map = (Map) vectorExtractRow.extractRowColumn(
-        colVector, typeInfo, objectInspector, adjustedBatchIndex);
+      final ColumnVector childColumnVector = colVector.child;
+      final Field elementField = field.children[0];
+      final int offset = (int) colVector.offsets[adjustedBatchIndex];
+      final int size = (int) colVector.lengths[adjustedBatchIndex];
 
-    serializeWrite.beginMap(map);
-    for (int i = 0; i < size; i++) {
-      if (i > 0) {
-        serializeWrite.separateKeyValuePair();
+      final ObjectInspector elementObjectInspector = objectInspector.getListElementObjectInspector();
+      final List list = (List) vectorExtractRow.extractRowColumn(
+              colVector, typeInfo, objectInspector, adjustedBatchIndex);
+
+      serializeWrite.beginList(list);
+      for (int i = 0; i < size; i++) {
+        if (i > 0) {
+          serializeWrite.separateList();
+        }
+        serializeWrite(
+                childColumnVector, elementField, offset + i);
       }
-      serializeWrite(keyColumnVector, keyField, offset + i);
-      serializeWrite.separateKey();
-      serializeWrite(valueColumnVector, valueField, offset + i);
+      serializeWrite.finishList();
     }
-    serializeWrite.finishMap();
   }
 
-  private void serializeListWrite(
-      ListColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
+  class VectorSerializeBooleanWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeBoolean(
+              ((LongColumnVector) colInfo).vector[adjustedBatchIndex] != 0);
+    }
+  }
 
-    final ListTypeInfo typeInfo = (ListTypeInfo) field.typeInfo;
-    final ListObjectInspector objectInspector = (ListObjectInspector) field.objectInspector;
+  class VectorSerializeByteWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeByte(
+              (byte) ((LongColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
 
-    final ColumnVector childColumnVector = colVector.child;
-    final Field elementField = field.children[0];
-    final int offset = (int) colVector.offsets[adjustedBatchIndex];
-    final int size = (int) colVector.lengths[adjustedBatchIndex];
+  class VectorSerializeShortWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeShort(
+              (short) ((LongColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
 
-    final ObjectInspector elementObjectInspector = objectInspector.getListElementObjectInspector();
-    final List list = (List) vectorExtractRow.extractRowColumn(
-        colVector, typeInfo, objectInspector, adjustedBatchIndex);
+  class VectorSerializeIntWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeInt(
+              (int) ((LongColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
 
-    serializeWrite.beginList(list);
-    for (int i = 0; i < size; i++) {
-      if (i > 0) {
-        serializeWrite.separateList();
-      }
-      serializeWrite(
-          childColumnVector, elementField, offset + i);
+  class VectorSerializeLongWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeLong(
+              ((LongColumnVector) colInfo).vector[adjustedBatchIndex]);
     }
-    serializeWrite.finishList();
-  }
-
-  private void serializePrimitiveWrite(
-      ColumnVector colVector, Field field, int adjustedBatchIndex) throws IOException {
-
-    final PrimitiveCategory primitiveCategory = field.primitiveCategory;
-    switch (primitiveCategory) {
-    case BOOLEAN:
-      serializeWrite.writeBoolean(((LongColumnVector) colVector).vector[adjustedBatchIndex] != 0);
-      break;
-    case BYTE:
-      serializeWrite.writeByte((byte) ((LongColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case SHORT:
-      serializeWrite.writeShort((short) ((LongColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case INT:
-      serializeWrite.writeInt((int) ((LongColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case LONG:
-      serializeWrite.writeLong(((LongColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case DATE:
-      serializeWrite.writeDate((int) ((LongColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case TIMESTAMP:
-      // From java.sql.Timestamp used by vectorization to serializable org.apache.hadoop.hive.common.type.Timestamp
-      java.sql.Timestamp ts = ((TimestampColumnVector) colVector).asScratchTimestamp(adjustedBatchIndex);
+  }
+
+  class VectorSerializeDateWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeDate(
+              (int) ((LongColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
+
+  class VectorSerializeTimestampWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      // From java.sql.Timestamp used by vectorization to serializable
+      // org.apache.hadoop.hive.common.type.Timestamp
+      java.sql.Timestamp ts =
+              ((TimestampColumnVector) colInfo).asScratchTimestamp(adjustedBatchIndex);
       Timestamp serializableTS = Timestamp.ofEpochMilli(ts.getTime(), ts.getNanos());
       serializeWrite.writeTimestamp(serializableTS);
-      break;
-    case FLOAT:
-      serializeWrite.writeFloat((float) ((DoubleColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case DOUBLE:
-      serializeWrite.writeDouble(((DoubleColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case STRING:
-    case CHAR:
-    case VARCHAR:
+    }
+  }
+
+  class VectorSerializeFloatWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeFloat(
+              (float) ((DoubleColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
+
+  class VectorSerializeDoubleWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeDouble(
+              ((DoubleColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
+
+  class VectorSerializeStringWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
       {
         // We store CHAR and VARCHAR without pads, so write with STRING.
-        final BytesColumnVector bytesColVector = (BytesColumnVector) colVector;
+        final BytesColumnVector bytesColVector = (BytesColumnVector) colInfo;
         serializeWrite.writeString(
-            bytesColVector.vector[adjustedBatchIndex],
-            bytesColVector.start[adjustedBatchIndex],
-            bytesColVector.length[adjustedBatchIndex]);
+                bytesColVector.vector[adjustedBatchIndex],
+                bytesColVector.start[adjustedBatchIndex],
+                bytesColVector.length[adjustedBatchIndex]);
       }
-      break;
-    case BINARY:
-      {
-        final BytesColumnVector bytesColVector = (BytesColumnVector) colVector;
-        serializeWrite.writeBinary(
-            bytesColVector.vector[adjustedBatchIndex],
-            bytesColVector.start[adjustedBatchIndex],
-            bytesColVector.length[adjustedBatchIndex]);
-      }
-      break;
-    case DECIMAL:
-      {
-        if (colVector instanceof Decimal64ColumnVector) {
-          final Decimal64ColumnVector decimal64ColVector = (Decimal64ColumnVector) colVector;
-          serializeWrite.writeDecimal64(decimal64ColVector.vector[adjustedBatchIndex], decimal64ColVector.scale);
-        } else {
-          final DecimalColumnVector decimalColVector = (DecimalColumnVector) colVector;
-          serializeWrite.writeHiveDecimal(decimalColVector.vector[adjustedBatchIndex], decimalColVector.scale);
-        }
+    }
+  }
+
+  class VectorSerializeBinaryWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      final BytesColumnVector bytesColVector = (BytesColumnVector) colInfo;
+      serializeWrite.writeBinary(
+              bytesColVector.vector[adjustedBatchIndex],
+              bytesColVector.start[adjustedBatchIndex],
+              bytesColVector.length[adjustedBatchIndex]);
+    }
+  }
+
+  class VectorSerializeDecimalWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      if (colInfo instanceof Decimal64ColumnVector) {
+        final Decimal64ColumnVector decimal64ColVector = (Decimal64ColumnVector) colInfo;
+        serializeWrite.writeDecimal64(
+                decimal64ColVector.vector[adjustedBatchIndex], decimal64ColVector.scale);
+      } else {
+        final DecimalColumnVector decimalColVector = (DecimalColumnVector) colInfo;
+        serializeWrite.writeHiveDecimal(
+                decimalColVector.vector[adjustedBatchIndex], decimalColVector.scale);
       }
-      break;
-    case INTERVAL_YEAR_MONTH:
-      serializeWrite.writeHiveIntervalYearMonth((int) ((LongColumnVector) colVector).vector[adjustedBatchIndex]);
-      break;
-    case INTERVAL_DAY_TIME:
-      serializeWrite.writeHiveIntervalDayTime(((IntervalDayTimeColumnVector) colVector).asScratchIntervalDayTime(adjustedBatchIndex));
-      break;
-    default:
-      throw new RuntimeException("Unexpected primitive category " + primitiveCategory);
+    }
+  }
+
+  class VectorSerializeHiveIntervalYearMonthWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeHiveIntervalYearMonth(
+              (int) ((LongColumnVector) colInfo).vector[adjustedBatchIndex]);
+    }
+  }
+
+  class VectorSerializeHiveIntervalDayTimeWriter extends VectorSerializeWriter {
+    @Override
+    void serialize(Object colInfo, Field field, int adjustedBatchIndex) throws IOException {
+      serializeWrite.writeHiveIntervalDayTime(
+              ((IntervalDayTimeColumnVector) colInfo).asScratchIntervalDayTime(adjustedBatchIndex));
     }
   }
 
