diff --git a/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java b/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
index 3ed24d072c..19172a7513 100644
--- a/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
+++ b/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
@@ -29,6 +29,7 @@
 import java.util.List;
 import java.util.ArrayList;
 import java.util.regex.Pattern;
+import java.util.HashMap;
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.tools.ant.AntClassLoader;
@@ -259,6 +260,7 @@ public void execute() throws BuildException {
     }
 
     List<File> qFiles = new ArrayList<File>();
+    HashMap<String, String> qFilesMap = new HashMap<String, String>();
     File outDir = null;
     File resultsDir = null;
     File logDir = null;
@@ -281,7 +283,7 @@ public void execute() throws BuildException {
       } else if (queryFileRegex != null && !queryFileRegex.equals("")) {
         qFiles.addAll(Arrays.asList(inpDir.listFiles(new QFileRegexFilter(queryFileRegex))));
       } else if (runDisabled != null && runDisabled.equals("true")) {
-        qFiles.addAll(Arrays.asList(inpDir.listFiles(new DisabledQFileFilter())));        
+        qFiles.addAll(Arrays.asList(inpDir.listFiles(new DisabledQFileFilter())));
       } else {
         qFiles.addAll(Arrays.asList(inpDir.listFiles(new QFileFilter())));
       }
@@ -298,6 +300,9 @@ public void execute() throws BuildException {
       }
       
       Collections.sort(qFiles);
+      for (File qFile : qFiles) {
+        qFilesMap.put(qFile.getName(), getEscapedCanonicalPath(qFile));
+      }
 
       // Make sure the output directory exists, if it doesn't
       // then create it.
@@ -348,8 +353,9 @@ public void execute() throws BuildException {
       VelocityContext ctx = new VelocityContext();
       ctx.put("className", className);
       ctx.put("qfiles", qFiles);
-      ctx.put("resultsDir", resultsDir);
-      ctx.put("logDir", logDir);
+      ctx.put("qfilesMap", qFilesMap);
+      ctx.put("resultsDir", getEscapedCanonicalPath(resultsDir));
+      ctx.put("logDir", getEscapedCanonicalPath(logDir));
       ctx.put("clusterMode", clusterMode);
       ctx.put("hadoopVersion", hadoopVersion);
 
@@ -373,4 +379,17 @@ public void execute() throws BuildException {
       throw new BuildException("Generation failed", e);
     }
   }
+  
+  private static String getEscapedCanonicalPath(File file) throws IOException {
+    if (System.getProperty("os.name").toLowerCase().startsWith("win")) {
+      // Escape the backward slash in CanonicalPath if the unit test runs on windows
+      // e.g. dir.getCanonicalPath() gets the absolute path of local
+      // directory. When we embed it directly in the generated java class it results
+      // in compiler error in windows. Reason : the canonical path contains backward
+      // slashes "C:\temp\etc\" and it is not a valid string in Java
+      // unless we escape the backward slashes.
+      return file.getCanonicalPath().replace("\\", "\\\\");
+    }
+    return file.getCanonicalPath();
+  }
 }
diff --git a/build-common.xml b/build-common.xml
index 40caea4b89..0b279f3a90 100644
--- a/build-common.xml
+++ b/build-common.xml
@@ -109,6 +109,8 @@
 
   <loadproperties srcfile="${ivy.conf.dir}/libraries.properties"/>
 
+  <osfamily property="os.family"/>
+
   <condition property="offline">
     <istrue value="${is-offline}"/>
   </condition>
@@ -366,7 +368,7 @@
 
   <!-- target to run the tests -->
   <target name="test"
-  	depends="test-conditions,gen-test,compile-test,test-jar,test-init">
+    depends="test-conditions,gen-test,compile-test,test-jar,test-init">
     <echo message="Project: ${ant.project.name}"/>
     <property name="hadoop.testcp" refid="test.classpath"/>
     <if>
@@ -378,6 +380,18 @@
         <property name="hadoop.opts" value="${hadoop.opts.20}" />
       </else>
     </if>
+    <!-- Set the OS specific settings to run junit tests on unix as well as on windows -->
+    <if>
+      <equals arg1="windows" arg2="${os.family}"/>
+      <then>
+        <property name="junit.script.extension" value=".cmd"/>
+        <property name="junit.file.schema" value=""/>
+      </then>
+      <else>
+        <property name="junit.script.extension" value=""/>
+        <property name="junit.file.schema" value="file://"/>
+      </else>
+    </if>
     <if>
       <equals arg1="${test.print.classpath}" arg2="true" />
       <then>
@@ -390,13 +404,13 @@
       <env key="LANG" value="${test.lang}"/>
       <env key="HIVE_HADOOP_TEST_CLASSPATH" value="${hadoop.testcp}"/>
       <env key="HADOOP_HOME" value="${hadoop.root}"/>
-      <env key="HADOOP_CLASSPATH" value="${test.src.data.dir}/conf:${build.dir.hive}/dist/lib/derby-${derby.version}.jar:${build.dir.hive}/dist/lib/JavaEWAH-${javaewah.version}.jar:${hadoop.root}/modules/*"/> <!-- Modules needed for Hadoop 0.23 -->
+      <env key="HADOOP_CLASSPATH" path="${test.src.data.dir}/conf:${build.dir.hive}/dist/lib/derby-${derby.version}.jar:${build.dir.hive}/dist/lib/JavaEWAH-${javaewah.version}.jar:${hadoop.root}/modules/*"/> <!-- Modules needed for Hadoop 0.23 -->
       <env key="TZ" value="US/Pacific"/>
       <sysproperty key="test.output.overwrite" value="${overwrite}"/>
       <sysproperty key="test.service.standalone.server" value="${standalone}"/>
-      <sysproperty key="log4j.configuration" value="file://${test.src.data.dir}/conf/hive-log4j.properties"/>
+      <sysproperty key="log4j.configuration" value="${junit.file.schema}${test.src.data.dir}/conf/hive-log4j.properties"/>
       <sysproperty key="derby.stream.error.file" value="${test.build.dir}/derby.log"/>
-      <sysproperty key="hive.aux.jars.path" value="file://${test.build.dir}/test-udfs.jar"/>
+      <sysproperty key="hive.aux.jars.path" value="${junit.file.schema}${test.build.dir}/test-udfs.jar"/>
       <sysproperty key="ql.test.query.clientpositive.dir" value="${ql.test.query.clientpositive.dir}"/>
       <sysproperty key="ql.test.results.clientpositive.dir" value="${ql.test.results.clientpositive.dir}"/>
       <sysproperty key="test.log.dir" value="${test.log.dir}"/>
@@ -413,7 +427,7 @@
       <sysproperty key="build.ivy.lib.dir" value="${build.ivy.lib.dir}"/>
       <sysproperty key="derby.version" value="${derby.version}"/>
       <sysproperty key="hive.version" value="${version}"/>
-      <sysproperty key="hadoop.bin.path" value="${test.hadoop.bin.path}"/>
+      <sysproperty key="hadoop.bin.path" value="${test.hadoop.bin.path}${junit.script.extension}"/>
 
       <classpath refid="test.local.classpath"/>
       <formatter type="${test.junit.output.format}" usefile="${test.junit.output.usefile}" />
diff --git a/build.xml b/build.xml
index fa4d65f1bf..6712af9354 100644
--- a/build.xml
+++ b/build.xml
@@ -532,9 +532,6 @@
       <fileset dir="${target.bin.dir}"/>
     </chmod>
 
-    <!-- create symlinks for libthrift.jar, libfb303.jar, etc. for backward compatibility -->
-    <symlink overwrite="true" link="${target.lib.dir}/libthrift.jar" resource="libthrift-${libthrift.version}.jar"/>
-    <symlink overwrite="true" link="${target.lib.dir}/libfb303.jar" resource="libfb303-${libfb303.version}.jar"/>
     <symlink overwrite="true" link="${target.lib.dir}/hive_contrib.jar" resource="hive-contrib-${version}.jar"/>
     <!-- special case because builtins compilation depends on packaging
          up everything else first -->
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index dd76f91fd2..c5f0556d3f 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -40,6 +40,7 @@
 import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.util.Shell;
 
 /**
  * Hive Configuration.
@@ -165,6 +166,7 @@ public static enum ConfVars {
     SCRIPTWRAPPER("hive.exec.script.wrapper", null),
     PLAN("hive.exec.plan", ""),
     SCRATCHDIR("hive.exec.scratchdir", "/tmp/hive-" + System.getProperty("user.name")),
+    LOCALSCRATCHDIR("hive.exec.local.scratchdir", "/tmp/" + System.getProperty("user.name")),
     SUBMITVIACHILD("hive.exec.submitviachild", false),
     SCRIPTERRORLIMIT("hive.exec.script.maxerrsize", 100000),
     ALLOWPARTIALCONSUMP("hive.exec.script.allow.partial.consumption", false),
@@ -692,8 +694,10 @@ private static String findHadoopBinary() {
         val = System.getenv("HADOOP_PREFIX");
       }
       // and if all else fails we can at least try /usr/bin/hadoop
-      return (val == null ? File.separator + "usr" : val)
+      val = (val == null ? File.separator + "usr" : val)
         + File.separator + "bin" + File.separator + "hadoop";
+      // Launch hadoop command file on windows.
+      return val + (Shell.WINDOWS ? ".cmd" : "");
     }
 
     enum VarType {
diff --git a/conf/hive-default.xml.template b/conf/hive-default.xml.template
index 0b517bec9a..c74404eb4e 100644
--- a/conf/hive-default.xml.template
+++ b/conf/hive-default.xml.template
@@ -68,6 +68,12 @@
   <description>Scratch space for Hive jobs</description>
 </property>
 
+<property>
+  <name>hive.exec.local.scratchdir</name>
+  <value>/tmp/${user.name}</value>
+  <description>Local scratch space for Hive jobs</description>
+</property>
+
 <property>
   <name>hive.test.mode</name>
   <value>false</value>
diff --git a/contrib/src/test/org/apache/hadoop/hive/contrib/mr/TestGenericMR.java b/contrib/src/test/org/apache/hadoop/hive/contrib/mr/TestGenericMR.java
index d077d23570..5fedb3db95 100644
--- a/contrib/src/test/org/apache/hadoop/hive/contrib/mr/TestGenericMR.java
+++ b/contrib/src/test/org/apache/hadoop/hive/contrib/mr/TestGenericMR.java
@@ -24,6 +24,8 @@
 
 import junit.framework.TestCase;
 
+import org.apache.hadoop.util.Shell;
+
 /**
  * TestGenericMR.
  *
@@ -61,8 +63,7 @@ public void testIdentityMap() throws Exception {
     final StringWriter out = new StringWriter();
 
     new GenericMR().map(new StringReader(in), out, identityMapper());
-
-    assertEquals(in + "\n", out.toString());
+    assertEquals(in + "\n", getOsSpecificOutput(out.toString()));
   }
 
   public void testKVSplitMap() throws Exception {
@@ -79,7 +80,7 @@ public void map(String[] record, Output output) throws Exception {
       }
     });
 
-    assertEquals(expected, out.toString());
+    assertEquals(expected, getOsSpecificOutput(out.toString()));
   }
 
   public void testIdentityReduce() throws Exception {
@@ -88,7 +89,7 @@ public void testIdentityReduce() throws Exception {
 
     new GenericMR().reduce(new StringReader(in), out, identityReducer());
 
-    assertEquals(in + "\n", out.toString());
+    assertEquals(in + "\n", getOsSpecificOutput(out.toString()));
   }
 
   public void testWordCountReduce() throws Exception {
@@ -111,7 +112,7 @@ public void reduce(String key, Iterator<String[]> records, Output output)
 
     final String expected = "hello\t3\nokay\t12\n";
 
-    assertEquals(expected, out.toString());
+    assertEquals(expected, getOsSpecificOutput(out.toString()));
   }
 
   private Mapper identityMapper() {
@@ -134,4 +135,9 @@ public void reduce(String key, Iterator<String[]> records, Output output)
       }
     };
   }
+
+  private static String getOsSpecificOutput(String outStr){
+    assert outStr != null;
+    return Shell.WINDOWS ? outStr.replaceAll("\\r", "") : outStr;
+  }
 }
diff --git a/data/conf/hive-site.xml b/data/conf/hive-site.xml
index f279f5aa14..907d333006 100644
--- a/data/conf/hive-site.xml
+++ b/data/conf/hive-site.xml
@@ -46,6 +46,12 @@
   <description>Scratch space for Hive jobs</description>
 </property>
 
+<property>
+  <name>hive.exec.local.scratchdir</name>
+  <value>${build.dir}/scratchdir/local/</value>
+  <description>Local scratch space for Hive jobs</description>
+</property>
+
 <property>
   <name>javax.jdo.option.ConnectionURL</name>
   <!-- note: variable substituion not working here because it's loaded by jdo, not Hive -->
diff --git a/hbase-handler/src/test/templates/TestHBaseCliDriver.vm b/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
index c38ad58956..3f2baafb72 100644
--- a/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
+++ b/hbase-handler/src/test/templates/TestHBaseCliDriver.vm
@@ -56,13 +56,10 @@ public class $className extends TestCase {
       if ("$clusterMode".equals("miniMR")) {
         miniMR = true;
       }
-
-      qt = new HBaseQTestUtil(
-        "$resultsDir.getCanonicalPath()",
-        "$logDir.getCanonicalPath()", miniMR, setup);
+      qt = new HBaseQTestUtil("$resultsDir", "$logDir", miniMR, setup);
 
 #foreach ($qf in $qfiles)
-      qt.addFile("$qf.getCanonicalPath()");
+      qt.addFile("$qfilesMap.get($qf.getName())");
 #end
     } catch (Exception e) {
       System.out.println("Exception: " + e.getMessage());
diff --git a/hbase-handler/src/test/templates/TestHBaseNegativeCliDriver.vm b/hbase-handler/src/test/templates/TestHBaseNegativeCliDriver.vm
index c61cbd5d32..000878cc80 100644
--- a/hbase-handler/src/test/templates/TestHBaseNegativeCliDriver.vm
+++ b/hbase-handler/src/test/templates/TestHBaseNegativeCliDriver.vm
@@ -40,12 +40,10 @@ public class $className extends TestCase {
         miniMR = true;
       }
 
-      qt = new HBaseQTestUtil(
-        "$resultsDir.getCanonicalPath()",
-        "$logDir.getCanonicalPath()", miniMR, setup);
+      qt = new HBaseQTestUtil("$resultsDir", "$logDir", miniMR, setup);
 
 #foreach ($qf in $qfiles)
-      qt.addFile("$qf.getCanonicalPath()");
+      qt.addFile("$qfilesMap.get($qf.getName())");
 #end
     } catch (Exception e) {
       System.out.println("Exception: " + e.getMessage());
diff --git a/odbc/build.xml b/odbc/build.xml
index 9f4ccd9430..2228f02d12 100644
--- a/odbc/build.xml
+++ b/odbc/build.xml
@@ -29,6 +29,21 @@
   <property name="make.cmd" value="make"/>
   <import file="../build-common.xml"/>
 
+  <!--Skip the Make file execution on windows-->
+  <condition property="execute.makefile">
+    <not>
+      <equals arg1="windows" arg2="${os.family}"/>
+    </not>
+  </condition>
+
+  <!-- Only run tests if thrift.home is defined and not on windows-->
+  <condition property="execute.tests">
+    <and>
+      <istrue value="${execute.makefile}"/>
+      <istrue value="${thrift.home.defined}"/>
+    </and>
+  </condition>
+
   <target name="set-test-classpath">
     <path id="test.classpath">
       <pathelement location="${test.build.classes}" />
@@ -49,7 +64,7 @@
     </condition>
   </target>
 
-  <target name="compile-cpp" depends="init,check-word-size">
+  <target name="compile-cpp" depends="init,check-word-size" if="execute.makefile">
     <echo message="Project: ${ant.project.name}"/>
     <exec dir="." executable="${make.cmd}" failonerror="true">
       <env key="WORD_SIZE" value="${word.size}"/>
@@ -63,7 +78,7 @@
     <copy file="${basedir}/src/cpp/hiveconstants.h" todir="${build.dir.hive}/odbc/include"/>
   </target>
 
-  <target name="clean">
+  <target name="clean" if="execute.makefile">
     <echo message="Project: ${ant.project.name}"/>
     <delete dir="${build.dir.hive}/odbc/include"/>
     <exec dir="." executable="${make.cmd}" failonerror="true">
@@ -73,7 +88,7 @@
     </exec>
   </target>
 
-  <target name="install" depends="check-word-size">
+  <target name="install" depends="check-word-size" if="execute.makefile">
     <echo message="Project: ${ant.project.name}"/>
     <exec dir="." executable="${make.cmd}" failonerror="true">
       <arg line="install"/>
@@ -84,7 +99,7 @@
     </exec>
   </target>
 
-  <target name="uninstall">
+  <target name="uninstall" if="execute.makefile">
     <echo message="Project: ${ant.project.name}"/>
     <exec dir="." executable="${make.cmd}" failonerror="true">
       <arg line="uninstall"/>
@@ -94,7 +109,7 @@
   </target>
 
   <!-- Only run tests if thrift.home is defined so that we don't break other tests -->
-  <target name="test" depends="check-word-size,check-thrift-home,set-test-classpath" if="thrift.home.defined">
+  <target name="test" depends="check-word-size,check-thrift-home,set-test-classpath" if="execute.tests">
     <echo message="Project: ${ant.project.name}"/>
     <exec dir="." executable="${make.cmd}" failonerror="true">
       <arg line="test"/>
@@ -104,7 +119,4 @@
       <env key="BASE_DIR" value="${basedir}"/>
     </exec>
   </target>
-
-
-
 </project>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Context.java b/ql/src/java/org/apache/hadoop/hive/ql/Context.java
index 68f4dc766e..4c00afd555 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Context.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Context.java
@@ -99,16 +99,13 @@ public Context(Configuration conf, String executionId)  {
     this.conf = conf;
     this.executionId = executionId;
 
-    // non-local tmp location is configurable. however it is the same across
+    // local & non-local tmp location is configurable. however it is the same across
     // all external file systems
     nonLocalScratchPath =
       new Path(HiveConf.getVar(conf, HiveConf.ConfVars.SCRATCHDIR),
                executionId);
-
-    // local tmp location is not configurable for now
-    localScratchDir = System.getProperty("java.io.tmpdir")
-      + Path.SEPARATOR + System.getProperty("user.name") + Path.SEPARATOR
-      + executionId;
+    localScratchDir = new Path(HiveConf.getVar(conf, HiveConf.ConfVars.LOCALSCRATCHDIR),
+            executionId).toUri().getPath();
   }
 
   /**
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index 3abc3da94e..b5134671c0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.hive.ql.plan.CopyWork;
 import org.apache.hadoop.hive.ql.plan.LoadTableDesc;
 import org.apache.hadoop.hive.ql.plan.MoveWork;
+import org.apache.hadoop.util.Shell;
 
 /**
  * LoadSemanticAnalyzer.
@@ -81,10 +82,10 @@ private URI initializeFromURI(String fromPath) throws IOException,
     // directory
     if (!path.startsWith("/")) {
       if (isLocal) {
-        path = new Path(System.getProperty("user.dir"), path).toString();
+        path = new Path(System.getProperty("user.dir"), path).toUri().toString();
       } else {
         path = new Path(new Path("/user/" + System.getProperty("user.name")),
-            path).toString();
+          path).toString();
       }
     }
 
@@ -231,7 +232,7 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     }
 
     // create final load/move work
-    
+
     String loadTmpPath = ctx.getExternalTmpFileURI(toURI);
     Map<String, String> partSpec = ts.getPartSpec();
     if (partSpec == null) {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
index 93d002a013..7ae9153e2c 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -33,7 +33,6 @@
 import java.io.PrintStream;
 import java.io.Serializable;
 import java.io.UnsupportedEncodingException;
-import java.lang.UnsupportedOperationException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Deque;
@@ -46,7 +45,6 @@
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
-import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.FileStatus;
@@ -82,6 +80,7 @@
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.util.Shell;
 import org.apache.thrift.protocol.TBinaryProtocol;
 import org.apache.zookeeper.ZooKeeper;
 
@@ -205,19 +204,64 @@ private String getHadoopMainVersion(String input) {
   }
 
   public void initConf() throws Exception {
+
+    if (Shell.WINDOWS) {
+      convertPathsFromWindowsToHdfs();
+    }
+
     if (miniMr) {
       assert dfs != null;
       assert mr != null;
       // set fs.default.name to the uri of mini-dfs
-      conf.setVar(HiveConf.ConfVars.HADOOPFS, dfs.getFileSystem().getUri().toString());
+      String dfsUriString = getHdfsUriString(dfs.getFileSystem().getUri().toString());
+      conf.setVar(HiveConf.ConfVars.HADOOPFS, dfsUriString);
       // hive.metastore.warehouse.dir needs to be set relative to the mini-dfs
       conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE,
-                  (new Path(dfs.getFileSystem().getUri().toString(),
+                  (new Path(dfsUriString,
                             "/build/ql/test/data/warehouse/")).toString());
       conf.setVar(HiveConf.ConfVars.HADOOPJT, "localhost:" + mr.getJobTrackerPort());
     }
   }
 
+  private void convertPathsFromWindowsToHdfs() {
+    // Following local paths are used as HDFS paths in unit tests.
+    // It works well in Unix as the path notation in Unix and HDFS is more or less same.
+    // But when it comes to Windows, drive letter separator ':' & backslash '\" are invalid
+    // characters in HDFS so we need to converts these local paths to HDFS paths before using them
+    // in unit tests.
+
+    // hive.exec.scratchdir needs to be set relative to the mini-dfs
+    String orgWarehouseDir = conf.getVar(HiveConf.ConfVars.METASTOREWAREHOUSE);
+    conf.setVar(HiveConf.ConfVars.METASTOREWAREHOUSE, getHdfsUriString(orgWarehouseDir));
+
+    String orgTestTempDir = System.getProperty("test.tmp.dir");
+    System.setProperty("test.tmp.dir", getHdfsUriString(orgTestTempDir));
+
+    String orgTestDataDir = System.getProperty("test.src.data.dir");
+    System.setProperty("test.src.data.dir", getHdfsUriString(orgTestDataDir));
+
+    String orgScratchDir = conf.getVar(HiveConf.ConfVars.SCRATCHDIR);
+    conf.setVar(HiveConf.ConfVars.SCRATCHDIR, getHdfsUriString(orgScratchDir));
+
+    if (miniMr) {
+      String orgAuxJarFolder = conf.getAuxJars();
+      conf.setAuxJars(getHdfsUriString("file://" + orgAuxJarFolder));
+    }
+  }
+
+  private String getHdfsUriString(String uriStr) {
+    assert uriStr != null;
+    if(Shell.WINDOWS) {
+      // If the URI conversion is from Windows to HDFS then replace the '\' with '/'
+      // and remove the windows single drive letter & colon from absolute path.
+      return uriStr.replace('\\', '/')
+        .replaceFirst("/[c-zC-Z]:", "/")
+        .replaceFirst("^[c-zC-Z]:", "");
+    }
+
+    return uriStr;
+  }
+
   public QTestUtil(String outDir, String logDir, boolean miniMr, String hadoopVer)
     throws Exception {
     this.outDir = outDir;
@@ -231,7 +275,7 @@ public QTestUtil(String outDir, String logDir, boolean miniMr, String hadoopVer)
     if (miniMr) {
       dfs = ShimLoader.getHadoopShims().getMiniDfs(conf, 4, true, null);
       FileSystem fs = dfs.getFileSystem();
-      mr = new MiniMRCluster(4, fs.getUri().toString(), 1);
+      mr = new MiniMRCluster(4, getHdfsUriString(fs.getUri().toString()), 1);
     }
 
     initConf();
@@ -242,8 +286,7 @@ public QTestUtil(String outDir, String logDir, boolean miniMr, String hadoopVer)
       dataDir = new File(".").getAbsolutePath() + "/data/files";
     }
 
-    testFiles = dataDir.replace('\\', '/')
-        .replace("c:", "");
+    testFiles = dataDir;
 
     String ow = System.getProperty("test.output.overwrite");
     if ((ow != null) && ow.equalsIgnoreCase("true")) {
@@ -285,7 +328,7 @@ public void addFile(File qf) throws Exception {
 
     // Look for a hint to not run a test on some Hadoop versions
     Pattern pattern = Pattern.compile("-- (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS\\((.*)\\)");
-    
+
     boolean excludeQuery = false;
     boolean includeQuery = false;
     Set<String> versionSet = new HashSet<String>();
@@ -313,7 +356,7 @@ public void addFile(File qf) throws Exception {
             + " contains more than one reference to (EX|IN)CLUDE_HADOOP_MAJOR_VERSIONS";
           throw new UnsupportedOperationException(message);
         }
-        
+
         String prefix = matcher.group(1);
         if ("EX".equals(prefix)) {
           excludeQuery = true;
@@ -330,7 +373,7 @@ public void addFile(File qf) throws Exception {
       qsb.append(line + "\n");
     }
     qMap.put(qf.getName(), qsb.toString());
-    
+
     if (excludeQuery && versionSet.contains(hadoopVer)) {
       System.out.println("QTestUtil: " + qf.getName()
         + " EXCLUDE list contains Hadoop Version " + hadoopVer + ". Skipping...");
@@ -469,7 +512,7 @@ public void createSources() throws Exception {
         // db.createPartition(srcpart, part_spec);
         fpath = new Path(testFiles, "kv1.txt");
         // db.loadPartition(fpath, srcpart.getName(), part_spec, true);
-        runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString()
+        runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
             + "' OVERWRITE INTO TABLE srcpart PARTITION (ds='" + ds + "',hr='"
             + hr + "')");
       }
@@ -481,7 +524,7 @@ public void createSources() throws Exception {
     // IgnoreKeyTextOutputFormat.class, 2, bucketCols);
     for (String fname : new String[] {"srcbucket0.txt", "srcbucket1.txt"}) {
       fpath = new Path(testFiles, fname);
-      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString()
+      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
           + "' INTO TABLE srcbucket");
     }
 
@@ -492,7 +535,7 @@ public void createSources() throws Exception {
     for (String fname : new String[] {"srcbucket20.txt", "srcbucket21.txt",
         "srcbucket22.txt", "srcbucket23.txt"}) {
       fpath = new Path(testFiles, fname);
-      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString()
+      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
           + "' INTO TABLE srcbucket2");
     }
 
@@ -520,25 +563,25 @@ public void createSources() throws Exception {
 
     // load the input data into the src table
     fpath = new Path(testFiles, "kv1.txt");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath() + "' INTO TABLE src");
 
     // load the input data into the src table
     fpath = new Path(testFiles, "kv3.txt");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src1");
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath() + "' INTO TABLE src1");
 
     // load the input data into the src_sequencefile table
     fpath = new Path(testFiles, "kv1.seq");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString()
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
         + "' INTO TABLE src_sequencefile");
 
     // load the input data into the src_thrift table
     fpath = new Path(testFiles, "complex.seq");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString()
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
         + "' INTO TABLE src_thrift");
 
     // load the json data into the src_json table
     fpath = new Path(testFiles, "json.txt");
-    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString()
+    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toUri().getPath()
         + "' INTO TABLE src_json");
     conf.setBoolean("hive.test.init.phase", false);
   }
@@ -712,26 +755,9 @@ public int checkNegativeResults(String tname, Exception e) throws Exception {
     outfd.write(e.getMessage());
     outfd.close();
 
-    String cmdLine = "diff " + outf.getPath() + " " + expf;
-    System.out.println(cmdLine);
-
-    Process executor = Runtime.getRuntime().exec(cmdLine);
-
-    StreamPrinter outPrinter = new StreamPrinter(
-        executor.getInputStream(), null, SessionState.getConsole().getChildOutStream());
-    StreamPrinter errPrinter = new StreamPrinter(
-        executor.getErrorStream(), null, SessionState.getConsole().getChildErrStream());
-
-    outPrinter.start();
-    errPrinter.start();
-
-    int exitVal = executor.waitFor();
-
+    int exitVal = executeDiffCommand(outf.getPath(), expf, false);
     if (exitVal != 0 && overWrite) {
-      System.out.println("Overwriting results");
-      cmdLine = "cp " + outf.getPath() + " " + expf;
-      executor = Runtime.getRuntime().exec(cmdLine);
-      exitVal = executor.waitFor();
+      exitVal = overwriteResults(outf.getPath(), expf);
     }
 
     return exitVal;
@@ -751,26 +777,10 @@ public int checkParseResults(String tname, ASTNode tree) throws Exception {
       outfd.write(tree.toStringTree());
       outfd.close();
 
-      String cmdLine = "diff " + outf.getPath() + " " + expf;
-      System.out.println(cmdLine);
-
-      Process executor = Runtime.getRuntime().exec(cmdLine);
-
-      StreamPrinter outPrinter = new StreamPrinter(
-          executor.getInputStream(), null, SessionState.getConsole().getChildOutStream());
-      StreamPrinter errPrinter = new StreamPrinter(
-          executor.getErrorStream(), null, SessionState.getConsole().getChildErrStream());
-
-      outPrinter.start();
-      errPrinter.start();
-
-      int exitVal = executor.waitFor();
+      int exitVal = executeDiffCommand(outf.getPath(), expf, false);
 
       if (exitVal != 0 && overWrite) {
-        System.out.println("Overwriting results");
-        cmdLine = "cp " + outf.getPath() + " " + expf;
-        executor = Runtime.getRuntime().exec(cmdLine);
-        exitVal = executor.waitFor();
+        exitVal = overwriteResults(outf.getPath(), expf);
       }
 
       return exitVal;
@@ -804,31 +814,10 @@ public int checkPlan(String tname, List<Task<? extends Serializable>> tasks) thr
       };
       maskPatterns(patterns, outf.getPath());
 
-      String[] cmdArray = new String[] {
-          "diff",
-          "-b",
-          outf.getPath(),
-          planFile
-      };
-      System.out.println(org.apache.commons.lang.StringUtils.join(cmdArray, ' '));
-
-      Process executor = Runtime.getRuntime().exec(cmdArray);
-
-      StreamPrinter outPrinter = new StreamPrinter(
-          executor.getInputStream(), null, SessionState.getConsole().getChildOutStream());
-      StreamPrinter errPrinter = new StreamPrinter(
-          executor.getErrorStream(), null, SessionState.getConsole().getChildErrStream());
-
-      outPrinter.start();
-      errPrinter.start();
-
-      int exitVal = executor.waitFor();
+      int exitVal = executeDiffCommand(outf.getPath(), planFile, true);
 
       if (exitVal != 0 && overWrite) {
-        System.out.println("Overwriting results");
-        String cmdLine = "cp " + outf.getPath() + " " + planFile;
-        executor = Runtime.getRuntime().exec(cmdLine);
-        exitVal = executor.waitFor();
+        exitVal = overwriteResults(outf.getPath(), planFile);
       }
 
       return exitVal;
@@ -954,13 +943,56 @@ public int checkCliDriverResults(String tname) throws Exception {
         "^Deleted.*",
     };
     maskPatterns(patterns, (new File(logDir, tname + ".out")).getPath());
+    int exitVal = executeDiffCommand((new File(logDir, tname + ".out")).getPath(),
+                    outFileName, false);
+
+    if (exitVal != 0 && overWrite) {
+      exitVal = overwriteResults((new File(logDir, tname + ".out")).getPath(), outFileName);
+    }
+
+    return exitVal;
+  }
 
-    cmdArray = new String[] {
-        "diff", "-a",
-        (new File(logDir, tname + ".out")).getPath(),
-        outFileName
+  private static int overwriteResults(String inFileName, String outFileName) throws Exception {
+    // This method can be replaced with Files.copy(source, target, REPLACE_EXISTING)
+    // once Hive uses JAVA 7.
+    System.out.println("Overwriting results");
+    String[] cmdArray = new String[] {
+        "cp",
+        Shell.WINDOWS ? getQuotedString(inFileName) : inFileName,
+        Shell.WINDOWS ? getQuotedString(outFileName) : outFileName
     };
+    Process executor = Runtime.getRuntime().exec(cmdArray);
+    return executor.waitFor();
+  }
+
+  private static int executeDiffCommand(String inFileName,
+      String outFileName,
+      boolean ignoreWhiteSpace) throws Exception {
+    ArrayList<String> diffCommandArgs = new ArrayList<String>();
+    diffCommandArgs.add("diff");
+
+    // Text file comparison
+    diffCommandArgs.add("-a");
+
+    // Ignore changes in the amount of white space
+    if (ignoreWhiteSpace || Shell.WINDOWS) {
+      diffCommandArgs.add("-b");
+    }
 
+    // Files created on Windows machines have different line endings
+    // than files created on Unix/Linux. Windows uses carriage return and line feed
+    // ("\r\n") as a line ending, whereas Unix uses just line feed ("\n").
+    // Also StringBuilder.toString(), Stream to String conversions adds extra
+    // spaces at the end of the line.
+    if (Shell.WINDOWS) {
+      diffCommandArgs.add("--strip-trailing-cr"); // Strip trailing carriage return on input
+      diffCommandArgs.add("-B"); // Ignore changes whose lines are all blank
+    }
+    // Add files to compare to the arguments list
+    diffCommandArgs.add(Shell.WINDOWS ? getQuotedString(inFileName) : inFileName);
+    diffCommandArgs.add(Shell.WINDOWS ? getQuotedString(outFileName) : outFileName);
+    String[] cmdArray =(String [])diffCommandArgs.toArray(new String [diffCommandArgs.size ()]);
     System.out.println(org.apache.commons.lang.StringUtils.join(cmdArray, ' '));
 
     Process executor = Runtime.getRuntime().exec(cmdArray);
@@ -973,19 +1005,11 @@ public int checkCliDriverResults(String tname) throws Exception {
     outPrinter.start();
     errPrinter.start();
 
-    int exitVal = executor.waitFor();
-
-    if (exitVal != 0 && overWrite) {
-      System.out.println("Overwriting results");
-      cmdArray = new String[3];
-      cmdArray[0] = "cp";
-      cmdArray[1] = (new File(logDir, tname + ".out")).getPath();
-      cmdArray[2] = outFileName;
-      executor = Runtime.getRuntime().exec(cmdArray);
-      exitVal = executor.waitFor();
-    }
+    return executor.waitFor();
+  }
 
-    return exitVal;
+  private static String getQuotedString(String str){
+    return String.format("\"%s\"", str);
   }
 
   public ASTNode parseQuery(String tname) throws Exception {
@@ -1210,4 +1234,4 @@ public static void outputTestFailureHelpMessage() {
         + "or try \"ant test ... -Dtest.silent=false\" to get more logs.");
     System.err.flush();
   }
-}
+}
\ No newline at end of file
diff --git a/ql/src/test/templates/TestCliDriver.vm b/ql/src/test/templates/TestCliDriver.vm
index 43b6910e9e..dc46aa4ff2 100644
--- a/ql/src/test/templates/TestCliDriver.vm
+++ b/ql/src/test/templates/TestCliDriver.vm
@@ -48,7 +48,7 @@ public class $className extends TestCase {
       if ("$clusterMode".equals("miniMR"))
         miniMR = true;
       hadoopVer = "$hadoopVersion";
-      qt = new QTestUtil("$resultsDir.getCanonicalPath()", "$logDir.getCanonicalPath()", miniMR, hadoopVer);
+      qt = new QTestUtil("$resultsDir", "$logDir", miniMR, hadoopVer);
 
       // do a one time initialization
       qt.cleanUp();
@@ -124,7 +124,7 @@ public class $className extends TestCase {
     try {
       System.out.println("Begin query: " + "$fname");
 
-      qt.addFile("$qf.getCanonicalPath()");
+      qt.addFile("$qfilesMap.get($fname)");
 
       if (qt.shouldBeSkipped("$fname")) {
         return;
diff --git a/ql/src/test/templates/TestNegativeCliDriver.vm b/ql/src/test/templates/TestNegativeCliDriver.vm
index e32cd194c2..d226c3f17f 100644
--- a/ql/src/test/templates/TestNegativeCliDriver.vm
+++ b/ql/src/test/templates/TestNegativeCliDriver.vm
@@ -41,7 +41,7 @@ public class $className extends TestCase {
       if ("$clusterMode".equals("miniMR"))
         miniMR = true;
       hadoopVer = "$hadoopVersion";
-      qt = new QTestUtil("$resultsDir.getCanonicalPath()", "$logDir.getCanonicalPath()", miniMR, hadoopVer);
+      qt = new QTestUtil("$resultsDir", "$logDir", miniMR, hadoopVer);
       // do a one time initialization
       qt.cleanUp();
       qt.createSources();
@@ -115,7 +115,7 @@ public class $className extends TestCase {
     try {
       System.out.println("Begin query: " + "$fname");
 
-      qt.addFile("$qf.getCanonicalPath()");
+      qt.addFile("$qfilesMap.get($fname)");
 
       if (qt.shouldBeSkipped("$fname")) {
         System.out.println("Test $fname skipped");
diff --git a/ql/src/test/templates/TestParse.vm b/ql/src/test/templates/TestParse.vm
index 2776aa3647..02bf7ea1e4 100644
--- a/ql/src/test/templates/TestParse.vm
+++ b/ql/src/test/templates/TestParse.vm
@@ -37,7 +37,7 @@ public class $className extends TestCase {
       if ("$clusterMode".equals("miniMR"))
         miniMR = true;
       String hadoopVer = "$hadoopVersion";
-      qt = new QTestUtil("$resultsDir.getCanonicalPath()", "$logDir.getCanonicalPath()", miniMR, hadoopVer);
+      qt = new QTestUtil("$resultsDir", "$logDir", miniMR, hadoopVer);
     } catch (Exception e) {
       System.out.println("Exception: " + e.getMessage());
       e.printStackTrace();
@@ -98,7 +98,7 @@ public class $className extends TestCase {
     try {
       System.out.println("Begin query: " + "$fname");
 
-      qt.addFile("$qf.getCanonicalPath()");
+      qt.addFile("$qfilesMap.get($fname)");
 
       qt.init("$fname");
       ASTNode tree = qt.parseQuery("$fname");
diff --git a/ql/src/test/templates/TestParseNegative.vm b/ql/src/test/templates/TestParseNegative.vm
index 83b6b01689..7c3345c6ec 100755
--- a/ql/src/test/templates/TestParseNegative.vm
+++ b/ql/src/test/templates/TestParseNegative.vm
@@ -37,8 +37,7 @@ public class $className extends TestCase {
       if ("$clusterMode".equals("miniMR"))
         miniMR = true;
       String hadoopVer = "$hadoopVersion";
-      qt = new QTestUtil("$resultsDir.getCanonicalPath()", "$logDir.getCanonicalPath()",
-                         miniMR, hadoopVer);
+      qt = new QTestUtil("$resultsDir", "$logDir", miniMR, hadoopVer);
     } catch (Exception e) {
       System.out.println("Exception: " + e.getMessage());
       e.printStackTrace();
@@ -98,7 +97,7 @@ public class $className extends TestCase {
     try {
       System.out.println("Begin query: " + "$fname");
 
-      qt.addFile("$qf.getCanonicalPath()");
+      qt.addFile("$qfilesMap.get($fname)");
 
       qt.init("$fname");
       ASTNode tree = qt.parseQuery("$fname");
diff --git a/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java b/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java
index 218236f81c..712e5c1d0f 100644
--- a/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java
+++ b/shims/src/common/java/org/apache/hadoop/fs/ProxyFileSystem.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.Shell;
 
 /****************************************************************
  * A FileSystem that can serve a given scheme/authority using some
@@ -42,14 +43,28 @@ public class ProxyFileSystem extends FilterFileSystem {
   protected String realAuthority;
   protected URI realUri;
 
-  
+
 
   private Path swizzleParamPath(Path p) {
-    return new Path (realScheme, realAuthority, p.toUri().getPath());
+    String pathUriString = p.toUri().toString();
+    if (Shell.WINDOWS) {
+      // Some of the file paths (Files with partition option) in HDFS '='
+      // but Windows file path doesn't support '=' so replace it with special string.
+      pathUriString = pathUriString.replaceAll("=", "------");
+    }
+    URI newPathUri = URI.create(pathUriString);
+    return new Path (realScheme, realAuthority, newPathUri.getPath());
   }
 
   private Path swizzleReturnPath(Path p) {
-    return new Path (myScheme, myAuthority, p.toUri().getPath());
+    String pathUriString = p.toUri().toString();
+    if (Shell.WINDOWS) {
+      // Revert back the special string '------' with '=' when we do the reverse conversion
+      // from Windows path to HDFS
+      pathUriString = pathUriString.replaceAll("------", "=");
+    }
+    URI newPathUri = URI.create(pathUriString);
+    return new Path (myScheme, myAuthority, newPathUri.getPath());
   }
 
   private FileStatus swizzleFileStatus(FileStatus orig, boolean isParam) {
@@ -66,14 +81,14 @@ private FileStatus swizzleFileStatus(FileStatus orig, boolean isParam) {
   public ProxyFileSystem() {
     throw new RuntimeException ("Unsupported constructor");
   }
-  
+
   public ProxyFileSystem(FileSystem fs) {
     throw new RuntimeException ("Unsupported constructor");
   }
 
   /**
    * Create a proxy file system for fs.
-   * 
+   *
    * @param fs FileSystem to create proxy for
    * @param myUri URI to use as proxy. Only the scheme and authority from
    *              this are used right now
@@ -158,7 +173,7 @@ public boolean setReplication(Path src, short replication) throws IOException {
   public boolean rename(Path src, Path dst) throws IOException {
     return super.rename(swizzleParamPath(src), swizzleParamPath(dst));
   }
-  
+
   @Override
   public boolean delete(Path f, boolean recursive) throws IOException {
     return super.delete(swizzleParamPath(f), recursive);
@@ -167,8 +182,8 @@ public boolean delete(Path f, boolean recursive) throws IOException {
   @Override
   public boolean deleteOnExit(Path f) throws IOException {
     return super.deleteOnExit(swizzleParamPath(f));
-  }    
-    
+  }
+
   @Override
   public FileStatus[] listStatus(Path f) throws IOException {
     FileStatus[] orig = super.listStatus(swizzleParamPath(f));
@@ -178,7 +193,7 @@ public FileStatus[] listStatus(Path f) throws IOException {
     }
     return ret;
   }
-  
+
   @Override
   public Path getHomeDirectory() {
     return swizzleReturnPath(super.getHomeDirectory());
@@ -188,12 +203,12 @@ public Path getHomeDirectory() {
   public void setWorkingDirectory(Path newDir) {
     super.setWorkingDirectory(swizzleParamPath(newDir));
   }
-  
+
   @Override
   public Path getWorkingDirectory() {
     return swizzleReturnPath(super.getWorkingDirectory());
   }
-  
+
   @Override
   public boolean mkdirs(Path f, FsPermission permission) throws IOException {
     return super.mkdirs(swizzleParamPath(f), permission);
@@ -206,14 +221,14 @@ public void copyFromLocalFile(boolean delSrc, Path src, Path dst)
   }
 
   @Override
-  public void copyFromLocalFile(boolean delSrc, boolean overwrite, 
+  public void copyFromLocalFile(boolean delSrc, boolean overwrite,
                                 Path[] srcs, Path dst)
     throws IOException {
     super.copyFromLocalFile(delSrc, overwrite, srcs, swizzleParamPath(dst));
   }
-  
+
   @Override
-  public void copyFromLocalFile(boolean delSrc, boolean overwrite, 
+  public void copyFromLocalFile(boolean delSrc, boolean overwrite,
                                 Path src, Path dst)
     throws IOException {
     super.copyFromLocalFile(delSrc, overwrite, src, swizzleParamPath(dst));
@@ -251,7 +266,7 @@ public FileStatus getFileStatus(Path f) throws IOException {
   public FileChecksum getFileChecksum(Path f) throws IOException {
     return super.getFileChecksum(swizzleParamPath(f));
   }
-  
+
   @Override
   public void setOwner(Path p, String username, String groupname
       ) throws IOException {
@@ -270,4 +285,4 @@ public void setPermission(Path p, FsPermission permission
     super.setPermission(swizzleParamPath(p), permission);
   }
 }
-  
+
diff --git a/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java b/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
index b28bf4e7f6..ed1e04725a 100644
--- a/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
+++ b/shims/src/common/java/org/apache/hadoop/fs/ProxyLocalFileSystem.java
@@ -18,19 +18,17 @@
 
 package org.apache.hadoop.fs;
 
-import java.io.*;
+import java.io.IOException;
 import java.net.URI;
-import java.net.URISyntaxException;
 
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.util.Progressable;
+import org.apache.hadoop.util.Shell;
 
 /****************************************************************
  * A Proxy for LocalFileSystem
  *
  * Serves uri's corresponding to 'pfile:///' namespace with using
- * a LocalFileSystem 
+ * a LocalFileSystem
  *****************************************************************/
 
 public class ProxyLocalFileSystem extends FilterFileSystem {
@@ -50,10 +48,21 @@ public void initialize(URI name, Configuration conf) throws IOException {
     // create a proxy for the local filesystem
     // the scheme/authority serving as the proxy is derived
     // from the supplied URI
-
     String scheme = name.getScheme();
+    String nameUriString = name.toString();
+    if (Shell.WINDOWS) {
+      // Replace the encoded backward slash with forward slash
+      // Remove the windows drive letter
+      // replace the '=' with special string '------' to handle the unsupported char '=' in windows.
+      nameUriString = nameUriString.replaceAll("%5C", "/")
+          .replaceFirst("/[c-zC-Z]:", "/")
+          .replaceFirst("^[c-zC-Z]:", "")
+          .replaceAll("=", "------");
+      name = URI.create(nameUriString);
+    }
+
     String authority = name.getAuthority() != null ? name.getAuthority() : "";
-    String proxyUriString = name + "://" + authority + "/";
+    String proxyUriString = nameUriString + "://" + authority + "/";
     fs = new ProxyFileSystem(localFs, URI.create(proxyUriString));
 
     fs.initialize(name, conf);
diff --git a/testutils/hadoop.cmd b/testutils/hadoop.cmd
new file mode 100644
index 0000000000..c31d0d9927
--- /dev/null
+++ b/testutils/hadoop.cmd
@@ -0,0 +1,252 @@
+@echo off
+@rem Licensed to the Apache Software Foundation (ASF) under one or more
+@rem contributor license agreements.  See the NOTICE file distributed with
+@rem this work for additional information regarding copyright ownership.
+@rem The ASF licenses this file to You under the Apache License, Version 2.0
+@rem (the "License"); you may not use this file except in compliance with
+@rem the License.  You may obtain a copy of the License at
+@rem
+@rem     http://www.apache.org/licenses/LICENSE-2.0
+@rem
+@rem Unless required by applicable law or agreed to in writing, software
+@rem distributed under the License is distributed on an "AS IS" BASIS,
+@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+@rem See the License for the specific language governing permissions and
+@rem limitations under the License.
+
+
+@rem The Hadoop command script
+@rem
+@rem Environment Variables
+@rem
+@rem   JAVA_HOME        The java implementation to use.  Overrides JAVA_HOME.
+@rem
+@rem   HADOOP_CLASSPATH Extra Java CLASSPATH entries.
+@rem
+@rem   HADOOP_HEAPSIZE  The maximum amount of heap to use, in MB.
+@rem                    Default is 1000.
+@rem
+@rem   HADOOP_OPTS      Extra Java runtime options.
+@rem
+@rem   HADOOP_NAMENODE_OPTS       These options are added to HADOOP_OPTS
+@rem   HADOOP_CLIENT_OPTS         when the respective command is run.
+@rem   HADOOP_{COMMAND}_OPTS etc  HADOOP_JT_OPTS applies to JobTracker
+@rem                              for e.g.  HADOOP_CLIENT_OPTS applies to
+@rem                              more than one command (fs, dfs, fsck,
+@rem                              dfsadmin etc)
+@rem
+@rem   HADOOP_CONF_DIR  Alternate conf dir. Default is ${HADOOP_HOME}/conf.
+@rem
+@rem   HADOOP_ROOT_LOGGER The root appender. Default is INFO,console
+@rem
+
+if not defined HADOOP_BIN_PATH ( 
+  set HADOOP_BIN_PATH=%~dp0
+)
+
+if "%HADOOP_BIN_PATH:~-1%" == "\" (
+  set HADOOP_BIN_PATH=%HADOOP_BIN_PATH:~0,-1%
+)
+call :updatepath %HADOOP_BIN_PATH%
+
+set BIN=%~dp0
+for %%i in (%BIN%.) do (
+  set BIN=%%~dpi
+)
+if "%BIN:~-1%" == "\" (
+  set BIN=%BIN:~0,-1%
+)
+
+
+@rem
+@rem setup java environment variables
+@rem
+
+if not defined JAVA_HOME (
+  echo Error: JAVA_HOME is not set.
+  goto :eof
+)
+
+if not exist %JAVA_HOME%\bin\java.exe (
+  echo Error: JAVA_HOME is incorrectly set.
+  goto :eof
+)
+
+set JAVA=%JAVA_HOME%\bin\java
+set JAVA_HEAP_MAX=-Xmx1000m
+
+@rem
+@rem check envvars which might override default args
+@rem
+
+if defined HADOOP_HEAPSIZE (
+  set JAVA_HEAP_MAX=-Xmx%HADOOP_HEAPSIZE%m
+)
+
+@rem
+@rem CLASSPATH initially contains %HADOOP_CONF_DIR%
+@rem
+
+set CLASSPATH=%HADOOP_CONF_DIR%
+set CLASSPATH=%CLASSPATH%;%JAVA_HOME%\lib\tools.jar
+
+
+set BUILD_ROOT="%BIN%"/build
+
+
+if not defined HIVE_HADOOP_TEST_CLASSPATH (
+  @echo Error: HIVE_HADOOP_TEST_CLASSPATH not defined.
+  goto :eof
+)
+
+
+
+set CLASSPATH=%CLASSPATH%;%HIVE_HADOOP_TEST_CLASSPATH%
+if not exist %BUILD_ROOT%/test/hadoop/logs (
+  mkdir %BUILD_ROOT%/test/hadoop/logs
+)
+
+@rem
+@rem add user-specified CLASSPATH last
+@rem
+
+if defined HADOOP_CLASSPATH (
+  set CLASSPATH=%CLASSPATH%;%HADOOP_CLASSPATH%
+)
+
+if not defined HADOOP_LOG_DIR (
+  set HADOOP_LOG_DIR=%BUILD_ROOT%\logs
+)
+
+if not defined HADOOP_LOGFILE (
+  set HADOOP_LOGFILE=hadoop.log
+)
+
+if not defined HADOOP_ROOT_LOGGER (
+  set HADOOP_ROOT_LOGGER=INFO,console,DRFA
+)
+
+@rem
+@rem default policy file for service-level authorization
+@rem
+
+if not defined HADOOP_POLICYFILE (
+  set HADOOP_POLICYFILE=hadoop-policy.xml
+)
+set HADOOP_OPTS=%HADOOP_OPTS% -Dhadoop.log.dir=%HADOOP_LOG_DIR%
+set HADOOP_OPTS=%HADOOP_OPTS% -Dhadoop.log.file=%HADOOP_LOGFILE%
+set HADOOP_OPTS=%HADOOP_OPTS% -Dhadoop.root.logger=%HADOOP_ROOT_LOGGER%
+
+if defined HADOOP_PREFIX (
+  set HADOOP_OPTS=%HADOOP_OPTS% -Dhadoop.home.dir=%HADOOP_PREFIX%
+)
+
+if defined HADOOP_IDENT_STRING (
+  set HADOOP_OPTS=%$HADOOP_OPTS% -Dhadoop.id.str=%HADOOP_IDENT_STRING%
+)
+
+if defined JAVA_LIBRARY_PATH (
+  set HADOOP_OPTS=%HADOOP_OPTS% -Djava.library.path=%JAVA_LIBRARY_PATH%
+)
+set HADOOP_OPTS=%HADOOP_OPTS% -Dhadoop.policy.file=%HADOOP_POLICYFILE%
+
+@rem Disable ipv6 as it can cause issues
+set HADOOP_OPTS=%HADOOP_OPTS% -Djava.net.preferIPv4Stack=true
+
+:main
+  setlocal enabledelayedexpansion
+  
+  set hadoop-command=%1
+  if not defined hadoop-command (
+      goto print_usage
+  )
+  
+  call :make_command_arguments %*
+  set corecommands=fs version jar distcp daemonlog archive
+  for %%i in ( %corecommands% ) do (
+    if %hadoop-command% == %%i set corecommand=true  
+  )
+  if defined corecommand (
+    call :%hadoop-command%
+  ) else (
+    set CLASSPATH=%CLASSPATH%;%CD%
+    set CLASS=%hadoop-command%
+  )
+  call %JAVA% %JAVA_HEAP_MAX% %HADOOP_OPTS% -classpath %CLASSPATH% %CLASS% %hadoop-command-arguments%
+
+  goto :eof
+
+:version 
+  set CLASS=org.apache.hadoop.util.VersionInfo
+  set HADOOP_OPTS=%HADOOP_OPTS% %HADOOP_CLIENT_OPTS%
+  goto :eof
+
+:jar
+  set CLASS=org.apache.hadoop.util.RunJar
+  goto :eof
+
+:distcp
+  set CLASS=org.apache.hadoop.tools.DistCp
+  set CLASSPATH=%CLASSPATH%;%TOOL_PATH%
+  set HADOOP_OPTS=%HADOOP_OPTS% %HADOOP_CLIENT_OPTS%
+  goto :eof
+
+:daemonlog
+  set CLASS=org.apache.hadoop.log.LogLevel
+  set HADOOP_OPTS=%HADOOP_OPTS% %HADOOP_CLIENT_OPTS%
+  goto :eof
+
+:archive
+  set CLASS=org.apache.hadoop.tools.HadoopArchives
+  set CLASSPATH=%CLASSPATH%;%TOOL_PATH%
+  set HADOOP_OPTS=%HADOOP_OPTS% %HADOOP_CLIENT_OPTS%
+  goto :eof
+
+:updatepath
+  set path_to_add=%*
+  set current_path_comparable=%path:(x86)=%
+  set current_path_comparable=%current_path_comparable: =_%
+  set path_to_add_comparable=%path_to_add:(x86)=%
+  set path_to_add_comparable=%path_to_add_comparable: =_%
+  for %%i in ( %current_path_comparable% ) do (
+    if /i "%%i" == "%path_to_add_comparable%" (
+      set path_to_add_exist=true
+    )
+  )
+  set system_path_comparable=
+  set path_to_add_comparable=
+  if not defined path_to_add_exist path=%path_to_add%;%path%
+  set path_to_add=
+  goto :eof
+
+:make_command_arguments
+  if "%2" == "" goto :eof
+  set _count=0
+  set _shift=1
+  for %%i in (%*) do (
+    set /a _count=!_count!+1
+    if !_count! GTR %_shift% ( 
+  if not defined _arguments (
+   set _arguments=%%i
+  ) else (
+          set _arguments=!_arguments! %%i
+  )
+    )
+  )
+  
+  set hadoop-command-arguments=%_arguments%
+  goto :eof
+
+:print_usage
+  @echo Usage: hadoop COMMAND
+  @echo where COMMAND is one of:
+  @echo   fs                   run a generic filesystem user client
+  @echo   version              print the version
+  @echo   jar ^<jar^>            run a jar file
+  @echo.
+  @echo   distcp ^<srcurl^> ^<desturl^> copy file or directories recursively
+  @echo   archive -archiveName NAME ^<src^>* ^<dest^> create a hadoop archive
+  @echo   daemonlog            get/set the log level for each daemon
+  @echo Most commands print help when invoked w/o parameters.
+
+endlocal
