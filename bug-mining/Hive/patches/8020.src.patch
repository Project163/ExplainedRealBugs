diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
index a7f1574102..285b099f89 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.apache.hive.iceberg.org.apache.orc.OrcConf;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.FileScanTask;
 import org.apache.iceberg.PartitionField;
@@ -42,6 +43,7 @@
 import org.apache.iceberg.io.CloseableIterator;
 import org.apache.iceberg.io.InputFile;
 import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;
+import org.apache.iceberg.orc.VectorizedReadUtils;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
 
 /**
@@ -98,6 +100,11 @@ public static <D> CloseableIterable<D> reader(InputFile inputFile, FileScanTask
     try {
       switch (format) {
         case ORC:
+          // Need to turn positional schema evolution off since we use column name based schema evolution for projection
+          // and Iceberg will make a mapping between the file schema and the current reading schema.
+          job.setBoolean(OrcConf.FORCE_POSITIONAL_EVOLUTION.getHiveConfName(), false);
+          VectorizedReadUtils.handleIcebergProjection(inputFile, task, job);
+
           InputSplit split = new OrcSplit(path, null, task.start(), task.length(), (String[]) null, null,
               false, false, com.google.common.collect.Lists.newArrayList(), 0, task.length(), path.getParent(), null);
           RecordReader<NullWritable, VectorizedRowBatch> recordReader = null;
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/ExpressionToOrcSearchArgument.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/ExpressionToOrcSearchArgument.java
new file mode 100644
index 0000000000..1b44e6fd68
--- /dev/null
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/ExpressionToOrcSearchArgument.java
@@ -0,0 +1,296 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.orc;
+
+import java.math.BigDecimal;
+import java.sql.Date;
+import java.sql.Timestamp;
+import java.time.Instant;
+import java.time.LocalDate;
+import java.util.Map;
+import java.util.Set;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;
+import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hive.iceberg.org.apache.orc.TypeDescription;
+import org.apache.iceberg.expressions.Bound;
+import org.apache.iceberg.expressions.BoundPredicate;
+import org.apache.iceberg.expressions.Expression;
+import org.apache.iceberg.expressions.ExpressionVisitors;
+import org.apache.iceberg.expressions.Literal;
+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;
+import org.apache.iceberg.types.Type;
+import org.apache.iceberg.types.Type.TypeID;
+
+/**
+ * Copy of ExpressionOrcSearchArgument from iceberg/orc module to provide java type compatibility between:
+ * org.apache.hadoop.hive.ql.io.sarg.SearchArgument and
+ * org.apache.hive.iceberg.org.apache.orc.storage.ql.io.sarg.SearchArgument
+ */
+class ExpressionToOrcSearchArgument extends ExpressionVisitors.BoundVisitor<ExpressionToOrcSearchArgument.Action> {
+
+  static SearchArgument convert(Expression expr, TypeDescription readSchema) {
+    Map<Integer, String> idToColumnName = ORCSchemaUtil.idToOrcName(ORCSchemaUtil.convert(readSchema));
+    SearchArgument.Builder builder = SearchArgumentFactory.newBuilder();
+    ExpressionVisitors.visit(expr, new ExpressionToOrcSearchArgument(builder, idToColumnName)).invoke();
+    return builder.build();
+  }
+
+  // Currently every predicate in ORC requires a PredicateLeaf.Type field which is not available for these Iceberg types
+  private static final Set<TypeID> UNSUPPORTED_TYPES = ImmutableSet.of(TypeID.BINARY, TypeID.FIXED, TypeID.UUID,
+      TypeID.STRUCT, TypeID.MAP, TypeID.LIST);
+
+  private SearchArgument.Builder builder;
+  private Map<Integer, String> idToColumnName;
+
+  private ExpressionToOrcSearchArgument(SearchArgument.Builder builder, Map<Integer, String> idToColumnName) {
+    this.builder = builder;
+    this.idToColumnName = idToColumnName;
+  }
+
+  @Override
+  public Action alwaysTrue() {
+    return () -> this.builder.literal(TruthValue.YES);
+  }
+
+  @Override
+  public Action alwaysFalse() {
+    return () -> this.builder.literal(TruthValue.NO);
+  }
+
+  @Override
+  public Action not(Action child) {
+    return () -> {
+      this.builder.startNot();
+      child.invoke();
+      this.builder.end();
+    };
+  }
+
+  @Override
+  public Action and(Action leftChild, Action rightChild) {
+    return () -> {
+      this.builder.startAnd();
+      leftChild.invoke();
+      rightChild.invoke();
+      this.builder.end();
+    };
+  }
+
+  @Override
+  public Action or(Action leftChild, Action rightChild) {
+    return () -> {
+      this.builder.startOr();
+      leftChild.invoke();
+      rightChild.invoke();
+      this.builder.end();
+    };
+  }
+
+  @Override
+  public <T> Action isNull(Bound<T> expr) {
+    return () -> this.builder.isNull(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()));
+  }
+
+  @Override
+  public <T> Action notNull(Bound<T> expr) {
+    return () -> this.builder.startNot().isNull(idToColumnName.get(expr.ref().fieldId()),
+        type(expr.ref().type())).end();
+  }
+
+  @Override
+  public <T> Action isNaN(Bound<T> expr) {
+    return () -> this.builder.equals(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()),
+        literal(expr.ref().type(), getNaNForType(expr.ref().type())));
+  }
+
+  private Object getNaNForType(Type type) {
+    switch (type.typeId()) {
+      case FLOAT:
+        return Float.NaN;
+      case DOUBLE:
+        return Double.NaN;
+      default:
+        throw new IllegalArgumentException("Cannot get NaN value for type " + type.typeId());
+    }
+  }
+
+  @Override
+  public <T> Action notNaN(Bound<T> expr) {
+    return () -> {
+      this.builder.startOr();
+      isNull(expr).invoke();
+      this.builder.startNot();
+      isNaN(expr).invoke();
+      this.builder.end(); // end NOT
+      this.builder.end(); // end OR
+    };
+  }
+
+  @Override
+  public <T> Action lt(Bound<T> expr, Literal<T> lit) {
+    return () -> this.builder.lessThan(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()),
+        literal(expr.ref().type(), lit.value()));
+  }
+
+  @Override
+  public <T> Action ltEq(Bound<T> expr, Literal<T> lit) {
+    return () -> this.builder.lessThanEquals(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()),
+        literal(expr.ref().type(), lit.value()));
+  }
+
+  @Override
+  public <T> Action gt(Bound<T> expr, Literal<T> lit) {
+    // ORC SearchArguments do not have a greaterThan predicate, so we use not(lessThanOrEquals)
+    // e.g. x > 5 => not(x <= 5)
+    return () -> this.builder.startNot().lessThanEquals(idToColumnName.get(expr.ref().fieldId()),
+        type(expr.ref().type()), literal(expr.ref().type(), lit.value())).end();
+  }
+
+  @Override
+  public <T> Action gtEq(Bound<T> expr, Literal<T> lit) {
+    // ORC SearchArguments do not have a greaterThanOrEquals predicate, so we use not(lessThan)
+    // e.g. x >= 5 => not(x < 5)
+    return () -> this.builder.startNot().lessThan(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()),
+        literal(expr.ref().type(), lit.value())).end();
+  }
+
+  @Override
+  public <T> Action eq(Bound<T> expr, Literal<T> lit) {
+    return () -> this.builder.equals(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()),
+        literal(expr.ref().type(), lit.value()));
+  }
+
+  @Override
+  public <T> Action notEq(Bound<T> expr, Literal<T> lit) {
+    // NOTE: ORC uses SQL semantics for Search Arguments, so an expression like
+    // `col != 1` will exclude rows where col is NULL along with rows where col = 1
+    // In contrast, Iceberg's Expressions will keep rows with NULL values
+    // So the equivalent ORC Search Argument for an Iceberg Expression `col != x`
+    // is `col IS NULL OR col != x`
+    return () -> {
+      this.builder.startOr();
+      isNull(expr).invoke();
+      this.builder.startNot();
+      eq(expr, lit).invoke();
+      this.builder.end(); // end NOT
+      this.builder.end(); // end OR
+    };
+  }
+
+  @Override
+  public <T> Action in(Bound<T> expr, Set<T> literalSet) {
+    return () -> this.builder.in(idToColumnName.get(expr.ref().fieldId()), type(expr.ref().type()),
+        literalSet.stream().map(lit -> literal(expr.ref().type(), lit)).toArray(Object[]::new));
+  }
+
+  @Override
+  public <T> Action notIn(Bound<T> expr, Set<T> literalSet) {
+    // NOTE: ORC uses SQL semantics for Search Arguments, so an expression like
+    // `col NOT IN {1}` will exclude rows where col is NULL along with rows where col = 1
+    // In contrast, Iceberg's Expressions will keep rows with NULL values
+    // So the equivalent ORC Search Argument for an Iceberg Expression `col NOT IN {x}`
+    // is `col IS NULL OR col NOT IN {x}`
+    return () -> {
+      this.builder.startOr();
+      isNull(expr).invoke();
+      this.builder.startNot();
+      in(expr, literalSet).invoke();
+      this.builder.end(); // end NOT
+      this.builder.end(); // end OR
+    };
+  }
+
+  @Override
+  public <T> Action startsWith(Bound<T> expr, Literal<T> lit) {
+    // Cannot push down STARTS_WITH operator to ORC, so return TruthValue.YES_NO_NULL which signifies
+    // that this predicate cannot help with filtering
+    return () -> this.builder.literal(TruthValue.YES_NO_NULL);
+  }
+
+  @Override
+  public <T> Action predicate(BoundPredicate<T> pred) {
+    if (UNSUPPORTED_TYPES.contains(pred.ref().type().typeId())) {
+      // Cannot push down predicates for types which cannot be represented in PredicateLeaf.Type, so return
+      // TruthValue.YES_NO_NULL which signifies that this predicate cannot help with filtering
+      return () -> this.builder.literal(TruthValue.YES_NO_NULL);
+    } else {
+      return super.predicate(pred);
+    }
+  }
+
+  @FunctionalInterface
+  interface Action {
+    void invoke();
+  }
+
+  private PredicateLeaf.Type type(Type icebergType) {
+    switch (icebergType.typeId()) {
+      case BOOLEAN:
+        return PredicateLeaf.Type.BOOLEAN;
+      case INTEGER:
+      case LONG:
+      case TIME:
+        return PredicateLeaf.Type.LONG;
+      case FLOAT:
+      case DOUBLE:
+        return PredicateLeaf.Type.FLOAT;
+      case DATE:
+        return PredicateLeaf.Type.DATE;
+      case TIMESTAMP:
+        return PredicateLeaf.Type.TIMESTAMP;
+      case STRING:
+        return PredicateLeaf.Type.STRING;
+      case DECIMAL:
+        return PredicateLeaf.Type.DECIMAL;
+      default:
+        throw new UnsupportedOperationException("Type " + icebergType + " not supported in ORC SearchArguments");
+    }
+  }
+
+  private <T> Object literal(Type icebergType, T icebergLiteral) {
+    switch (icebergType.typeId()) {
+      case BOOLEAN:
+      case LONG:
+      case TIME:
+      case DOUBLE:
+        return icebergLiteral;
+      case INTEGER:
+        return ((Integer) icebergLiteral).longValue();
+      case FLOAT:
+        return ((Float) icebergLiteral).doubleValue();
+      case STRING:
+        return icebergLiteral.toString();
+      case DATE:
+        return Date.valueOf(LocalDate.ofEpochDay((Integer) icebergLiteral));
+      case TIMESTAMP:
+        long microsFromEpoch = (Long) icebergLiteral;
+        return Timestamp.from(Instant.ofEpochSecond(Math.floorDiv(microsFromEpoch, 1_000_000),
+            Math.floorMod(microsFromEpoch, 1_000_000) * 1_000));
+      case DECIMAL:
+        return new HiveDecimalWritable(HiveDecimal.create((BigDecimal) icebergLiteral, false));
+      default:
+        throw new UnsupportedOperationException("Type " + icebergType + " not supported in ORC SearchArguments");
+    }
+  }
+}
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java
new file mode 100644
index 0000000000..2068d5bc89
--- /dev/null
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/orc/VectorizedReadUtils.java
@@ -0,0 +1,92 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.apache.iceberg.orc;
+
+import java.io.IOException;
+import org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg;
+import org.apache.hadoop.hive.ql.plan.TableScanDesc;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hive.iceberg.org.apache.orc.Reader;
+import org.apache.hive.iceberg.org.apache.orc.TypeDescription;
+import org.apache.iceberg.FileScanTask;
+import org.apache.iceberg.Schema;
+import org.apache.iceberg.expressions.Binder;
+import org.apache.iceberg.expressions.Expression;
+import org.apache.iceberg.io.InputFile;
+import org.apache.iceberg.mapping.MappingUtil;
+
+/**
+ * Utilities that rely on Iceberg code from org.apache.iceberg.orc package.
+ */
+public class VectorizedReadUtils {
+
+  private VectorizedReadUtils() {
+
+  }
+
+  /**
+   * Adjusts the jobConf so that column reorders and renames that might have happened since this ORC file was written
+   * are properly mapped to the schema of the original file.
+   * @param inputFile - the original ORC file - this needs to be accessed to retrieve the original schema for mapping
+   * @param task - Iceberg task - required for
+   * @param job - JobConf instance to adjust
+   * @throws IOException - errors relating to accessing the ORC file
+   */
+  public static void handleIcebergProjection(InputFile inputFile, FileScanTask task, JobConf job)
+      throws IOException {
+    Reader orcFileReader = ORC.newFileReader(inputFile, job);
+
+    try {
+      // We need to map with the current (i.e. current Hive table columns) full schema (without projections),
+      // as OrcInputFormat will take care of the projections by the use of an include boolean array
+      Schema currentSchema = task.spec().schema();
+      TypeDescription fileSchema = orcFileReader.getSchema();
+
+      TypeDescription readOrcSchema;
+      if (ORCSchemaUtil.hasIds(fileSchema)) {
+        readOrcSchema = ORCSchemaUtil.buildOrcProjection(currentSchema, fileSchema);
+      } else {
+        TypeDescription typeWithIds =
+            ORCSchemaUtil.applyNameMapping(fileSchema, MappingUtil.create(currentSchema));
+        readOrcSchema = ORCSchemaUtil.buildOrcProjection(currentSchema, typeWithIds);
+      }
+
+      job.set(ColumnProjectionUtils.ICEBERG_ORC_SCHEMA_STRING, readOrcSchema.toString());
+
+      // Predicate pushdowns needs to be adjusted too in case of column renames, we let Iceberg generate this into job
+      if (task.residual() != null) {
+        Expression boundFilter = Binder.bind(currentSchema.asStruct(), task.residual(), false);
+
+        // Note the use of the unshaded version of this class here (required for SARG deseralization later)
+        org.apache.hadoop.hive.ql.io.sarg.SearchArgument sarg =
+            ExpressionToOrcSearchArgument.convert(boundFilter, readOrcSchema);
+        if (sarg != null) {
+          job.unset(TableScanDesc.FILTER_EXPR_CONF_STR);
+          job.unset(ConvertAstToSearchArg.SARG_PUSHDOWN);
+
+          job.set(ConvertAstToSearchArg.SARG_PUSHDOWN, ConvertAstToSearchArg.sargToKryo(sarg));
+        }
+      }
+    } finally {
+      orcFileReader.close();
+    }
+  }
+}
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
index 8585fb643a..cf8a893dd3 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
@@ -582,9 +582,6 @@ public void testInsertFromSelect() throws IOException {
 
   @Test
   public void testAlterChangeColumn() throws IOException {
-    // TODO: remove once vectorized execution can handle column reorders/renames
-    Assume.assumeTrue(!isVectorized);
-
     testTables.createTable(shell, "customers", HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA,
         fileFormat, HiveIcebergStorageHandlerTestUtils.CUSTOMER_RECORDS);
 
@@ -609,6 +606,121 @@ public void testAlterChangeColumn() throws IOException {
     Assert.assertArrayEquals(new Object[]{0L, "Brown"}, result.get(0));
     Assert.assertArrayEquals(new Object[]{1L, "Green"}, result.get(1));
     Assert.assertArrayEquals(new Object[]{2L, "Pink"}, result.get(2));
+
+  }
+
+  // Tests CHANGE COLUMN feature similarly like above, but with a more complex schema, aimed to verify vectorized
+  // reads support the feature properly, also combining with other schema changes e.g. ADD COLUMN
+  @Test
+  public void testSchemaEvolutionOnVectorizedReads() throws Exception {
+    // Currently only ORC, but in the future this should run against each fileformat with vectorized read support.
+    Assume.assumeTrue("Vectorized reads only.", isVectorized);
+
+    Schema orderSchema = new Schema(
+        optional(1, "order_id", Types.IntegerType.get()),
+        optional(2, "customer_first_name", Types.StringType.get()),
+        optional(3, "customer_last_name", Types.StringType.get()),
+        optional(4, "quantity", Types.IntegerType.get()),
+        optional(5, "price", Types.IntegerType.get()),
+        optional(6, "item", Types.StringType.get())
+    );
+
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(orderSchema)
+        .add(1, "Doctor", "Strange", 100, 3, "apple")
+        .add(2, "Tony", "Stark", 150, 2, "apple")
+        .add(3, "Tony", "Stark", 200, 6, "orange")
+        .add(4, "Steve", "Rogers", 100, 8, "banana")
+        .add(5, "Doctor", "Strange", 800, 7, "orange")
+        .add(6, "Thor", "Odinson", 650, 3, "apple")
+        .build();
+
+    testTables.createTable(shell, "orders", orderSchema, fileFormat, records);
+
+    // Reorder columns and rename one column
+    shell.executeStatement("ALTER TABLE orders CHANGE COLUMN " +
+        "customer_first_name customer_first_name string AFTER customer_last_name");
+    shell.executeStatement("ALTER TABLE orders CHANGE COLUMN " +
+        "quantity quantity int AFTER price");
+    shell.executeStatement("ALTER TABLE orders CHANGE COLUMN " +
+        "item fruit string");
+    List<Object[]> result = shell.executeStatement("SELECT customer_first_name, customer_last_name, SUM(quantity) " +
+        "FROM orders where price >= 3 group by customer_first_name, customer_last_name");
+
+    assertQueryResult(result, 4,
+        "Doctor", "Strange", 900L,
+        "Steve", "Rogers", 100L,
+        "Thor", "Odinson", 650L,
+        "Tony", "Stark", 200L);
+
+
+    // Adding a new column (will end up as last col of the schema)
+    shell.executeStatement("ALTER TABLE orders ADD COLUMNS (nickname string)");
+    shell.executeStatement("INSERT INTO orders VALUES (7, 'Romanoff', 'Natasha', 3, 250, 'apple', 'Black Widow')");
+    result = shell.executeStatement("SELECT customer_first_name, customer_last_name, nickname, SUM(quantity) " +
+        " FROM orders where price >= 3 group by customer_first_name, customer_last_name, nickname");
+    assertQueryResult(result, 5,
+        "Doctor", "Strange", null, 900L,
+        "Natasha", "Romanoff", "Black Widow", 250L,
+        "Steve", "Rogers", null, 100L,
+        "Thor", "Odinson", null, 650L,
+        "Tony", "Stark", null, 200L);
+
+    // Re-order newly added column (move it away from being the last column)
+    shell.executeStatement("ALTER TABLE orders CHANGE COLUMN fruit fruit string AFTER nickname");
+    result = shell.executeStatement("SELECT customer_first_name, customer_last_name, nickname, fruit, SUM(quantity) " +
+        " FROM orders where price >= 3 and fruit < 'o' group by customer_first_name, customer_last_name, nickname, " +
+        "fruit");
+    assertQueryResult(result, 4,
+        "Doctor", "Strange", null, "apple", 100L,
+        "Natasha", "Romanoff", "Black Widow", "apple", 250L,
+        "Steve", "Rogers", null, "banana", 100L,
+        "Thor", "Odinson", null, "apple", 650L);
+
+    // Rename newly added column (+ reading with different file includes)
+    shell.executeStatement("ALTER TABLE orders CHANGE COLUMN nickname nick string");
+    result = shell.executeStatement("SELECT customer_first_name, nick, SUM(quantity) " +
+        " FROM orders where fruit < 'o'and nick IS NOT NULL group by customer_first_name, nick");
+    assertQueryResult(result, 1, "Natasha", "Black Widow", 250L);
+
+    // Re-order between different types
+    shell.executeStatement("ALTER TABLE orders CHANGE COLUMN order_id order_id int AFTER customer_first_name");
+    result = shell.executeStatement("SELECT customer_first_name, nick, SUM(quantity), MIN(order_id) " +
+        " FROM orders where fruit < 'o'and nick IS NOT NULL group by customer_first_name, nick");
+    assertQueryResult(result, 1, "Natasha", "Black Widow", 250L, 7);
+
+    // Drop columns via REPLACE COLUMNS (as per query there's also a column re-order, but it should be handled as no-op)
+    shell.executeStatement("ALTER TABLE orders REPLACE COLUMNS (" +
+        "order_id int, customer_last_name string, nick string, quantity int, fruit string)");
+
+    result = shell.executeStatement("DESCRIBE orders");
+    assertQueryResult(result, 5,
+        "customer_last_name", "string", "from deserializer",
+        "order_id", "int", "from deserializer",
+        "quantity", "int", "from deserializer",
+        "nick", "string", "from deserializer",
+        "fruit", "string", "from deserializer");
+    result = shell.executeStatement("SELECT * FROM orders ORDER BY order_id");
+    assertQueryResult(result, 7,
+        "Strange", 1, 100, null, "apple",
+        "Stark", 2, 150, null, "apple",
+        "Stark", 3, 200, null, "orange",
+        "Rogers", 4, 100, null, "banana",
+        "Strange", 5, 800, null, "orange",
+        "Odinson", 6, 650, null, "apple",
+        "Romanoff", 7, 250, "Black Widow", "apple");
+
+  }
+
+  private static void assertQueryResult(List<Object[]> result, int expectedCount, Object... expectedRows) {
+    Assert.assertEquals(expectedCount, result.size());
+    int colCount = expectedRows.length / expectedCount;
+    for (int i = 0; i < expectedCount; ++i) {
+      Object[] rows = new Object[colCount];
+      for (int j = 0; j < colCount; ++j) {
+        rows[j] = expectedRows[i * colCount + j];
+      }
+      Assert.assertArrayEquals(rows, result.get(i));
+    }
   }
 
   @Test
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index 85dc1246c0..384a2f271b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -585,7 +585,14 @@ private static String[] extractNeededColNames(
   }
 
   static String getNeededColumnNamesString(Configuration conf) {
-    return conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR);
+    String icebergOrcSchema = conf.get(ColumnProjectionUtils.ICEBERG_ORC_SCHEMA_STRING);
+
+    if (icebergOrcSchema != null) {
+      final String columnNameDelimiter = conf.get(serdeConstants.COLUMN_NAME_DELIMITER, String.valueOf(SerDeUtils.COMMA));
+      return String.join(columnNameDelimiter, TypeDescription.fromString(icebergOrcSchema).getFieldNames());
+    } else {
+      return conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR);
+    }
   }
 
   static String getSargColumnIDsString(Configuration conf) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
index 8cb9af787b..de6fdb26a8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hive.ql.io.AcidUtils;
 import org.apache.hadoop.hive.ql.io.InputFormatChecker;
 import org.apache.hadoop.hive.ql.io.SelfDescribingInputFormatInterface;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapred.FileInputFormat;
 import org.apache.hadoop.mapred.FileSplit;
@@ -76,8 +77,10 @@ static class VectorizedOrcRecordReader
        * Do we have schema on read in the configuration variables?
        */
       int dataColumns = rbCtx.getDataColumnCount();
-      TypeDescription schema =
-          OrcInputFormat.getDesiredRowTypeDescr(conf, false, dataColumns);
+      String icebergOrcSchema = conf.get(ColumnProjectionUtils.ICEBERG_ORC_SCHEMA_STRING);
+      TypeDescription schema = icebergOrcSchema == null ?
+          OrcInputFormat.getDesiredRowTypeDescr(conf, false, dataColumns) :
+          TypeDescription.fromString(icebergOrcSchema);
       if (schema == null) {
         schema = file.getSchema();
         // Even if the user isn't doing schema evolution, cut the schema
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
index 6dff5332e7..aace83e9de 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java
@@ -56,6 +56,7 @@ public final class ColumnProjectionUtils {
   private static final String READ_NESTED_COLUMN_PATH_CONF_STR_DEFAULT = "";
   private static final boolean READ_ALL_COLUMNS_DEFAULT = true;
   private static final Joiner CSV_JOINER = Joiner.on(",").skipNulls();
+  public static final String ICEBERG_ORC_SCHEMA_STRING = "hive.iceberg.orc.schema.string";
 
   /**
    * @deprecated for backwards compatibility with &lt;= 0.12, use setReadAllColumns
