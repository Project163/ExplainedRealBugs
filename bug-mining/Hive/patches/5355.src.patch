diff --git a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
index 70b0d6429c..0da55334ec 100644
--- a/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
+++ b/contrib/src/java/org/apache/hadoop/hive/contrib/util/typedbytes/TypedBytesWritableOutput.java
@@ -24,6 +24,7 @@
 import java.io.IOException;
 import java.util.Arrays;
 import java.util.Map;
+import java.util.Set;
 
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
@@ -212,7 +213,10 @@ public void writeMap(MapWritable mw) throws IOException {
 
   public void writeSortedMap(SortedMapWritable smw) throws IOException {
     out.writeMapHeader(smw.size());
-    for (Map.Entry<WritableComparable, Writable> entry : smw.entrySet()) {
+    // Make sure it compiles with both Hadoop 2 and Hadoop 3.
+    Set<Map.Entry<? extends WritableComparable, Writable>> entrySet =
+      (Set<Map.Entry<? extends WritableComparable, Writable>>)((Object)smw.entrySet());
+    for (Map.Entry<? extends WritableComparable, Writable> entry : entrySet) {
       write(entry.getKey());
       write(entry.getValue());
     }
diff --git a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index e6af00ded4..21a18f870a 100644
--- a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -1158,9 +1158,10 @@ private boolean isEncryptionEnabled(DFSClient client, Configuration conf) {
       try {
         DFSClient.class.getMethod("isHDFSEncryptionEnabled");
       } catch (NoSuchMethodException e) {
-        // the method is available since Hadoop-2.7.1
-        // if we run with an older Hadoop, check this ourselves
-        return !conf.getTrimmed(DFSConfigKeys.DFS_ENCRYPTION_KEY_PROVIDER_URI, "").isEmpty();
+        // The method is available since Hadoop-2.7.1; if we run with an older Hadoop, check this
+        // ourselves. Note that this setting is in turn deprected in newer versions of Hadoop, but
+        // we only care for it in the older versions; so we will hardcode the old name here.
+        return !conf.getTrimmed("dfs.encryption.key.provider.uri", "").isEmpty();
       }
       return client.isHDFSEncryptionEnabled();
     }
