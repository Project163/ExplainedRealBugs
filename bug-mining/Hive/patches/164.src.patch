diff --git a/CHANGES.txt b/CHANGES.txt
index dc8dc70b2d..c313c38943 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -183,6 +183,9 @@ Trunk - Unreleased
     HIVE-514. Partition key names should be case insensitive in alter table add
     partition statement. (Prasad Chakka via zshao)
 
+    HIVE-523. FIx PartitionPruner not to fetch all partitions at once.
+    (Prasad Chakka via zshao)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
index f70822640e..a2024c982c 100755
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
@@ -20,11 +20,13 @@
 
 import java.io.FileNotFoundException;
 import java.io.IOException;
-import java.net.URI;
+import java.util.ArrayList;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
@@ -37,7 +39,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.common.FileUtils;
 
 /**
  * This class represents a warehouse where data of Hive tables is stored
@@ -170,6 +171,43 @@ public static String makePartName(Map<String, String> spec) throws MetaException
     }
     return suffixBuf.toString();
   }
+  
+  static final Pattern pat = Pattern.compile("([^/]+)=([^/]+)");
+  public static LinkedHashMap<String, String> makeSpecFromName(String name) throws MetaException {
+    LinkedHashMap<String, String> partSpec = new LinkedHashMap<String, String>();
+    if (name == null || name.isEmpty()) {
+      throw new MetaException("Partition name is invalid. " + name);
+    }
+    List<String[]> kvs = new ArrayList<String[]>();
+    Path currPath = new Path(name);
+    do {
+      String component = currPath.getName();
+      Matcher m = pat.matcher(component);
+      if (m.matches()) {
+        String k = m.group(1);
+        String v = m.group(2);
+
+        if (partSpec.containsKey(k)) {
+          throw new MetaException("Partition name is invalid. Key " + k + " defined at two levels");
+        }
+        String[] kv = new String[2];
+        kv[0] = k;
+        kv[1] = v;
+        kvs.add(kv);
+      }
+      else {
+        throw new MetaException("Partition name is invalid. " + name);
+      }
+      currPath = currPath.getParent();
+    } while(currPath != null && !currPath.getName().isEmpty());
+    
+    // reverse the list since we checked the part from leaf dir to table's base dir
+    for(int i = kvs.size(); i > 0; i--) { 
+      partSpec.put(kvs.get(i-1)[0], kvs.get(i-1)[1]);
+    }
+    return partSpec;
+  }
+
 
   public Path getPartitionPath(String dbName, String tableName, LinkedHashMap<String, String> pm) throws MetaException {
     return new Path(getDefaultTablePath(dbName, tableName), makePartName(pm)); 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
index 44e1022087..c74d91f0e9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java
@@ -20,10 +20,15 @@
 
 import java.util.*;
 
+import org.apache.hadoop.hive.metastore.MetaStoreUtils;
+import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.*;
+import org.apache.hadoop.hive.ql.metadata.Hive;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.metadata.Partition;
+import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.plan.exprNodeColumnDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeConstantDesc;
 import org.apache.hadoop.hive.ql.plan.exprNodeDesc;
@@ -469,54 +474,57 @@ public PrunedPartitionList prune() throws HiveException {
     try {
       StructObjectInspector rowObjectInspector = (StructObjectInspector)this.tab.getDeserializer().getObjectInspector();
       Object[] rowWithPart = new Object[2];
-      
-      for(Partition part: Hive.get().getPartitions(this.tab)) {
-        // Set all the variables here
-        LinkedHashMap<String, String> partSpec = part.getSpec();
-
-        // Create the row object
-        ArrayList<String> partNames = new ArrayList<String>();
-        ArrayList<String> partValues = new ArrayList<String>();
-        ArrayList<ObjectInspector> partObjectInspectors = new ArrayList<ObjectInspector>();
-        for(Map.Entry<String,String>entry : partSpec.entrySet()) {
-          partNames.add(entry.getKey());
-          partValues.add(entry.getValue());
-          partObjectInspectors.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); 
-        }
-        StructObjectInspector partObjectInspector = ObjectInspectorFactory.getStandardStructObjectInspector(partNames, partObjectInspectors);
-        
-        rowWithPart[1] = partValues;
-        ArrayList<StructObjectInspector> ois = new ArrayList<StructObjectInspector>(2);
-        ois.add(rowObjectInspector);
-        ois.add(partObjectInspector);
-        StructObjectInspector rowWithPartObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(ois);
-        
-        // evaluate the expression tree
-        if (this.prunerExpr != null) {
-          ExprNodeEvaluator evaluator = ExprNodeEvaluatorFactory.get(this.prunerExpr);
-          ObjectInspector evaluateResultOI = evaluator.initialize(rowWithPartObjectInspector);
-          Object evaluateResultO = evaluator.evaluate(rowWithPart);
-          Boolean r = (Boolean) ((PrimitiveObjectInspector)evaluateResultOI).getPrimitiveJavaObject(evaluateResultO);
-          LOG.trace("prune result for partition " + partSpec + ": " + r);
-          if (Boolean.TRUE.equals(r)) {
-            LOG.debug("retained partition: " + partSpec);
-            true_parts.add(part);
-          } 
-          else if (Boolean.FALSE.equals(r)) {
-            LOG.trace("pruned partition: " + partSpec);
-          } 
-          else {
-            LOG.debug("unknown partition: " + partSpec);
-            unkn_parts.add(part);
+
+      if(tab.isPartitioned()) {
+        for(String partName: Hive.get().getPartitionNames(MetaStoreUtils.DEFAULT_DATABASE_NAME, tab.getName(), (short) -1)) {
+          // Set all the variables here
+          LinkedHashMap<String, String> partSpec = Warehouse.makeSpecFromName(partName);
+          LOG.debug("part name: " + partName);
+          // Create the row object
+          ArrayList<String> partNames = new ArrayList<String>();
+          ArrayList<String> partValues = new ArrayList<String>();
+          ArrayList<ObjectInspector> partObjectInspectors = new ArrayList<ObjectInspector>();
+          for(Map.Entry<String,String>entry : partSpec.entrySet()) {
+            partNames.add(entry.getKey());
+            partValues.add(entry.getValue());
+            partObjectInspectors.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector); 
+          }
+          StructObjectInspector partObjectInspector = ObjectInspectorFactory.getStandardStructObjectInspector(partNames, partObjectInspectors);
+
+          rowWithPart[1] = partValues;
+          ArrayList<StructObjectInspector> ois = new ArrayList<StructObjectInspector>(2);
+          ois.add(rowObjectInspector);
+          ois.add(partObjectInspector);
+          StructObjectInspector rowWithPartObjectInspector = ObjectInspectorFactory.getUnionStructObjectInspector(ois);
+
+          // evaluate the expression tree
+          if (this.prunerExpr != null) {
+            ExprNodeEvaluator evaluator = ExprNodeEvaluatorFactory.get(this.prunerExpr);
+            ObjectInspector evaluateResultOI = evaluator.initialize(rowWithPartObjectInspector);
+            Object evaluateResultO = evaluator.evaluate(rowWithPart);
+            Boolean r = (Boolean) ((PrimitiveObjectInspector)evaluateResultOI).getPrimitiveJavaObject(evaluateResultO);
+            LOG.trace("prune result for partition " + partSpec + ": " + r);
+            if (Boolean.FALSE.equals(r)) {
+              LOG.trace("pruned partition: " + partSpec);
+            } else {
+              Partition part = Hive.get().getPartition(tab, partSpec, Boolean.FALSE);
+              if (Boolean.TRUE.equals(r)) {
+                LOG.debug("retained partition: " + partSpec);
+                true_parts.add(part);
+              } else {             
+                LOG.debug("unknown partition: " + partSpec);
+                unkn_parts.add(part);
+              }
+            }
+          } else {
+            // is there is no parition pruning, all of them are needed
+            true_parts.add(Hive.get().getPartition(tab, partSpec, Boolean.FALSE));
           }
         }
-        else {
-          // is there is no parition pruning, all of them are needed
-          true_parts.add(part);
-        }
+      } else {
+        true_parts.addAll(Hive.get().getPartitions(tab));
       }
-    }
-    catch (Exception e) {
+    } catch (Exception e) {
       throw new HiveException(e);
     }
 
diff --git a/ql/src/test/results/clientpositive/nullgroup3.q.out b/ql/src/test/results/clientpositive/nullgroup3.q.out
index 506cf160b9..9389279cc2 100644
--- a/ql/src/test/results/clientpositive/nullgroup3.q.out
+++ b/ql/src/test/results/clientpositive/nullgroup3.q.out
@@ -49,9 +49,9 @@ STAGE PLANS:
 
 
 query: select count(1) from tstparttbl
-Input: default/tstparttbl/ds=2008-04-09
 Input: default/tstparttbl/ds=2008-04-08
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/94234533/163827544.10000
+Input: default/tstparttbl/ds=2008-04-09
+Output: file:/Users/pchakka/workspace/oshive/build/ql/tmp/1242360371/10000
 500
 query: DROP TABLE tstparttbl2
 query: CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -104,9 +104,9 @@ STAGE PLANS:
 
 
 query: select count(1) from tstparttbl2
-Input: default/tstparttbl2/ds=2008-04-09
 Input: default/tstparttbl2/ds=2008-04-08
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/726239292/465785380.10000
+Input: default/tstparttbl2/ds=2008-04-09
+Output: file:/Users/pchakka/workspace/oshive/build/ql/tmp/361074813/10000
 0
 query: DROP TABLE tstparttbl
 query: CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -159,9 +159,9 @@ STAGE PLANS:
 
 
 query: select count(1) from tstparttbl
-Input: default/tstparttbl/ds=2008-04-09
 Input: default/tstparttbl/ds=2008-04-08
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/115674074/219051439.10000
+Input: default/tstparttbl/ds=2008-04-09
+Output: file:/Users/pchakka/workspace/oshive/build/ql/tmp/813815719/10000
 500
 query: DROP TABLE tstparttbl2
 query: CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE
@@ -214,9 +214,9 @@ STAGE PLANS:
 
 
 query: select count(1) from tstparttbl2
-Input: default/tstparttbl2/ds=2008-04-09
 Input: default/tstparttbl2/ds=2008-04-08
-Output: /data/users/njain/hive5/hive5/ql/../build/ql/tmp/702539461/654313985.10000
+Input: default/tstparttbl2/ds=2008-04-09
+Output: file:/Users/pchakka/workspace/oshive/build/ql/tmp/18671157/10000
 0
 query: DROP TABLE tstparttbl
 query: DROP TABLE tstparttbl2
