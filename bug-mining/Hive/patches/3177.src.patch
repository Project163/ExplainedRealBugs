diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
index 9557efd795..3fd7ea9aaf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/SparkHashTableSinkOperator.java
@@ -18,7 +18,6 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.BufferedOutputStream;
-import java.io.IOException;
 import java.io.ObjectOutputStream;
 import java.io.OutputStream;
 import java.io.Serializable;
@@ -81,8 +80,18 @@ public void closeOp(boolean abort) throws HiveException {
       if (mapJoinTables == null || mapJoinTables.length < tag
           || mapJoinTables[tag] == null) {
         LOG.debug("mapJoinTable is null");
+      } else if (abort) {
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Aborting, skip dumping side-table for tag: " + tag);
+        }
       } else {
-        flushToFile(mapJoinTables[tag], tag);
+        String method = PerfLogger.SPARK_FLUSH_HASHTABLE + getName();
+        perfLogger.PerfLogBegin(CLASS_NAME, method);
+        try {
+          flushToFile(mapJoinTables[tag], tag);
+        } finally {
+          perfLogger.PerfLogEnd(CLASS_NAME, method);
+        }
       }
       super.closeOp(abort);
     } catch (HiveException e) {
@@ -93,8 +102,7 @@ public void closeOp(boolean abort) throws HiveException {
   }
 
   protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
-      byte tag) throws IOException, HiveException {
-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.SPARK_FLUSH_HASHTABLE + this.getName());
+      byte tag) throws Exception {
     MapredLocalWork localWork = getExecContext().getLocalWork();
     BucketMapJoinContext mapJoinCtx = localWork.getBucketMapjoinContext();
     Path inputPath = getExecContext().getCurrentInputPath();
@@ -136,7 +144,6 @@ protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
     htsOperator.console.printInfo(Utilities.now() + "\tDump the side-table for tag: " + tag
       + " with group count: " + tableContainer.size() + " into file: " + path);
     // get the hashtable file and path
-    // get the hashtable file and path
     OutputStream os = null;
     ObjectOutputStream out = null;
     try {
@@ -144,6 +151,18 @@ protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
       out = new ObjectOutputStream(new BufferedOutputStream(os, 4096));
       MapJoinTableContainerSerDe mapJoinTableSerde = htsOperator.mapJoinTableSerdes[tag];
       mapJoinTableSerde.persist(out, tableContainer);
+      FileStatus status = fs.getFileStatus(path);
+      htsOperator.console.printInfo(Utilities.now() + "\tUploaded 1 File to: " + path
+        + " (" + status.getLen() + " bytes)");
+    } catch (Exception e) {
+      // Failed to dump the side-table, remove the partial file
+      try {
+        fs.delete(path, false);
+      } catch (Exception ex) {
+        LOG.warn("Got exception in deleting partial side-table dump for tag: "
+          + tag + ", file " + path, ex);
+      }
+      throw e;
     } finally {
       if (out != null) {
         out.close();
@@ -152,10 +171,6 @@ protected void flushToFile(MapJoinPersistableTableContainer tableContainer,
       }
     }
     tableContainer.clear();
-    FileStatus status = fs.getFileStatus(path);
-    htsOperator.console.printInfo(Utilities.now() + "\tUploaded 1 File to: " + path
-      + " (" + status.getLen() + " bytes)");
-    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.SPARK_FLUSH_HASHTABLE + this.getName());
   }
 
   public void setTag(byte tag) {
