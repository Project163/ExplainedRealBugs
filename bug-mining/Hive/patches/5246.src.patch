diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapOptionsProcessor.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapOptionsProcessor.java
index e680dfcb70..6463b305ce 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapOptionsProcessor.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapOptionsProcessor.java
@@ -67,7 +67,7 @@ public class LlapOptionsProcessor {
   public static final String OPTION_SLIDER_PLACEMENT = "slider-placement";
   public static final String OPTION_SLIDER_DEFAULT_KEYTAB = "slider-default-keytab";
   public static final String OPTION_OUTPUT_DIR = "output";
-
+  public static final String OPTION_START = "startImmediately";
 
   public static class LlapOptions {
     private final int instances;
@@ -84,12 +84,13 @@ public static class LlapOptions {
     private final String javaPath;
     private final String llapQueueName;
     private final String logger;
+    private final boolean isStarting;
+    private final String output;
 
     public LlapOptions(String name, int instances, String directory, int executors, int ioThreads,
                        long cache, long size, long xmx, String jars, boolean isHbase,
                        @Nonnull Properties hiveconf, String javaPath, String llapQueueName,
-                       String logger)
-        throws ParseException {
+                       String logger, boolean isStarting, String output) throws ParseException {
       if (instances <= 0) {
         throw new ParseException("Invalid configuration: " + instances
             + " (should be greater than 0)");
@@ -108,6 +109,12 @@ public LlapOptions(String name, int instances, String directory, int executors,
       this.javaPath = javaPath;
       this.llapQueueName = llapQueueName;
       this.logger = logger;
+      this.isStarting = isStarting;
+      this.output = output;
+    }
+
+    public String getOutput() {
+      return output;
     }
 
     public String getName() {
@@ -165,6 +172,10 @@ public String getLlapQueueName() {
     public String getLogger() {
       return logger;
     }
+
+    public boolean isStarting() {
+      return isStarting;
+    }
   }
 
   protected static final Logger l4j = LoggerFactory.getLogger(LlapOptionsProcessor.class.getName());
@@ -265,6 +276,10 @@ public LlapOptionsProcessor() {
     options.addOption(OptionBuilder.hasArg().withArgName(OPTION_IO_THREADS)
         .withLongOpt(OPTION_IO_THREADS).withDescription("executor per instance").create('t'));
 
+    options.addOption(OptionBuilder.hasArg(false).withArgName(OPTION_START)
+        .withLongOpt(OPTION_START).withDescription("immediately start the cluster")
+        .create('z'));
+
     // [-H|--help]
     options.addOption(new Option("H", "help", false, "Print help information"));
   }
@@ -293,7 +308,10 @@ public LlapOptions processOptions(String argv[]) throws ParseException, IOExcept
     final long cache = parseSuffixed(commandLine.getOptionValue(OPTION_CACHE, "-1"));
     final long size = parseSuffixed(commandLine.getOptionValue(OPTION_SIZE, "-1"));
     final long xmx = parseSuffixed(commandLine.getOptionValue(OPTION_XMX, "-1"));
-    final boolean isHbase = Boolean.parseBoolean(commandLine.getOptionValue(OPTION_AUXHBASE, "true"));
+    final boolean isHbase = Boolean.parseBoolean(
+        commandLine.getOptionValue(OPTION_AUXHBASE, "true"));
+    final boolean doStart = commandLine.hasOption(OPTION_START);
+    final String output = commandLine.getOptionValue(OPTION_OUTPUT_DIR, null);
 
     final String queueName = commandLine.getOptionValue(OPTION_LLAP_QUEUE,
         HiveConf.ConfVars.LLAP_DAEMON_QUEUE_NAME.getDefaultValue());
@@ -323,7 +341,7 @@ public LlapOptions processOptions(String argv[]) throws ParseException, IOExcept
     // loglevel, chaosmonkey & args are parsed by the python processor
 
     return new LlapOptions(name, instances, directory, executors, ioThreads, cache,
-        size, xmx, jars, isHbase, hiveconf, javaHome, queueName, logger);
+        size, xmx, jars, isHbase, hiveconf, javaHome, queueName, logger, doStart, output);
   }
 
   private void printUsage() {
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
index f0e7b6b5ff..a93d53a5ed 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.llap.cli;
 
+import java.io.File;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStreamWriter;
@@ -25,6 +26,8 @@
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.net.URL;
+import java.nio.file.Paths;
+import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -36,7 +39,6 @@
 import java.util.Set;
 import java.util.concurrent.Callable;
 import java.util.concurrent.CompletionService;
-import java.util.concurrent.Executor;
 import java.util.concurrent.ExecutorCompletionService;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
@@ -51,6 +53,12 @@
 import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos;
 import org.apache.hadoop.hive.llap.tezplugins.LlapTezUtils;
 import org.apache.hadoop.registry.client.binding.RegistryUtils;
+import org.apache.slider.client.SliderClient;
+import org.apache.slider.common.params.ActionCreateArgs;
+import org.apache.slider.common.params.ActionDestroyArgs;
+import org.apache.slider.common.params.ActionFreezeArgs;
+import org.apache.slider.common.params.ActionInstallPackageArgs;
+import org.apache.slider.core.exceptions.UnknownApplicationInstanceException;
 import org.apache.tez.dag.api.TezConfiguration;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -76,14 +84,16 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.yarn.conf.YarnConfiguration;
+import org.apache.hadoop.yarn.exceptions.YarnException;
 import org.eclipse.jetty.server.ssl.SslSocketConnector;
+import org.joda.time.DateTime;
+import org.json.JSONException;
 import org.json.JSONObject;
 
 import com.google.common.base.Preconditions;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 
 public class LlapServiceDriver {
-
   protected static final Logger LOG = LoggerFactory.getLogger(LlapServiceDriver.class.getName());
 
   private static final String[] DEFAULT_AUX_CLASSES = new String[] {
@@ -91,6 +101,7 @@ public class LlapServiceDriver {
   private static final String HBASE_SERDE_CLASS = "org.apache.hadoop.hive.hbase.HBaseSerDe";
   private static final String[] NEEDED_CONFIGS = LlapDaemonConfiguration.DAEMON_CONFIGS;
   private static final String[] OPTIONAL_CONFIGS = LlapDaemonConfiguration.SSL_DAEMON_CONFIGS;
+  private static final String OUTPUT_DIR_PREFIX = "llap-slider-";
 
   // This is not a config that users set in hive-site. It's only use is to share information
   // between the java component of the service driver and the python component.
@@ -111,7 +122,7 @@ public static void main(String[] args) throws Exception {
     LOG.info("LLAP service driver invoked with arguments={}", args);
     int ret = 0;
     try {
-      new LlapServiceDriver().run(args);
+      ret = new LlapServiceDriver().run(args);
     } catch (Throwable t) {
       System.err.println("Failed: " + t.getMessage());
       t.printStackTrace();
@@ -173,7 +184,7 @@ public String getName() {
     }
   }
 
-  private void run(String[] args) throws Exception {
+  private int run(String[] args) throws Exception {
     LlapOptionsProcessor optionsProcessor = new LlapOptionsProcessor();
     final LlapOptions options = optionsProcessor.processOptions(args);
 
@@ -181,7 +192,7 @@ private void run(String[] args) throws Exception {
 
     if (options == null) {
       // help
-      return;
+      return 1;
     }
 
     // Working directory.
@@ -201,6 +212,7 @@ private void run(String[] args) throws Exception {
             new ThreadFactoryBuilder().setNameFormat("llap-pkg-%d").build());
     final CompletionService<Void> asyncRunner = new ExecutorCompletionService<Void>(executor);
 
+    int rc = 0;
     try {
 
       // needed so that the file is actually loaded into configuration.
@@ -332,7 +344,8 @@ private void run(String[] args) throws Exception {
       }
 
       Path home = new Path(System.getenv("HIVE_HOME"));
-      Path scripts = new Path(new Path(new Path(home, "scripts"), "llap"), "bin");
+      Path scriptParent = new Path(new Path(home, "scripts"), "llap");
+      Path scripts = new Path(scriptParent, "bin");
 
       if (!lfs.exists(home)) {
         throw new Exception("Unable to find HIVE_HOME:" + home);
@@ -505,29 +518,7 @@ public Void call() throws Exception {
             }
           }
           createLlapDaemonConfig(lfs, confPath, conf, propsDirectOptions, options.getConfig());
-
-          // logger can be a resource stream or a real file (cannot use copy)
-          InputStream loggerContent = logger.openStream();
-          IOUtils.copyBytes(loggerContent,
-              lfs.create(new Path(confPath, "llap-daemon-log4j2.properties"), true), conf, true);
-
-          String metricsFile = LlapConstants.LLAP_HADOOP_METRICS2_PROPERTIES_FILE;
-          URL metrics2 = conf.getResource(metricsFile);
-          if (metrics2 == null) {
-            LOG.warn(LlapConstants.LLAP_HADOOP_METRICS2_PROPERTIES_FILE + " cannot be found."
-                + " Looking for " + LlapConstants.HADOOP_METRICS2_PROPERTIES_FILE);
-            metricsFile = LlapConstants.HADOOP_METRICS2_PROPERTIES_FILE;
-            metrics2 = conf.getResource(metricsFile);
-          }
-          if (metrics2 != null) {
-            InputStream metrics2FileStream = metrics2.openStream();
-            IOUtils.copyBytes(metrics2FileStream,
-                lfs.create(new Path(confPath, metricsFile), true), conf, true);
-            LOG.info("Copied hadoop metrics2 properties file from " + metrics2);
-          } else {
-            LOG.warn("Cannot find " + LlapConstants.LLAP_HADOOP_METRICS2_PROPERTIES_FILE + " or "
-                + LlapConstants.HADOOP_METRICS2_PROPERTIES_FILE + " in classpath.");
-          }
+          setUpLogAndMetricConfigs(lfs, logger, confPath);
           return null;
         }
       };
@@ -546,60 +537,9 @@ public Void call() throws Exception {
         asyncResults[i] = asyncRunner.submit(asyncWork[i]);
       }
 
-      // extract configs for processing by the python fragments in Slider
-      JSONObject configs = new JSONObject();
-
-      configs.put("java.home", java_home);
-
-      configs.put(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,
-          HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB));
-      configs.put(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);
-
-      configs.put(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,
-          HiveConf.getSizeVar(conf, HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE));
-
-      configs.put(HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT.varname,
-          HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT));
-
-      configs.put(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,
-          HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB));
-
-      configs.put(ConfVars.LLAP_DAEMON_HEADROOM_MEMORY_PER_INSTANCE_MB.varname,
-        HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_HEADROOM_MEMORY_PER_INSTANCE_MB));
-
-      configs.put(ConfVars.LLAP_DAEMON_VCPUS_PER_INSTANCE.varname,
-          HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_VCPUS_PER_INSTANCE));
-
-      configs.put(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,
-          HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_NUM_EXECUTORS));
-
-      // Let YARN pick the queue name, if it isn't provided in hive-site, or via the command-line
-      if (HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME) != null) {
-        configs.put(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,
-            HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));
-      }
-
-      // Propagate the cluster name to the script.
-      String clusterHosts = HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_SERVICE_HOSTS);
-      if (!StringUtils.isEmpty(clusterHosts) && clusterHosts.startsWith("@")
-          && clusterHosts.length() > 1) {
-        configs.put(CONFIG_CLUSTER_NAME, clusterHosts.substring(1));
-      }
-
-      configs.put(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,
-          conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1));
-
-      configs.put(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_VCORES,
-          conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_VCORES, -1));
-
-      long maxDirect = (xmx > 0 && cache > 0 && xmx < cache * 1.25) ? (long) (cache * 1.25) : -1;
-      configs.put("max_direct_memory", Long.toString(maxDirect));
-
-      FSDataOutputStream os = lfs.create(new Path(tmpDir, "config.json"));
-      OutputStreamWriter w = new OutputStreamWriter(os);
-      configs.write(w);
-      w.close();
-      os.close();
+      // TODO: need to move from Python to Java for the rest of the script.
+      JSONObject configs = createConfigJson(containerSize, cache, xmx, java_home);
+      writeConfigJson(tmpDir, lfs, configs);
 
       if (LOG.isDebugEnabled()) {
         LOG.debug("Config generation took " + (System.nanoTime() - t0) + " ns");
@@ -612,15 +552,127 @@ public Void call() throws Exception {
           LOG.debug(asyncWork[i].getName() + " waited for " + (t2 - t1) + " ns");
         }
       }
+      if (options.isStarting()) {
+        String version = System.getenv("HIVE_VERSION");
+        if (version == null || version.isEmpty()) {
+          version = DateTime.now().toString("ddMMMyyyy");
+        }
+
+        String outputDir = options.getOutput();
+        Path packageDir = null;
+        if (outputDir == null) {
+          outputDir = OUTPUT_DIR_PREFIX + version;
+          packageDir = new Path(Paths.get(".").toAbsolutePath().toString(),
+              OUTPUT_DIR_PREFIX + version);
+        } else {
+          packageDir = new Path(outputDir);
+        }
+        rc = runPackagePy(args, tmpDir, scriptParent, version, outputDir);
+        if (rc == 0) {
+          LlapSliderUtils.startCluster(conf, options.getName(), "llap-" + version + ".zip",
+              packageDir, HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));
+        }
+      } else {
+        rc = 0;
+      }
     } finally {
       executor.shutdown();
       lfs.close();
       fs.close();
     }
 
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Exiting successfully");
+    if (rc == 0) {
+      if (LOG.isDebugEnabled()) {
+        LOG.debug("Exiting successfully");
+      }
+    } else {
+      LOG.info("Exiting with rc = " + rc);
+    }
+    return rc;
+  }
+
+  private int runPackagePy(String[] args, Path tmpDir, Path scriptParent,
+      String version, String outputDir) throws IOException, InterruptedException {
+    Path scriptPath = new Path(new Path(scriptParent, "slider"), "package.py");
+    List<String> scriptArgs = new ArrayList<>(args.length + 7);
+    scriptArgs.add("python");
+    scriptArgs.add(scriptPath.toString());
+    scriptArgs.add("--input");
+    scriptArgs.add(tmpDir.toString());
+    scriptArgs.add("--output");
+    scriptArgs.add(outputDir);
+    scriptArgs.add("--javaChild");
+    for (String arg : args) {
+      scriptArgs.add(arg);
+    }
+    LOG.debug("Calling package.py via: " + scriptArgs);
+    ProcessBuilder builder = new ProcessBuilder(scriptArgs);
+    builder.redirectError(ProcessBuilder.Redirect.INHERIT);
+    builder.redirectOutput(ProcessBuilder.Redirect.INHERIT);
+    builder.environment().put("HIVE_VERSION", version);
+    return builder.start().waitFor();
+  }
+
+  private void writeConfigJson(Path tmpDir, final FileSystem lfs,
+      JSONObject configs) throws IOException, JSONException {
+    FSDataOutputStream os = lfs.create(new Path(tmpDir, "config.json"));
+    OutputStreamWriter w = new OutputStreamWriter(os);
+    configs.write(w);
+    w.close();
+    os.close();
+  }
+
+  private JSONObject createConfigJson(long containerSize, long cache, long xmx,
+      String java_home) throws JSONException {
+    // extract configs for processing by the python fragments in Slider
+    JSONObject configs = new JSONObject();
+
+    configs.put("java.home", java_home);
+
+    configs.put(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname,
+        HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB));
+    configs.put(ConfVars.LLAP_DAEMON_YARN_CONTAINER_MB.varname, containerSize);
+
+    configs.put(HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE.varname,
+        HiveConf.getSizeVar(conf, HiveConf.ConfVars.LLAP_IO_MEMORY_MAX_SIZE));
+
+    configs.put(HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT.varname,
+        HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_ALLOCATOR_DIRECT));
+
+    configs.put(ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB.varname,
+        HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_MEMORY_PER_INSTANCE_MB));
+
+    configs.put(ConfVars.LLAP_DAEMON_HEADROOM_MEMORY_PER_INSTANCE_MB.varname,
+      HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_HEADROOM_MEMORY_PER_INSTANCE_MB));
+
+    configs.put(ConfVars.LLAP_DAEMON_VCPUS_PER_INSTANCE.varname,
+        HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_VCPUS_PER_INSTANCE));
+
+    configs.put(ConfVars.LLAP_DAEMON_NUM_EXECUTORS.varname,
+        HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_NUM_EXECUTORS));
+
+    // Let YARN pick the queue name, if it isn't provided in hive-site, or via the command-line
+    if (HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME) != null) {
+      configs.put(ConfVars.LLAP_DAEMON_QUEUE_NAME.varname,
+          HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_QUEUE_NAME));
+    }
+
+    // Propagate the cluster name to the script.
+    String clusterHosts = HiveConf.getVar(conf, ConfVars.LLAP_DAEMON_SERVICE_HOSTS);
+    if (!StringUtils.isEmpty(clusterHosts) && clusterHosts.startsWith("@")
+        && clusterHosts.length() > 1) {
+      configs.put(CONFIG_CLUSTER_NAME, clusterHosts.substring(1));
     }
+
+    configs.put(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,
+        conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, -1));
+
+    configs.put(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_VCORES,
+        conf.getInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_VCORES, -1));
+
+    long maxDirect = (xmx > 0 && cache > 0 && xmx < cache * 1.25) ? (long) (cache * 1.25) : -1;
+    configs.put("max_direct_memory", Long.toString(maxDirect));
+    return configs;
   }
 
   private Set<String> downloadPermanentFunctions(Configuration conf, Path udfDir) throws HiveException,
@@ -715,4 +767,30 @@ private void copyConfig(FileSystem lfs, Path confPath, String f) throws IOExcept
     // they will be file:// URLs
     lfs.copyFromLocalFile(new Path(conf.getResource(f).toString()), confPath);
   }
+
+  private void setUpLogAndMetricConfigs(final FileSystem lfs, final URL logger,
+      final Path confPath) throws IOException {
+    // logger can be a resource stream or a real file (cannot use copy)
+    InputStream loggerContent = logger.openStream();
+    IOUtils.copyBytes(loggerContent,
+        lfs.create(new Path(confPath, "llap-daemon-log4j2.properties"), true), conf, true);
+
+    String metricsFile = LlapConstants.LLAP_HADOOP_METRICS2_PROPERTIES_FILE;
+    URL metrics2 = conf.getResource(metricsFile);
+    if (metrics2 == null) {
+      LOG.warn(LlapConstants.LLAP_HADOOP_METRICS2_PROPERTIES_FILE + " cannot be found."
+          + " Looking for " + LlapConstants.HADOOP_METRICS2_PROPERTIES_FILE);
+      metricsFile = LlapConstants.HADOOP_METRICS2_PROPERTIES_FILE;
+      metrics2 = conf.getResource(metricsFile);
+    }
+    if (metrics2 != null) {
+      InputStream metrics2FileStream = metrics2.openStream();
+      IOUtils.copyBytes(metrics2FileStream,
+          lfs.create(new Path(confPath, metricsFile), true), conf, true);
+      LOG.info("Copied hadoop metrics2 properties file from " + metrics2);
+    } else {
+      LOG.warn("Cannot find " + LlapConstants.LLAP_HADOOP_METRICS2_PROPERTIES_FILE + " or "
+          + LlapConstants.HADOOP_METRICS2_PROPERTIES_FILE + " in classpath.");
+    }
+  }
 }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapSliderUtils.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapSliderUtils.java
new file mode 100644
index 0000000000..8342067146
--- /dev/null
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapSliderUtils.java
@@ -0,0 +1,188 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.llap.cli;
+
+import java.io.File;
+import java.io.IOException;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.yarn.exceptions.YarnException;
+import org.apache.slider.client.SliderClient;
+import org.apache.slider.common.params.ActionCreateArgs;
+import org.apache.slider.common.params.ActionDestroyArgs;
+import org.apache.slider.common.params.ActionFreezeArgs;
+import org.apache.slider.common.params.ActionInstallPackageArgs;
+import org.apache.slider.common.tools.SliderUtils;
+import org.apache.slider.core.exceptions.UnknownApplicationInstanceException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.io.Files;
+
+public class LlapSliderUtils {
+  private static final String SLIDER_GZ = "slider-agent.tar.gz";
+  private static final Logger LOG = LoggerFactory.getLogger(LlapSliderUtils.class);
+
+  public static SliderClient createSliderClient(
+      Configuration conf) throws Exception {
+    SliderClient sliderClient = new SliderClient() {
+      @Override
+      public void serviceInit(Configuration conf) throws Exception {
+        super.serviceInit(conf);
+        initHadoopBinding();
+      }
+    };
+    Configuration sliderClientConf = new Configuration(conf);
+    sliderClientConf = sliderClient.bindArgs(sliderClientConf,
+      new String[]{"help"});
+    sliderClient.init(sliderClientConf);
+    sliderClient.start();
+    return sliderClient;
+  }
+
+  public static void startCluster(Configuration conf, String name,
+      String packageName, Path packageDir, String queue) {
+    LOG.info("Starting cluster with " + name + ", "
+      + packageName + ", " + queue + ", " + packageDir);
+    SliderClient sc;
+    try {
+      sc = createSliderClient(conf);
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+    try {
+      LOG.info("Executing the freeze command");
+      ActionFreezeArgs freezeArgs = new ActionFreezeArgs();
+      freezeArgs.force = true;
+      freezeArgs.setWaittime(3600); // Wait forever (or at least for an hour).
+      try {
+        sc.actionFreeze(name, freezeArgs);
+      } catch (UnknownApplicationInstanceException ex) {
+        LOG.info("There was no old application instance to freeze");
+      }
+      LOG.info("Executing the destroy command");
+      ActionDestroyArgs destroyArg = new ActionDestroyArgs();
+      destroyArg.force = true;
+      try {
+        sc.actionDestroy(name, destroyArg);
+      } catch (UnknownApplicationInstanceException ex) {
+        LOG.info("There was no old application instance to destroy");
+      }
+      LOG.info("Executing the install command");
+      ActionInstallPackageArgs installArgs = new ActionInstallPackageArgs();
+      installArgs.name = "LLAP";
+      installArgs.packageURI = new Path(packageDir, packageName).toString();
+      installArgs.replacePkg = true;
+      sc.actionInstallPkg(installArgs);
+      LOG.info("Executing the create command");
+      ActionCreateArgs createArgs = new ActionCreateArgs();
+      createArgs.resources = new File(new Path(packageDir, "resources.json").toString());
+      createArgs.template = new File(new Path(packageDir, "appConfig.json").toString());
+      createArgs.setWaittime(3600);
+      if (queue != null) {
+        createArgs.queue = queue;
+      }
+      // See the comments in the method. SliderClient doesn't work in normal circumstances.
+      File bogusSliderFile = startSetSliderLibDir();
+      try {
+        sc.actionCreate(name, createArgs);
+      } finally {
+        endSetSliderLibDir(bogusSliderFile);
+      }
+      LOG.debug("Started the cluster via slider API");
+    } catch (YarnException | IOException e) {
+      throw new RuntimeException(e);
+    } finally {
+      try {
+        sc.close();
+      } catch (IOException e) {
+        LOG.info("Failed to close slider client", e);
+      }
+    }
+  }
+
+  public static File startSetSliderLibDir() throws IOException {
+    // TODO: this is currently required for the use of slider create API. Need SLIDER-1192.
+    File sliderJarDir = SliderUtils.findContainingJar(SliderClient.class).getParentFile();
+    File gz = new File(sliderJarDir, SLIDER_GZ);
+    if (gz.exists()) {
+      String path = sliderJarDir.getAbsolutePath();
+      LOG.info("Setting slider.libdir based on jar file location: " + path);
+      System.setProperty("slider.libdir", path);
+      return null;
+    }
+
+    // There's no gz file next to slider jars. Due to the horror that is SliderClient, we'd have
+    // to find it and copy it there. Let's try to find it. Also set slider.libdir.
+    String path = System.getProperty("slider.libdir");
+    gz = null;
+    if (path != null && !path.isEmpty()) {
+      LOG.info("slider.libdir was already set: " + path);
+      gz = new File(path, SLIDER_GZ);
+      if (!gz.exists()) {
+        gz = null;
+      }
+    }
+    if (gz == null) {
+      path = System.getenv("SLIDER_HOME");
+      if (path != null && !path.isEmpty()) {
+        gz = new File(new File(path, "lib"), SLIDER_GZ);
+        if (gz.exists()) {
+          path = gz.getParentFile().getAbsolutePath();
+          LOG.info("Setting slider.libdir based on SLIDER_HOME: " + path);
+          System.setProperty("slider.libdir", path);
+        } else {
+          gz = null;
+        }
+      }
+    }
+    if (gz == null) {
+      // This is a terrible hack trying to find slider on a typical installation. Sigh...
+      File rootDir = SliderUtils.findContainingJar(HiveConf.class)
+          .getParentFile().getParentFile().getParentFile();
+      File sliderJarDir2 = new File(new File(rootDir, "slider"), "lib");
+      if (sliderJarDir2.exists()) {
+        gz = new File(sliderJarDir2, SLIDER_GZ);
+        if (gz.exists()) {
+          path = sliderJarDir2.getAbsolutePath();
+          LOG.info("Setting slider.libdir based on guesswork: " + path);
+          System.setProperty("slider.libdir", path);
+        } else {
+          gz = null;
+        }
+      }
+    }
+    if (gz == null) {
+      throw new IOException("Cannot find " + SLIDER_GZ + ". Please ensure SLIDER_HOME is set.");
+    }
+    File newGz = new File(sliderJarDir, SLIDER_GZ);
+    LOG.info("Copying " + gz + " to " + newGz);
+    Files.copy(gz, newGz);
+    newGz.deleteOnExit();
+    return newGz;
+  }
+
+  public static void endSetSliderLibDir(File newGz) throws IOException {
+    if (newGz == null || !newGz.exists()) return;
+    LOG.info("Deleting " + newGz);
+    newGz.delete();
+  }
+}
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java
index 1b9eba6e11..b36d4ff766 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java
@@ -288,23 +288,12 @@ private SliderClient createSliderClient() throws LlapStatusCliException {
     }
 
     try {
-      sliderClient = new SliderClient() {
-        @Override
-        public void serviceInit(Configuration conf) throws Exception {
-          super.serviceInit(conf);
-          initHadoopBinding();
-        }
-      };
-      Configuration sliderClientConf = new Configuration(conf);
-      sliderClientConf = sliderClient.bindArgs(sliderClientConf,
-        new String[]{"help"});
-      sliderClient.init(sliderClientConf);
-      sliderClient.start();
-      return sliderClient;
+      sliderClient = LlapSliderUtils.createSliderClient(conf);
     } catch (Exception e) {
       throw new LlapStatusCliException(ExitCode.SLIDER_CLIENT_ERROR_CREATE_FAILED,
         "Failed to create slider client", e);
     }
+    return sliderClient;
   }
 
   private ApplicationReport getAppReport(String appName, SliderClient sliderClient,
diff --git a/llap-server/src/main/resources/package.py b/llap-server/src/main/resources/package.py
index 62a8d0885f..66648b69e6 100644
--- a/llap-server/src/main/resources/package.py
+++ b/llap-server/src/main/resources/package.py
@@ -93,11 +93,23 @@ def main(args):
 	parser.add_argument("--slider-keytab", default="")
 	parser.add_argument("--slider-principal", default="")
 	parser.add_argument("--slider-default-keytab", dest='slider_default_keytab', action='store_true')
-	parser.set_defaults(slider_default_keytab=False)
 	parser.add_argument("--slider-placement", type=int, default=4)
+	parser.set_defaults(slider_default_keytab=False)
+	parser.add_argument("--startImmediately", dest='start_immediately', action='store_true')
+	parser.add_argument("--javaChild", dest='java_child', action='store_true')
+	parser.set_defaults(start_immediately=False)
+	parser.set_defaults(java_child=False)
 	# Unneeded here for now: parser.add_argument("--hiveconf", action='append')
 	#parser.add_argument("--size") parser.add_argument("--xmx") parser.add_argument("--cache") parser.add_argument("--executors")
 	(args, unknown_args) = parser.parse_known_args(args)
+	if args.start_immediately and not args.java_child:
+		sys.exit(0)
+		return
+	if args.java_child:
+		print "%s Running as a child of LlapServiceDriver" % (strftime("%H:%M:%S", gmtime()))
+	else:
+		print "%s Running after LlapServiceDriver" % (strftime("%H:%M:%S", gmtime()))
+
 	input = args.input
 	output = args.output
 	slider_am_jvm_heapsize = max(args.slider_am_container_mb * 0.8, args.slider_am_container_mb - 1024)
@@ -160,7 +172,6 @@ def main(args):
 		shutil.rmtree(dst)
 	shutil.copytree(src, dst)
 
-
 	# Make the zip package
 	tmp = join(output, "tmp")
 	pkg = join(tmp, "package")
@@ -175,6 +186,8 @@ def main(args):
 		f.write(metainfo % vars)
 
 	os.mkdir(join(pkg, "files"))
+	print "%s Prepared the files" % (strftime("%H:%M:%S", gmtime()))
+
 	tarball = tarfile.open(join(pkg, "files", "llap-%s.tar.gz" %  version), "w:gz")
 	# recursive add + -C chdir inside
 	tarball.add(input, "")
@@ -183,6 +196,7 @@ def main(args):
 	zipped = zipfile.ZipFile(join(output, "llap-%s.zip" % version), "w")
 	zipdir(tmp, zipped)
 	zipped.close()
+	print "%s Packaged the files" % (strftime("%H:%M:%S", gmtime()))
 
 	# cleanup after making zip pkg
 	shutil.rmtree(tmp)
@@ -197,7 +211,8 @@ def main(args):
 		f.write(runner % vars)
 	os.chmod(join(output, "run.sh"), 0700)
 
-	print "Prepared %s/run.sh for running LLAP on Slider" % (output)
+	if not args.java_child:
+		print "%s Prepared %s/run.sh for running LLAP on Slider" % (strftime("%H:%M:%S", gmtime()), output)
 
 if __name__ == "__main__":
 	main(sys.argv[1:])
