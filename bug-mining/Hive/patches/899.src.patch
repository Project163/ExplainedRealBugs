diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
index bff6438cb2..c2f3bf6640 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java
@@ -219,7 +219,10 @@ public enum ErrorMsg {
   NUM_BUCKETS_CHANGE_NOT_ALLOWED(10130, "Changing the number of buckets for a " +
     "partitioned table is not allowed. It may lead to wrong results for " +
     "older partitions"),
-
+  ALTER_COMMAND_FOR_VIEWS(10131, "To alter a view you need to use the ALTER VIEW command."),
+  ALTER_COMMAND_FOR_TABLES(10132, "To alter a base table you need to use the ALTER TABLE command."),
+  ALTER_VIEW_DISALLOWED_OP(10133, "Cannot use this form of ALTER on a view"),
+  ALTER_TABLE_NON_NATIVE(10134, "ALTER TABLE cannot be used for a non-native table"),
 
   SCRIPT_INIT_ERROR(20000, "Unable to initialize custom script."),
   SCRIPT_IO_ERROR(20001, "An error occurred while reading or writing to your custom script. "
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index d3e940b183..2e7027d4e6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -975,10 +975,6 @@ private int addPartition(Hive db, AddPartitionDesc addPartitionDesc) throws Hive
 
     Table tbl = db.getTable(addPartitionDesc.getDbName(), addPartitionDesc.getTableName());
 
-    validateAlterTableType(
-      tbl, AlterTableDesc.AlterTableTypes.ADDPARTITION,
-      addPartitionDesc.getExpectView());
-
     // If the add partition was created with IF NOT EXISTS, then we should
     // not throw an error if the specified part does exist.
     Partition checkPart = db.getPartition(tbl, addPartitionDesc.getPartSpec(), false);
@@ -1038,9 +1034,6 @@ private int renamePartition(Hive db, RenamePartitionDesc renamePartitionDesc) th
 
     Table tbl = db.getTable(renamePartitionDesc.getDbName(), renamePartitionDesc.getTableName());
 
-    validateAlterTableType(
-      tbl, AlterTableDesc.AlterTableTypes.RENAMEPARTITION,
-      false);
     Partition oldPart = db.getPartition(tbl, renamePartitionDesc.getOldPartSpec(), false);
     Partition part = db.getPartition(tbl, renamePartitionDesc.getOldPartSpec(), false);
     part.setValues(renamePartitionDesc.getNewPartSpec());
@@ -1069,8 +1062,6 @@ private int touch(Hive db, AlterTableSimpleDesc touchDesc)
 
     Table tbl = db.getTable(dbName, tblName);
 
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.TOUCH);
-
     if (touchDesc.getPartSpec() == null) {
       try {
         db.alterTable(tblName, tbl);
@@ -1228,7 +1219,6 @@ private int archive(Hive db, AlterTableSimpleDesc simpleDesc,
     String tblName = simpleDesc.getTableName();
 
     Table tbl = db.getTable(dbName, tblName);
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.ARCHIVE);
 
     if (tbl.getTableType() != TableType.MANAGED_TABLE) {
       throw new HiveException("ARCHIVE can only be performed on managed tables");
@@ -1445,7 +1435,6 @@ private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)
     String tblName = simpleDesc.getTableName();
 
     Table tbl = db.getTable(dbName, tblName);
-    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.UNARCHIVE);
 
     // Means user specified a table, not a partition
     if (simpleDesc.getPartSpec() == null) {
@@ -1670,43 +1659,6 @@ private void checkArchiveProperty(int partSpecLevel,
     }
   }
 
-  private void validateAlterTableType(
-    Table tbl, AlterTableDesc.AlterTableTypes alterType) throws HiveException {
-
-    validateAlterTableType(tbl, alterType, false);
-  }
-
-  private void validateAlterTableType(
-    Table tbl, AlterTableDesc.AlterTableTypes alterType,
-    boolean expectView) throws HiveException {
-
-    if (tbl.isView()) {
-      if (!expectView) {
-        throw new HiveException("Cannot alter a view with ALTER TABLE");
-      }
-      switch (alterType) {
-      case ADDPARTITION:
-      case DROPPARTITION:
-      case RENAMEPARTITION:
-      case ADDPROPS:
-      case RENAME:
-        // allow this form
-        break;
-      default:
-        throw new HiveException(
-          "Cannot use this form of ALTER on a view");
-      }
-    } else {
-      if (expectView) {
-        throw new HiveException("Cannot alter a base table with ALTER VIEW");
-      }
-    }
-
-    if (tbl.isNonNative()) {
-      throw new HiveException("Cannot use ALTER TABLE on a non-native table");
-    }
-  }
-
   /**
    * MetastoreCheck, see if the data in the metastore matches what is on the
    * dfs. Current version checks for tables and partitions that are either
@@ -2734,8 +2686,6 @@ private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {
       }
     }
 
-    validateAlterTableType(tbl, alterTbl.getOp(), alterTbl.getExpectView());
-
     Table oldTbl = tbl.copy();
 
     if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {
@@ -3117,12 +3067,6 @@ private int dropTable(Hive db, DropTableDesc dropTbl)
       }
     } else {
       // This is actually an ALTER TABLE DROP PARTITION
-      if (tbl != null) {
-        validateAlterTableType(
-          tbl, AlterTableDesc.AlterTableTypes.DROPPARTITION,
-          dropTbl.getExpectView());
-      }
-
       List<Partition> partsToDelete = new ArrayList<Partition>();
       for (PartitionSpec partSpec : dropTbl.getPartSpecs()) {
         List<Partition> partitions = null;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
index 0813f3f2b7..31d7698e71 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java
@@ -934,6 +934,38 @@ private List<Partition> preparePartitions(
     return baseTblPartitions;
   }
 
+  private void validateAlterTableType(Table tbl, AlterTableTypes op) throws SemanticException {
+    validateAlterTableType(tbl, op, false);
+  }
+
+  private void validateAlterTableType(Table tbl, AlterTableTypes op, boolean expectView)
+      throws SemanticException {
+    if (tbl.isView()) {
+      if (!expectView) {
+        throw new SemanticException(ErrorMsg.ALTER_COMMAND_FOR_VIEWS.getMsg());
+      }
+
+      switch (op) {
+        case ADDPARTITION:
+        case DROPPARTITION:
+        case RENAMEPARTITION:
+        case ADDPROPS:
+        case RENAME:
+          // allow this form
+          break;
+        default:
+          throw new SemanticException(ErrorMsg.ALTER_VIEW_DISALLOWED_OP.getMsg(op.toString()));
+      }
+    } else {
+      if (expectView) {
+        throw new SemanticException(ErrorMsg.ALTER_COMMAND_FOR_TABLES.getMsg());
+      }
+    }
+    if (tbl.isNonNative()) {
+      throw new SemanticException(ErrorMsg.ALTER_TABLE_NON_NATIVE.getMsg(tbl.getTableName()));
+    }
+  }
+
   private void analyzeAlterTableProps(ASTNode ast, boolean expectView)
     throws SemanticException {
 
@@ -945,15 +977,7 @@ private void analyzeAlterTableProps(ASTNode ast, boolean expectView)
     alterTblDesc.setProps(mapProp);
     alterTblDesc.setOldName(tableName);
 
-    try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tableName, false);
-      if (tab != null) {
-        inputs.add(new ReadEntity(tab));
-        outputs.add(new WriteEntity(tab));
-      }
-    } catch (HiveException e) {
-      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tableName));
-    }
+    addInputsOutputsAlterTable(tableName, null, alterTblDesc);
 
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
@@ -970,7 +994,7 @@ private void analyzeAlterTableSerdeProps(ASTNode ast, String tableName,
     alterTblDesc.setOldName(tableName);
     alterTblDesc.setPartSpec(partSpec);
 
-    addInputsOutputsAlterTable(tableName, partSpec);
+    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -990,7 +1014,7 @@ private void analyzeAlterTableSerde(ASTNode ast, String tableName,
     alterTblDesc.setSerdeName(serdeName);
     alterTblDesc.setPartSpec(partSpec);
 
-    addInputsOutputsAlterTable(tableName, partSpec);
+    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -1048,31 +1072,39 @@ private void analyzeAlterTableFileFormat(ASTNode ast, String tableName,
     AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, inputFormat,
         outputFormat, serde, storageHandler, partSpec);
 
-    addInputsOutputsAlterTable(tableName, partSpec);
+    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
 
   private void addInputsOutputsAlterTable(String tableName, HashMap<String, String> partSpec)
-    throws SemanticException {
+      throws SemanticException {
+    addInputsOutputsAlterTable(tableName, partSpec, null);
+  }
+
+  private void addInputsOutputsAlterTable(String tableName, HashMap<String, String> partSpec,
+      AlterTableDesc desc) throws SemanticException {
+    Table tab = null;
     try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tableName, false);
-      if (tab != null) {
-        inputs.add(new ReadEntity(tab));
+      tab = db.getTable(db.getCurrentDatabase(), tableName, true);
+      inputs.add(new ReadEntity(tab));
 
-        if ((partSpec == null) || (partSpec.isEmpty())) {
-          outputs.add(new WriteEntity(tab));
-        }
-        else {
-          Partition part = db.getPartition(tab, partSpec, false);
-          if (part != null) {
-            outputs.add(new WriteEntity(part));
-          }
+      if ((partSpec == null) || (partSpec.isEmpty())) {
+        outputs.add(new WriteEntity(tab));
+      }
+      else {
+        Partition part = db.getPartition(tab, partSpec, false);
+        if (part != null) {
+          outputs.add(new WriteEntity(part));
         }
       }
     } catch (HiveException e) {
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tableName));
     }
+
+    if (desc != null) {
+      validateAlterTableType(tab, desc.getOp(), desc.getExpectView());
+    }
   }
 
   private void analyzeAlterTableLocation(ASTNode ast, String tableName,
@@ -1082,7 +1114,7 @@ private void analyzeAlterTableLocation(ASTNode ast, String tableName,
 
     AlterTableDesc alterTblDesc = new AlterTableDesc (tableName, newLocation, partSpec);
 
-    addInputsOutputsAlterTable(tableName, partSpec);
+    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -1132,7 +1164,7 @@ private void analyzeAlterTableProtectMode(ASTNode ast, String tableName,
           "Only protect mode NO_DROP or OFFLINE supported");
     }
 
-    addInputsOutputsAlterTable(tableName, partSpec);
+    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -1268,6 +1300,8 @@ private void analyzeAlterTableClusterSort(ASTNode ast)
     inputs.add(new ReadEntity(tab));
     outputs.add(new WriteEntity(tab));
 
+    validateAlterTableType(tab, AlterTableTypes.ADDCLUSTERSORTCOLUMN);
+
     if (ast.getChildCount() == 1) {
       // This means that we want to turn off bucketing
       AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, -1,
@@ -1738,16 +1772,8 @@ private void analyzeAlterTableRename(ASTNode ast, boolean expectView) throws Sem
     String tblName = getUnescapedName((ASTNode)ast.getChild(0));
     AlterTableDesc alterTblDesc = new AlterTableDesc(tblName,
         getUnescapedName((ASTNode)ast.getChild(1)), expectView);
-    try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tblName, false);
-      if (tab != null) {
-        inputs.add(new ReadEntity(tab));
-        outputs.add(new WriteEntity(tab));
-      }
-    } catch (HiveException e) {
-      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
-    }
 
+    addInputsOutputsAlterTable(tblName, null, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -1782,16 +1808,7 @@ private void analyzeAlterTableRenameCol(ASTNode ast) throws SemanticException {
     AlterTableDesc alterTblDesc = new AlterTableDesc(tblName,
         unescapeIdentifier(ast.getChild(1).getText()), unescapeIdentifier(ast
         .getChild(2).getText()), newType, newComment, first, flagCol);
-
-    try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tblName, false);
-      if (tab != null) {
-        inputs.add(new ReadEntity(tab));
-        outputs.add(new WriteEntity(tab));
-      }
-    } catch (HiveException e) {
-      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
-    }
+    addInputsOutputsAlterTable(tblName, null, alterTblDesc);
 
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
@@ -1803,8 +1820,9 @@ private void analyzeAlterTableRenamePart(ASTNode ast, String tblName,
     if (newPartSpec == null) {
       throw new SemanticException("RENAME PARTITION Missing Destination" + ast);
     }
+    Table tab = null;
     try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tblName, false);
+      tab = db.getTable(db.getCurrentDatabase(), tblName, false);
       if (tab != null) {
         inputs.add(new ReadEntity(tab));
       } else {
@@ -1813,6 +1831,8 @@ private void analyzeAlterTableRenamePart(ASTNode ast, String tblName,
     } catch (HiveException e) {
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
     }
+    validateAlterTableType(tab, AlterTableTypes.RENAMEPARTITION);
+
     List<Map<String, String>> partSpecs = new ArrayList<Map<String, String>>();
     partSpecs.add(oldPartSpec);
     partSpecs.add(newPartSpec);
@@ -1830,16 +1850,7 @@ private void analyzeAlterTableModifyCols(ASTNode ast,
     AlterTableDesc alterTblDesc = new AlterTableDesc(tblName, newCols,
         alterType);
 
-    try {
-      Table tab = db.getTable(db.getCurrentDatabase(), tblName, false);
-      if (tab != null) {
-        inputs.add(new ReadEntity(tab));
-        outputs.add(new WriteEntity(tab));
-      }
-    } catch (HiveException e) {
-      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
-    }
-
+    addInputsOutputsAlterTable(tblName, null, alterTblDesc);
     rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),
         alterTblDesc), conf));
   }
@@ -1860,6 +1871,7 @@ private void analyzeAlterTableDropParts(ASTNode ast, boolean expectView)
     } catch (HiveException e) {
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
     }
+    validateAlterTableType(tab, AlterTableTypes.DROPPARTITION, expectView);
 
     // Find out if all partition columns are strings. This is needed for JDO
     boolean stringPartitionColumns = true;
@@ -1916,7 +1928,7 @@ private void analyzeAlterTableAddParts(CommonTree ast, boolean expectView)
 
     String tblName = getUnescapedName((ASTNode)ast.getChild(0));
     boolean isView = false;
-    Table tab;
+    Table tab = null;
     try {
       tab = db.getTable(db.getCurrentDatabase(), tblName, false);
       if (tab != null) {
@@ -1926,6 +1938,7 @@ private void analyzeAlterTableAddParts(CommonTree ast, boolean expectView)
     } catch (HiveException e) {
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
     }
+    validateAlterTableType(tab, AlterTableTypes.ADDPARTITION, expectView);
 
     // partition name to value
     List<Map<String, String>> partSpecs = getPartitionSpecs(ast);
@@ -2052,6 +2065,7 @@ private void analyzeAlterTableTouch(CommonTree ast)
     } catch (HiveException e) {
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
     }
+    validateAlterTableType(tab, AlterTableTypes.TOUCH);
 
     // partition name to value
     List<Map<String, String>> partSpecs = getPartitionSpecs(ast);
@@ -2096,6 +2110,7 @@ private void analyzeAlterTableArchive(CommonTree ast, boolean isUnArchive)
       throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));
     }
     addTablePartsOutputs(tblName, partSpecs, true);
+    validateAlterTableType(tab, AlterTableTypes.ARCHIVE);
 
     if (partSpecs.size() > 1 ) {
       throw new SemanticException(isUnArchive ?
diff --git a/ql/src/test/results/clientnegative/alter_non_native.q.out b/ql/src/test/results/clientnegative/alter_non_native.q.out
index 8be2c3b4f0..7872d1543c 100644
--- a/ql/src/test/results/clientnegative/alter_non_native.q.out
+++ b/ql/src/test/results/clientnegative/alter_non_native.q.out
@@ -5,10 +5,4 @@ POSTHOOK: query: CREATE TABLE non_native1(key int, value string)
 STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler'
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@non_native1
-PREHOOK: query: -- we do not support ALTER TABLE on non-native tables yet
-ALTER TABLE non_native1 RENAME TO new_non_native
-PREHOOK: type: ALTERTABLE_RENAME
-PREHOOK: Input: default@non_native1
-PREHOOK: Output: default@non_native1
-FAILED: Error in metadata: Cannot use ALTER TABLE on a non-native table
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: SemanticException [Error 10134]: ALTER TABLE cannot be used for a non-native table non_native1
diff --git a/ql/src/test/results/clientnegative/alter_view_failure.q.out b/ql/src/test/results/clientnegative/alter_view_failure.q.out
index 6964d280ec..80d512ae49 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure.q.out
@@ -9,9 +9,4 @@ POSTHOOK: query: CREATE VIEW xxx3 AS SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx3
 #### A masked pattern was here ####
-PREHOOK: query: ALTER TABLE xxx3 REPLACE COLUMNS (xyz int)
-PREHOOK: type: ALTERTABLE_REPLACECOLS
-PREHOOK: Input: default@xxx3
-PREHOOK: Output: default@xxx3
-FAILED: Error in metadata: Cannot alter a view with ALTER TABLE
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: SemanticException [Error 10131]: To alter a view you need to use the ALTER VIEW command.
diff --git a/ql/src/test/results/clientnegative/alter_view_failure2.q.out b/ql/src/test/results/clientnegative/alter_view_failure2.q.out
index 56e0100fad..7c60a21878 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure2.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure2.q.out
@@ -15,10 +15,4 @@ SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx4
 #### A masked pattern was here ####
-PREHOOK: query: -- should fail:  need to use ALTER VIEW, not ALTER TABLE
-ALTER TABLE xxx4 ADD PARTITION (value='val_86')
-PREHOOK: type: ALTERTABLE_ADDPARTS
-PREHOOK: Input: default@src
-PREHOOK: Input: default@xxx4
-FAILED: Error in metadata: Cannot alter a view with ALTER TABLE
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: SemanticException [Error 10131]: To alter a view you need to use the ALTER VIEW command.
diff --git a/ql/src/test/results/clientnegative/alter_view_failure3.q.out b/ql/src/test/results/clientnegative/alter_view_failure3.q.out
index d79877174b..c1e82dfb92 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure3.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure3.q.out
@@ -1,6 +1 @@
-PREHOOK: query: -- should fail:  can't use ALTER VIEW on a table
-ALTER VIEW srcpart ADD PARTITION (ds='2012-12-31', hr='23')
-PREHOOK: type: ALTERTABLE_ADDPARTS
-PREHOOK: Input: default@srcpart
-FAILED: Error in metadata: Cannot alter a base table with ALTER VIEW
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: SemanticException [Error 10132]: To alter a base table you need to use the ALTER TABLE command.
diff --git a/ql/src/test/results/clientnegative/alter_view_failure8.q.out b/ql/src/test/results/clientnegative/alter_view_failure8.q.out
index 4420c5789a..31388941c1 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure8.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure8.q.out
@@ -5,9 +5,4 @@ POSTHOOK: query: -- should fail:  can't use ALTER VIEW on a table
 CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: default@invites
-PREHOOK: query: ALTER VIEW invites RENAME TO invites2
-PREHOOK: type: null
-PREHOOK: Input: default@invites
-PREHOOK: Output: default@invites
-FAILED: Error in metadata: Cannot alter a base table with ALTER VIEW
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: SemanticException [Error 10132]: To alter a base table you need to use the ALTER TABLE command.
diff --git a/ql/src/test/results/clientnegative/alter_view_failure9.q.out b/ql/src/test/results/clientnegative/alter_view_failure9.q.out
index 8b17afc41a..77534778ea 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure9.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure9.q.out
@@ -13,10 +13,4 @@ SELECT * FROM src
 POSTHOOK: type: CREATEVIEW
 POSTHOOK: Output: default@xxx4
 #### A masked pattern was here ####
-PREHOOK: query: -- should fail:  need to use ALTER VIEW, not ALTER TABLE
-ALTER TABLE xxx4 RENAME TO xxx4a
-PREHOOK: type: ALTERTABLE_RENAME
-PREHOOK: Input: default@xxx4
-PREHOOK: Output: default@xxx4
-FAILED: Error in metadata: Cannot alter a view with ALTER TABLE
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
+FAILED: SemanticException [Error 10131]: To alter a view you need to use the ALTER VIEW command.
