diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index f4ca4a0ed5..ff376a853b 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -791,14 +791,21 @@ public static enum ConfVars {
         "hive.txn.valid.txns,hive.script.operator.env.blacklist",
         "Comma separated list of keys from the configuration file not to convert to environment " +
         "variables when envoking the script operator"),
-    HIVEMAPREDMODE("hive.mapred.mode", "strict",
-        "The mode in which the Hive operations are being performed. \n" +
-        "In strict mode, some risky queries are not allowed to run. They include:\n" +
-        "  Cartesian Product.\n" +
-        "  No partition being picked up for a query.\n" +
+    HIVE_STRICT_CHECKS_LARGE_QUERY("hive.strict.checks.large.query", false,
+        "Enabling strict large query checks disallows the following:\n" +
+        "  Orderby without limit.\n" +
+        "  No partition being picked up for a query against partitioned table.\n" +
+        "Note that these checks currently do not consider data size, only the query pattern."),
+    HIVE_STRICT_CHECKS_TYPE_SAFETY("hive.strict.checks.type.safety", true,
+        "Enabling strict type safety checks disallows the following:\n" +
         "  Comparing bigints and strings.\n" +
-        "  Comparing bigints and doubles.\n" +
-        "  Orderby without limit."),
+        "  Comparing bigints and doubles."),
+    HIVE_STRICT_CHECKS_CARTESIAN("hive.strict.checks.cartesian.product", true,
+        "Enabling strict large query checks disallows the following:\n" +
+        "  Cartesian product (cross join)."),
+    @Deprecated
+    HIVEMAPREDMODE("hive.mapred.mode", "nonstrict",
+        "Deprecated; use hive.strict.checks.* settings instead."),
     HIVEALIAS("hive.alias", "", ""),
     HIVEMAPSIDEAGGREGATE("hive.map.aggr", true, "Whether to use map-side aggregation in Hive Group By queries"),
     HIVEGROUPBYSKEW("hive.groupby.skewindata", false, "Whether there is skew in data to optimize group by queries"),
@@ -3746,6 +3753,47 @@ public static void setLoadHiveServer2Config(boolean loadHiveServer2Config) {
     HiveConf.loadHiveServer2Config = loadHiveServer2Config;
   }
 
+  public static class StrictChecks {
+
+    private static final String NO_LIMIT_MSG = makeMessage(
+        "Order by-s without limit", ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY);
+    private static final String NO_PARTITIONLESS_MSG = makeMessage(
+        "Queries against partitioned tables without a partition filter",
+        ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY);
+    private static final String NO_COMPARES_MSG = makeMessage(
+        "Unsafe compares between different types", ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY);
+    private static final String NO_CARTESIAN_MSG = makeMessage(
+        "Cartesian products", ConfVars.HIVE_STRICT_CHECKS_CARTESIAN);
+
+    private static String makeMessage(String what, ConfVars setting) {
+      return what + " are disabled for safety reasons. If you know what you are doing, please make"
+          + " sure that " + setting.varname + " is set to false and that "
+          + ConfVars.HIVEMAPREDMODE.varname + " is not set to 'strict' to enable them.";
+    }
+
+    public static String checkNoLimit(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY) ? null : NO_LIMIT_MSG;
+    }
+
+    public static String checkNoPartitionFilter(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY)
+          ? null : NO_PARTITIONLESS_MSG;
+    }
+
+    public static String checkTypeSafety(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_TYPE_SAFETY) ? null : NO_COMPARES_MSG;
+    }
+
+    public static String checkCartesian(Configuration conf) {
+      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_CARTESIAN) ? null : NO_CARTESIAN_MSG;
+    }
+
+    private static boolean isAllowed(Configuration conf, ConfVars setting) {
+      String mode = HiveConf.getVar(conf, ConfVars.HIVEMAPREDMODE, null);
+      return (mode != null) ? !"strict".equals(mode) : !HiveConf.getBoolVar(conf, setting);
+    }
+  }
+
   public static String getNonMrEngines() {
     String result = "";
     for (String s : ConfVars.HIVE_EXECUTION_ENGINE.getValidStringValues()) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
index c79b1bed7a..a0e374c744 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.optimizer.calcite.translator;
 
+
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashMap;
@@ -43,7 +44,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.ErrorMsg;
+import org.apache.hadoop.hive.conf.HiveConf.StrictChecks;
 import org.apache.hadoop.hive.ql.exec.ColumnInfo;
 import org.apache.hadoop.hive.ql.exec.FilterOperator;
 import org.apache.hadoop.hive.ql.exec.JoinOperator;
@@ -120,17 +121,14 @@ public static enum HIVEAGGOPMODE {
   private final HiveConf                                      hiveConf;
   private final UnparseTranslator                             unparseTranslator;
   private final Map<String, TableScanOperator>                topOps;
-  private final boolean                                       strictMode;
   private int                                                 uniqueCounter;
 
   public HiveOpConverter(SemanticAnalyzer semanticAnalyzer, HiveConf hiveConf,
-      UnparseTranslator unparseTranslator, Map<String, TableScanOperator> topOps,
-      boolean strictMode) {
+      UnparseTranslator unparseTranslator, Map<String, TableScanOperator> topOps) {
     this.semanticAnalyzer = semanticAnalyzer;
     this.hiveConf = hiveConf;
     this.unparseTranslator = unparseTranslator;
     this.topOps = topOps;
-    this.strictMode = strictMode;
     this.uniqueCounter = 0;
   }
 
@@ -424,10 +422,10 @@ OpAttr visit(HiveSortLimit sortRel) throws SemanticException {
     // of their columns
     if (sortRel.getCollation() != RelCollations.EMPTY) {
 
-      // In strict mode, in the presence of order by, limit must be
-      // specified
-      if (strictMode && sortRel.fetch == null) {
-        throw new SemanticException(ErrorMsg.NO_LIMIT_WITH_ORDERBY.getMsg());
+      // In strict mode, in the presence of order by, limit must be specified.
+      if (sortRel.fetch == null) {
+        String error = StrictChecks.checkNoLimit(hiveConf);
+        if (error != null) throw new SemanticException(error);
       }
 
       // 1.a. Extract order for each column from collation
@@ -476,7 +474,7 @@ OpAttr visit(HiveSortLimit sortRel) throws SemanticException {
       // 1.b. Generate reduce sink and project operator
       resultOp = genReduceSinkAndBacktrackSelect(resultOp,
           sortCols.toArray(new ExprNodeDesc[sortCols.size()]), 0, new ArrayList<ExprNodeDesc>(),
-          order.toString(), numReducers, Operation.NOT_ACID, strictMode, keepColumns);
+          order.toString(), numReducers, Operation.NOT_ACID, hiveConf, keepColumns);
     }
 
     // 2. If we need to generate limit
@@ -606,7 +604,7 @@ OpAttr visit(HiveSortExchange exchangeRel) throws SemanticException {
     exchangeRel.setJoinExpressions(expressions);
 
     ReduceSinkOperator rsOp = genReduceSink(inputOpAf.inputs.get(0), tabAlias, expressions,
-        -1, -1, Operation.NOT_ACID, strictMode);
+        -1, -1, Operation.NOT_ACID, hiveConf);
 
     return new OpAttr(tabAlias, inputOpAf.vcolsInCalcite, rsOp);
   }
@@ -654,7 +652,7 @@ private OpAttr genPTF(OpAttr inputOpAf, WindowingSpec wSpec) throws SemanticExce
 
       SelectOperator selectOp = genReduceSinkAndBacktrackSelect(input,
           keyCols.toArray(new ExprNodeDesc[keyCols.size()]), 0, partCols,
-          order.toString(), -1, Operation.NOT_ACID, strictMode);
+          order.toString(), -1, Operation.NOT_ACID, hiveConf);
 
       // 2. Finally create PTF
       PTFTranslator translator = new PTFTranslator();
@@ -679,14 +677,15 @@ private OpAttr genPTF(OpAttr inputOpAf, WindowingSpec wSpec) throws SemanticExce
 
   private static SelectOperator genReduceSinkAndBacktrackSelect(Operator<?> input,
           ExprNodeDesc[] keys, int tag, ArrayList<ExprNodeDesc> partitionCols, String order,
-          int numReducers, Operation acidOperation, boolean strictMode) throws SemanticException {
+          int numReducers, Operation acidOperation, HiveConf hiveConf)
+              throws SemanticException {
     return genReduceSinkAndBacktrackSelect(input, keys, tag, partitionCols, order,
-        numReducers, acidOperation, strictMode, input.getSchema().getColumnNames());
+        numReducers, acidOperation, hiveConf, input.getSchema().getColumnNames());
   }
 
   private static SelectOperator genReduceSinkAndBacktrackSelect(Operator<?> input,
       ExprNodeDesc[] keys, int tag, ArrayList<ExprNodeDesc> partitionCols, String order,
-      int numReducers, Operation acidOperation, boolean strictMode,
+      int numReducers, Operation acidOperation, HiveConf hiveConf,
       List<String> keepColNames) throws SemanticException {
     // 1. Generate RS operator
     // 1.1 Prune the tableNames, only count the tableNames that are not empty strings
@@ -716,7 +715,7 @@ private static SelectOperator genReduceSinkAndBacktrackSelect(Operator<?> input,
           "In CBO return path, genReduceSinkAndBacktrackSelect is expecting only one tableAlias but there is none");
     }
     // 1.2 Now generate RS operator
-    ReduceSinkOperator rsOp = genReduceSink(input, tableAlias, keys, tag, partitionCols, order, numReducers, acidOperation, strictMode);
+    ReduceSinkOperator rsOp = genReduceSink(input, tableAlias, keys, tag, partitionCols, order, numReducers, acidOperation, hiveConf);
 
     // 2. Generate backtrack Select operator
     Map<String, ExprNodeDesc> descriptors = buildBacktrackFromReduceSink(keepColNames,
@@ -737,15 +736,15 @@ private static SelectOperator genReduceSinkAndBacktrackSelect(Operator<?> input,
   }
 
   private static ReduceSinkOperator genReduceSink(Operator<?> input, String tableAlias, ExprNodeDesc[] keys, int tag,
-      int numReducers, Operation acidOperation, boolean strictMode) throws SemanticException {
+      int numReducers, Operation acidOperation, HiveConf hiveConf) throws SemanticException {
     return genReduceSink(input, tableAlias, keys, tag, new ArrayList<ExprNodeDesc>(), "", numReducers,
-        acidOperation, strictMode);
+        acidOperation, hiveConf);
   }
 
   @SuppressWarnings({ "rawtypes", "unchecked" })
   private static ReduceSinkOperator genReduceSink(Operator<?> input, String tableAlias, ExprNodeDesc[] keys, int tag,
       ArrayList<ExprNodeDesc> partitionCols, String order, int numReducers,
-      Operation acidOperation, boolean strictMode) throws SemanticException {
+      Operation acidOperation, HiveConf hiveConf) throws SemanticException {
     Operator dummy = Operator.createDummy(); // dummy for backtracking
     dummy.setParentOperators(Arrays.asList(input));
 
@@ -809,9 +808,8 @@ private static ReduceSinkOperator genReduceSink(Operator<?> input, String tableA
       numReducers = 1;
 
       // Cartesian product is not supported in strict mode
-      if (strictMode) {
-        throw new SemanticException(ErrorMsg.NO_CARTESIAN_PRODUCT.getMsg());
-      }
+      String error = StrictChecks.checkCartesian(hiveConf);
+      if (error != null) throw new SemanticException(error);
     }
 
     ReduceSinkDesc rsDesc;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
index 90e9b3fcc3..02c5a89df3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
@@ -33,11 +33,11 @@
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.common.ObjectPair;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.conf.HiveConf.StrictChecks;
 import org.apache.hadoop.hive.metastore.IMetaStoreClient;
 import org.apache.hadoop.hive.metastore.Warehouse;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.MetaException;
-import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.TableScanOperator;
@@ -181,11 +181,13 @@ public static PrunedPartitionList prune(Table tab, ExprNodeDesc prunerExpr,
       return getAllPartsFromCacheOrServer(tab, key, false, prunedPartitionsMap);
     }
 
-    if ("strict".equalsIgnoreCase(HiveConf.getVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE))
-        && !hasColumnExpr(prunerExpr)) {
+    if (!hasColumnExpr(prunerExpr)) {
       // If the "strict" mode is on, we have to provide partition pruner for each table.
-      throw new SemanticException(ErrorMsg.NO_PARTITION_PREDICATE
-          .getMsg("for Alias \"" + alias + "\" Table \"" + tab.getTableName() + "\""));
+      String error = StrictChecks.checkNoPartitionFilter(conf);
+      if (error != null) {
+        throw new SemanticException(error + " No partition predicate for Alias \""
+            + alias + "\" Table \"" + tab.getTableName() + "\"");
+      }
     }
 
     if (prunerExpr == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
index cc9dc23eed..3b3e8402dc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
@@ -98,6 +98,7 @@
 import org.apache.calcite.util.Pair;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConf.StrictChecks;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.QueryProperties;
@@ -438,8 +439,9 @@ boolean canCBOHandleAst(ASTNode ast, QB qb, PreCboCtx cboCtx) {
   static String canHandleQbForCbo(QueryProperties queryProperties, HiveConf conf,
       boolean topLevelQB, boolean verbose, QB qb) {
     boolean isInTest = conf.getBoolVar(ConfVars.HIVE_IN_TEST);
+    // TODO: HIVEMAPREDMODE is deprecated. Why does this test-only exception exist?
     boolean isStrictTest = isInTest
-        && !conf.getVar(ConfVars.HIVEMAPREDMODE).equalsIgnoreCase("nonstrict");
+        && "strict".equals(HiveConf.getVar(conf, ConfVars.HIVEMAPREDMODE));
 
     if (!isStrictTest
         && !queryProperties.hasClusterBy() && !queryProperties.hasDistributeBy()
@@ -453,7 +455,7 @@ static String canHandleQbForCbo(QueryProperties queryProperties, HiveConf conf,
     String msg = "";
     if (verbose) {
       if (isStrictTest)
-        msg += "is in test running in mode other than nonstrict; ";
+        msg += "is in test running in strict mode (deprecated); ";
       if (queryProperties.hasClusterBy())
         msg += "has cluster by; ";
       if (queryProperties.hasDistributeBy())
@@ -683,8 +685,8 @@ Operator getOptimizedHiveOPDag() throws SemanticException {
         optimizedOptiqPlan, resultSchema, this.getQB().getTableDesc() != null);
 
     LOG.debug("Translating the following plan:\n" + RelOptUtil.toString(modifiedOptimizedOptiqPlan));
-    Operator<?> hiveRoot = new HiveOpConverter(this, conf, unparseTranslator, topOps,
-        conf.getVar(HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase("strict")).convert(modifiedOptimizedOptiqPlan);
+    Operator<?> hiveRoot = new HiveOpConverter(this, conf, unparseTranslator, topOps)
+                                  .convert(modifiedOptimizedOptiqPlan);
     RowResolver hiveRootRR = genRowResolver(hiveRoot, getQB());
     opParseCtx.put(hiveRoot, new OpParseContext(hiveRootRR));
     String dest = getQB().getParseInfo().getClauseNames().iterator().next();
@@ -2331,10 +2333,11 @@ private Pair<RelNode, RelNode> genOBLogicalPlan(QB qb, RelNode srcRel, boolean o
         // 1. OB Expr sanity test
         // in strict mode, in the presence of order by, limit must be specified
         Integer limit = qb.getParseInfo().getDestLimit(dest);
-        if (conf.getVar(HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase("strict")
-            && limit == null) {
-          throw new SemanticException(SemanticAnalyzer.generateErrorMessage(obAST,
-              ErrorMsg.NO_LIMIT_WITH_ORDERBY.getMsg()));
+        if (limit == null) {
+          String error = StrictChecks.checkNoLimit(conf);
+          if (error != null) {
+            throw new SemanticException(SemanticAnalyzer.generateErrorMessage(obAST, error));
+          }
         }
 
         // 2. Walk through OB exprs and extract field collations and additional
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 8c880c3982..607c2f32e0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -58,6 +58,7 @@
 import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
+import org.apache.hadoop.hive.conf.HiveConf.StrictChecks;
 import org.apache.hadoop.hive.metastore.MetaStoreUtils;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.Warehouse;
@@ -7122,12 +7123,11 @@ private Operator genReduceSinkPlan(String dest, QB qb, Operator<?> input,
       if (sortExprs != null) {
         assert numReducers == 1;
         // in strict mode, in the presence of order by, limit must be specified
-        Integer limit = qb.getParseInfo().getDestLimit(dest);
-        if (conf.getVar(HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase(
-            "strict")
-            && limit == null) {
-          throw new SemanticException(generateErrorMessage(sortExprs,
-              ErrorMsg.NO_LIMIT_WITH_ORDERBY.getMsg()));
+        if (qb.getParseInfo().getDestLimit(dest) == null) {
+          String error = StrictChecks.checkNoLimit(conf);
+          if (error != null) {
+            throw new SemanticException(generateErrorMessage(sortExprs, error));
+          }
         }
       }
     }
@@ -7488,12 +7488,8 @@ private Operator genJoinReduceSinkChild(QB qb, ExprNodeDesc[] joinKeys,
     // Use only 1 reducer in case of cartesian product
     if (reduceKeys.size() == 0) {
       numReds = 1;
-
-      // Cartesian product is not supported in strict mode
-      if (conf.getVar(HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase(
-          "strict")) {
-        throw new SemanticException(ErrorMsg.NO_CARTESIAN_PRODUCT.getMsg());
-      }
+      String error = StrictChecks.checkCartesian(conf);
+      if (error != null) throw new SemanticException(error);
     }
 
     ReduceSinkDesc rsDesc = PlanUtils.getReduceSinkDesc(reduceKeys,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
index 25c2ff3f9b..b7c144566c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java
@@ -27,8 +27,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.ErrorMsg;
+import org.apache.hadoop.hive.conf.HiveConf.StrictChecks;
 import org.apache.hadoop.hive.ql.exec.FunctionRegistry;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -217,18 +216,14 @@ public static ExprNodeGenericFuncDesc newInstance(GenericUDF genericUDF,
       // For now, if a bigint is going to be cast to a double throw an error or warning
       if ((oiTypeInfo0.equals(TypeInfoFactory.stringTypeInfo) && oiTypeInfo1.equals(TypeInfoFactory.longTypeInfo)) ||
           (oiTypeInfo0.equals(TypeInfoFactory.longTypeInfo) && oiTypeInfo1.equals(TypeInfoFactory.stringTypeInfo))) {
-        if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase("strict")) {
-          throw new UDFArgumentException(ErrorMsg.NO_COMPARE_BIGINT_STRING.getMsg());
-        } else {
-          console.printError("WARNING: Comparing a bigint and a string may result in a loss of precision.");
-        }
+        String error = StrictChecks.checkTypeSafety(conf);
+        if (error != null) throw new UDFArgumentException(error);
+        console.printError("WARNING: Comparing a bigint and a string may result in a loss of precision.");
       } else if ((oiTypeInfo0.equals(TypeInfoFactory.doubleTypeInfo) && oiTypeInfo1.equals(TypeInfoFactory.longTypeInfo)) ||
           (oiTypeInfo0.equals(TypeInfoFactory.longTypeInfo) && oiTypeInfo1.equals(TypeInfoFactory.doubleTypeInfo))) {
-        if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE).equalsIgnoreCase("strict")) {
-          throw new UDFArgumentException(ErrorMsg.NO_COMPARE_BIGINT_DOUBLE.getMsg());
-        } else {
-          console.printError("WARNING: Comparing a bigint and a double may result in a loss of precision.");
-        }
+        String error = StrictChecks.checkTypeSafety(conf);
+        if (error != null) throw new UDFArgumentException(error);
+        console.printError("WARNING: Comparing a bigint and a double may result in a loss of precision.");
       }
     }
 
diff --git a/ql/src/test/queries/clientnegative/invalid_distinct1.q b/ql/src/test/queries/clientnegative/invalid_distinct1.q
index 538ef8164f..6f688e386e 100644
--- a/ql/src/test/queries/clientnegative/invalid_distinct1.q
+++ b/ql/src/test/queries/clientnegative/invalid_distinct1.q
@@ -1 +1,2 @@
+set hive.cbo.enable=false;
 explain select hash(distinct value) from src;
diff --git a/ql/src/test/queries/clientnegative/invalid_distinct3.q b/ql/src/test/queries/clientnegative/invalid_distinct3.q
index ec8026b130..e05acca097 100644
--- a/ql/src/test/queries/clientnegative/invalid_distinct3.q
+++ b/ql/src/test/queries/clientnegative/invalid_distinct3.q
@@ -1 +1,2 @@
+set hive.cbo.enable=false;
 explain select hash(upper(distinct value)) from src;
diff --git a/ql/src/test/queries/clientpositive/perf/query12.q b/ql/src/test/queries/clientpositive/perf/query12.q
index 89dd2de4e1..29976fc144 100644
--- a/ql/src/test/queries/clientpositive/perf/query12.q
+++ b/ql/src/test/queries/clientpositive/perf/query12.q
@@ -1 +1,2 @@
+set hive.cbo.enable=false;
 explain select i_item_desc ,i_category ,i_class ,i_current_price ,i_item_id ,sum(ws_ext_sales_price) as itemrevenue ,sum(ws_ext_sales_price)*100/sum(sum(ws_ext_sales_price)) over (partition by i_class) as revenueratio from web_sales ,item ,date_dim where web_sales.ws_item_sk = item.i_item_sk and item.i_category in ('Jewelry', 'Sports', 'Books') and web_sales.ws_sold_date_sk = date_dim.d_date_sk and date_dim.d_date between '2001-01-12' and '2001-02-11' group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratio limit 100;
diff --git a/ql/src/test/results/clientnegative/alter_view_failure6.q.out b/ql/src/test/results/clientnegative/alter_view_failure6.q.out
index 8726ca0c53..70d7622366 100644
--- a/ql/src/test/results/clientnegative/alter_view_failure6.q.out
+++ b/ql/src/test/results/clientnegative/alter_view_failure6.q.out
@@ -18,5 +18,5 @@ POSTHOOK: type: CREATEVIEW
 POSTHOOK: Input: default@srcpart
 POSTHOOK: Output: database:default
 POSTHOOK: Output: default@xxx7
-FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "default.xxx7:srcpart" Table "srcpart"
+FAILED: SemanticException Queries against partitioned tables without a partition filter are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.large.query is set to false and that hive.mapred.mode is not set to 'strict' to enable them. No partition predicate for Alias "default.xxx7:srcpart" Table "srcpart"
 FAILED: SemanticException [Error 10056]: The query does not reference any valid partition. To run this query, set hive.mapred.mode=nonstrict
diff --git a/ql/src/test/results/clientnegative/compare_double_bigint.q.out b/ql/src/test/results/clientnegative/compare_double_bigint.q.out
index 68a295a313..10a5667e7a 100644
--- a/ql/src/test/results/clientnegative/compare_double_bigint.q.out
+++ b/ql/src/test/results/clientnegative/compare_double_bigint.q.out
@@ -1 +1 @@
-FAILED: SemanticException Line 0:-1 Wrong arguments '1.0': In strict mode, comparing bigints and doubles is not allowed, it may result in a loss of precision. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException Line 0:-1 Wrong arguments '1.0': Unsafe compares between different types are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.type.safety is set to false and that hive.mapred.mode is not set to 'strict' to enable them.
diff --git a/ql/src/test/results/clientnegative/compare_string_bigint.q.out b/ql/src/test/results/clientnegative/compare_string_bigint.q.out
index 9a604fa71b..c8eb290f5d 100644
--- a/ql/src/test/results/clientnegative/compare_string_bigint.q.out
+++ b/ql/src/test/results/clientnegative/compare_string_bigint.q.out
@@ -1 +1 @@
-FAILED: SemanticException Line 0:-1 Wrong arguments ''1'': In strict mode, comparing bigints and strings is not allowed, it may result in a loss of precision. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException Line 0:-1 Wrong arguments ''1'': Unsafe compares between different types are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.type.safety is set to false and that hive.mapred.mode is not set to 'strict' to enable them.
diff --git a/ql/src/test/results/clientnegative/input4.q.out b/ql/src/test/results/clientnegative/input4.q.out
index eb1707577f..887865d9f2 100644
--- a/ql/src/test/results/clientnegative/input4.q.out
+++ b/ql/src/test/results/clientnegative/input4.q.out
@@ -1 +1 @@
-FAILED: SemanticException [Error 10052]: In strict mode, cartesian product is not allowed. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException Cartesian products are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.cartesian.product is set to false and that hive.mapred.mode is not set to 'strict' to enable them.
diff --git a/ql/src/test/results/clientnegative/input_part0_neg.q.out b/ql/src/test/results/clientnegative/input_part0_neg.q.out
index 4c717b749a..3abe4b7ecd 100644
--- a/ql/src/test/results/clientnegative/input_part0_neg.q.out
+++ b/ql/src/test/results/clientnegative/input_part0_neg.q.out
@@ -1 +1 @@
-FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "x" Table "srcpart"
+FAILED: SemanticException Queries against partitioned tables without a partition filter are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.large.query is set to false and that hive.mapred.mode is not set to 'strict' to enable them. No partition predicate for Alias "x" Table "srcpart"
diff --git a/ql/src/test/results/clientnegative/invalid_distinct1.q.out b/ql/src/test/results/clientnegative/invalid_distinct1.q.out
index c1c95a97c5..c08b1c350c 100644
--- a/ql/src/test/results/clientnegative/invalid_distinct1.q.out
+++ b/ql/src/test/results/clientnegative/invalid_distinct1.q.out
@@ -1 +1 @@
-FAILED: SemanticException 1:15 Distinct keyword is not support in current context. Error encountered near token 'value'
+FAILED: SemanticException 2:15 Distinct keyword is not support in current context. Error encountered near token 'value'
diff --git a/ql/src/test/results/clientnegative/invalid_distinct3.q.out b/ql/src/test/results/clientnegative/invalid_distinct3.q.out
index e478860e2a..444af62ec8 100644
--- a/ql/src/test/results/clientnegative/invalid_distinct3.q.out
+++ b/ql/src/test/results/clientnegative/invalid_distinct3.q.out
@@ -1 +1 @@
-FAILED: SemanticException 1:20 Distinct keyword is not support in current context. Error encountered near token 'value'
+FAILED: SemanticException 2:20 Distinct keyword is not support in current context. Error encountered near token 'value'
diff --git a/ql/src/test/results/clientnegative/strict_join.q.out b/ql/src/test/results/clientnegative/strict_join.q.out
index eb1707577f..887865d9f2 100644
--- a/ql/src/test/results/clientnegative/strict_join.q.out
+++ b/ql/src/test/results/clientnegative/strict_join.q.out
@@ -1 +1 @@
-FAILED: SemanticException [Error 10052]: In strict mode, cartesian product is not allowed. If you really want to perform the operation, set hive.mapred.mode=nonstrict
+FAILED: SemanticException Cartesian products are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.cartesian.product is set to false and that hive.mapred.mode is not set to 'strict' to enable them.
diff --git a/ql/src/test/results/clientnegative/strict_orderby.q.out b/ql/src/test/results/clientnegative/strict_orderby.q.out
index 82d1f9c382..08c1062169 100644
--- a/ql/src/test/results/clientnegative/strict_orderby.q.out
+++ b/ql/src/test/results/clientnegative/strict_orderby.q.out
@@ -1 +1 @@
-FAILED: SemanticException 4:47 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'key'
+FAILED: SemanticException 4:47 Order by-s without limit are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.large.query is set to false and that hive.mapred.mode is not set to 'strict' to enable them.. Error encountered near token 'key'
diff --git a/ql/src/test/results/clientnegative/strict_pruning.q.out b/ql/src/test/results/clientnegative/strict_pruning.q.out
index ff50b28d33..d45a2b8509 100644
--- a/ql/src/test/results/clientnegative/strict_pruning.q.out
+++ b/ql/src/test/results/clientnegative/strict_pruning.q.out
@@ -1 +1 @@
-FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "srcpart" Table "srcpart"
+FAILED: SemanticException Queries against partitioned tables without a partition filter are disabled for safety reasons. If you know what you are doing, please make sure that hive.strict.checks.large.query is set to false and that hive.mapred.mode is not set to 'strict' to enable them. No partition predicate for Alias "srcpart" Table "srcpart"
diff --git a/ql/src/test/results/clientnegative/subquery_notexists_implicit_gby.q.out b/ql/src/test/results/clientnegative/subquery_notexists_implicit_gby.q.out
index 6d9fa0aecf..da38f5ff50 100644
--- a/ql/src/test/results/clientnegative/subquery_notexists_implicit_gby.q.out
+++ b/ql/src/test/results/clientnegative/subquery_notexists_implicit_gby.q.out
@@ -1 +1 @@
-FAILED: SemanticException [Error 10250]: Line 7:7 Invalid SubQuery expression 'key': A Not Exists predicate on SubQuery with implicit Aggregation(no Group By clause) cannot be rewritten. (predicate will always return false).
+FAILED: SemanticException [Error 10002]: Line 8:20 Invalid column reference 'value'
diff --git a/ql/src/test/results/clientpositive/create_genericudf.q.out b/ql/src/test/results/clientpositive/create_genericudf.q.out
index 586f0bae7f..db3a9b5a90 100644
--- a/ql/src/test/results/clientpositive/create_genericudf.q.out
+++ b/ql/src/test/results/clientpositive/create_genericudf.q.out
@@ -50,13 +50,13 @@ SELECT
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.c1 EXPRESSION []
-POSTHOOK: Lineage: dest1.c2 EXPRESSION []
+POSTHOOK: Lineage: dest1.c1 SIMPLE []
+POSTHOOK: Lineage: dest1.c2 SIMPLE []
 POSTHOOK: Lineage: dest1.c3 EXPRESSION []
 POSTHOOK: Lineage: dest1.c4 EXPRESSION []
 POSTHOOK: Lineage: dest1.c5 EXPRESSION []
-POSTHOOK: Lineage: dest1.c6 EXPRESSION []
-POSTHOOK: Lineage: dest1.c7 EXPRESSION []
+POSTHOOK: Lineage: dest1.c6 SIMPLE []
+POSTHOOK: Lineage: dest1.c7 SIMPLE []
 PREHOOK: query: SELECT dest1.* FROM dest1 LIMIT 1
 PREHOOK: type: QUERY
 PREHOOK: Input: default@dest1
diff --git a/ql/src/test/results/clientpositive/cte_2.q.out b/ql/src/test/results/clientpositive/cte_2.q.out
index a8bc760120..d6923ba30d 100644
--- a/ql/src/test/results/clientpositive/cte_2.q.out
+++ b/ql/src/test/results/clientpositive/cte_2.q.out
@@ -40,7 +40,7 @@ select *
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@s1
-POSTHOOK: Lineage: s1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: s1.key SIMPLE []
 POSTHOOK: Lineage: s1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: select * from s1
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/empty_join.q.out b/ql/src/test/results/clientpositive/empty_join.q.out
index 3f8aedf429..2abba0e080 100644
--- a/ql/src/test/results/clientpositive/empty_join.q.out
+++ b/ql/src/test/results/clientpositive/empty_join.q.out
@@ -59,31 +59,45 @@ STAGE PLANS:
   Stage: Stage-5
     Map Reduce Local Work
       Alias -> Map Local Tables:
-        t2 
+        $hdt$_1:t2
           Fetch Operator
             limit: -1
-        t3 
+        $hdt$_2:t3
           Fetch Operator
             limit: -1
       Alias -> Map Local Operator Tree:
-        t2 
+        $hdt$_1:t2
           TableScan
             alias: t2
             Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
-            HashTable Sink Operator
-              keys:
-                0 id (type: int)
-                1 id (type: int)
-                2 id (type: int)
-        t3 
+            Filter Operator
+              predicate: id is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
+              Select Operator
+                expressions: id (type: int)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
+                HashTable Sink Operator
+                  keys:
+                    0 _col0 (type: int)
+                    1 _col0 (type: int)
+                    2 _col0 (type: int)
+        $hdt$_2:t3
           TableScan
             alias: t3
             Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
-            HashTable Sink Operator
-              keys:
-                0 id (type: int)
-                1 id (type: int)
-                2 id (type: int)
+            Filter Operator
+              predicate: id is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: id (type: int)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
+                HashTable Sink Operator
+                  keys:
+                    0 _col0 (type: int)
+                    1 _col0 (type: int)
+                    2 _col0 (type: int)
 
   Stage: Stage-4
     Map Reduce
@@ -91,27 +105,30 @@ STAGE PLANS:
           TableScan
             alias: t1
             Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
-            Map Join Operator
-              condition map:
-                   Left Outer Join0 to 1
-                   Inner Join 0 to 2
-              keys:
-                0 id (type: int)
-                1 id (type: int)
-                2 id (type: int)
-              outputColumnNames: _col0, _col4, _col8
-              Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: id is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
               Select Operator
-                expressions: _col0 (type: int), _col4 (type: int), _col8 (type: int)
-                outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
+                expressions: id (type: int)
+                outputColumnNames: _col0
+                Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: NONE
+                Map Join Operator
+                  condition map:
+                       Left Outer Join0 to 1
+                       Inner Join 0 to 2
+                  keys:
+                    0 _col0 (type: int)
+                    1 _col0 (type: int)
+                    2 _col0 (type: int)
+                  outputColumnNames: _col0, _col1, _col2
                   Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 2 Data size: 2 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
       Local Work:
         Map Reduce Local Work
 
diff --git a/ql/src/test/results/clientpositive/groupby_duplicate_key.q.out b/ql/src/test/results/clientpositive/groupby_duplicate_key.q.out
index 47233c8dbd..b45cd04fce 100644
--- a/ql/src/test/results/clientpositive/groupby_duplicate_key.q.out
+++ b/ql/src/test/results/clientpositive/groupby_duplicate_key.q.out
@@ -194,11 +194,11 @@ STAGE PLANS:
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: string)
-              outputColumnNames: key
+              outputColumnNames: _col0
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
                 aggregations: max('pants'), max('pANTS')
-                keys: key (type: string)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
diff --git a/ql/src/test/results/clientpositive/insert1.q.out b/ql/src/test/results/clientpositive/insert1.q.out
index 49dd2d5891..7a2c4297ee 100644
--- a/ql/src/test/results/clientpositive/insert1.q.out
+++ b/ql/src/test/results/clientpositive/insert1.q.out
@@ -26,7 +26,7 @@ POSTHOOK: query: insert overwrite table insert1 select a.key, a.value from inser
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@insert2
 POSTHOOK: Output: default@insert1
-POSTHOOK: Lineage: insert1.key SIMPLE [(insert2)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: insert1.key SIMPLE []
 POSTHOOK: Lineage: insert1.value SIMPLE [(insert2)a.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: explain insert into table insert1 select a.key, a.value from insert2 a WHERE (a.key=-1)
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/join_cond_pushdown_unqual5.q.out b/ql/src/test/results/clientpositive/join_cond_pushdown_unqual5.q.out
index ab333b0f1f..7d4fa71e35 100644
--- a/ql/src/test/results/clientpositive/join_cond_pushdown_unqual5.q.out
+++ b/ql/src/test/results/clientpositive/join_cond_pushdown_unqual5.q.out
@@ -54,44 +54,48 @@ STAGE PLANS:
           TableScan
             alias: l
             Statistics: Num rows: 2 Data size: 52 Basic stats: COMPLETE Column stats: NONE
-            Reduce Output Operator
-              key expressions: lk1 (type: string), lk2 (type: string)
-              sort order: ++
-              Map-reduce partition columns: lk1 (type: string), lk2 (type: string)
+            Select Operator
+              expressions: index (type: int), la (type: int), lk1 (type: string), lk2 (type: string)
+              outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 2 Data size: 52 Basic stats: COMPLETE Column stats: NONE
-              value expressions: index (type: int), la (type: int)
+              Reduce Output Operator
+                key expressions: _col2 (type: string), _col3 (type: string)
+                sort order: ++
+                Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+                Statistics: Num rows: 2 Data size: 52 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col0 (type: int), _col1 (type: int)
           TableScan
             alias: r
             Statistics: Num rows: 2 Data size: 47 Basic stats: COMPLETE Column stats: NONE
-            Reduce Output Operator
-              key expressions: rk1 (type: string), rk2 (type: string)
-              sort order: ++
-              Map-reduce partition columns: rk1 (type: string), rk2 (type: string)
+            Select Operator
+              expressions: ra (type: int), rk1 (type: string), rk2 (type: string)
+              outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 2 Data size: 47 Basic stats: COMPLETE Column stats: NONE
-              value expressions: ra (type: int)
+              Reduce Output Operator
+                key expressions: _col1 (type: string), _col2 (type: string)
+                sort order: ++
+                Map-reduce partition columns: _col1 (type: string), _col2 (type: string)
+                Statistics: Num rows: 2 Data size: 47 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col0 (type: int)
       Reduce Operator Tree:
         Join Operator
           condition map:
                Left Outer Join0 to 1
           keys:
-            0 lk1 (type: string), lk2 (type: string)
-            1 rk1 (type: string), rk2 (type: string)
-          outputColumnNames: _col0, _col1, _col2, _col3, _col7, _col8, _col9
+            0 _col2 (type: string), _col3 (type: string)
+            1 _col1 (type: string), _col2 (type: string)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
           Statistics: Num rows: 2 Data size: 57 Basic stats: COMPLETE Column stats: NONE
           Filter Operator
-            predicate: (COALESCE(_col1,'EMPTY') = COALESCE(_col7,'EMPTY')) (type: boolean)
+            predicate: (COALESCE(_col1,'EMPTY') = COALESCE(_col4,'EMPTY')) (type: boolean)
             Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: string), _col7 (type: int), _col8 (type: string), _col9 (type: string)
-              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+            File Output Operator
+              compressed: false
               Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
-              File Output Operator
-                compressed: false
-                Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
-                table:
-                    input format: org.apache.hadoop.mapred.TextInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
 
   Stage: Stage-0
     Fetch Operator
@@ -123,18 +127,22 @@ STAGE PLANS:
   Stage: Stage-4
     Map Reduce Local Work
       Alias -> Map Local Tables:
-        r 
+        $hdt$_1:r
           Fetch Operator
             limit: -1
       Alias -> Map Local Operator Tree:
-        r 
+        $hdt$_1:r
           TableScan
             alias: r
             Statistics: Num rows: 2 Data size: 47 Basic stats: COMPLETE Column stats: NONE
-            HashTable Sink Operator
-              keys:
-                0 lk1 (type: string), lk2 (type: string)
-                1 rk1 (type: string), rk2 (type: string)
+            Select Operator
+              expressions: ra (type: int), rk1 (type: string), rk2 (type: string)
+              outputColumnNames: _col0, _col1, _col2
+              Statistics: Num rows: 2 Data size: 47 Basic stats: COMPLETE Column stats: NONE
+              HashTable Sink Operator
+                keys:
+                  0 _col2 (type: string), _col3 (type: string)
+                  1 _col1 (type: string), _col2 (type: string)
 
   Stage: Stage-3
     Map Reduce
@@ -142,20 +150,20 @@ STAGE PLANS:
           TableScan
             alias: l
             Statistics: Num rows: 2 Data size: 52 Basic stats: COMPLETE Column stats: NONE
-            Map Join Operator
-              condition map:
-                   Left Outer Join0 to 1
-              keys:
-                0 lk1 (type: string), lk2 (type: string)
-                1 rk1 (type: string), rk2 (type: string)
-              outputColumnNames: _col0, _col1, _col2, _col3, _col7, _col8, _col9
-              Statistics: Num rows: 2 Data size: 57 Basic stats: COMPLETE Column stats: NONE
-              Filter Operator
-                predicate: (COALESCE(_col1,'EMPTY') = COALESCE(_col7,'EMPTY')) (type: boolean)
-                Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
-                Select Operator
-                  expressions: _col0 (type: int), _col1 (type: int), _col2 (type: string), _col3 (type: string), _col7 (type: int), _col8 (type: string), _col9 (type: string)
-                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+            Select Operator
+              expressions: index (type: int), la (type: int), lk1 (type: string), lk2 (type: string)
+              outputColumnNames: _col0, _col1, _col2, _col3
+              Statistics: Num rows: 2 Data size: 52 Basic stats: COMPLETE Column stats: NONE
+              Map Join Operator
+                condition map:
+                     Left Outer Join0 to 1
+                keys:
+                  0 _col2 (type: string), _col3 (type: string)
+                  1 _col1 (type: string), _col2 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 2 Data size: 57 Basic stats: COMPLETE Column stats: NONE
+                Filter Operator
+                  predicate: (COALESCE(_col1,'EMPTY') = COALESCE(_col4,'EMPTY')) (type: boolean)
                   Statistics: Num rows: 1 Data size: 28 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
diff --git a/ql/src/test/results/clientpositive/literal_decimal.q.out b/ql/src/test/results/clientpositive/literal_decimal.q.out
index eddc1a44e5..0b6299b077 100644
--- a/ql/src/test/results/clientpositive/literal_decimal.q.out
+++ b/ql/src/test/results/clientpositive/literal_decimal.q.out
@@ -14,12 +14,12 @@ STAGE PLANS:
           alias: src
           Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
           Select Operator
-            expressions: -1 (type: decimal(1,0)), 0 (type: decimal(1,0)), 1 (type: decimal(1,0)), 3.14 (type: decimal(3,2)), -3.14 (type: decimal(3,2)), 99999999999999999 (type: decimal(17,0)), 99999999999999999.9999999999999 (type: decimal(30,13)), null (type: decimal(1,0))
+            expressions: -1 (type: int), 0 (type: int), 1 (type: int), 3.14 (type: decimal(3,2)), -3.14 (type: decimal(3,2)), 99999999999999999 (type: bigint), 99999999999999999.9999999999999 (type: decimal(30,13)), null (type: void)
             outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7
-            Statistics: Num rows: 500 Data size: 392000 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 500 Data size: 178000 Basic stats: COMPLETE Column stats: COMPLETE
             Limit
               Number of rows: 1
-              Statistics: Num rows: 1 Data size: 784 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 1 Data size: 356 Basic stats: COMPLETE Column stats: COMPLETE
               ListSink
 
 PREHOOK: query: SELECT -1BD, 0BD, 1BD, 3.14BD, -3.14BD, 99999999999999999BD, 99999999999999999.9999999999999BD, 1E99BD FROM src LIMIT 1
diff --git a/ql/src/test/results/clientpositive/load_dyn_part14.q.out b/ql/src/test/results/clientpositive/load_dyn_part14.q.out
index b35cfaf959..53e9df367f 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part14.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part14.q.out
@@ -74,13 +74,13 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
             Select Operator
-              Statistics: Num rows: 500 Data size: 43000 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 500 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
               Limit
                 Number of rows: 2
-                Statistics: Num rows: 2 Data size: 172 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                 Reduce Output Operator
                   sort order: 
-                  Statistics: Num rows: 2 Data size: 172 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                   TopN Hash Memory Usage: 0.1
       Reduce Operator Tree:
         Limit
@@ -195,13 +195,13 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
             Select Operator
-              Statistics: Num rows: 500 Data size: 85000 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 500 Data size: 2000 Basic stats: COMPLETE Column stats: COMPLETE
               Limit
                 Number of rows: 2
-                Statistics: Num rows: 2 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                 Reduce Output Operator
                   sort order: 
-                  Statistics: Num rows: 2 Data size: 340 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                   TopN Hash Memory Usage: 0.1
       Reduce Operator Tree:
         Limit
@@ -225,13 +225,13 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
             Select Operator
-              Statistics: Num rows: 500 Data size: 85500 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 500 Data size: 2000 Basic stats: COMPLETE Column stats: COMPLETE
               Limit
                 Number of rows: 2
-                Statistics: Num rows: 2 Data size: 342 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                 Reduce Output Operator
                   sort order: 
-                  Statistics: Num rows: 2 Data size: 342 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                   TopN Hash Memory Usage: 0.1
       Reduce Operator Tree:
         Limit
diff --git a/ql/src/test/results/clientpositive/offset_limit.q.out b/ql/src/test/results/clientpositive/offset_limit.q.out
index 2092c1d1bf..0838a68ca9 100644
--- a/ql/src/test/results/clientpositive/offset_limit.q.out
+++ b/ql/src/test/results/clientpositive/offset_limit.q.out
@@ -17,12 +17,12 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: key (type: string), value (type: string)
-              outputColumnNames: key, value
+              expressions: key (type: string), substr(value, 5) (type: string)
+              outputColumnNames: _col0, _col1
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: sum(substr(value, 5))
-                keys: key (type: string)
+                aggregations: sum(_col1)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
diff --git a/ql/src/test/results/clientpositive/offset_limit_ppd_optimizer.q.out b/ql/src/test/results/clientpositive/offset_limit_ppd_optimizer.q.out
index b91bf3d422..8b30daa3a8 100644
--- a/ql/src/test/results/clientpositive/offset_limit_ppd_optimizer.q.out
+++ b/ql/src/test/results/clientpositive/offset_limit_ppd_optimizer.q.out
@@ -172,12 +172,12 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: value (type: string), key (type: string)
-              outputColumnNames: value, key
+              expressions: value (type: string), (UDFToDouble(key) + 1.0) (type: double)
+              outputColumnNames: _col0, _col1
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: sum((key + 1))
-                keys: value (type: string)
+                aggregations: sum(_col1)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
@@ -261,12 +261,12 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: value (type: string), key (type: string)
-              outputColumnNames: value, key
+              expressions: value (type: string), (UDFToDouble(key) + 1.0) (type: double)
+              outputColumnNames: _col0, _col1
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: avg((key + 1))
-                keys: value (type: string)
+                aggregations: avg(_col1)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
@@ -1050,15 +1050,15 @@ STAGE PLANS:
             alias: src
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: key (type: string), value (type: string)
-              outputColumnNames: key, value
+              expressions: concat(key, value, value, value, value, value, value, value, value, value) (type: string), key (type: string)
+              outputColumnNames: _col0, _col1
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Reduce Output Operator
-                key expressions: concat(key, value, value, value, value, value, value, value, value, value) (type: string)
+                key expressions: _col0 (type: string)
                 sort order: +
-                Map-reduce partition columns: concat(key, value, value, value, value, value, value, value, value, value) (type: string)
+                Map-reduce partition columns: _col0 (type: string)
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                value expressions: key (type: string)
+                value expressions: _col1 (type: string)
       Reduce Operator Tree:
         Group By Operator
           aggregations: sum(VALUE._col0)
@@ -1219,12 +1219,12 @@ STAGE PLANS:
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: string)
-              outputColumnNames: key
+              outputColumnNames: _col0
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Reduce Output Operator
-                key expressions: key (type: string)
+                key expressions: _col0 (type: string)
                 sort order: +
-                Map-reduce partition columns: key (type: string)
+                Map-reduce partition columns: _col0 (type: string)
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                 TopN Hash Memory Usage: 2.0E-5
       Reduce Operator Tree:
@@ -1294,12 +1294,12 @@ STAGE PLANS:
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: string)
-              outputColumnNames: key
+              outputColumnNames: _col0
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
               Reduce Output Operator
-                key expressions: key (type: string)
+                key expressions: _col0 (type: string)
                 sort order: +
-                Map-reduce partition columns: key (type: string)
+                Map-reduce partition columns: _col0 (type: string)
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                 TopN Hash Memory Usage: 2.0E-5
       Reduce Operator Tree:
diff --git a/ql/src/test/results/clientpositive/perf/query12.q.out b/ql/src/test/results/clientpositive/perf/query12.q.out
index ce3654ab7e..f326a926e8 100644
--- a/ql/src/test/results/clientpositive/perf/query12.q.out
+++ b/ql/src/test/results/clientpositive/perf/query12.q.out
@@ -2,8 +2,6 @@ PREHOOK: query: explain select i_item_desc ,i_category ,i_class ,i_current_price
 PREHOOK: type: QUERY
 POSTHOOK: query: explain select i_item_desc ,i_category ,i_class ,i_current_price ,i_item_id ,sum(ws_ext_sales_price) as itemrevenue ,sum(ws_ext_sales_price)*100/sum(sum(ws_ext_sales_price)) over (partition by i_class) as revenueratio from web_sales ,item ,date_dim where web_sales.ws_item_sk = item.i_item_sk and item.i_category in ('Jewelry', 'Sports', 'Books') and web_sales.ws_sold_date_sk = date_dim.d_date_sk and date_dim.d_date between '2001-01-12' and '2001-02-11' group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratio limit 100
 POSTHOOK: type: QUERY
-Plan not optimized by CBO.
-
 Vertex dependency in root stage
 Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 7 (SIMPLE_EDGE)
 Reducer 3 <- Map 8 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)
diff --git a/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out b/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
index 41b6fa284b..4559b39c0b 100644
--- a/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
+++ b/ql/src/test/results/clientpositive/rand_partitionpruner2.q.out
@@ -396,7 +396,7 @@ POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
 POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
 POSTHOOK: Output: default@tmptable
 POSTHOOK: Lineage: tmptable.ds SIMPLE [(srcpart)a.FieldSchema(name:hr, type:string, comment:null), ]
-POSTHOOK: Lineage: tmptable.hr SIMPLE [(srcpart)a.FieldSchema(name:ds, type:string, comment:null), ]
+POSTHOOK: Lineage: tmptable.hr SIMPLE []
 POSTHOOK: Lineage: tmptable.key SIMPLE [(srcpart)a.FieldSchema(name:key, type:string, comment:default), ]
 POSTHOOK: Lineage: tmptable.value SIMPLE [(srcpart)a.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: select * from tmptable x sort by x.key,x.value,x.ds,x.hr
diff --git a/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out b/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out
index 610618838a..4b29056379 100644
--- a/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out
+++ b/ql/src/test/results/clientpositive/smb_mapjoin_18.q.out
@@ -238,17 +238,28 @@ STAGE PLANS:
               predicate: (key = 238) (type: boolean)
               Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Select Operator
-                expressions: key (type: int), value (type: string)
+                expressions: 238 (type: int), value (type: string)
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
+                Reduce Output Operator
+                  key expressions: _col0 (type: int)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: int)
                   Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                      name: default.test_table2
+                  value expressions: _col1 (type: string)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string)
+          outputColumnNames: _col0, _col1
+          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                name: default.test_table2
 
   Stage: Stage-0
     Move Operator
@@ -277,7 +288,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
 POSTHOOK: Input: default@test_table1@ds=1
 POSTHOOK: Output: default@test_table2@ds=2
-POSTHOOK: Lineage: test_table2 PARTITION(ds=2).key SIMPLE [(test_table1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=2).key SIMPLE []
 POSTHOOK: Lineage: test_table2 PARTITION(ds=2).value SIMPLE [(test_table1)a.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: select count(*) from test_table2 where ds = '2'
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/spark/insert1.q.out b/ql/src/test/results/clientpositive/spark/insert1.q.out
index e72ba16605..50e83767d1 100644
--- a/ql/src/test/results/clientpositive/spark/insert1.q.out
+++ b/ql/src/test/results/clientpositive/spark/insert1.q.out
@@ -26,7 +26,7 @@ POSTHOOK: query: insert overwrite table insert1 select a.key, a.value from inser
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@insert2
 POSTHOOK: Output: default@insert1
-POSTHOOK: Lineage: insert1.key SIMPLE [(insert2)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: insert1.key SIMPLE []
 POSTHOOK: Lineage: insert1.value SIMPLE [(insert2)a.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: explain insert into table insert1 select a.key, a.value from insert2 a WHERE (a.key=-1)
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/spark/load_dyn_part14.q.out b/ql/src/test/results/clientpositive/spark/load_dyn_part14.q.out
index 05ccf8bb30..84d99c39ef 100644
--- a/ql/src/test/results/clientpositive/spark/load_dyn_part14.q.out
+++ b/ql/src/test/results/clientpositive/spark/load_dyn_part14.q.out
@@ -73,13 +73,13 @@ STAGE PLANS:
                   alias: src
                   Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: COMPLETE
                   Select Operator
-                    Statistics: Num rows: 500 Data size: 43000 Basic stats: COMPLETE Column stats: COMPLETE
+                    Statistics: Num rows: 500 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                     Limit
                       Number of rows: 2
-                      Statistics: Num rows: 2 Data size: 172 Basic stats: COMPLETE Column stats: COMPLETE
+                      Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                       Reduce Output Operator
                         sort order: 
-                        Statistics: Num rows: 2 Data size: 172 Basic stats: COMPLETE Column stats: COMPLETE
+                        Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                         TopN Hash Memory Usage: 0.1
         Reducer 2 
             Reduce Operator Tree:
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out
index 3076e0639b..8822447851 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_18.q.out
@@ -233,6 +233,8 @@ STAGE DEPENDENCIES:
 STAGE PLANS:
   Stage: Stage-1
     Spark
+      Edges:
+        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 1)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -244,17 +246,29 @@ STAGE PLANS:
                     predicate: (key = 238) (type: boolean)
                     Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
-                      expressions: key (type: int), value (type: string)
+                      expressions: 238 (type: int), value (type: string)
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
+                      Reduce Output Operator
+                        key expressions: _col0 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: int)
                         Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            name: default.test_table2
+                        value expressions: _col1 (type: string)
+        Reducer 2
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string)
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.test_table2
 
   Stage: Stage-0
     Move Operator
@@ -283,7 +297,7 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@test_table1
 POSTHOOK: Input: default@test_table1@ds=1
 POSTHOOK: Output: default@test_table2@ds=2
-POSTHOOK: Lineage: test_table2 PARTITION(ds=2).key SIMPLE [(test_table1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=2).key SIMPLE []
 POSTHOOK: Lineage: test_table2 PARTITION(ds=2).value SIMPLE [(test_table1)a.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: select count(*) from test_table2 where ds = '2'
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_8.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_8.q.out
index babaff814a..17821a893c 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_8.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_8.q.out
@@ -150,7 +150,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_1 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=5000
 PREHOOK: type: QUERY
@@ -160,7 +160,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_2
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 PREHOOK: type: QUERY
@@ -335,7 +335,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_1 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=5000
 PREHOOK: type: QUERY
@@ -345,7 +345,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_2
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=4000
 PREHOOK: type: QUERY
@@ -355,7 +355,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_3 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_3
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
@@ -381,7 +381,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_1 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=4000
 PREHOOK: type: QUERY
@@ -391,7 +391,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_2
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=5000
 PREHOOK: type: QUERY
@@ -401,7 +401,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_3 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_3
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
@@ -427,7 +427,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_1 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_1
-POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_1.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_1.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=4000
 PREHOOK: type: QUERY
@@ -437,7 +437,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_2 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_2
-POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_2.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=5000
 PREHOOK: type: QUERY
@@ -447,7 +447,7 @@ POSTHOOK: query: insert overwrite table smb_bucket4_3 select * from smb_bucket_i
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_input
 POSTHOOK: Output: default@smb_bucket4_3
-POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_bucket4_3.key SIMPLE []
 POSTHOOK: Lineage: smb_bucket4_3.value SIMPLE [(smb_bucket_input)smb_bucket_input.FieldSchema(name:value, type:string, comment:null), ]
 PREHOOK: query: select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
 full outer join smb_bucket4_3 c on a.key=c.key
diff --git a/ql/src/test/results/clientpositive/spark/union_date_trim.q.out b/ql/src/test/results/clientpositive/spark/union_date_trim.q.out
index e2f5269345..324e8b7ddb 100644
--- a/ql/src/test/results/clientpositive/spark/union_date_trim.q.out
+++ b/ql/src/test/results/clientpositive/spark/union_date_trim.q.out
@@ -51,4 +51,4 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@testdate
 POSTHOOK: Output: default@testdate
 POSTHOOK: Lineage: testdate.dt EXPRESSION [(testdate)testdate.FieldSchema(name:dt, type:date, comment:null), ]
-POSTHOOK: Lineage: testdate.id EXPRESSION [(testdate)testdate.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: testdate.id EXPRESSION []
diff --git a/ql/src/test/results/clientpositive/stats_empty_partition.q.out b/ql/src/test/results/clientpositive/stats_empty_partition.q.out
index 0ad031c48c..d7f2e735d7 100644
--- a/ql/src/test/results/clientpositive/stats_empty_partition.q.out
+++ b/ql/src/test/results/clientpositive/stats_empty_partition.q.out
@@ -20,7 +20,7 @@ POSTHOOK: query: insert overwrite table tmptable partition (part = '1') select *
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@tmptable@part=1
-POSTHOOK: Lineage: tmptable PARTITION(part=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: tmptable PARTITION(part=1).key SIMPLE []
 POSTHOOK: Lineage: tmptable PARTITION(part=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: describe formatted tmptable partition (part = '1')
 PREHOOK: type: DESCTABLE
diff --git a/ql/src/test/results/clientpositive/tez/empty_join.q.out b/ql/src/test/results/clientpositive/tez/empty_join.q.out
index 06eea6f1d0..4c89317ade 100644
--- a/ql/src/test/results/clientpositive/tez/empty_join.q.out
+++ b/ql/src/test/results/clientpositive/tez/empty_join.q.out
@@ -50,7 +50,7 @@ FROM test_1 t1
 LEFT JOIN test_2 t2 ON t1.id = t2.id
 INNER JOIN test_3 t3 ON t1.id = t3.id
 POSTHOOK: type: QUERY
-Plan not optimized by CBO.
+Plan optimized by CBO.
 
 Vertex dependency in root stage
 Map 1 <- Map 2 (BROADCAST_EDGE), Map 3 (BROADCAST_EDGE)
@@ -60,23 +60,33 @@ Stage-0
     limit:-1
     Stage-1
       Map 1
-      File Output Operator [FS_8]
-        Select Operator [SEL_7] (rows=2 width=1)
-          Output:["_col0","_col1","_col2"]
-          Map Join Operator [MAPJOIN_16] (rows=2 width=1)
-            Conds:TS_0.id=RS_4.id(Left Outer),TS_0.id=RS_5.id(Inner),Output:["_col0","_col4","_col8"]
-          <-Map 2 [BROADCAST_EDGE]
-            BROADCAST [RS_4]
-              PartitionCols:id
-              TableScan [TS_1] (rows=1 width=0)
-                default@test_2,t2,Tbl:PARTIAL,Col:NONE,Output:["id"]
-          <-Map 3 [BROADCAST_EDGE]
-            BROADCAST [RS_5]
-              PartitionCols:id
-              TableScan [TS_2] (rows=1 width=1)
-                default@test_3,t3,Tbl:COMPLETE,Col:NONE,Output:["id"]
-          <-TableScan [TS_0] (rows=1 width=1)
-              default@test_1,t1,Tbl:COMPLETE,Col:NONE,Output:["id"]
+      File Output Operator [FS_14]
+        Map Join Operator [MAPJOIN_22] (rows=2 width=1)
+          Conds:SEL_2._col0=RS_10._col0(Left Outer),SEL_2._col0=RS_11._col0(Inner),Output:["_col0","_col1","_col2"]
+        <-Map 2 [BROADCAST_EDGE]
+          BROADCAST [RS_10]
+            PartitionCols:_col0
+            Select Operator [SEL_5] (rows=1 width=0)
+              Output:["_col0"]
+              Filter Operator [FIL_20] (rows=1 width=0)
+                predicate:id is not null
+                TableScan [TS_3] (rows=1 width=0)
+                  default@test_2,t2,Tbl:PARTIAL,Col:NONE,Output:["id"]
+        <-Map 3 [BROADCAST_EDGE]
+          BROADCAST [RS_11]
+            PartitionCols:_col0
+            Select Operator [SEL_8] (rows=1 width=1)
+              Output:["_col0"]
+              Filter Operator [FIL_21] (rows=1 width=1)
+                predicate:id is not null
+                TableScan [TS_6] (rows=1 width=1)
+                  default@test_3,t3,Tbl:COMPLETE,Col:NONE,Output:["id"]
+        <-Select Operator [SEL_2] (rows=1 width=1)
+            Output:["_col0"]
+            Filter Operator [FIL_19] (rows=1 width=1)
+              predicate:id is not null
+              TableScan [TS_0] (rows=1 width=1)
+                default@test_1,t1,Tbl:COMPLETE,Col:NONE,Output:["id"]
 
 PREHOOK: query: SELECT t1.id, t2.id, t3.id
 FROM test_1 t1
diff --git a/ql/src/test/results/clientpositive/tez/vector_when_case_null.q.out b/ql/src/test/results/clientpositive/tez/vector_when_case_null.q.out
index b32dd30081..eb5806aafb 100644
--- a/ql/src/test/results/clientpositive/tez/vector_when_case_null.q.out
+++ b/ql/src/test/results/clientpositive/tez/vector_when_case_null.q.out
@@ -44,12 +44,12 @@ STAGE PLANS:
                   alias: count_case_groupby
                   Statistics: Num rows: 5 Data size: 452 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
-                    expressions: key (type: string), bool (type: boolean)
-                    outputColumnNames: key, bool
+                    expressions: key (type: string), CASE WHEN (bool) THEN (1) WHEN ((not bool)) THEN (0) ELSE (null) END (type: int)
+                    outputColumnNames: _col0, _col1
                     Statistics: Num rows: 5 Data size: 452 Basic stats: COMPLETE Column stats: NONE
                     Group By Operator
-                      aggregations: count(CASE WHEN (bool) THEN (1) WHEN ((not bool)) THEN (0) ELSE (null) END)
-                      keys: key (type: string)
+                      aggregations: count(_col1)
+                      keys: _col0 (type: string)
                       mode: hash
                       outputColumnNames: _col0, _col1
                       Statistics: Num rows: 5 Data size: 452 Basic stats: COMPLETE Column stats: NONE
diff --git a/ql/src/test/results/clientpositive/udf_concat_insert2.q.out b/ql/src/test/results/clientpositive/udf_concat_insert2.q.out
index f1b70fe40f..d68bd768c4 100644
--- a/ql/src/test/results/clientpositive/udf_concat_insert2.q.out
+++ b/ql/src/test/results/clientpositive/udf_concat_insert2.q.out
@@ -16,7 +16,7 @@ INSERT OVERWRITE TABLE dest1 SELECT concat('1234', 'abc', 'extra argument'), src
 POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src
 POSTHOOK: Output: default@dest1
-POSTHOOK: Lineage: dest1.key EXPRESSION []
+POSTHOOK: Lineage: dest1.key SIMPLE []
 POSTHOOK: Lineage: dest1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
 PREHOOK: query: SELECT dest1.* FROM dest1
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/union_date_trim.q.out b/ql/src/test/results/clientpositive/union_date_trim.q.out
index e2f5269345..324e8b7ddb 100644
--- a/ql/src/test/results/clientpositive/union_date_trim.q.out
+++ b/ql/src/test/results/clientpositive/union_date_trim.q.out
@@ -51,4 +51,4 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@testdate
 POSTHOOK: Output: default@testdate
 POSTHOOK: Lineage: testdate.dt EXPRESSION [(testdate)testdate.FieldSchema(name:dt, type:date, comment:null), ]
-POSTHOOK: Lineage: testdate.id EXPRESSION [(testdate)testdate.FieldSchema(name:id, type:int, comment:null), ]
+POSTHOOK: Lineage: testdate.id EXPRESSION []
diff --git a/ql/src/test/results/clientpositive/union_remove_26.q.out b/ql/src/test/results/clientpositive/union_remove_26.q.out
index 8afaf08514..3e2cc18577 100644
--- a/ql/src/test/results/clientpositive/union_remove_26.q.out
+++ b/ql/src/test/results/clientpositive/union_remove_26.q.out
@@ -151,10 +151,10 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -184,10 +184,10 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -217,10 +217,10 @@ STAGE PLANS:
             Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -359,11 +359,11 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: string), val (type: int)
-              outputColumnNames: key, val
+              outputColumnNames: _col0, _col2
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
-                keys: key (type: string)
+                aggregations: count(1), min(_col2), max(_col2)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
@@ -396,11 +396,11 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: string), val (type: int)
-              outputColumnNames: key, val
+              outputColumnNames: _col0, _col2
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
-                keys: key (type: string)
+                aggregations: count(1), min(_col2), max(_col2)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
@@ -433,11 +433,11 @@ STAGE PLANS:
             Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: key (type: string), val (type: int)
-              outputColumnNames: key, val
+              outputColumnNames: _col0, _col2
               Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
-                keys: key (type: string)
+                aggregations: count(1), min(_col2), max(_col2)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -520,10 +520,10 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -553,10 +553,10 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -586,10 +586,10 @@ STAGE PLANS:
             Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -670,10 +670,10 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -736,10 +736,10 @@ STAGE PLANS:
             Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 6 Data size: 24 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
@@ -768,10 +768,10 @@ STAGE PLANS:
             Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
             Select Operator
               expressions: val (type: int)
-              outputColumnNames: val
+              outputColumnNames: _col1
               Statistics: Num rows: 4 Data size: 16 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(1), min(val), max(val)
+                aggregations: count(1), min(_col1), max(_col1)
                 mode: hash
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE
diff --git a/ql/src/test/results/clientpositive/vector_when_case_null.q.out b/ql/src/test/results/clientpositive/vector_when_case_null.q.out
index 679fddc37f..b21bc90175 100644
--- a/ql/src/test/results/clientpositive/vector_when_case_null.q.out
+++ b/ql/src/test/results/clientpositive/vector_when_case_null.q.out
@@ -38,12 +38,12 @@ STAGE PLANS:
             alias: count_case_groupby
             Statistics: Num rows: 5 Data size: 452 Basic stats: COMPLETE Column stats: NONE
             Select Operator
-              expressions: key (type: string), bool (type: boolean)
-              outputColumnNames: key, bool
+              expressions: key (type: string), CASE WHEN (bool) THEN (1) WHEN ((not bool)) THEN (0) ELSE (null) END (type: int)
+              outputColumnNames: _col0, _col1
               Statistics: Num rows: 5 Data size: 452 Basic stats: COMPLETE Column stats: NONE
               Group By Operator
-                aggregations: count(CASE WHEN (bool) THEN (1) WHEN ((not bool)) THEN (0) ELSE (null) END)
-                keys: key (type: string)
+                aggregations: count(_col1)
+                keys: _col0 (type: string)
                 mode: hash
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 5 Data size: 452 Basic stats: COMPLETE Column stats: NONE
