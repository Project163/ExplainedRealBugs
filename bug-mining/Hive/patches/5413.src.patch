diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
index 86d964c920..4b76d746b9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java
@@ -815,6 +815,12 @@ public void doProcessBatch(VectorizedRowBatch batch, boolean isFirstGroupingSet,
       if (first) {
         // Copy the group key to output batch now.  We'll copy in the aggregates at the end of the group.
         first = false;
+
+        // Evaluate the key expressions of just this first batch to get the correct key.
+        for (int i = 0; i < outputKeyLength; i++) {
+          keyExpressions[i].evaluate(batch);
+        }
+
         groupKeyHelper.copyGroupKey(batch, outputBatch, buffer);
       }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java
index 0ff389e832..64706ad19b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java
@@ -32,6 +32,8 @@
  */
 public class VectorGroupKeyHelper extends VectorColumnSetInfo {
 
+  private int[] outputColumnNums;
+
   public VectorGroupKeyHelper(int keyCount) {
     super(keyCount);
    }
@@ -41,12 +43,14 @@ void init(VectorExpression[] keyExpressions) throws HiveException {
     // NOTE: To support pruning the grouping set id dummy key by VectorGroupbyOpeator MERGE_PARTIAL
     // case, we use the keyCount passed to the constructor and not keyExpressions.length.
 
-    // Inspect the output type of each key expression.
+    // Inspect the output type of each key expression.  And, remember the output columns.
+    outputColumnNums = new int[keyCount];
     for(int i=0; i < keyCount; ++i) {
       String typeName = VectorizationContext.mapTypeNameSynonyms(keyExpressions[i].getOutputType());
       TypeInfo typeInfo = TypeInfoUtils.getTypeInfoFromTypeString(typeName);
       Type columnVectorType = VectorizationContext.getColumnVectorTypeFromTypeInfo(typeInfo);
       addKey(columnVectorType);
+      outputColumnNums[i] = keyExpressions[i].getOutputColumn();
     }
     finishAdding();
   }
@@ -61,9 +65,9 @@ void init(VectorExpression[] keyExpressions) throws HiveException {
   public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outputBatch,
           DataOutputBuffer buffer) throws HiveException {
     for(int i = 0; i< longIndices.length; ++i) {
-      int keyIndex = longIndices[i];
-      LongColumnVector inputColumnVector = (LongColumnVector) inputBatch.cols[keyIndex];
-      LongColumnVector outputColumnVector = (LongColumnVector) outputBatch.cols[keyIndex];
+      final int columnIndex = outputColumnNums[longIndices[i]];
+      LongColumnVector inputColumnVector = (LongColumnVector) inputBatch.cols[columnIndex];
+      LongColumnVector outputColumnVector = (LongColumnVector) outputBatch.cols[columnIndex];
 
       // This vectorized code pattern says: 
       //    If the input batch has no nulls at all (noNulls is true) OR
@@ -87,9 +91,9 @@ public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outpu
       }
     }
     for(int i=0;i<doubleIndices.length; ++i) {
-      int keyIndex = doubleIndices[i];
-      DoubleColumnVector inputColumnVector = (DoubleColumnVector) inputBatch.cols[keyIndex];
-      DoubleColumnVector outputColumnVector = (DoubleColumnVector) outputBatch.cols[keyIndex];
+      final int columnIndex = outputColumnNums[doubleIndices[i]];
+      DoubleColumnVector inputColumnVector = (DoubleColumnVector) inputBatch.cols[columnIndex];
+      DoubleColumnVector outputColumnVector = (DoubleColumnVector) outputBatch.cols[columnIndex];
       if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
         outputColumnVector.vector[outputBatch.size] = inputColumnVector.vector[0];
       } else {
@@ -98,9 +102,9 @@ public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outpu
       }
     }
     for(int i=0;i<stringIndices.length; ++i) {
-      int keyIndex = stringIndices[i];
-      BytesColumnVector inputColumnVector = (BytesColumnVector) inputBatch.cols[keyIndex];
-      BytesColumnVector outputColumnVector = (BytesColumnVector) outputBatch.cols[keyIndex];
+      final int columnIndex = outputColumnNums[stringIndices[i]];
+      BytesColumnVector inputColumnVector = (BytesColumnVector) inputBatch.cols[columnIndex];
+      BytesColumnVector outputColumnVector = (BytesColumnVector) outputBatch.cols[columnIndex];
       if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
         // Copy bytes into scratch buffer.
         int start = buffer.getLength();
@@ -117,9 +121,9 @@ public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outpu
       }
     }
     for(int i=0;i<decimalIndices.length; ++i) {
-      int keyIndex = decimalIndices[i];
-      DecimalColumnVector inputColumnVector = (DecimalColumnVector) inputBatch.cols[keyIndex];
-      DecimalColumnVector outputColumnVector = (DecimalColumnVector) outputBatch.cols[keyIndex];
+      final int columnIndex = outputColumnNums[decimalIndices[i]];
+      DecimalColumnVector inputColumnVector = (DecimalColumnVector) inputBatch.cols[columnIndex];
+      DecimalColumnVector outputColumnVector = (DecimalColumnVector) outputBatch.cols[columnIndex];
       if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
 
         // Since we store references to HiveDecimalWritable instances, we must use the update method instead
@@ -131,9 +135,9 @@ public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outpu
       }
     }
     for(int i=0;i<timestampIndices.length; ++i) {
-      int keyIndex = timestampIndices[i];
-      TimestampColumnVector inputColumnVector = (TimestampColumnVector) inputBatch.cols[keyIndex];
-      TimestampColumnVector outputColumnVector = (TimestampColumnVector) outputBatch.cols[keyIndex];
+      final int columnIndex = outputColumnNums[timestampIndices[i]];
+      TimestampColumnVector inputColumnVector = (TimestampColumnVector) inputBatch.cols[columnIndex];
+      TimestampColumnVector outputColumnVector = (TimestampColumnVector) outputBatch.cols[columnIndex];
       if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
 
         outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);
@@ -143,9 +147,9 @@ public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outpu
       }
     }
     for(int i=0;i<intervalDayTimeIndices.length; ++i) {
-      int keyIndex = intervalDayTimeIndices[i];
-      IntervalDayTimeColumnVector inputColumnVector = (IntervalDayTimeColumnVector) inputBatch.cols[keyIndex];
-      IntervalDayTimeColumnVector outputColumnVector = (IntervalDayTimeColumnVector) outputBatch.cols[keyIndex];
+      final int columnIndex = outputColumnNums[intervalDayTimeIndices[i]];
+      IntervalDayTimeColumnVector inputColumnVector = (IntervalDayTimeColumnVector) inputBatch.cols[columnIndex];
+      IntervalDayTimeColumnVector outputColumnVector = (IntervalDayTimeColumnVector) outputBatch.cols[columnIndex];
       if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
 
         outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
index 2f853795a6..1572384780 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java
@@ -2022,17 +2022,6 @@ private boolean validateGroupByOperator(GroupByOperator op, boolean isReduce, bo
       return false;
     }
 
-    if (processingMode == ProcessingMode.MERGE_PARTIAL) {
-      // For now, VectorGroupByOperator ProcessingModeReduceMergePartial cannot handle key
-      // expressions.
-      for (ExprNodeDesc keyExpr : desc.getKeys()) {
-        if (!(keyExpr instanceof ExprNodeColumnDesc)) {
-          setExpressionIssue("Key", "Non-column key expressions not supported for MERGEPARTIAL");
-          return false;
-        }
-      }
-    }
-
     Pair<Boolean,Boolean> retPair =
         validateAggregationDescs(desc.getAggregators(), processingMode, hasKeys);
     if (!retPair.left) {
diff --git a/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out b/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out
index 9eb2747938..c3809d36a9 100644
--- a/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out
+++ b/ql/src/test/results/clientpositive/llap/vector_groupby_grouping_id3.q.out
@@ -122,15 +122,29 @@ STAGE PLANS:
                     partitionColumnCount: 0
                     scratchColumnTypeNames: bigint
         Reducer 2 
-            Execution mode: llap
+            Execution mode: vectorized, llap
             Reduce Vectorization:
                 enabled: true
                 enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
-                notVectorizedReason: Key expression for GROUPBY operator: Non-column key expressions not supported for MERGEPARTIAL
-                vectorized: false
+                groupByVectorOutput: true
+                allNative: false
+                usesVectorUDFAdaptor: false
+                vectorized: true
+                rowBatchContext:
+                    dataColumnCount: 4
+                    dataColumns: KEY._col0:int, KEY._col1:int, KEY._col2:int, VALUE._col0:bigint
+                    partitionColumnCount: 0
+                    scratchColumnTypeNames: bigint
             Reduce Operator Tree:
               Group By Operator
                 aggregations: count(VALUE._col0)
+                Group By Vectorization:
+                    aggregators: VectorUDAFCountMerge(col 3) -> bigint
+                    className: VectorGroupByOperator
+                    vectorOutput: true
+                    keyExpressions: col 0, col 1, ConstantVectorExpression(val 1) -> 4:long
+                    native: false
+                    projectedOutputColumns: [0]
                 keys: KEY._col0 (type: int), KEY._col1 (type: int), 1 (type: int)
                 mode: mergepartial
                 outputColumnNames: _col0, _col1, _col3
@@ -139,9 +153,17 @@ STAGE PLANS:
                 Select Operator
                   expressions: _col0 (type: int), _col1 (type: int), 1 (type: int), _col3 (type: bigint)
                   outputColumnNames: _col0, _col1, _col2, _col3
+                  Select Vectorization:
+                      className: VectorSelectOperator
+                      native: true
+                      projectedOutputColumns: [0, 1, 3, 2]
+                      selectExpressions: ConstantVectorExpression(val 1) -> 3:long
                   Statistics: Num rows: 3 Data size: 20 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
+                    File Sink Vectorization:
+                        className: VectorFileSinkOperator
+                        native: false
                     Statistics: Num rows: 3 Data size: 20 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.SequenceFileInputFormat
