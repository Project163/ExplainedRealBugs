diff --git a/kafka-handler/pom.xml b/kafka-handler/pom.xml
index 0e2a6c3995..daf8ecb862 100644
--- a/kafka-handler/pom.xml
+++ b/kafka-handler/pom.xml
@@ -30,7 +30,6 @@
 
   <properties>
     <hive.path.to.root>..</hive.path.to.root>
-    <kafka.version>2.5.0</kafka.version>
   </properties>
 
   <artifactId>kafka-handler</artifactId>
diff --git a/kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaUtils.java b/kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaUtils.java
index be7c28cbcf..6958409467 100644
--- a/kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaUtils.java
+++ b/kafka-handler/src/java/org/apache/hadoop/hive/kafka/KafkaUtils.java
@@ -27,8 +27,12 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.serde2.AbstractSerDe;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.security.Credentials;
 import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hive.common.util.ReflectionUtil;
 import org.apache.kafka.clients.CommonClientConfigs;
@@ -52,6 +56,7 @@
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.util.Arrays;
+import java.util.Base64;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Objects;
@@ -67,6 +72,9 @@ final class KafkaUtils {
   private final static Logger log = LoggerFactory.getLogger(KafkaUtils.class);
   private static final String JAAS_TEMPLATE = "com.sun.security.auth.module.Krb5LoginModule required "
       + "useKeyTab=true storeKey=true keyTab=\"%s\" principal=\"%s\";";
+  private static final String JAAS_TEMPLATE_SCRAM =
+      "org.apache.kafka.common.security.scram.ScramLoginModule required "
+          + "username=\"%s\" password=\"%s\" serviceName=\"%s\" tokenauth=true;";
 
   private KafkaUtils() {
   }
@@ -370,10 +378,22 @@ static void addKerberosJaasConf(Configuration configuration, Properties props) {
       log.error("Can not construct kerberos principal", e);
       throw new RuntimeException(e);
     }
-    final String jaasConf = String.format(JAAS_TEMPLATE, keyTab, principal);
+    String jaasConf = String.format(JAAS_TEMPLATE, keyTab, principal);
     props.setProperty("sasl.jaas.config", jaasConf);
-    log.info("Kafka client running with following JAAS = [{}]", jaasConf);
-  }
 
+    if (configuration instanceof JobConf) {
+      Credentials creds = ((JobConf) configuration).getCredentials();
+      Token<?> token = creds.getToken(new Text("KAFKA_DELEGATION_TOKEN"));
 
+      if (token != null) {
+        log.info("Kafka delegation token has been found: {}", token);
+        props.setProperty("sasl.mechanism", "SCRAM-SHA-256");
+
+        jaasConf = String.format(JAAS_TEMPLATE_SCRAM, new String(token.getIdentifier()),
+            Base64.getEncoder().encodeToString(token.getPassword()), token.getService());
+        props.setProperty("sasl.jaas.config", jaasConf);
+      }
+    }
+    log.info("Kafka client running with following JAAS = [{}]", jaasConf);
+  }
 }
diff --git a/pom.xml b/pom.xml
index 80132f27af..2e6e9bdd55 100644
--- a/pom.xml
+++ b/pom.xml
@@ -169,6 +169,7 @@
     <junit.version>4.13</junit.version>
     <junit.jupiter.version>5.6.2</junit.jupiter.version>
     <junit.vintage.version>5.6.2</junit.vintage.version>
+    <kafka.version>2.5.0</kafka.version>
     <kryo.version>4.0.2</kryo.version>
     <kudu.version>1.12.0</kudu.version>
     <!-- Leaving libfb303 at 0.9.3 regardless of libthrift: As per THRIFT-4613 The Apache Thrift project does not publish items related to fb303 at this point -->
diff --git a/ql/pom.xml b/ql/pom.xml
index 34b8172d7f..b51def5dc4 100644
--- a/ql/pom.xml
+++ b/ql/pom.xml
@@ -857,6 +857,17 @@
       <artifactId>jersey-multipart</artifactId>
       <version>${jersey.version}</version>
     </dependency>
+    <dependency>
+      <groupId>org.apache.kafka</groupId>
+      <artifactId>kafka-clients</artifactId>
+      <version>${kafka.version}</version>
+      <exclusions>
+        <exclusion>
+          <groupId>org.apache.shiro</groupId>
+          <artifactId>shiro-crypto-cipher</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
   </dependencies>
   <profiles>
     <profile>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index d21aa45d79..dee1842bae 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -19,6 +19,7 @@
 
 import java.util.Collection;
 import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ExecutionException;
 
 import com.google.common.base.Function;
 import com.google.common.base.Preconditions;
@@ -41,6 +42,7 @@
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Properties;
 import java.util.Set;
 import java.util.Stack;
 import java.util.concurrent.TimeUnit;
@@ -52,6 +54,13 @@
 import org.apache.commons.io.FilenameUtils;
 import org.apache.commons.lang3.StringUtils;
 import org.apache.hive.common.util.HiveStringUtils;
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.AdminClientConfig;
+import org.apache.kafka.clients.admin.CreateDelegationTokenOptions;
+import org.apache.kafka.clients.admin.CreateDelegationTokenResult;
+import org.apache.kafka.clients.CommonClientConfigs;
+import org.apache.kafka.common.config.SaslConfigs;
+import org.apache.kafka.common.security.token.delegation.DelegationToken;
 import org.apache.tez.mapreduce.common.MRInputSplitDistributor;
 import org.apache.tez.mapreduce.hadoop.InputSplitInfo;
 import org.apache.tez.mapreduce.output.MROutput;
@@ -103,7 +112,9 @@
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.MergeJoinWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
+import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
+import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.plan.TezEdgeProperty;
 import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;
 import org.apache.hadoop.hive.ql.plan.TezWork;
@@ -116,11 +127,14 @@
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.OutputFormat;
+import org.apache.hadoop.security.SecurityUtil;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hadoop.security.token.Token;
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.hadoop.yarn.api.records.LocalResourceType;
 import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
@@ -180,7 +194,7 @@ public class DagUtils {
       "hive.tez.current.merge.file.prefix";
   // "A comma separated list of work names used as prefix.
   public static final String TEZ_MERGE_WORK_FILE_PREFIXES = "hive.tez.merge.file.prefixes";
-
+  private static final Text KAFKA_DELEGATION_TOKEN_KEY = new Text("KAFKA_DELEGATION_TOKEN");
   /**
    * Notifiers to synchronize resource localization across threads. If one thread is localizing
    * a file, other threads can wait on the corresponding notifier object instead of just sleeping
@@ -244,7 +258,7 @@ private void collectFileSinkUris(List<Node> topNodes, Set<URI> uris) {
   /**
    * Set up credentials for the base work on secure clusters
    */
-  public void addCredentials(BaseWork work, DAG dag) {
+  public void addCredentials(BaseWork work, DAG dag, JobConf conf) {
     if (work instanceof MapWork){
       Set<Path> paths = ((MapWork)work).getPathToAliases().keySet();
       if (!paths.isEmpty()) {
@@ -265,11 +279,74 @@ public URI apply(Path path) {
         }
         dag.addURIsForCredentials(uris);
       }
+      getKafkaCredentials((MapWork)work, dag, conf);
     }
-
     getCredentialsForFileSinks(work, dag);
   }
 
+  private void getKafkaCredentials(MapWork work, DAG dag, JobConf conf) {
+    Token<?> tokenCheck = dag.getCredentials().getToken(KAFKA_DELEGATION_TOKEN_KEY);
+    if (tokenCheck != null) {
+      LOG.debug("Kafka credentials already added, skipping...");
+      return;
+    }
+    LOG.info("Getting kafka credentials for mapwork: " + work.getName());
+
+    String kafkaBrokers = null;
+    Map<String, PartitionDesc> partitions = work.getAliasToPartnInfo();
+    for (PartitionDesc partition : partitions.values()) {
+      TableDesc tableDesc = partition.getTableDesc();
+      kafkaBrokers = (String) tableDesc.getProperties().get("kafka.bootstrap.servers"); //FIXME: KafkaTableProperties
+      if (kafkaBrokers != null && !kafkaBrokers.isEmpty()) {
+        getKafkaDelegationTokenForBrokers(dag, conf, kafkaBrokers);
+        return;
+      } else {
+        // we don't need to iterate on all partitions, and checking the same TableDesc
+        // if partitions[0].getTableDesc() doesn't show kafka table, let's return quickly
+        break;
+      }
+    }
+  }
+
+  private void getKafkaDelegationTokenForBrokers(DAG dag, JobConf conf, String kafkaBrokers) {
+    LOG.debug("Getting kafka credentials for brokers: {}", kafkaBrokers);
+
+    String keytab = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB);
+    String principal = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL);
+    try {
+      principal = SecurityUtil.getServerPrincipal(principal, "0.0.0.0");
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    }
+
+    Properties config = new Properties();
+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokers);
+    config.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_PLAINTEXT");
+
+    String jaasConfig =
+        String.format("%s %s %s %s serviceName=\"%s\" keyTab=\"%s\" principal=\"%s\";",
+            "com.sun.security.auth.module.Krb5LoginModule required", "debug=true", "useKeyTab=true",
+            "storeKey=true", "kafka", keytab, principal);
+    config.put(SaslConfigs.SASL_JAAS_CONFIG, jaasConfig);
+
+    LOG.debug("Jaas config for requesting kafka credentials: {}", jaasConfig);
+    AdminClient admin = AdminClient.create(config);
+
+    CreateDelegationTokenOptions createDelegationTokenOptions = new CreateDelegationTokenOptions();
+    CreateDelegationTokenResult createResult =
+        admin.createDelegationToken(createDelegationTokenOptions);
+    DelegationToken token;
+    try {
+      token = createResult.delegationToken().get();
+    } catch (InterruptedException | ExecutionException e) {
+      throw new RuntimeException("Exception while getting kafka delegation tokens", e);
+    }
+    LOG.info("Got kafka delegation token: {}", token);
+
+    dag.getCredentials().addToken(KAFKA_DELEGATION_TOKEN_KEY,
+        new Token<>(token.tokenInfo().tokenId().getBytes(), token.hmac(), null, new Text("kafka")));
+  }
+
   private void getCredentialsForFileSinks(BaseWork baseWork, DAG dag) {
     Set<URI> fileSinkUris = new HashSet<URI>();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
index 323906c9b1..1d836600c2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
@@ -504,7 +504,7 @@ DAG build(JobConf conf, TezWork tezWork, Path scratchDir, Context ctx,
           wx.setConf(TEZ_MEMORY_RESERVE_FRACTION, Double.toString(frac));
         } // Otherwise just leave it up to Tez to decide how much memory to allocate
         dag.addVertex(wx);
-        utils.addCredentials(workUnit, dag);
+        utils.addCredentials(workUnit, dag, conf);
         perfLogger.perfLogEnd(CLASS_NAME, PerfLogger.TEZ_CREATE_VERTEX + workUnit.getName());
         workToVertex.put(workUnit, wx);
         workToConf.put(workUnit, wxConf);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java
index 5e1be4ee4e..3239b369f8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFGetSplits.java
@@ -474,7 +474,7 @@ private SplitResult getSplits(JobConf job, TezWork work, Schema schema, Applicat
           DagUtils.createTezLrMap(appJarLr, null));
       String vertexName = wx.getName();
       dag.addVertex(wx);
-      utils.addCredentials(mapWork, dag);
+      utils.addCredentials(mapWork, dag, job);
 
 
       // we have the dag now proceed to get the splits:
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestDagUtils.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestDagUtils.java
index 84e3b107ec..37618a631f 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestDagUtils.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestDagUtils.java
@@ -57,7 +57,7 @@ public void testCredentialsNotOverwritten() throws Exception {
     testUser.doAs(new PrivilegedExceptionAction<Void>() {
       @Override
       public Void run() throws Exception {
-        dagUtils.addCredentials(work, dag);
+        dagUtils.addCredentials(work, dag, null);
         return null;
       }
     });
