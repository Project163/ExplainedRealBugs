diff --git a/common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java b/common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java
index 5ed2386b47..5cc12e6cec 100644
--- a/common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java
+++ b/common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java
@@ -101,6 +101,8 @@ public class PerfLogger {
   public static final String LOAD_PARTITION = "LoadPartition";
   public static final String LOAD_DYNAMIC_PARTITIONS = "LoadDynamicPartitions";
 
+  public static final String STATS_TASK = "StatsTask";
+
   public static final String HIVE_GET_TABLE = "getTablesByType";
   public static final String HIVE_GET_DATABASE = "getDatabase";
   public static final String HIVE_GET_DATABASE_2 = "getDatabase2";
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 6ab137b418..e1f16d338d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -78,6 +78,7 @@
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.util.DirectionUtils;
 import org.apache.hadoop.util.StringUtils;
+import org.apache.hadoop.util.Time;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -104,6 +105,7 @@ public class MoveTask extends Task<MoveWork> implements Serializable {
 
   private static final long serialVersionUID = 1L;
   private static transient final Logger LOG = LoggerFactory.getLogger(MoveTask.class);
+  private final PerfLogger perfLogger = SessionState.getPerfLogger();
 
   public MoveTask() {
     super();
@@ -169,7 +171,6 @@ public void flattenUnionSubdirectories(Path sourcePath) throws HiveException {
   private void moveFile(Path sourcePath, Path targetPath, boolean isDfsDir)
       throws HiveException {
     try {
-      PerfLogger perfLogger = SessionState.getPerfLogger();
       perfLogger.perfLogBegin("MoveTask", PerfLogger.FILE_MOVES);
 
       String mesg = "Moving data to " + (isDfsDir ? "" : "local ") + "directory "
@@ -571,7 +572,8 @@ public int execute() {
         }
         releaseLocks(tbd);
       }
-
+      long moveFilesDuration = perfLogger.getDuration(PerfLogger.FILE_MOVES);
+      console.printInfo(String.format("Time taken to move files:\t %d ms", moveFilesDuration));
       return 0;
     } catch (HiveException he) {
       return processHiveException(he);
@@ -687,7 +689,7 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
     Map<Path, Utilities.PartitionDetails> dps = Utilities.getFullDPSpecs(conf, dpCtx, dynamicPartitionSpecs);
 
     console.printInfo(System.getProperty("line.separator"));
-    long startTime = System.currentTimeMillis();
+    long startTime = Time.monotonicNow();
     // load the list of DP partitions and return the list of partition specs
     // TODO: In a follow-up to HIVE-1361, we should refactor loadDynamicPartitions
     // to use Utilities.getFullDPSpecs() to get the list of full partSpecs.
@@ -712,8 +714,8 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
       pushFeed(FeedType.DYNAMIC_PARTITIONS, dp.values());
     }
 
-    String loadTime = "\t Time taken to load dynamic partitions: "  +
-        (System.currentTimeMillis() - startTime)/1000.0 + " seconds";
+    String loadTime = String.format("Time taken to load dynamic partitions:\t %.3f seconds",
+        (Time.monotonicNow() - startTime) / 1000.0);
     console.printInfo(loadTime);
     LOG.info(loadTime);
 
@@ -722,7 +724,7 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
           " To turn off this error, set hive.error.on.empty.partition=false.");
     }
 
-    startTime = System.currentTimeMillis();
+    startTime = Time.monotonicNow();
     // for each partition spec, get the partition
     // and put it to WriteEntity for post-exec hook
     for(Map.Entry<Map<String, String>, Partition> entry : dp.entrySet()) {
@@ -760,8 +762,8 @@ private DataContainer handleDynParts(Hive db, Table table, LoadTableDesc tbd,
       }
       LOG.info("Loading partition " + entry.getKey());
     }
-    console.printInfo("\t Time taken for adding to write entity : " +
-        (System.currentTimeMillis() - startTime)/1000.0 + " seconds");
+    console.printInfo(String.format("Time taken for adding to write entity:\t %.3f seconds",
+        (Time.monotonicNow() - startTime) / 1000.0));
     dc = null; // reset data container to prevent it being added again.
     return dc;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
index 9e05834680..f322e1a0ce 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
@@ -28,6 +28,8 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.Context;
+import org.apache.hadoop.hive.ql.log.PerfLogger;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.ql.TaskQueue;
@@ -55,6 +57,7 @@
 public class StatsTask extends Task<StatsWork> implements Serializable {
   private static final long serialVersionUID = 1L;
   private static transient final Logger LOG = LoggerFactory.getLogger(StatsTask.class);
+  private final PerfLogger perfLogger = SessionState.getPerfLogger();
 
   public StatsTask() {
     super();
@@ -94,6 +97,7 @@ public int execute() {
     }
     int ret = 0;
     try {
+      perfLogger.perfLogBegin("StatsTask", PerfLogger.STATS_TASK);
 
       if (work.isFooterScan()) {
         work.getBasicStatsNoJobWork().setPartitions(work.getPartitions());
@@ -113,6 +117,9 @@ public int execute() {
       LOG.error("Failed to run stats task", e);
       setException(e);
       return 1;
+    } finally {
+      perfLogger.perfLogEnd("StatsTask", PerfLogger.STATS_TASK);
+      console.printInfo(String.format("StatsTask took %d", perfLogger.getDuration(PerfLogger.STATS_TASK)));
     }
     return 0;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index 165c119286..818a1c7af5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -5150,7 +5150,7 @@ private static void deleteAndRename(FileSystem destFs, Path destFile, FileStatus
     try {
       // rename cannot overwrite non empty destination directory, so deleting the destination before renaming.
       destFs.delete(destFile);
-      LOG.info("Deleted destination file" + destFile.toUri());
+      LOG.info("Deleted destination file: {}", destFile.toUri());
     } catch (FileNotFoundException e) {
       // no worries
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
index a0b331b2bd..52e163246e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/BasicStatsTask.java
@@ -101,7 +101,6 @@ public BasicStatsTask(HiveConf conf, BasicStatsWork work) {
 
   @Override
   public int process(Hive db, Table tbl) throws Exception {
-
     LOG.info("Executing stats task");
     table = tbl;
     return aggregateStats(db, tbl);
