diff --git a/CHANGES.txt b/CHANGES.txt
index 3747685427..ac20abb63d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -214,3 +214,6 @@ Trunk (unreleased changes)
 
     HIVE-239. Check that replace columns in alter table does not have names
     that are same as the partitioning columns (Prasad Chakka via athusoo)
+
+    HIVE-25. Enable Table aliases in cluster by, distribute by and sort
+    by clauses (Prasad Chakka via athusoo)
diff --git a/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java b/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
index 882ec2a065..24c170e59e 100644
--- a/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
+++ b/ant/src/org/apache/hadoop/hive/ant/QTestGenTask.java
@@ -20,6 +20,7 @@
 
 import java.io.*;
 import java.util.StringTokenizer;
+import java.util.Arrays;
 
 import org.apache.tools.ant.AntClassLoader;
 import org.apache.tools.ant.BuildException;
@@ -206,6 +207,7 @@ public void execute() throws BuildException {
       }
       else {
         qFiles = inpDir.listFiles(new QFileFilter());
+        Arrays.sort(qFiles);
       }
 
       // Make sure the output directory exists, if it doesn't
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreClient.java b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreClient.java
index 7a0e7d3fa1..f5e380ce9b 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreClient.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreClient.java
@@ -329,8 +329,11 @@ public Table getTable(String tableName) throws MetaException, TException, NoSuch
    */
   public Table getTable(String dbName, String tableName) throws MetaException,
     TException, NoSuchObjectException {
-    throw new UnsupportedOperationException("getTable from a specific db " +
-    		"not supported by this metastore");
+    if(dbName.equalsIgnoreCase(MetaStoreUtils.DEFAULT_DATABASE_NAME)) {
+      Properties schema = this.getSchema(tableName);
+      return MetaStoreUtils.getTable(conf, schema);
+    }
+    throw new UnsupportedOperationException("Operation not supported in this metastore");
   }
   
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g b/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
index 0ca85ad6f0..ce2e8dd3c8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/Hive.g
@@ -345,6 +345,12 @@ columnNameOrder
     ->                  ^(TOK_TABSORTCOLNAMEDESC Identifier)
     ;
 
+columnRefOrder
+    : tableColumn (asc=KW_ASC | desc=KW_DESC)? 
+    -> {$desc == null}? ^(TOK_TABSORTCOLNAMEASC tableColumn)
+    ->                  ^(TOK_TABSORTCOLNAMEDESC tableColumn)
+    ;
+
 columnNameType
     : colName=Identifier colType (KW_COMMENT comment=StringLiteral)?    
     -> {$comment == null}? ^(TOK_TABCOL $colName colType)
@@ -619,20 +625,20 @@ orderByExpression
 clusterByClause
     :
     KW_CLUSTER KW_BY
-    Identifier
-    ( COMMA Identifier )* -> ^(TOK_CLUSTERBY Identifier+)
+    tableColumn
+    ( COMMA tableColumn )* -> ^(TOK_CLUSTERBY tableColumn+)
     ;
 
 distributeByClause:
     KW_DISTRIBUTE KW_BY
-    Identifier
-    ( COMMA Identifier )* -> ^(TOK_DISTRIBUTEBY Identifier+)
+    tableColumn
+    ( COMMA tableColumn )* -> ^(TOK_DISTRIBUTEBY tableColumn+)
     ;
 
 sortByClause:
     KW_SORT KW_BY
-    columnNameOrder
-    ( COMMA columnNameOrder)* -> ^(TOK_SORTBY columnNameOrder+)
+    columnRefOrder
+    ( COMMA columnRefOrder)* -> ^(TOK_SORTBY columnRefOrder+)
     ;
 
 // fun(par1, par2, par3)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index a24e20bda7..188aef4c14 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -953,7 +953,7 @@ private void genColList(String tabAlias, String alias, ASTNode sel,
  
       exprNodeColumnDesc expr = new exprNodeColumnDesc(colInfo.getType(), name);
       col_list.add(expr);
-      output.put(alias, tmp[1], new ColumnInfo(pos.toString(), colInfo.getType()));
+      output.put(tmp[0], tmp[1], new ColumnInfo(pos.toString(), colInfo.getType()));
       pos = Integer.valueOf(pos.intValue() + 1);
     }
   }
@@ -1063,13 +1063,24 @@ static List<ASTNode> getGroupByForClause(QBParseInfo parseInfo, String dest) {
     }
   }
   
-  private static String getColAlias(ASTNode selExpr, String defaultName) {
+  private static String[] getColAlias(ASTNode selExpr, String defaultName) {
+    String colAlias = null;
+    String tabAlias = null;
+    String[] colRef = new String[2];
     if (selExpr.getChildCount() == 2) {
       // return zz for "xx + yy AS zz"
-      return unescapeIdentifier(selExpr.getChild(1).getText()); 
+      colAlias  = unescapeIdentifier(selExpr.getChild(1).getText());
+      colRef[0] = tabAlias;
+      colRef[1] = colAlias;
+      return colRef;
     }
 
-    ASTNode root = (ASTNode)selExpr.getChild(0);
+    ASTNode root = (ASTNode) selExpr.getChild(0);
+    if (root.getType() == HiveParser.TOK_COLREF && root.getChildCount() > 1) {
+      ASTNode tab = (ASTNode) root.getChild(0);
+      tabAlias = unescapeIdentifier(tab.getText());
+    }
+  
     while (root.getType() == HiveParser.DOT || root.getType() == HiveParser.TOK_COLREF) {
       if (root.getType() == HiveParser.TOK_COLREF && root.getChildCount() == 1) {
         root = (ASTNode) root.getChild(0);
@@ -1081,11 +1092,15 @@ private static String getColAlias(ASTNode selExpr, String defaultName) {
     }
     if (root.getType() == HiveParser.Identifier) {
       // Return zz for "xx.zz" and "xx.yy.zz"
-      return unescapeIdentifier(root.getText());
-    } else {
+      colAlias = unescapeIdentifier(root.getText());
+    }
+    if(colAlias == null) {
       // Return defaultName if selExpr is not a simple xx.yy.zz 
-      return defaultName;
+      colAlias = defaultName;
     }
+    colRef[0] = tabAlias;
+    colRef[1] = colAlias;
+    return colRef;
   }
   
   @SuppressWarnings("nls")
@@ -1108,11 +1123,13 @@ private Operator genSelectPlan(String dest, QB qb,
 
       // list of the columns
       ASTNode selExpr = (ASTNode) selExprList.getChild(i);
-      String colAlias = getColAlias(selExpr, "_C" + i);
+      String[] colRef = getColAlias(selExpr, "_C" + i);
+      String colAlias = colRef[1];
+      String tabAlias = colRef[0];
       ASTNode sel = (ASTNode)selExpr.getChild(0);
       
       if (sel.getToken().getType() == HiveParser.TOK_ALLCOLREF) {
-        String tabAlias = null;
+        tabAlias = null;
         if (sel.getChildCount() == 1)
           tabAlias = unescapeIdentifier(sel.getChild(0).getText().toLowerCase());
         genColList(tabAlias, alias, sel, col_list, inputRR, pos, out_rwsch);
@@ -1126,7 +1143,7 @@ private Operator genSelectPlan(String dest, QB qb,
         for (int j = 0; j < cols.getChildCount(); ++j) {
           ASTNode expr = (ASTNode) cols.getChild(j);
           if (expr.getToken().getType() == HiveParser.TOK_ALLCOLREF) {
-            String tabAlias = null;
+            tabAlias = null;
             if (sel.getChildCount() == 1)
               tabAlias = unescapeIdentifier(sel.getChild(0).getText().toLowerCase());
 
@@ -1136,11 +1153,11 @@ private Operator genSelectPlan(String dest, QB qb,
             exprNodeDesc exp = genExprNodeDesc(qb.getMetaData(), expr, inputRR);
             col_list.add(exp);
             if (!StringUtils.isEmpty(alias) &&
-                (out_rwsch.get(alias, colAlias) != null)) {
+                (out_rwsch.get(null, colAlias) != null)) {
               throw new SemanticException(ErrorMsg.AMBIGOUS_COLUMN.getMsg(expr.getChild(1)));
             }
 
-            out_rwsch.put(alias, unescapeIdentifier(expr.getText()),
+            out_rwsch.put(tabAlias, unescapeIdentifier(expr.getText()),
                           new ColumnInfo((Integer.valueOf(pos)).toString(),
                                          exp.getTypeInfo()));
           }
@@ -1150,12 +1167,12 @@ private Operator genSelectPlan(String dest, QB qb,
         exprNodeDesc exp = genExprNodeDesc(qb.getMetaData(), sel, inputRR);
         col_list.add(exp);
         if (!StringUtils.isEmpty(alias) &&
-            (out_rwsch.get(alias, colAlias) != null)) {
+            (out_rwsch.get(null, colAlias) != null)) {
           throw new SemanticException(ErrorMsg.AMBIGOUS_COLUMN.getMsg(sel.getChild(1)));
         }
         // Since the as clause is lacking we just use the text representation
         // of the expression as the column name
-        out_rwsch.put(alias, colAlias,
+        out_rwsch.put(tabAlias, colAlias,
                       new ColumnInfo((Integer.valueOf(pos)).toString(),
                                      exp.getTypeInfo()));
       }
@@ -2166,12 +2183,7 @@ private Operator genReduceSinkPlan(String dest, QB qb,
       int ccount = partitionExprs.getChildCount();
       for(int i=0; i<ccount; ++i) {
         ASTNode cl = (ASTNode)partitionExprs.getChild(i);
-        ColumnInfo colInfo = inputRR.get(qb.getParseInfo().getAlias(),
-                                         unescapeIdentifier(cl.getText()));
-        if (colInfo == null) {
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(cl));
-        }
-        partitionCols.add(new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName()));
+        partitionCols.add(genExprNodeDescFromColRef(cl, inputRR));
       }
     }
 
@@ -2200,13 +2212,7 @@ private Operator genReduceSinkPlan(String dest, QB qb,
           order.append("+");
         }
 
-        ColumnInfo colInfo = inputRR.get(qb.getParseInfo().getAlias(),
-                                         unescapeIdentifier(cl.getText()));
-        if (colInfo == null) {
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(cl));
-        }
-        
-        sortCols.add(new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName()));
+        sortCols.add(genExprNodeDescFromColRef(cl, inputRR));
       }
     }
 
@@ -2720,6 +2726,19 @@ private Operator genBodyPlan(QB qb, Operator input)
         }
         curr = genFileSinkPlan(dest, qb, curr);
       }
+      
+      // change curr ops row resolver's tab aliases to query alias if it exists
+      if(qb.getParseInfo().getAlias() != null) {
+        RowResolver rr = opParseCtx.get(curr).getRR();
+        RowResolver newRR = new RowResolver();
+        String alias = qb.getParseInfo().getAlias();
+        for(ColumnInfo colInfo: rr.getColumnInfos()) {
+          String name = colInfo.getInternalName();
+          String [] tmp = rr.reverseLookup(name);
+          newRR.put(alias, tmp[1], colInfo);
+        }
+        opParseCtx.get(curr).setRR(newRR);
+      }
     }
 
     LOG.debug("Created Body Plan for Query Block " + qb.getId());
@@ -3352,33 +3371,7 @@ private exprNodeDesc genExprNodeDesc(QBMetaData qbm, ASTNode expr, RowResolver i
     int tokType = expr.getType();
     switch (tokType) {
       case HiveParser.TOK_COLREF: {
-
-        String tabAlias = null;
-        String colName = null;
-        if (expr.getChildCount() != 1) {
-          tabAlias = unescapeIdentifier(expr.getChild(0).getText());
-          colName = unescapeIdentifier(expr.getChild(1).getText());
-        }
-        else {
-          colName = unescapeIdentifier(expr.getChild(0).getText());
-        }
-
-        if (colName == null) {
-          throw new SemanticException(ErrorMsg.INVALID_XPATH.getMsg(expr));
-        }
-
-        colInfo = input.get(tabAlias, colName);
-
-        if (colInfo == null && input.getIsExprResolver()) {
-          throw new SemanticException(ErrorMsg.NON_KEY_EXPR_IN_GROUPBY.getMsg(expr));
-        }         
-        else if (tabAlias != null && !input.hasTableAlias(tabAlias)) {
-          throw new SemanticException(ErrorMsg.INVALID_TABLE_ALIAS.getMsg(expr.getChild(0)));
-        } else if (colInfo == null) {
-          throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(tabAlias == null? expr.getChild(0) : expr.getChild(1)));
-        }
-
-        desc = new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName());
+        desc = genExprNodeDescFromColRef(expr, input);
         break;
       }
   
@@ -3401,6 +3394,49 @@ else if (tabAlias != null && !input.hasTableAlias(tabAlias)) {
     return desc;
   }
 
+  /**
+   * Generates expression node from a TOK_COLREF AST Node
+   * @param expr Antrl node
+   * @param input row resolver for this col reference
+   * @return exprNodeDesc or null if ASTNode is not a TOK_COLREF
+   * @throws SemanticException
+   */
+  private exprNodeDesc genExprNodeDescFromColRef(ASTNode expr, RowResolver input)
+      throws SemanticException {
+    if(expr.getType() != HiveParser.TOK_COLREF) {
+      throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(expr));
+    }
+    exprNodeDesc desc;
+    ColumnInfo colInfo;
+    String tabAlias = null;
+    String colName = null;
+    if (expr.getChildCount() != 1) {
+      tabAlias = unescapeIdentifier(expr.getChild(0).getText());
+      colName = unescapeIdentifier(expr.getChild(1).getText());
+    }
+    else {
+      colName = unescapeIdentifier(expr.getChild(0).getText());
+    }
+
+    if (colName == null) {
+      throw new SemanticException(ErrorMsg.INVALID_XPATH.getMsg(expr));
+    }
+
+    colInfo = input.get(tabAlias, colName);
+
+    if (colInfo == null && input.getIsExprResolver()) {
+      throw new SemanticException(ErrorMsg.NON_KEY_EXPR_IN_GROUPBY.getMsg(expr));
+    }         
+    else if (tabAlias != null && !input.hasTableAlias(tabAlias)) {
+      throw new SemanticException(ErrorMsg.INVALID_TABLE_ALIAS.getMsg(expr.getChild(0)));
+    } else if (colInfo == null) {
+      throw new SemanticException(ErrorMsg.INVALID_COLUMN.getMsg(tabAlias == null? expr.getChild(0) : expr.getChild(1)));
+    }
+
+    desc = new exprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName());
+    return desc;
+  }
+
   static HashMap<Integer, String> specialUnaryOperatorTextHashMap;
   static HashMap<Integer, String> specialFunctionTextHashMap;
   static HashMap<Integer, String> conversionFunctionTextHashMap;
diff --git a/ql/src/test/queries/clientnegative/clustern1.q b/ql/src/test/queries/clientnegative/clustern1.q
new file mode 100644
index 0000000000..0ff4477965
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/clustern1.q
@@ -0,0 +1,2 @@
+EXPLAIN
+SELECT x.key, x.value as key FROM SRC x CLUSTER BY key;
diff --git a/ql/src/test/queries/clientnegative/clustern2.q b/ql/src/test/queries/clientnegative/clustern2.q
new file mode 100644
index 0000000000..9ed8944d2b
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/clustern2.q
@@ -0,0 +1,3 @@
+EXPLAIN
+SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) CLUSTER BY key;
+
diff --git a/ql/src/test/queries/clientnegative/clustern3.q b/ql/src/test/queries/clientnegative/clustern3.q
new file mode 100644
index 0000000000..23f73667ed
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/clustern3.q
@@ -0,0 +1,2 @@
+EXPLAIN
+SELECT x.key as k1, x.value FROM SRC x CLUSTER BY x.key;
diff --git a/ql/src/test/queries/clientnegative/clustern4.q b/ql/src/test/queries/clientnegative/clustern4.q
new file mode 100644
index 0000000000..3a9b45ca60
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/clustern4.q
@@ -0,0 +1,2 @@
+EXPLAIN
+SELECT x.key as k1, x.value FROM SRC x CLUSTER BY key;
diff --git a/ql/src/test/queries/clientpositive/alter1.q b/ql/src/test/queries/clientpositive/alter1.q
index 4c238f9895..f72fd37cd6 100644
--- a/ql/src/test/queries/clientpositive/alter1.q
+++ b/ql/src/test/queries/clientpositive/alter1.q
@@ -17,4 +17,7 @@ describe extended alter1;
 alter table alter1 set serde 'org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe';
 describe extended alter1;
 
+alter table alter1 replace columns (a int, b int, c string);
+describe alter1;
+
 drop table alter1;
diff --git a/ql/src/test/queries/clientpositive/cluster.q b/ql/src/test/queries/clientpositive/cluster.q
index dd278ed7a8..2481970718 100644
--- a/ql/src/test/queries/clientpositive/cluster.q
+++ b/ql/src/test/queries/clientpositive/cluster.q
@@ -1,4 +1,65 @@
 EXPLAIN
-SELECT x.* FROM SRC x CLUSTER BY key;
+SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;
+SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;
 
-SELECT x.* FROM SRC x CLUSTER BY key;
+EXPLAIN
+SELECT * FROM SRC x  where x.key = 20 CLUSTER BY key ;
+SELECT * FROM SRC x where x.key = 20 CLUSTER BY key ;
+
+EXPLAIN
+SELECT x.* FROM SRC x where x.key = 20 CLUSTER BY key;
+SELECT x.* FROM SRC x where x.key = 20 CLUSTER BY key;
+
+EXPLAIN
+SELECT x.*  FROM SRC x where x.key = 20 CLUSTER BY x.key;
+SELECT x.*  FROM SRC x where x.key = 20 CLUSTER BY x.key;
+
+EXPLAIN
+SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY key ;
+SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY key ;
+
+EXPLAIN
+SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY x.key;
+SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY x.key;
+
+EXPLAIN
+SELECT x.key, x.value as v1  FROM SRC x where x.key = 20 CLUSTER BY v1;
+SELECT x.key, x.value as v1  FROM SRC x where x.key = 20 CLUSTER BY v1;
+
+EXPLAIN
+SELECT y.* from (SELECT x.* FROM SRC x CLUSTER BY x.key) y where y.key = 20;
+SELECT y.* from (SELECT x.* FROM SRC x CLUSTER BY x.key) y where y.key = 20;
+
+
+EXPLAIN 
+SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key)  where x.key = 20 CLUSTER BY v1;;
+SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;
+
+EXPLAIN 
+SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;
+SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;
+
+EXPLAIN
+SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY x.key ;
+SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY x.key ;
+
+EXPLAIN
+SELECT x.key, x.value as v1, y.key as yk  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY key ;
+SELECT x.key, x.value as v1, y.key as yk  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY key ;
+
+EXPLAIN
+SELECT unioninput.*
+FROM (
+  FROM src select src.key, src.value WHERE src.key < 100
+  UNION ALL
+  FROM src SELECT src.* WHERE src.key > 100
+) unioninput
+CLUSTER BY unioninput.key;
+
+SELECT unioninput.*
+FROM (
+  FROM src select src.key, src.value WHERE src.key < 100
+  UNION ALL
+  FROM src SELECT src.* WHERE src.key > 100
+) unioninput
+CLUSTER BY unioninput.key;
diff --git a/ql/src/test/queries/clientpositive/input16.q b/ql/src/test/queries/clientpositive/input16.q
index e2387152a3..6b401827f6 100644
--- a/ql/src/test/queries/clientpositive/input16.q
+++ b/ql/src/test/queries/clientpositive/input16.q
@@ -1,4 +1,5 @@
 -- TestSerDe is a user defined serde where the default delimiter is Ctrl-B
+DROP TABLE INPUT16;
 CREATE TABLE INPUT16(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' STORED AS TEXTFILE;
 LOAD DATA LOCAL INPATH '../data/files/kv1_cb.txt' INTO TABLE INPUT16;
 SELECT INPUT16.VALUE, INPUT16.KEY FROM INPUT16;
diff --git a/ql/src/test/results/clientnegative/clustern1.q.out b/ql/src/test/results/clientnegative/clustern1.q.out
new file mode 100644
index 0000000000..7ad3f8650d
--- /dev/null
+++ b/ql/src/test/results/clientnegative/clustern1.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: Column key Found in more than One Tables/Subqueries
diff --git a/ql/src/test/results/clientnegative/clustern2.q.out b/ql/src/test/results/clientnegative/clustern2.q.out
new file mode 100644
index 0000000000..7ad3f8650d
--- /dev/null
+++ b/ql/src/test/results/clientnegative/clustern2.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: Column key Found in more than One Tables/Subqueries
diff --git a/ql/src/test/results/clientnegative/clustern3.q.out b/ql/src/test/results/clientnegative/clustern3.q.out
new file mode 100644
index 0000000000..37c103fd82
--- /dev/null
+++ b/ql/src/test/results/clientnegative/clustern3.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: line 2:52 Invalid Column Reference key
diff --git a/ql/src/test/results/clientnegative/clustern4.q.out b/ql/src/test/results/clientnegative/clustern4.q.out
new file mode 100644
index 0000000000..563a5beb6d
--- /dev/null
+++ b/ql/src/test/results/clientnegative/clustern4.q.out
@@ -0,0 +1 @@
+FAILED: Error in semantic analysis: line 2:50 Invalid Column Reference key
diff --git a/ql/src/test/results/clientpositive/alter1.q.out b/ql/src/test/results/clientpositive/alter1.q.out
index b0b67e4b5a..91974c1371 100644
--- a/ql/src/test/results/clientpositive/alter1.q.out
+++ b/ql/src/test/results/clientpositive/alter1.q.out
@@ -1,28 +1,31 @@
 a	int
 b	int
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,parameters:{serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{})
 a	int
 b	int
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{last_modified_by=njain,c=3,last_modified_time=1225994182,a=1})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,parameters:{serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{last_modified_by=pchakka,c=3,last_modified_time=1232158150,a=1})
 a	int
 b	int
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=njain,c=4,last_modified_time=1225994182,a=1})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,parameters:{serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=pchakka,c=4,last_modified_time=1232158150,a=1})
 a	int
 b	int
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{s1=9,serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=njain,c=4,last_modified_time=1225994182,a=1})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,parameters:{s1=9,serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=pchakka,c=4,last_modified_time=1232158150,a=1})
 a	int
 b	int
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{s2=20,s1=10,serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=njain,c=4,last_modified_time=1225994182,a=1})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:int,comment:null), FieldSchema(name:b,type:int,comment:null)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe,parameters:{s2=20,s1=10,serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=pchakka,c=4,last_modified_time=1232158150,a=1})
 a	string	'from deserializer'
 b	string	'from deserializer'
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:string,comment:from deserializer), FieldSchema(name:b,type:string,comment:from deserializer)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.TestSerDe,parameters:{s2=20,s1=9,serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=njain,c=4,last_modified_time=1225994182,a=1})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:string,comment:from deserializer), FieldSchema(name:b,type:string,comment:from deserializer)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.TestSerDe,parameters:{s2=20,s1=9,serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=pchakka,c=4,last_modified_time=1232158150,a=1})
 a	string	'from deserializer'
 b	string	'from deserializer'
 Detailed Table Information:
-Table(tableName:alter1,dbName:default,owner:njain,createTime:1225994182,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:string,comment:from deserializer), FieldSchema(name:b,type:string,comment:from deserializer)],location:file:/home/njain/workspace/hadoop-0.17/build/contrib/hive/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{s2=20,s1=9,serialization.format=1}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=njain,c=4,last_modified_time=1225994182,a=1})
+Table(tableName:alter1,dbName:default,owner:pchakka,createTime:1232158150,lastAccessTime:0,retention:0,sd:StorageDescriptor(cols:[FieldSchema(name:a,type:string,comment:from deserializer), FieldSchema(name:b,type:string,comment:from deserializer)],location:file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/alter1,inputFormat:org.apache.hadoop.mapred.TextInputFormat,outputFormat:org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat,compressed:false,numBuckets:-1,serdeInfo:SerDeInfo(name:null,serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe,parameters:{s2=20,s1=9,serialization.format=org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol}),bucketCols:[],sortCols:[],parameters:{}),partitionKeys:[],parameters:{d=3,last_modified_by=pchakka,c=4,last_modified_time=1232158150,a=1})
+a	int
+b	int
+c	string
diff --git a/ql/src/test/results/clientpositive/cluster.q.out b/ql/src/test/results/clientpositive/cluster.q.out
index 433f00f109..e3ddf0d0f6 100644
--- a/ql/src/test/results/clientpositive/cluster.q.out
+++ b/ql/src/test/results/clientpositive/cluster.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_CLUSTERBY key)))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (TOK_COLREF x key) 10)) (TOK_CLUSTERBY (TOK_COLREF x key))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -10,6 +10,356 @@ STAGE PLANS:
     Map Reduce
       Alias -> Map Operator Tree:
         x 
+            Filter Operator
+              predicate:
+                  expr: (key = 10)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+10	val_10
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+            Filter Operator
+              predicate:
+                  expr: (key = 20)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+            Filter Operator
+              predicate:
+                  expr: (key = 20)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF x key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+            Filter Operator
+              predicate:
+                  expr: (key = 20)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1)) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+            Filter Operator
+              predicate:
+                  expr: (key = 20)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1)) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF x key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+            Filter Operator
+              predicate:
+                  expr: (key = 20)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 0
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 0
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1)) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF v1))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        x 
+            Filter Operator
+              predicate:
+                  expr: (key = 20)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                Reduce Output Operator
+                  key expressions:
+                        expr: 1
+                        type: string
+                  sort order: +
+                  Map-reduce partition columns:
+                        expr: 1
+                        type: string
+                  tag: -1
+                  value expressions:
+                        expr: 0
+                        type: string
+                        expr: 1
+                        type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_CLUSTERBY (TOK_COLREF x key)))) y)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF y))) (TOK_WHERE (= (TOK_COLREF y key) 20))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y:x 
             Select Operator
               expressions:
                     expr: key
@@ -30,6 +380,523 @@ STAGE PLANS:
                       type: string
                       expr: 1
                       type: string
+      Reduce Operator Tree:
+        Extract
+          Filter Operator
+            predicate:
+                expr: (0 = 20)
+                type: boolean
+            Select Operator
+              expressions:
+                    expr: 0
+                    type: string
+                    expr: 1
+                    type: string
+              File Output Operator
+                compressed: false
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF SRC x) (TOK_TABREF SRC y) (= (TOK_COLREF x key) (TOK_COLREF y key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1) (TOK_SELEXPR (TOK_COLREF y key))) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF v1))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y 
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+              Reduce Output Operator
+                key expressions:
+                      expr: 0
+                      type: string
+                sort order: +
+                Map-reduce partition columns:
+                      expr: 0
+                      type: string
+                tag: 1
+                value expressions:
+                      expr: 0
+                      type: string
+        x 
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 {VALUE.0} {VALUE.1}
+            1 {VALUE.0}
+          Filter Operator
+            predicate:
+                expr: (0 = 20)
+                type: boolean
+            Select Operator
+              expressions:
+                    expr: 0
+                    type: string
+                    expr: 1
+                    type: string
+                    expr: 2
+                    type: string
+              File Output Operator
+                compressed: false
+                table:
+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                    output format: org.apache.hadoop.mapred.SequenceFileOutputFormat
+                    name: binary_table
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/4095681/630596716.10002 
+          Reduce Output Operator
+            key expressions:
+                  expr: 1
+                  type: string
+            sort order: +
+            Map-reduce partition columns:
+                  expr: 1
+                  type: string
+            tag: -1
+            value expressions:
+                  expr: 0
+                  type: string
+                  expr: 1
+                  type: string
+                  expr: 2
+                  type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20	20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF SRC x) (TOK_TABREF SRC y) (= (TOK_COLREF x key) (TOK_COLREF y key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1) (TOK_SELEXPR (TOK_ALLCOLREF y))) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF v1))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y 
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 1
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+        x 
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 {VALUE.0} {VALUE.1}
+            1 {VALUE.0} {VALUE.1}
+          Filter Operator
+            predicate:
+                expr: (0 = 20)
+                type: boolean
+            Select Operator
+              expressions:
+                    expr: 0
+                    type: string
+                    expr: 1
+                    type: string
+                    expr: 2
+                    type: string
+                    expr: 3
+                    type: string
+              File Output Operator
+                compressed: false
+                table:
+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                    output format: org.apache.hadoop.mapred.SequenceFileOutputFormat
+                    name: binary_table
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/462292647/163669153.10002 
+          Reduce Output Operator
+            key expressions:
+                  expr: 1
+                  type: string
+            sort order: +
+            Map-reduce partition columns:
+                  expr: 1
+                  type: string
+            tag: -1
+            value expressions:
+                  expr: 0
+                  type: string
+                  expr: 1
+                  type: string
+                  expr: 2
+                  type: string
+                  expr: 3
+                  type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20	20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF SRC x) (TOK_TABREF SRC y) (= (TOK_COLREF x key) (TOK_COLREF y key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1) (TOK_SELEXPR (TOK_ALLCOLREF y))) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF x key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y 
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 1
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+        x 
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 {VALUE.0} {VALUE.1}
+            1 {VALUE.0} {VALUE.1}
+          Filter Operator
+            predicate:
+                expr: (0 = 20)
+                type: boolean
+            Select Operator
+              expressions:
+                    expr: 0
+                    type: string
+                    expr: 1
+                    type: string
+                    expr: 2
+                    type: string
+                    expr: 3
+                    type: string
+              File Output Operator
+                compressed: false
+                table:
+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                    output format: org.apache.hadoop.mapred.SequenceFileOutputFormat
+                    name: binary_table
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/72781939/1364102870.10002 
+          Reduce Output Operator
+            key expressions:
+                  expr: 0
+                  type: string
+            sort order: +
+            Map-reduce partition columns:
+                  expr: 0
+                  type: string
+            tag: -1
+            value expressions:
+                  expr: 0
+                  type: string
+                  expr: 1
+                  type: string
+                  expr: 2
+                  type: string
+                  expr: 3
+                  type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20	20	val_20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF SRC x) (TOK_TABREF SRC y) (= (TOK_COLREF x key) (TOK_COLREF y key)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF x key)) (TOK_SELEXPR (TOK_COLREF x value) v1) (TOK_SELEXPR (TOK_COLREF y key) yk)) (TOK_WHERE (= (TOK_COLREF x key) 20)) (TOK_CLUSTERBY (TOK_COLREF key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        y 
+            Select Operator
+              expressions:
+                    expr: key
+                    type: string
+              Reduce Output Operator
+                key expressions:
+                      expr: 0
+                      type: string
+                sort order: +
+                Map-reduce partition columns:
+                      expr: 0
+                      type: string
+                tag: 1
+                value expressions:
+                      expr: 0
+                      type: string
+        x 
+            Reduce Output Operator
+              key expressions:
+                    expr: key
+                    type: string
+              sort order: +
+              Map-reduce partition columns:
+                    expr: key
+                    type: string
+              tag: 0
+              value expressions:
+                    expr: key
+                    type: string
+                    expr: value
+                    type: string
+      Reduce Operator Tree:
+        Join Operator
+          condition map:
+               Inner Join 0 to 1
+          condition expressions:
+            0 {VALUE.0} {VALUE.1}
+            1 {VALUE.0}
+          Filter Operator
+            predicate:
+                expr: (0 = 20)
+                type: boolean
+            Select Operator
+              expressions:
+                    expr: 0
+                    type: string
+                    expr: 1
+                    type: string
+                    expr: 2
+                    type: string
+              File Output Operator
+                compressed: false
+                table:
+                    input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                    output format: org.apache.hadoop.mapred.SequenceFileOutputFormat
+                    name: binary_table
+
+  Stage: Stage-2
+    Map Reduce
+      Alias -> Map Operator Tree:
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/808635/840293573.10002 
+          Reduce Output Operator
+            key expressions:
+                  expr: 0
+                  type: string
+            sort order: +
+            Map-reduce partition columns:
+                  expr: 0
+                  type: string
+            tag: -1
+            value expressions:
+                  expr: 0
+                  type: string
+                  expr: 1
+                  type: string
+                  expr: 2
+                  type: string
+      Reduce Operator Tree:
+        Extract
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+
+
+20	val_20	20
+ABSTRACT SYNTAX TREE:
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_UNION (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_COLREF src value))) (TOK_WHERE (< (TOK_COLREF src key) 100)))) (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF src))) (TOK_WHERE (> (TOK_COLREF src key) 100))))) unioninput)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF unioninput))) (TOK_CLUSTERBY (TOK_COLREF unioninput key))))
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Alias -> Map Operator Tree:
+        null-subquery1:unioninput-subquery1:src 
+            Filter Operator
+              predicate:
+                  expr: (key < 100)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                  Select Operator
+                    expressions:
+                          expr: 0
+                          type: string
+                          expr: 1
+                          type: string
+                    Reduce Output Operator
+                      key expressions:
+                            expr: 0
+                            type: string
+                      sort order: +
+                      Map-reduce partition columns:
+                            expr: 0
+                            type: string
+                      tag: -1
+                      value expressions:
+                            expr: 0
+                            type: string
+                            expr: 1
+                            type: string
+        null-subquery2:unioninput-subquery2:src 
+            Filter Operator
+              predicate:
+                  expr: (key > 100)
+                  type: boolean
+              Select Operator
+                expressions:
+                      expr: key
+                      type: string
+                      expr: value
+                      type: string
+                  Select Operator
+                    expressions:
+                          expr: 0
+                          type: string
+                          expr: 1
+                          type: string
+                    Reduce Output Operator
+                      key expressions:
+                            expr: 0
+                            type: string
+                      sort order: +
+                      Map-reduce partition columns:
+                            expr: 0
+                            type: string
+                      tag: -1
+                      value expressions:
+                            expr: 0
+                            type: string
+                            expr: 1
+                            type: string
       Reduce Operator Tree:
         Extract
           File Output Operator
@@ -47,8 +914,6 @@ STAGE PLANS:
 0	val_0
 0	val_0
 10	val_10
-100	val_100
-100	val_100
 103	val_103
 103	val_103
 104	val_104
diff --git a/ql/src/test/results/clientpositive/input14.q.out b/ql/src/test/results/clientpositive/input14.q.out
index 43d827263e..351976ee62 100644
--- a/ql/src/test/results/clientpositive/input14.q.out
+++ b/ql/src/test/results/clientpositive/input14.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY tkey))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))) (TOK_WHERE (< (TOK_COLREF tmap tkey) 100))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY (TOK_COLREF tkey)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))) (TOK_WHERE (< (TOK_COLREF tmap tkey) 100))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/input14_limit.q.out b/ql/src/test/results/clientpositive/input14_limit.q.out
index d7a4f333f0..a3a0c18cc2 100644
--- a/ql/src/test/results/clientpositive/input14_limit.q.out
+++ b/ql/src/test/results/clientpositive/input14_limit.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY tkey) (TOK_LIMIT 20))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))) (TOK_WHERE (< (TOK_COLREF tmap tkey) 100))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY (TOK_COLREF tkey)) (TOK_LIMIT 20))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))) (TOK_WHERE (< (TOK_COLREF tmap tkey) 100))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -49,7 +49,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-zshao/2396737/195622561.10001 
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/557187098/238412176.10001 
           Reduce Output Operator
             key expressions:
                   expr: 0
diff --git a/ql/src/test/results/clientpositive/input17.q.out b/ql/src/test/results/clientpositive/input17.q.out
index 12f99e2489..c67a1ff1bd 100644
--- a/ql/src/test/results/clientpositive/input17.q.out
+++ b/ql/src/test/results/clientpositive/input17.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (+ (TOK_COLREF src_thrift aint) ([ (TOK_COLREF src_thrift lint) 0)) ([ (TOK_COLREF src_thrift lintstring) 0)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY tkey))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue)))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (+ (TOK_COLREF src_thrift aint) ([ (TOK_COLREF src_thrift lint) 0)) ([ (TOK_COLREF src_thrift lintstring) 0)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY (TOK_COLREF tkey)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/input18.q.out b/ql/src/test/results/clientpositive/input18.q.out
index b7e1dfe502..dd886a20d1 100644
--- a/ql/src/test/results/clientpositive/input18.q.out
+++ b/ql/src/test/results/clientpositive/input18.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value) (+ 1 2) (+ 3 4)) '/bin/cat'))) (TOK_CLUSTERBY key))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap key)) (TOK_SELEXPR (TOK_FUNCTION regexp_replace (TOK_COLREF tmap value) '\t' '+'))) (TOK_WHERE (< (TOK_COLREF tmap key) 100))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value) (+ 1 2) (+ 3 4)) '/bin/cat'))) (TOK_CLUSTERBY (TOK_COLREF key)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap key)) (TOK_SELEXPR (TOK_FUNCTION regexp_replace (TOK_COLREF tmap value) '\t' '+'))) (TOK_WHERE (< (TOK_COLREF tmap key) 100))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/input20.q.out b/ql/src/test/results/clientpositive/input20.q.out
index 8cc136b132..54530b45b0 100644
--- a/ql/src/test/results/clientpositive/input20.q.out
+++ b/ql/src/test/results/clientpositive/input20.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (% (TOK_COLREF src key) 2) (% (TOK_COLREF src key) 5)) 'cat'))) (TOK_CLUSTERBY key))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF tmap key) (TOK_COLREF tmap value)) 'uniq -c | sed "s@^ *@@" | sed "s@\t@_@" | sed "s@ @\t@"' (TOK_ALIASLIST key value))))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (% (TOK_COLREF src key) 2) (% (TOK_COLREF src key) 5)) 'cat'))) (TOK_CLUSTERBY (TOK_COLREF key)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF tmap key) (TOK_COLREF tmap value)) 'uniq -c | sed "s@^ *@@" | sed "s@\t@_@" | sed "s@ @\t@"' (TOK_ALIASLIST key value))))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/input21.q.out b/ql/src/test/results/clientpositive/input21.q.out
index 47b1e51698..554acb11c3 100644
--- a/ql/src/test/results/clientpositive/input21.q.out
+++ b/ql/src/test/results/clientpositive/input21.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src_null)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_DISTRIBUTEBY c) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC d))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src_null)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_DISTRIBUTEBY (TOK_COLREF c)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_COLREF d)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/input5.q.out b/ql/src/test/results/clientpositive/input5.q.out
index 24a417dd80..dd78187e1a 100644
--- a/ql/src/test/results/clientpositive/input5.q.out
+++ b/ql/src/test/results/clientpositive/input5.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src_thrift lint) (TOK_COLREF src_thrift lintstring)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY tkey))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue)))))
+  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src_thrift lint) (TOK_COLREF src_thrift lintstring)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY (TOK_COLREF tkey)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/join0.q.out b/ql/src/test/results/clientpositive/join0.q.out
index 71ac447285..83134cfefc 100644
--- a/ql/src/test/results/clientpositive/join0.q.out
+++ b/ql/src/test/results/clientpositive/join0.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (< (TOK_COLREF src key) 10)))) src1) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (< (TOK_COLREF src key) 10)))) src2))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src1 key) k1) (TOK_SELEXPR (TOK_COLREF src1 value) v1) (TOK_SELEXPR (TOK_COLREF src2 key) k2) (TOK_SELEXPR (TOK_COLREF src2 value) v2)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC k1) (TOK_TABSORTCOLNAMEASC v1) (TOK_TABSORTCOLNAMEASC k2) (TOK_TABSORTCOLNAMEASC v2))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (< (TOK_COLREF src key) 10)))) src1) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (< (TOK_COLREF src key) 10)))) src2))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src1 key) k1) (TOK_SELEXPR (TOK_COLREF src1 value) v1) (TOK_SELEXPR (TOK_COLREF src2 key) k2) (TOK_SELEXPR (TOK_COLREF src2 value) v2)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_COLREF k1)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF v1)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF k2)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF v2)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -75,7 +75,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-zshao/909665613/33076823.10002 
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/1608930740/15144587.10002 
           Reduce Output Operator
             key expressions:
                   expr: 0
diff --git a/ql/src/test/results/clientpositive/mapreduce1.q.out b/ql/src/test/results/clientpositive/mapreduce1.q.out
index 97ab7d4850..153ba6c4e6 100644
--- a/ql/src/test/results/clientpositive/mapreduce1.q.out
+++ b/ql/src/test/results/clientpositive/mapreduce1.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_DISTRIBUTEBY tvalue tkey) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC ten) (TOK_TABSORTCOLNAMEASC one))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_DISTRIBUTEBY (TOK_COLREF tvalue) (TOK_COLREF tkey)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_COLREF ten)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF one)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/mapreduce2.q.out b/ql/src/test/results/clientpositive/mapreduce2.q.out
index 9daa5198f5..2f499f221e 100644
--- a/ql/src/test/results/clientpositive/mapreduce2.q.out
+++ b/ql/src/test/results/clientpositive/mapreduce2.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_DISTRIBUTEBY tvalue tkey)))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_DISTRIBUTEBY (TOK_COLREF tvalue) (TOK_COLREF tkey))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/mapreduce3.q.out b/ql/src/test/results/clientpositive/mapreduce3.q.out
index 8cc1191f87..60d8b02987 100644
--- a/ql/src/test/results/clientpositive/mapreduce3.q.out
+++ b/ql/src/test/results/clientpositive/mapreduce3.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC tvalue) (TOK_TABSORTCOLNAMEASC tkey))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_COLREF tvalue)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF tkey)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/mapreduce4.q.out b/ql/src/test/results/clientpositive/mapreduce4.q.out
index e022c58bef..5897aa0934 100644
--- a/ql/src/test/results/clientpositive/mapreduce4.q.out
+++ b/ql/src/test/results/clientpositive/mapreduce4.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_DISTRIBUTEBY tvalue tkey) (TOK_SORTBY (TOK_TABSORTCOLNAMEDESC ten) (TOK_TABSORTCOLNAMEASC one))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey ten one tvalue)))) (TOK_DISTRIBUTEBY (TOK_COLREF tvalue) (TOK_COLREF tkey)) (TOK_SORTBY (TOK_TABSORTCOLNAMEDESC (TOK_COLREF ten)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF one)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/mapreduce5.q.out b/ql/src/test/results/clientpositive/mapreduce5.q.out
index b4750013b2..ca305e5823 100644
--- a/ql/src/test/results/clientpositive/mapreduce5.q.out
+++ b/ql/src/test/results/clientpositive/mapreduce5.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key) c1) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) c2) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) c3) (TOK_SELEXPR (TOK_COLREF src value) c4)) (TOK_DISTRIBUTEBY c4 c1) (TOK_SORTBY (TOK_TABSORTCOLNAMEDESC c2) (TOK_TABSORTCOLNAMEASC c3))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key) c1) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) c2) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) c3) (TOK_SELEXPR (TOK_COLREF src value) c4)) (TOK_DISTRIBUTEBY (TOK_COLREF c4) (TOK_COLREF c1)) (TOK_SORTBY (TOK_TABSORTCOLNAMEDESC (TOK_COLREF c2)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF c3)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/mapreduce6.q.out b/ql/src/test/results/clientpositive/mapreduce6.q.out
index bcbecb97bf..56477d7341 100644
--- a/ql/src/test/results/clientpositive/mapreduce6.q.out
+++ b/ql/src/test/results/clientpositive/mapreduce6.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) c2) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) c3) (TOK_SELEXPR (TOK_COLREF src value))) (TOK_DISTRIBUTEBY value key) (TOK_SORTBY (TOK_TABSORTCOLNAMEDESC c2) (TOK_TABSORTCOLNAMEASC c3))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (/ (TOK_COLREF src key) 10)) c2) (TOK_SELEXPR (TOK_FUNCTION TOK_INT (% (TOK_COLREF src key) 10)) c3) (TOK_SELEXPR (TOK_COLREF src value))) (TOK_DISTRIBUTEBY (TOK_COLREF value) (TOK_COLREF key)) (TOK_SORTBY (TOK_TABSORTCOLNAMEDESC (TOK_COLREF c2)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF c3)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/clientpositive/sample8.q.out b/ql/src/test/results/clientpositive/sample8.q.out
index d5f36e0aaf..5d3a440432 100644
--- a/ql/src/test/results/clientpositive/sample8.q.out
+++ b/ql/src/test/results/clientpositive/sample8.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF srcpart (TOK_TABLESAMPLE 1 1 (TOK_COLREF key)) s) (TOK_TABREF srcpart (TOK_TABLESAMPLE 1 10 (TOK_COLREF key)) t))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF s))) (TOK_WHERE (and (and (and (and (and (= (TOK_COLREF t key) (TOK_COLREF s key)) (= (TOK_COLREF t value) (TOK_COLREF s value))) (= (TOK_COLREF s ds) '2008-04-08')) (= (TOK_COLREF s hr) '11')) (= (TOK_COLREF s ds) '2008-04-08')) (= (TOK_COLREF s hr) '11'))) (TOK_DISTRIBUTEBY key value) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC key) (TOK_TABSORTCOLNAMEASC value))))
+  (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF srcpart (TOK_TABLESAMPLE 1 1 (TOK_COLREF key)) s) (TOK_TABREF srcpart (TOK_TABLESAMPLE 1 10 (TOK_COLREF key)) t))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF s))) (TOK_WHERE (and (and (and (and (and (= (TOK_COLREF t key) (TOK_COLREF s key)) (= (TOK_COLREF t value) (TOK_COLREF s value))) (= (TOK_COLREF s ds) '2008-04-08')) (= (TOK_COLREF s hr) '11')) (= (TOK_COLREF s ds) '2008-04-08')) (= (TOK_COLREF s hr) '11'))) (TOK_DISTRIBUTEBY (TOK_COLREF key) (TOK_COLREF value)) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_COLREF key)) (TOK_TABSORTCOLNAMEASC (TOK_COLREF value)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
@@ -48,12 +48,12 @@ STAGE PLANS:
                       type: string
       Needs Tagging: true
       Path -> Alias:
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
       Path -> Partition:
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11 
           Partition
             partition values:
               ds 2008-04-08
@@ -71,10 +71,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: srcpart
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12 
           Partition
             partition values:
               ds 2008-04-08
@@ -92,10 +92,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: srcpart
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=11 
           Partition
             partition values:
               ds 2008-04-09
@@ -113,10 +113,10 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: srcpart
-        file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
+        file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-09/hr=12 
           Partition
             partition values:
               ds 2008-04-09
@@ -134,7 +134,7 @@ STAGE PLANS:
                 serialization.lib org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
                 file.inputformat org.apache.hadoop.mapred.TextInputFormat
                 file.outputformat org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
-                location file:/mnt/vol/devrs005.snc1/rmurthy/hive/build/ql/test/data/warehouse/srcpart
+                location file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart
               serde: org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe
               name: srcpart
       Reduce Operator Tree:
@@ -160,7 +160,7 @@ STAGE PLANS:
                     type: string
               File Output Operator
                 compressed: false
-                directory: /tmp/hive-rmurthy/352392290/1281901534.10002
+                directory: /data/users/pchakka/workspace/oshive/build/ql/tmp/502758895/785997617.10002
                 table:
                     input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                     output format: org.apache.hadoop.mapred.SequenceFileOutputFormat
@@ -173,7 +173,7 @@ STAGE PLANS:
   Stage: Stage-2
     Map Reduce
       Alias -> Map Operator Tree:
-        /tmp/hive-rmurthy/352392290/1281901534.10002 
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/502758895/785997617.10002 
           Reduce Output Operator
             key expressions:
                   expr: 0
@@ -198,9 +198,9 @@ STAGE PLANS:
                   type: string
       Needs Tagging: false
       Path -> Alias:
-        /tmp/hive-rmurthy/352392290/1281901534.10002 
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/502758895/785997617.10002 
       Path -> Partition:
-        /tmp/hive-rmurthy/352392290/1281901534.10002 
+        /data/users/pchakka/workspace/oshive/build/ql/tmp/502758895/785997617.10002 
           Partition
           
               input format: org.apache.hadoop.mapred.SequenceFileInputFormat
@@ -214,12 +214,12 @@ STAGE PLANS:
         Extract
           File Output Operator
             compressed: false
-            directory: /tmp/hive-rmurthy/1317038554.10001.insclause-0
+            directory: /data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/67235665.10001.insclause-0
             table:
                 input format: org.apache.hadoop.mapred.TextInputFormat
                 output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat
                 properties:
-                  columns key,value,ds,hr
+                  columns s.key,s.value,s.ds,s.hr
                   serialization.format 1
 
   Stage: Stage-0
diff --git a/ql/src/test/results/clientpositive/sort.q.out b/ql/src/test/results/clientpositive/sort.q.out
index b1106d28da..7d05787928 100644
--- a/ql/src/test/results/clientpositive/sort.q.out
+++ b/ql/src/test/results/clientpositive/sort.q.out
@@ -1,5 +1,5 @@
 ABSTRACT SYNTAX TREE:
-  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC key))))
+  (TOK_QUERY (TOK_FROM (TOK_TABREF SRC x)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_ALLCOLREF x))) (TOK_SORTBY (TOK_TABSORTCOLNAMEASC (TOK_COLREF key)))))
 
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
diff --git a/ql/src/test/results/compiler/parse/input20.q.out b/ql/src/test/results/compiler/parse/input20.q.out
index 155abc7615..48485c796e 100644
--- a/ql/src/test/results/compiler/parse/input20.q.out
+++ b/ql/src/test/results/compiler/parse/input20.q.out
@@ -1 +1 @@
-(TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (% (TOK_COLREF src key) 2) (% (TOK_COLREF src key) 5)) 'cat'))) (TOK_CLUSTERBY key))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF tmap key) (TOK_COLREF tmap value)) 'uniq -c | sed "s@^ *@@" | sed "s@\t@_@" | sed "s@ @\t@"' (TOK_ALIASLIST key value)))))) null
\ No newline at end of file
+(TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (% (TOK_COLREF src key) 2) (% (TOK_COLREF src key) 5)) 'cat'))) (TOK_CLUSTERBY (TOK_COLREF key)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF tmap key) (TOK_COLREF tmap value)) 'uniq -c | sed "s@^ *@@" | sed "s@\t@_@" | sed "s@ @\t@"' (TOK_ALIASLIST key value)))))) null
\ No newline at end of file
diff --git a/ql/src/test/results/compiler/parse/input4.q.out b/ql/src/test/results/compiler/parse/input4.q.out
index e5ddc56b46..837613d4c8 100755
--- a/ql/src/test/results/compiler/parse/input4.q.out
+++ b/ql/src/test/results/compiler/parse/input4.q.out
@@ -1 +1 @@
-(TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY tkey))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))) (TOK_WHERE (< (TOK_COLREF tmap tkey) 100)))) null
\ No newline at end of file
+(TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src key) (TOK_COLREF src value)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY (TOK_COLREF tkey)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))) (TOK_WHERE (< (TOK_COLREF tmap tkey) 100)))) null
\ No newline at end of file
diff --git a/ql/src/test/results/compiler/parse/input5.q.out b/ql/src/test/results/compiler/parse/input5.q.out
index fe636c7bc5..1977765cfa 100644
--- a/ql/src/test/results/compiler/parse/input5.q.out
+++ b/ql/src/test/results/compiler/parse/input5.q.out
@@ -1 +1 @@
-(TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src_thrift lint) (TOK_COLREF src_thrift lintstring)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY tkey))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))))) null
\ No newline at end of file
+(TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_TABREF src_thrift)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TRANSFORM (TOK_EXPLIST (TOK_COLREF src_thrift lint) (TOK_COLREF src_thrift lintstring)) '/bin/cat' (TOK_ALIASLIST tkey tvalue)))) (TOK_CLUSTERBY (TOK_COLREF tkey)))) tmap)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF tmap tkey)) (TOK_SELEXPR (TOK_COLREF tmap tvalue))))) null
\ No newline at end of file
diff --git a/ql/src/test/results/compiler/plan/groupby5.q.xml b/ql/src/test/results/compiler/plan/groupby5.q.xml
index aee72caced..f935385d95 100644
--- a/ql/src/test/results/compiler/plan/groupby5.q.xml
+++ b/ql/src/test/results/compiler/plan/groupby5.q.xml
@@ -20,7 +20,7 @@
         <void property="aliasToWork"> 
          <object class="java.util.HashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/25539710/241360638.10002</string> 
+           <string>/data/users/pchakka/workspace/oshive/build/ql/tmp/1685965608/18485413.10002</string> 
            <object id="ReduceSinkOperator0" class="org.apache.hadoop.hive.ql.exec.ReduceSinkOperator"> 
             <void property="conf"> 
              <object class="org.apache.hadoop.hive.ql.plan.reduceSinkDesc"> 
@@ -147,7 +147,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/25539710/241360638.10002</string> 
+                       <string>/data/users/pchakka/workspace/oshive/build/ql/tmp/1685965608/18485413.10002</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -582,10 +582,10 @@
         <void property="pathToAliases"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/25539710/241360638.10002</string> 
+           <string>/data/users/pchakka/workspace/oshive/build/ql/tmp/1685965608/18485413.10002</string> 
            <object class="java.util.ArrayList"> 
             <void method="add"> 
-             <string>/tmp/hive-njain/25539710/241360638.10002</string> 
+             <string>/data/users/pchakka/workspace/oshive/build/ql/tmp/1685965608/18485413.10002</string> 
             </void> 
            </object> 
           </void> 
@@ -594,7 +594,7 @@
         <void property="pathToPartitionInfo"> 
          <object class="java.util.LinkedHashMap"> 
           <void method="put"> 
-           <string>/tmp/hive-njain/25539710/241360638.10002</string> 
+           <string>/data/users/pchakka/workspace/oshive/build/ql/tmp/1685965608/18485413.10002</string> 
            <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
             <void property="tableDesc"> 
              <object idref="tableDesc2"/> 
@@ -616,7 +616,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/25539710/241360638.10001.insclause-0</string> 
+                     <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/1685965608/18485413.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -633,7 +633,7 @@
                        <object class="java.util.Properties"> 
                         <void method="put"> 
                          <string>columns</string> 
-                         <string>key,_c1</string> 
+                         <string>src.key,_c1</string> 
                         </void> 
                         <void method="put"> 
                          <string>serialization.format</string> 
@@ -847,7 +847,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -859,7 +859,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -911,7 +911,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input3.q.xml b/ql/src/test/results/compiler/plan/input3.q.xml
index aaa32628d4..bce18a84d2 100755
--- a/ql/src/test/results/compiler/plan/input3.q.xml
+++ b/ql/src/test/results/compiler/plan/input3.q.xml
@@ -22,13 +22,13 @@
           <void method="add"> 
            <object class="org.apache.hadoop.hive.ql.plan.loadFileDesc"> 
             <void property="columns"> 
-             <string>value</string> 
+             <string>src.value</string> 
             </void> 
             <void property="isDfsDir"> 
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/44210915/741228445.10003.insclause-3</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10003.insclause-3</string> 
             </void> 
             <void property="targetDir"> 
              <string>../../../../build/contrib/hive/ql/test/data/warehouse/dest4.out</string> 
@@ -48,7 +48,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/44210915/741228445.10000.insclause-0</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -97,7 +97,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -117,7 +117,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/44210915/741228445.10001.insclause-1</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10001.insclause-1</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc1" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -166,7 +166,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest2</string> 
+                 <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest2</string> 
                 </void> 
                </object> 
               </void> 
@@ -195,7 +195,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/44210915/741228445.10002.insclause-2</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10002.insclause-2</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc2" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -248,7 +248,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest3</string> 
+                 <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest3</string> 
                 </void> 
                </object> 
               </void> 
@@ -292,7 +292,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/44210915/741228445.10000.insclause-0</string> 
+                       <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10000.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc0"/> 
@@ -500,7 +500,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/44210915/741228445.10001.insclause-1</string> 
+                       <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10001.insclause-1</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc1"/> 
@@ -746,7 +746,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/44210915/741228445.10002.insclause-2</string> 
+                       <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10002.insclause-2</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object idref="tableDesc2"/> 
@@ -992,7 +992,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/44210915/741228445.10003.insclause-3</string> 
+                       <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/382185459/89077292.10003.insclause-3</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1009,7 +1009,7 @@
                          <object class="java.util.Properties"> 
                           <void method="put"> 
                            <string>columns</string> 
-                           <string>value</string> 
+                           <string>src.value</string> 
                           </void> 
                           <void method="put"> 
                            <string>serialization.format</string> 
@@ -1169,7 +1169,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>src</string> 
@@ -1181,7 +1181,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1233,7 +1233,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/input4.q.xml b/ql/src/test/results/compiler/plan/input4.q.xml
index 7a082a42ad..3f715ccf9b 100755
--- a/ql/src/test/results/compiler/plan/input4.q.xml
+++ b/ql/src/test/results/compiler/plan/input4.q.xml
@@ -31,7 +31,7 @@
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/89591678/324016511.10000.insclause-0</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/952811677/1477355585.10000.insclause-0</string> 
             </void> 
             <void property="table"> 
              <object id="tableDesc0" class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -80,7 +80,7 @@
                 </void> 
                 <void method="put"> 
                  <string>location</string> 
-                 <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/dest1</string> 
+                 <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/dest1</string> 
                 </void> 
                </object> 
               </void> 
@@ -468,7 +468,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>tmap:src</string> 
@@ -480,7 +480,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -532,7 +532,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -562,7 +562,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/89591678/324016511.10000.insclause-0</string> 
+                     <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/952811677/1477355585.10000.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object idref="tableDesc0"/> 
@@ -723,9 +723,9 @@
           <void property="schema"> 
            <object class="org.apache.hadoop.hive.ql.exec.RowSchema"> 
             <void property="signature"> 
-             <object id="Vector2" class="java.util.Vector"> 
+             <object class="java.util.Vector"> 
               <void method="add"> 
-               <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+               <object id="ColumnInfo0" class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
                 <void property="internalName"> 
                  <string>0</string> 
                 </void> 
@@ -735,7 +735,7 @@
                </object> 
               </void> 
               <void method="add"> 
-               <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+               <object id="ColumnInfo1" class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
                 <void property="internalName"> 
                  <string>1</string> 
                 </void> 
@@ -776,7 +776,14 @@
       <void property="schema"> 
        <object class="org.apache.hadoop.hive.ql.exec.RowSchema"> 
         <void property="signature"> 
-         <object idref="Vector2"/> 
+         <object class="java.util.Vector"> 
+          <void method="add"> 
+           <object idref="ColumnInfo0"/> 
+          </void> 
+          <void method="add"> 
+           <object idref="ColumnInfo1"/> 
+          </void> 
+         </object> 
         </void> 
        </object> 
       </void> 
diff --git a/ql/src/test/results/compiler/plan/input_part1.q.xml b/ql/src/test/results/compiler/plan/input_part1.q.xml
index bb1fd1681b..96ca059aee 100644
--- a/ql/src/test/results/compiler/plan/input_part1.q.xml
+++ b/ql/src/test/results/compiler/plan/input_part1.q.xml
@@ -26,7 +26,7 @@
                     <void property="conf"> 
                      <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                       <void property="dirName"> 
-                       <string>/tmp/hive-njain/381790525/715994016.10001.insclause-0</string> 
+                       <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/60338291/166255580.10001.insclause-0</string> 
                       </void> 
                       <void property="tableInfo"> 
                        <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -43,7 +43,7 @@
                          <object class="java.util.Properties"> 
                           <void method="put"> 
                            <string>columns</string> 
-                           <string>key,value,hr,ds</string> 
+                           <string>srcpart.key,srcpart.value,srcpart.hr,srcpart.ds</string> 
                           </void> 
                           <void method="put"> 
                            <string>serialization.format</string> 
@@ -464,7 +464,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>srcpart</string> 
@@ -476,7 +476,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=12</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"> 
@@ -541,7 +541,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/join4.q.xml b/ql/src/test/results/compiler/plan/join4.q.xml
index e95df793ee..0bb8d9d62d 100644
--- a/ql/src/test/results/compiler/plan/join4.q.xml
+++ b/ql/src/test/results/compiler/plan/join4.q.xml
@@ -842,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -857,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -909,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -939,7 +939,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/291874807/86598252.10001.insclause-0</string> 
+                     <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/46163837/318815753.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -956,7 +956,7 @@
                        <object class="java.util.Properties"> 
                         <void method="put"> 
                          <string>columns</string> 
-                         <string>c1,c2,c3,c4</string> 
+                         <string>c.c1,c.c2,c.c3,c.c4</string> 
                         </void> 
                         <void method="put"> 
                          <string>serialization.format</string> 
diff --git a/ql/src/test/results/compiler/plan/join5.q.xml b/ql/src/test/results/compiler/plan/join5.q.xml
index 68a13e17e1..ea66a96255 100644
--- a/ql/src/test/results/compiler/plan/join5.q.xml
+++ b/ql/src/test/results/compiler/plan/join5.q.xml
@@ -842,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -857,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -909,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -939,7 +939,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/1109739262/425333867.10001.insclause-0</string> 
+                     <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/67901132/10357416.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -956,7 +956,7 @@
                        <object class="java.util.Properties"> 
                         <void method="put"> 
                          <string>columns</string> 
-                         <string>c1,c2,c3,c4</string> 
+                         <string>c.c1,c.c2,c.c3,c.c4</string> 
                         </void> 
                         <void method="put"> 
                          <string>serialization.format</string> 
diff --git a/ql/src/test/results/compiler/plan/join6.q.xml b/ql/src/test/results/compiler/plan/join6.q.xml
index 56a681b25a..366ef53208 100644
--- a/ql/src/test/results/compiler/plan/join6.q.xml
+++ b/ql/src/test/results/compiler/plan/join6.q.xml
@@ -842,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -857,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -909,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -939,7 +939,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/166521027/637281771.10001.insclause-0</string> 
+                     <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/849791247/24076830.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -956,7 +956,7 @@
                        <object class="java.util.Properties"> 
                         <void method="put"> 
                          <string>columns</string> 
-                         <string>c1,c2,c3,c4</string> 
+                         <string>c.c1,c.c2,c.c3,c.c4</string> 
                         </void> 
                         <void method="put"> 
                          <string>serialization.format</string> 
diff --git a/ql/src/test/results/compiler/plan/join7.q.xml b/ql/src/test/results/compiler/plan/join7.q.xml
index ecc9be7ef5..e1a814eeae 100644
--- a/ql/src/test/results/compiler/plan/join7.q.xml
+++ b/ql/src/test/results/compiler/plan/join7.q.xml
@@ -1249,7 +1249,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -1267,7 +1267,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -1319,7 +1319,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -1349,7 +1349,7 @@
                   <void property="conf"> 
                    <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                     <void property="dirName"> 
-                     <string>/tmp/hive-njain/759592261/286929524.10001.insclause-0</string> 
+                     <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/144356131/476198727.10001.insclause-0</string> 
                     </void> 
                     <void property="tableInfo"> 
                      <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -1366,7 +1366,7 @@
                        <object class="java.util.Properties"> 
                         <void method="put"> 
                          <string>columns</string> 
-                         <string>c1,c2,c3,c4,c5,c6</string> 
+                         <string>c.c1,c.c2,c.c3,c.c4,c.c5,c.c6</string> 
                         </void> 
                         <void method="put"> 
                          <string>serialization.format</string> 
diff --git a/ql/src/test/results/compiler/plan/join8.q.xml b/ql/src/test/results/compiler/plan/join8.q.xml
index 486420a4d6..7cfc9b0a3b 100644
--- a/ql/src/test/results/compiler/plan/join8.q.xml
+++ b/ql/src/test/results/compiler/plan/join8.q.xml
@@ -842,7 +842,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>c:a:src1</string> 
@@ -857,7 +857,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -909,7 +909,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
@@ -943,7 +943,7 @@
                       <void property="conf"> 
                        <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                         <void property="dirName"> 
-                         <string>/tmp/hive-njain/19854159/32645100.10001.insclause-0</string> 
+                         <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/27316771/18134100.10001.insclause-0</string> 
                         </void> 
                         <void property="tableInfo"> 
                          <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -960,7 +960,7 @@
                            <object class="java.util.Properties"> 
                             <void method="put"> 
                              <string>columns</string> 
-                             <string>c1,c2,c3,c4</string> 
+                             <string>c.c1,c.c2,c.c3,c.c4</string> 
                             </void> 
                             <void method="put"> 
                              <string>serialization.format</string> 
@@ -1206,9 +1206,9 @@
               <void property="schema"> 
                <object class="org.apache.hadoop.hive.ql.exec.RowSchema"> 
                 <void property="signature"> 
-                 <object id="Vector3" class="java.util.Vector"> 
+                 <object class="java.util.Vector"> 
                   <void method="add"> 
-                   <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                   <object id="ColumnInfo0" class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
                     <void property="internalName"> 
                      <string>0</string> 
                     </void> 
@@ -1218,7 +1218,7 @@
                    </object> 
                   </void> 
                   <void method="add"> 
-                   <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                   <object id="ColumnInfo1" class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
                     <void property="internalName"> 
                      <string>1</string> 
                     </void> 
@@ -1228,7 +1228,7 @@
                    </object> 
                   </void> 
                   <void method="add"> 
-                   <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                   <object id="ColumnInfo2" class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
                     <void property="internalName"> 
                      <string>2</string> 
                     </void> 
@@ -1238,7 +1238,7 @@
                    </object> 
                   </void> 
                   <void method="add"> 
-                   <object class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
+                   <object id="ColumnInfo3" class="org.apache.hadoop.hive.ql.exec.ColumnInfo"> 
                     <void property="internalName"> 
                      <string>3</string> 
                     </void> 
@@ -1313,7 +1313,20 @@
           <void property="schema"> 
            <object class="org.apache.hadoop.hive.ql.exec.RowSchema"> 
             <void property="signature"> 
-             <object idref="Vector3"/> 
+             <object class="java.util.Vector"> 
+              <void method="add"> 
+               <object idref="ColumnInfo0"/> 
+              </void> 
+              <void method="add"> 
+               <object idref="ColumnInfo1"/> 
+              </void> 
+              <void method="add"> 
+               <object idref="ColumnInfo2"/> 
+              </void> 
+              <void method="add"> 
+               <object idref="ColumnInfo3"/> 
+              </void> 
+             </object> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/sample1.q.xml b/ql/src/test/results/compiler/plan/sample1.q.xml
index 9661aef3ad..f394b68679 100644
--- a/ql/src/test/results/compiler/plan/sample1.q.xml
+++ b/ql/src/test/results/compiler/plan/sample1.q.xml
@@ -30,7 +30,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/425404034/89912765.10001.insclause-0</string> 
+                           <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/248620454/1149518232.10001.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -47,7 +47,7 @@
                              <object class="java.util.Properties"> 
                               <void method="put"> 
                                <string>columns</string> 
-                               <string>key,value,ds,hr</string> 
+                               <string>s.key,s.value,s.ds,s.hr</string> 
                               </void> 
                               <void method="put"> 
                                <string>serialization.format</string> 
@@ -570,7 +570,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>s</string> 
@@ -582,7 +582,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart/ds=2008-04-08/hr=11</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"> 
@@ -647,7 +647,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/srcpart</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/srcpart</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/subq.q.xml b/ql/src/test/results/compiler/plan/subq.q.xml
index 5a560ae306..e7144806ae 100644
--- a/ql/src/test/results/compiler/plan/subq.q.xml
+++ b/ql/src/test/results/compiler/plan/subq.q.xml
@@ -22,13 +22,13 @@
           <void method="add"> 
            <object class="org.apache.hadoop.hive.ql.plan.loadFileDesc"> 
             <void property="columns"> 
-             <string>key,value</string> 
+             <string>unioninput.key,unioninput.value</string> 
             </void> 
             <void property="isDfsDir"> 
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/276583968/36258042.10000.insclause-0</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/797066151/209780209.10000.insclause-0</string> 
             </void> 
             <void property="targetDir"> 
              <string>../build/ql/test/data/warehouse/union.out</string> 
@@ -75,7 +75,7 @@
                         <void property="conf"> 
                          <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                           <void property="dirName"> 
-                           <string>/tmp/hive-njain/276583968/36258042.10000.insclause-0</string> 
+                           <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/797066151/209780209.10000.insclause-0</string> 
                           </void> 
                           <void property="tableInfo"> 
                            <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -92,7 +92,7 @@
                              <object class="java.util.Properties"> 
                               <void method="put"> 
                                <string>columns</string> 
-                               <string>key,value</string> 
+                               <string>unioninput.key,unioninput.value</string> 
                               </void> 
                               <void method="put"> 
                                <string>serialization.format</string> 
@@ -378,7 +378,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>unioninput:src</string> 
@@ -390,7 +390,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -442,7 +442,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
diff --git a/ql/src/test/results/compiler/plan/union.q.xml b/ql/src/test/results/compiler/plan/union.q.xml
index af0b1d87c6..f90118f9fb 100644
--- a/ql/src/test/results/compiler/plan/union.q.xml
+++ b/ql/src/test/results/compiler/plan/union.q.xml
@@ -22,13 +22,13 @@
           <void method="add"> 
            <object class="org.apache.hadoop.hive.ql.plan.loadFileDesc"> 
             <void property="columns"> 
-             <string>key,value</string> 
+             <string>unioninput.key,unioninput.value</string> 
             </void> 
             <void property="isDfsDir"> 
              <boolean>true</boolean> 
             </void> 
             <void property="sourceDir"> 
-             <string>/tmp/hive-njain/1238482884/612877942.10000.insclause-0</string> 
+             <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/1982541537/686730669.10000.insclause-0</string> 
             </void> 
             <void property="targetDir"> 
              <string>../build/ql/test/data/warehouse/union.out</string> 
@@ -79,7 +79,7 @@
                             <void property="conf"> 
                              <object class="org.apache.hadoop.hive.ql.plan.fileSinkDesc"> 
                               <void property="dirName"> 
-                               <string>/tmp/hive-njain/1238482884/612877942.10000.insclause-0</string> 
+                               <string>/data/users/pchakka/workspace/oshive/ql/../build/ql/tmp/1982541537/686730669.10000.insclause-0</string> 
                               </void> 
                               <void property="tableInfo"> 
                                <object class="org.apache.hadoop.hive.ql.plan.tableDesc"> 
@@ -96,7 +96,7 @@
                                  <object class="java.util.Properties"> 
                                   <void method="put"> 
                                    <string>columns</string> 
-                                   <string>key,value</string> 
+                                   <string>unioninput.key,unioninput.value</string> 
                                   </void> 
                                   <void method="put"> 
                                    <string>serialization.format</string> 
@@ -590,7 +590,7 @@
     <void property="pathToAliases"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="java.util.ArrayList"> 
         <void method="add"> 
          <string>null-subquery1:unioninput-subquery1:src</string> 
@@ -605,7 +605,7 @@
     <void property="pathToPartitionInfo"> 
      <object class="java.util.LinkedHashMap"> 
       <void method="put"> 
-       <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+       <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
        <object class="org.apache.hadoop.hive.ql.plan.partitionDesc"> 
         <void property="partSpec"> 
          <object class="java.util.LinkedHashMap"/> 
@@ -657,7 +657,7 @@
             </void> 
             <void method="put"> 
              <string>location</string> 
-             <string>file:/home/njain/workspace/hadoophive/trunk/build/ql/test/data/warehouse/src</string> 
+             <string>file:/data/users/pchakka/workspace/oshive/build/ql/test/data/warehouse/src</string> 
             </void> 
            </object> 
           </void> 
