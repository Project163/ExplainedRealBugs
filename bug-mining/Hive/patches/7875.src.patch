diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
index 9f07cf7e0d..9660f48012 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
@@ -126,7 +126,6 @@
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;
-import org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
 import org.apache.hadoop.hive.ql.plan.GroupByDesc;
 import org.apache.hadoop.hive.ql.plan.MapJoinDesc;
@@ -1936,12 +1935,10 @@ private void removeSemijoinOptimizationByBenefit(OptimizeTezProcContext procCtx)
           if (filterStats != null) {
             ImmutableSet.Builder<String> colNames = ImmutableSet.builder();
             for (ExprNodeDesc tsExpr : targetColumns) {
-              // tsExpr might actually be a ExprNodeFieldDesc and we need to extract the column expression
-              if (tsExpr instanceof ExprNodeFieldDesc) {
-                LOG.debug("Unwrapped column expression from ExprNodeFieldDesc");
-                tsExpr = ((ExprNodeFieldDesc) tsExpr).getDesc();
+              Set<ExprNodeColumnDesc> allReferencedColumns = ExprNodeDescUtils.findAllColumnDescs(tsExpr);
+              for (ExprNodeColumnDesc col : allReferencedColumns) {
+                colNames.add(col.getColumn());
               }
-              colNames.add(ExprNodeDescUtils.getColumnExpr(tsExpr).getColumn());
             }
             // We check whether there was already another SJ over this TS that was selected
             // in previous iteration
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java
index 232e248418..c151320b0b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java
@@ -23,6 +23,8 @@
 
 import java.util.Arrays;
 import java.util.Collection;
+import java.util.HashSet;
+
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -60,6 +62,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 
 public class ExprNodeDescUtils {
@@ -839,6 +842,28 @@ public static ExprNodeColumnDesc getColumnExpr(ExprNodeDesc expr) {
     return (expr instanceof ExprNodeColumnDesc) ? (ExprNodeColumnDesc)expr : null;
   }
 
+  /*
+   * Extracts all referenced columns from the subtree.
+   */
+  public static Set<ExprNodeColumnDesc> findAllColumnDescs(ExprNodeDesc expr) {
+    Set<ExprNodeColumnDesc> ret = new HashSet<>();
+    findAllColumnDescs(ret, expr);
+    return ret;
+  }
+
+  private static void findAllColumnDescs(Set<ExprNodeColumnDesc> ret, ExprNodeDesc expr) {
+    if (expr instanceof ExprNodeGenericFuncDesc) {
+      ExprNodeGenericFuncDesc func = (ExprNodeGenericFuncDesc) expr;
+      for (ExprNodeDesc c : func.getChildren()) {
+        findAllColumnDescs(ret, c);
+      }
+    }
+
+    if (expr instanceof ExprNodeColumnDesc) {
+      ret.add((ExprNodeColumnDesc) expr);
+    }
+  }
+
   // Find the constant origin of a certain column if it is originated from a constant
   // Otherwise, it returns the expression that originated the column
   public static ExprNodeDesc findConstantExprOrigin(String dpCol, Operator<? extends OperatorDesc> op) {
diff --git a/ql/src/test/queries/clientpositive/semijoin_removal_udf.q b/ql/src/test/queries/clientpositive/semijoin_removal_udf.q
new file mode 100644
index 0000000000..5fadeb04a3
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/semijoin_removal_udf.q
@@ -0,0 +1,32 @@
+set hive.optimize.index.filter=true;
+set hive.support.concurrency=true;
+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
+set hive.exec.dynamic.partition.mode=nonstrict;
+set hive.exec.dynamic.partition=true;
+set hive.vectorized.execution.enabled=true;
+
+drop table if exists t1;
+drop table if exists t2;
+
+create table t1 (
+	v1 string
+);
+
+create table t2 (
+	v2 string
+);
+
+insert into t1 values ('e123456789'),('x123456789');
+insert into t2 values
+('123'),
+ ('e123456789');
+
+
+alter table t1 update statistics set ('numRows'='934884357','rawDataSize'='0');
+alter table t2 update statistics set ('numRows'='9348','rawDataSize'='0');
+
+alter table t1 update statistics for column v1 set ('numNulls'='0','numDVs'='15541355','avgColLen'='10.0','maxColLen'='10');
+alter table t2 update statistics for column v2 set ('numNulls'='0','numDVs'='155','avgColLen'='5.0','maxColLen'='10');
+
+explain
+select v1,v2 from t1 join t2 on (substr(v1,1,3) = v2);
diff --git a/ql/src/test/results/clientpositive/llap/semijoin_removal_udf.q.out b/ql/src/test/results/clientpositive/llap/semijoin_removal_udf.q.out
new file mode 100644
index 0000000000..79d284275c
--- /dev/null
+++ b/ql/src/test/results/clientpositive/llap/semijoin_removal_udf.q.out
@@ -0,0 +1,212 @@
+PREHOOK: query: drop table if exists t1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists t1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table if exists t2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists t2
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table t1 (
+	v1 string
+)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@t1
+POSTHOOK: query: create table t1 (
+	v1 string
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@t1
+PREHOOK: query: create table t2 (
+	v2 string
+)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@t2
+POSTHOOK: query: create table t2 (
+	v2 string
+)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@t2
+PREHOOK: query: insert into t1 values ('e123456789'),('x123456789')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@t1
+POSTHOOK: query: insert into t1 values ('e123456789'),('x123456789')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@t1
+POSTHOOK: Lineage: t1.v1 SCRIPT []
+PREHOOK: query: insert into t2 values
+('123'),
+ ('e123456789')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@t2
+POSTHOOK: query: insert into t2 values
+('123'),
+ ('e123456789')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@t2
+POSTHOOK: Lineage: t2.v2 SCRIPT []
+PREHOOK: query: alter table t1 update statistics set ('numRows'='934884357','rawDataSize'='0')
+PREHOOK: type: ALTERTABLE_UPDATETABLESTATS
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@t1
+POSTHOOK: query: alter table t1 update statistics set ('numRows'='934884357','rawDataSize'='0')
+POSTHOOK: type: ALTERTABLE_UPDATETABLESTATS
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@t1
+PREHOOK: query: alter table t2 update statistics set ('numRows'='9348','rawDataSize'='0')
+PREHOOK: type: ALTERTABLE_UPDATETABLESTATS
+PREHOOK: Input: default@t2
+PREHOOK: Output: default@t2
+POSTHOOK: query: alter table t2 update statistics set ('numRows'='9348','rawDataSize'='0')
+POSTHOOK: type: ALTERTABLE_UPDATETABLESTATS
+POSTHOOK: Input: default@t2
+POSTHOOK: Output: default@t2
+PREHOOK: query: alter table t1 update statistics for column v1 set ('numNulls'='0','numDVs'='15541355','avgColLen'='10.0','maxColLen'='10')
+PREHOOK: type: ALTERTABLE_UPDATETABLESTATS
+PREHOOK: Input: default@t1
+PREHOOK: Output: default@t1
+POSTHOOK: query: alter table t1 update statistics for column v1 set ('numNulls'='0','numDVs'='15541355','avgColLen'='10.0','maxColLen'='10')
+POSTHOOK: type: ALTERTABLE_UPDATETABLESTATS
+POSTHOOK: Input: default@t1
+POSTHOOK: Output: default@t1
+PREHOOK: query: alter table t2 update statistics for column v2 set ('numNulls'='0','numDVs'='155','avgColLen'='5.0','maxColLen'='10')
+PREHOOK: type: ALTERTABLE_UPDATETABLESTATS
+PREHOOK: Input: default@t2
+PREHOOK: Output: default@t2
+POSTHOOK: query: alter table t2 update statistics for column v2 set ('numNulls'='0','numDVs'='155','avgColLen'='5.0','maxColLen'='10')
+POSTHOOK: type: ALTERTABLE_UPDATETABLESTATS
+POSTHOOK: Input: default@t2
+POSTHOOK: Output: default@t2
+PREHOOK: query: explain
+select v1,v2 from t1 join t2 on (substr(v1,1,3) = v2)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@t1
+PREHOOK: Input: default@t2
+#### A masked pattern was here ####
+POSTHOOK: query: explain
+select v1,v2 from t1 join t2 on (substr(v1,1,3) = v2)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@t1
+POSTHOOK: Input: default@t2
+#### A masked pattern was here ####
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Map 1 <- Reducer 4 (BROADCAST_EDGE)
+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)
+        Reducer 4 <- Map 3 (CUSTOM_SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: t1
+                  filterExpr: (substr(v1, 1, 3) is not null and substr(v1, 1, 3) BETWEEN DynamicValue(RS_7_t2_v2_min) AND DynamicValue(RS_7_t2_v2_max) and in_bloom_filter(substr(v1, 1, 3), DynamicValue(RS_7_t2_v2_bloom_filter))) (type: boolean)
+                  Statistics: Num rows: 934884357 Data size: 87879129558 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: (substr(v1, 1, 3) is not null and substr(v1, 1, 3) BETWEEN DynamicValue(RS_7_t2_v2_min) AND DynamicValue(RS_7_t2_v2_max) and in_bloom_filter(substr(v1, 1, 3), DynamicValue(RS_7_t2_v2_bloom_filter))) (type: boolean)
+                    Statistics: Num rows: 934884357 Data size: 87879129558 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: v1 (type: string), substr(v1, 1, 3) (type: string)
+                      outputColumnNames: _col0, _col1
+                      Statistics: Num rows: 934884357 Data size: 169214068617 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col1 (type: string)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col1 (type: string)
+                        Statistics: Num rows: 934884357 Data size: 169214068617 Basic stats: COMPLETE Column stats: COMPLETE
+                        value expressions: _col0 (type: string)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+        Map 3 
+            Map Operator Tree:
+                TableScan
+                  alias: t2
+                  filterExpr: v2 is not null (type: boolean)
+                  Statistics: Num rows: 9348 Data size: 831972 Basic stats: COMPLETE Column stats: COMPLETE
+                  Filter Operator
+                    predicate: v2 is not null (type: boolean)
+                    Statistics: Num rows: 9348 Data size: 831972 Basic stats: COMPLETE Column stats: COMPLETE
+                    Select Operator
+                      expressions: v2 (type: string)
+                      outputColumnNames: _col0
+                      Statistics: Num rows: 9348 Data size: 831972 Basic stats: COMPLETE Column stats: COMPLETE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string)
+                        null sort order: z
+                        sort order: +
+                        Map-reduce partition columns: _col0 (type: string)
+                        Statistics: Num rows: 9348 Data size: 831972 Basic stats: COMPLETE Column stats: COMPLETE
+                      Select Operator
+                        expressions: _col0 (type: string)
+                        outputColumnNames: _col0
+                        Statistics: Num rows: 9348 Data size: 831972 Basic stats: COMPLETE Column stats: COMPLETE
+                        Group By Operator
+                          aggregations: min(_col0), max(_col0), bloom_filter(_col0, expectedEntries=1000000)
+                          minReductionHashAggr: 0.99
+                          mode: hash
+                          outputColumnNames: _col0, _col1, _col2
+                          Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE
+                          Reduce Output Operator
+                            null sort order: 
+                            sort order: 
+                            Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE
+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)
+            Execution mode: vectorized, llap
+            LLAP IO: all inputs
+        Reducer 2 
+            Execution mode: llap
+            Reduce Operator Tree:
+              Merge Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 _col1 (type: string)
+                  1 _col0 (type: string)
+                outputColumnNames: _col0, _col2
+                Statistics: Num rows: 562325 Data size: 102905475 Basic stats: COMPLETE Column stats: COMPLETE
+                Select Operator
+                  expressions: _col0 (type: string), _col2 (type: string)
+                  outputColumnNames: _col0, _col1
+                  Statistics: Num rows: 562325 Data size: 102905475 Basic stats: COMPLETE Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 562325 Data size: 102905475 Basic stats: COMPLETE Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+        Reducer 4 
+            Execution mode: vectorized, llap
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=1000000)
+                mode: final
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE
+                Reduce Output Operator
+                  null sort order: 
+                  sort order: 
+                  Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE
+                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
