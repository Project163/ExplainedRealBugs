diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 86f1a7164d..bd27d99ac9 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -499,6 +499,7 @@ public static enum ConfVars {
 
     HIVEMERGEMAPFILES("hive.merge.mapfiles", true),
     HIVEMERGEMAPREDFILES("hive.merge.mapredfiles", false),
+    HIVEMERGETEZFILES("hive.merge.tezfiles", false),
     HIVEMERGEMAPFILESSIZE("hive.merge.size.per.task", (long) (256 * 1000 * 1000)),
     HIVEMERGEMAPFILESAVGSIZE("hive.merge.smallfiles.avgsize", (long) (16 * 1000 * 1000)),
     HIVEMERGERCFILEBLOCKLEVEL("hive.merge.rcfile.block.level", true),
@@ -560,6 +561,10 @@ public static enum ConfVars {
     HIVEDEBUGLOCALTASK("hive.debug.localtask",false),
 
     HIVEINPUTFORMAT("hive.input.format", "org.apache.hadoop.hive.ql.io.CombineHiveInputFormat"),
+    HIVETEZINPUTFORMAT("hive.tez.input.format", "org.apache.hadoop.hive.ql.io.HiveInputFormat"),
+
+    HIVETEZCONTAINERSIZE("hive.tez.container.size", -1),
+    HIVETEZJAVAOPTS("hive.tez.java.opts", null),
 
     HIVEENFORCEBUCKETING("hive.enforce.bucketing", false),
     HIVEENFORCESORTING("hive.enforce.sorting", false),
diff --git a/conf/hive-default.xml.template b/conf/hive-default.xml.template
index 8c64633929..065c397b45 100644
--- a/conf/hive-default.xml.template
+++ b/conf/hive-default.xml.template
@@ -793,6 +793,12 @@
   <description>Merge small files at the end of a map-reduce job</description>
 </property>
 
+<property>
+  <name>hive.merge.tezfiles</name>
+  <value>false</value>
+  <description>Merge small files at the end of a Tez DAG</description>
+</property>
+
 <property>
   <name>hive.heartbeat.interval</name>
   <value>1000</value>
@@ -959,6 +965,12 @@
   <description>The default input format. Set this to HiveInputFormat if you encounter problems with CombineHiveInputFormat.</description>
 </property>
 
+<property>
+  <name>hive.tez.input.format</name>
+  <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
+  <description>The default input format for tez. Tez groups splits in the AM.</description>
+</property>
+
 <property>
   <name>hive.udtf.auto.progress</name>
   <value>false</value>
@@ -2360,4 +2372,16 @@
   </description>
 </property>
 
+<property>
+  <name>hive.tez.container.size</name>
+  <value>-1</value>
+  <description>By default tez will spawn containers of the size of a mapper. This can be used to overwrite.</description>
+</property>
+
+<property>
+  <name>hive.tez.java.opts</name>
+  <value></value>
+  <description>By default tez will use the java opts from map tasks. This can be used to overwrite.</description>
+</property>
+
 </configuration>
diff --git a/data/conf/tez/hive-site.xml b/data/conf/tez/hive-site.xml
index d240056d64..5ff5b4c302 100644
--- a/data/conf/tez/hive-site.xml
+++ b/data/conf/tez/hive-site.xml
@@ -33,8 +33,26 @@
 </property>
 
 <property>
-  <name>mapred.child.java.opts</name>
-  <value>-Xmx200m</value>
+  <name>mapred.tez.java.opts</name>
+  <value>-Xmx128m</value>
+</property>
+
+<property>
+  <name>hive.tez.container.size</name>
+  <value>128</value>
+  <description></description>
+</property>
+
+<property>
+  <name>hive.merge.tezfiles</name>
+  <value>false</value>
+  <description>Merge small files at the end of a Tez DAG</description>
+</property>
+
+<property>
+  <name>hive.tez.input.format</name>
+  <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
+  <description>The default input format for tez. Tez groups splits in the AM.</description>
 </property>
 
 <property>
@@ -172,7 +190,7 @@
 
 <property>
   <name>hive.input.format</name>
-  <value>org.apache.hadoop.hive.ql.io.HiveInputFormat</value>
+  <value>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</value>
   <description>The default input format, if it is not specified, the system assigns it. It is set to HiveInputFormat for hadoop versions 17, 18 and 19, whereas it is set to CombineHiveInputFormat for hadoop 20. The user can always overwrite it - if there is a bug in CombineHiveInputFormat, it can always be manually set to HiveInputFormat. </description>
 </property>
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 0418d18917..23ef69bd69 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -3144,8 +3144,10 @@ public static void setInputPaths(JobConf job, List<Path> pathsToAdd) {
    * Set hive input format, and input format file if necessary.
    */
   public static void setInputAttributes(Configuration conf, MapWork mWork) {
+    HiveConf.ConfVars var = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ?
+      HiveConf.ConfVars.HIVETEZINPUTFORMAT : HiveConf.ConfVars.HIVEINPUTFORMAT;
     if (mWork.getInputformat() != null) {
-      HiveConf.setVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT, mWork.getInputformat());
+      HiveConf.setVar(conf, var, mWork.getInputformat());
     }
     if (mWork.getIndexIntermediateFile() != null) {
       conf.set("hive.index.compact.file", mWork.getIndexIntermediateFile());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index 608c1edfcf..1fbc57d208 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -74,6 +74,7 @@
 import org.apache.hadoop.yarn.api.records.LocalResource;
 import org.apache.hadoop.yarn.api.records.LocalResourceType;
 import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
+import org.apache.hadoop.yarn.api.records.Resource;
 import org.apache.hadoop.yarn.api.records.URL;
 import org.apache.hadoop.yarn.util.ConverterUtils;
 import org.apache.hadoop.yarn.util.Records;
@@ -178,7 +179,7 @@ private JobConf initializeVertexConf(JobConf baseConf, MapWork mapWork) {
 
     Utilities.setInputAttributes(conf, mapWork);
 
-    String inpFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT);
+    String inpFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVETEZINPUTFORMAT);
     if ((inpFormat == null) || (!StringUtils.isNotBlank(inpFormat))) {
       inpFormat = ShimLoader.getHadoopShims().getInputFormatClassName();
     }
@@ -293,6 +294,35 @@ private EdgeProperty createEdgeProperty(EdgeType edgeType) {
     return edgeProperty;
   }
 
+  /*
+   * Helper to determine the size of the container requested
+   * from yarn. Falls back to Map-reduce's map size if tez
+   * container size isn't set.
+   */
+  private Resource getContainerResource(Configuration conf) {
+    Resource containerResource;
+    int memory = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVETEZCONTAINERSIZE) > 0 ?
+      HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVETEZCONTAINERSIZE) :
+      conf.getInt(MRJobConfig.MAP_MEMORY_MB, MRJobConfig.DEFAULT_MAP_MEMORY_MB);
+    int cpus = conf.getInt(MRJobConfig.MAP_CPU_VCORES,
+                           MRJobConfig.DEFAULT_MAP_CPU_VCORES);
+    return Resource.newInstance(memory, cpus);
+  }
+
+  /*
+   * Helper to determine what java options to use for the containers
+   * Falls back to Map-reduces map java opts if no tez specific options
+   * are set
+   */
+  private String getContainerJavaOpts(Configuration conf) {
+    String javaOpts = HiveConf.getVar(conf, HiveConf.ConfVars.HIVETEZJAVAOPTS);
+    if (javaOpts != null && !javaOpts.isEmpty()) {
+      return javaOpts;
+    }
+    return MRHelpers.getMapJavaOpts(conf);
+  }
+
+
   /*
    * Helper function to create Vertex from MapWork.
    */
@@ -344,12 +374,11 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,
     byte[] serializedConf = MRHelpers.createUserPayloadFromConf(conf);
     map = new Vertex(mapWork.getName(),
         new ProcessorDescriptor(MapTezProcessor.class.getName()).
-             setUserPayload(serializedConf), numTasks,
-        MRHelpers.getMapResource(conf));
+             setUserPayload(serializedConf), numTasks, getContainerResource(conf));
     Map<String, String> environment = new HashMap<String, String>();
     MRHelpers.updateEnvironmentForMRTasks(conf, environment, true);
     map.setTaskEnvironment(environment);
-    map.setJavaOpts(MRHelpers.getMapJavaOpts(conf));
+    map.setJavaOpts(getContainerJavaOpts(conf));
 
     assert mapWork.getAliasToWork().keySet().size() == 1;
 
@@ -419,14 +448,14 @@ private Vertex createVertex(JobConf conf, ReduceWork reduceWork,
     Vertex reducer = new Vertex(reduceWork.getName(),
         new ProcessorDescriptor(ReduceTezProcessor.class.getName()).
              setUserPayload(MRHelpers.createUserPayloadFromConf(conf)),
-        reduceWork.getNumReduceTasks(), MRHelpers.getReduceResource(conf));
+        reduceWork.getNumReduceTasks(), getContainerResource(conf));
 
     Map<String, String> environment = new HashMap<String, String>();
 
     MRHelpers.updateEnvironmentForMRTasks(conf, environment, false);
     reducer.setTaskEnvironment(environment);
 
-    reducer.setJavaOpts(MRHelpers.getReduceJavaOpts(conf));
+    reducer.setJavaOpts(getContainerJavaOpts(conf));
 
     Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();
     localResources.put(getBaseName(appJarLr), appJarLr);
@@ -479,7 +508,7 @@ public PreWarmContext createPreWarmContext(TezSessionConfiguration sessionConfig
     ProcessorDescriptor prewarmProcDescriptor = new ProcessorDescriptor(HivePreWarmProcessor.class.getName());
     prewarmProcDescriptor.setUserPayload(MRHelpers.createUserPayloadFromConf(conf));
 
-    PreWarmContext context = new PreWarmContext(prewarmProcDescriptor, MRHelpers.getMapResource(conf),
+    PreWarmContext context = new PreWarmContext(prewarmProcDescriptor, getContainerResource(conf),
         numContainers, new VertexLocationHint(null));
 
     Map<String, LocalResource> combinedResources = new HashMap<String, LocalResource>();
@@ -504,7 +533,7 @@ public PreWarmContext createPreWarmContext(TezSessionConfiguration sessionConfig
     Map<String, String> environment = new HashMap<String, String>();
     MRHelpers.updateEnvironmentForMRTasks(conf, environment, true);
     context.setEnvironment(environment);
-    context.setJavaOpts(MRHelpers.getMapJavaOpts(conf));
+    context.setJavaOpts(getContainerJavaOpts(conf));
     return context;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
index 427ea12ca4..c39e82300d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
@@ -152,6 +152,7 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs,
       HiveConf builderConf = new HiveConf(getConf(), AggregateIndexHandler.class);
       builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPFILES, false);
       builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPREDFILES, false);
+      builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
       Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
           command, (LinkedHashMap<String, String>) partSpec, indexTableName, dbName);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
index 11ddcae2cc..0135a71fa1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
@@ -144,6 +144,7 @@ protected Task<?> getIndexBuilderMapRedTask(Set<ReadEntity> inputs, Set<WriteEnt
     HiveConf builderConf = new HiveConf(getConf(), CompactIndexHandler.class);
     builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPFILES, false);
     builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGEMAPREDFILES, false);
+    builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
     Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
         command, partSpec, indexTableName, dbName);
     return rootTask;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 9a2182aba2..c0ca404b90 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -1619,6 +1619,13 @@ public static boolean isMergeRequired(List<Task<MoveWork>> mvTasks, HiveConf hco
       }
 
       if ((mvTask != null) && !mvTask.isLocal() && fsOp.getConf().canBeMerged()) {
+
+        if (currTask.getWork() instanceof TezWork) {
+          // tez blurs the boundary between map and reduce, thus it has it's own
+          // config
+          return hconf.getBoolVar(ConfVars.HIVEMERGETEZFILES);
+        }
+
         if (fsOp.getConf().isLinkedFileSink()) {
           // If the user has HIVEMERGEMAPREDFILES set to false, the idea was the
           // number of reducers are few, so the number of files anyway are small.
@@ -1632,16 +1639,13 @@ public static boolean isMergeRequired(List<Task<MoveWork>> mvTasks, HiveConf hco
           // There are separate configuration parameters to control whether to
           // merge for a map-only job
           // or for a map-reduce job
-          if (currTask.getWork() instanceof TezWork) {
-            return hconf.getBoolVar(ConfVars.HIVEMERGEMAPFILES) || 
-                hconf.getBoolVar(ConfVars.HIVEMERGEMAPREDFILES);
-          } else if (currTask.getWork() instanceof MapredWork) {
+          if (currTask.getWork() instanceof MapredWork) {  
             ReduceWork reduceWork = ((MapredWork) currTask.getWork()).getReduceWork();
             boolean mergeMapOnly =
-                hconf.getBoolVar(ConfVars.HIVEMERGEMAPFILES) && reduceWork == null;
+              hconf.getBoolVar(ConfVars.HIVEMERGEMAPFILES) && reduceWork == null;
             boolean mergeMapRed =
-                hconf.getBoolVar(ConfVars.HIVEMERGEMAPREDFILES) &&
-                reduceWork != null;
+              hconf.getBoolVar(ConfVars.HIVEMERGEMAPREDFILES) &&
+              reduceWork != null;
             if (mergeMapOnly || mergeMapRed) {
               return true;
             }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 77388dd75c..1acd29fdb4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -643,7 +643,9 @@ private String processTable(QB qb, ASTNode tabref) throws SemanticException {
   }
 
   private void assertCombineInputFormat(Tree numerator, String message) throws SemanticException {
-    String inputFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT);
+    String inputFormat = conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals("tez") ?
+      HiveConf.getVar(conf, HiveConf.ConfVars.HIVETEZINPUTFORMAT):
+      HiveConf.getVar(conf, HiveConf.ConfVars.HIVEINPUTFORMAT);
     if (!inputFormat.equals(CombineHiveInputFormat.class.getName())) {
       throw new SemanticException(generateErrorMessage((ASTNode) numerator,
           message + " sampling is not supported in " + inputFormat));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
index fa16d0c026..a5e6cbfa34 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java
@@ -88,10 +88,6 @@ public void init(HiveConf conf, LogHelper console, Hive db) {
     // We require the use of recursive input dirs for union processing
     conf.setBoolean("mapred.input.dir.recursive", true);
     HiveConf.setBoolVar(conf, ConfVars.HIVE_HADOOP_SUPPORTS_SUBDIRECTORIES, true);
-
-    // Don't auto-merge files in tez
-    HiveConf.setBoolVar(conf, ConfVars.HIVEMERGEMAPFILES, false);
-    HiveConf.setBoolVar(conf, ConfVars.HIVEMERGEMAPREDFILES, false);
   }
 
   @Override
