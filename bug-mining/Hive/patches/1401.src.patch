diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
index 1ba19e456e..b5c85d2a0d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java
@@ -168,57 +168,51 @@ public void initializeAsRoot(Configuration hconf, MapWork mapWork)
     initialize(hconf, null);
   }
 
-  private MapOpCtx initObjectInspector(MapWork conf,
-      Configuration hconf, String onefile, Map<TableDesc, StructObjectInspector> convertedOI)
-          throws HiveException,
-      ClassNotFoundException, InstantiationException, IllegalAccessException,
-      SerDeException {
-    PartitionDesc pd = conf.getPathToPartitionInfo().get(onefile);
-    LinkedHashMap<String, String> partSpec = pd.getPartSpec();
+  private MapOpCtx initObjectInspector(Configuration hconf, MapInputPath ctx,
+      Map<TableDesc, StructObjectInspector> convertedOI) throws Exception {
+
+    PartitionDesc pd = ctx.partDesc;
+    TableDesc td = pd.getTableDesc();
+
+    MapOpCtx opCtx = new MapOpCtx();
     // Use table properties in case of unpartitioned tables,
     // and the union of table properties and partition properties, with partition
     // taking precedence
-    Properties partProps =
-        (pd.getPartSpec() == null || pd.getPartSpec().isEmpty()) ?
-            pd.getTableDesc().getProperties() : pd.getOverlayedProperties();
+    Properties partProps = isPartitioned(pd) ?
+        pd.getOverlayedProperties() : pd.getTableDesc().getProperties();
+
+    Map<String, String> partSpec = pd.getPartSpec();
+
+    opCtx.tableName = String.valueOf(partProps.getProperty("name"));
+    opCtx.partName = String.valueOf(partSpec);
 
     Class serdeclass = pd.getDeserializerClass();
     if (serdeclass == null) {
-      String className = pd.getSerdeClassName();
-      if ((className == null) || (className.isEmpty())) {
-        throw new HiveException(
-            "SerDe class or the SerDe class name is not set for table: "
-                + pd.getProperties().getProperty("name"));
-      }
+      String className = checkSerdeClassName(pd.getSerdeClassName(), opCtx.tableName);
       serdeclass = hconf.getClassByName(className);
     }
 
-    String tableName = String.valueOf(partProps.getProperty("name"));
-    String partName = String.valueOf(partSpec);
-    Deserializer partDeserializer = (Deserializer) serdeclass.newInstance();
-    partDeserializer.initialize(hconf, partProps);
-    StructObjectInspector partRawRowObjectInspector = (StructObjectInspector) partDeserializer
-        .getObjectInspector();
+    opCtx.deserializer = (Deserializer) serdeclass.newInstance();
+    opCtx.deserializer.initialize(hconf, partProps);
+
+    StructObjectInspector partRawRowObjectInspector =
+        (StructObjectInspector) opCtx.deserializer.getObjectInspector();
 
-    StructObjectInspector tblRawRowObjectInspector = convertedOI.get(pd.getTableDesc());
+    opCtx.tblRawRowObjectInspector = convertedOI.get(td);
 
-    partTblObjectInspectorConverter =
-    ObjectInspectorConverters.getConverter(partRawRowObjectInspector,
-        tblRawRowObjectInspector);
+    opCtx.partTblObjectInspectorConverter = ObjectInspectorConverters.getConverter(
+        partRawRowObjectInspector, opCtx.tblRawRowObjectInspector);
 
-    MapOpCtx opCtx = null;
     // Next check if this table has partitions and if so
     // get the list of partition names as well as allocate
     // the serdes for the partition columns
-    String pcols = partProps
-        .getProperty(org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
+    String pcols = partProps.getProperty(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);
     // Log LOG = LogFactory.getLog(MapOperator.class.getName());
     if (pcols != null && pcols.length() > 0) {
       String[] partKeys = pcols.trim().split("/");
       List<String> partNames = new ArrayList<String>(partKeys.length);
       Object[] partValues = new Object[partKeys.length];
-      List<ObjectInspector> partObjectInspectors = new ArrayList<ObjectInspector>(
-          partKeys.length);
+      List<ObjectInspector> partObjectInspectors = new ArrayList<ObjectInspector>(partKeys.length);
       for (int i = 0; i < partKeys.length; i++) {
         String key = partKeys[i];
         partNames.add(key);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
index 725ba7cf19..5879042462 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java
@@ -319,8 +319,8 @@ static private void pruneByPushDown(Table tab, Set<Partition> true_parts, String
    * @param conf Hive Configuration object, can not be NULL.
    * @throws Exception
    */
-   static private void pruneBySequentialScan(Table tab, Set<Partition> true_parts,
-       Set<Partition> unkn_parts, Set<Partition> denied_parts, ExprNodeDesc prunerExpr,
+  static private void pruneBySequentialScan(Table tab, Set<Partition> true_parts,
+      Set<Partition> unkn_parts, Set<Partition> denied_parts, ExprNodeDesc prunerExpr,
        StructObjectInspector rowObjectInspector, List<VirtualColumn> vcs, HiveConf conf)
       throws Exception {
 
@@ -338,7 +338,7 @@ static private void pruneBySequentialScan(Table tab, Set<Partition> true_parts,
     List<String> partCols = new ArrayList<String>(pCols.size());
     List<String> values = new ArrayList<String>(pCols.size());
     String defaultPartitionName = conf.getVar(HiveConf.ConfVars.DEFAULTPARTITIONNAME);
- 
+
     boolean hasVC = vcs != null && !vcs.isEmpty();
     Object[] objectWithPart = new Object[hasVC ? 3 : 2];
 
