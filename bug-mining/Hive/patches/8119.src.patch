diff --git a/RELEASE_NOTES.txt b/RELEASE_NOTES.txt
index bde815ee8c..c64f5cf07b 100644
--- a/RELEASE_NOTES.txt
+++ b/RELEASE_NOTES.txt
@@ -831,7 +831,7 @@ Release Notes - Hive - Version 3.0.0
     * [HIVE-16125] - Split work between reducers.
     * [HIVE-16130] - Remove jackson classes from hive-jdbc standalone jar
     * [HIVE-16147] - Rename a partitioned table should not drop its partition columns stats
-    * [HIVE-16174] - Update MetricsConstant.WAITING_COMPILE_OPS metric when we aquire lock failed in Driver
+    * [HIVE-16174] - Update MetricsConstant.WAITING_COMPILE_OPS metric when we acquire lock failed in Driver
     * [HIVE-16177] - non Acid to acid conversion doesn't handle _copy_N files
     * [HIVE-16188] - beeline should block the connection if given invalid database name.
     * [HIVE-16193] - Hive show compactions not reflecting the status of the application
@@ -1152,7 +1152,7 @@ Release Notes - Hive - Version 3.0.0
     * [HIVE-17321] - HoS: analyze ORC table doesn't compute raw data size when noscan/partialscan is not specified
     * [HIVE-17322] - Serialise BeeLine qtest execution to prevent flakyness
     * [HIVE-17327] - LLAP IO: restrict native file ID usage to default FS to avoid hypothetical collisions when HDFS federation is used
-    * [HIVE-17331] - Path must be used as key type of the pathToAlises
+    * [HIVE-17331] - Path must be used as key type of the pathToAliases
     * [HIVE-17333] - Schema changes in HIVE-12274 for Oracle may not work for upgrade
     * [HIVE-17336] - Missing class 'org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat' from Hive on Spark when inserting into hbase based table
     * [HIVE-17338] - Utilities.get*Tasks multiple methods duplicate code
@@ -1823,7 +1823,7 @@ Release Notes - Hive - Version 3.0.0
     * [HIVE-16855] - org.apache.hadoop.hive.ql.exec.mr.HashTableLoader Improvements
     * [HIVE-16856] - Allow For Customization Of Buffer Size In MapJoinTableContainerSerDe
     * [HIVE-16857] - SparkPartitionPruningSinkOperator Buffer Size
-    * [HIVE-16858] - Acumulo Utils Improvements
+    * [HIVE-16858] - Accumulo Utils Improvements
     * [HIVE-16866] - existing available UDF is used in TestReplicationScenariosAcrossInstances#testDropFunctionIncrementalReplication 
     * [HIVE-16867] - Extend shared scan optimizer to reuse computation from other operators
     * [HIVE-16873] - Remove Thread Cache From Logging
diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloHiveConstants.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloHiveConstants.java
index 6cdfe1bc96..552577f8e8 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloHiveConstants.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/AccumuloHiveConstants.java
@@ -35,7 +35,7 @@ public class AccumuloHiveConstants {
       + Character.toString(ASTERISK);
 
   // Escape the escape, and escape the asterisk
-  public static final String ESCAPED_ASERTISK_REGEX = Character.toString(ESCAPE)
+  public static final String ESCAPED_ASTERISK_REGEX = Character.toString(ESCAPE)
       + Character.toString(ESCAPE) + Character.toString(ESCAPE) + Character.toString(ASTERISK);
 
   public static final Charset UTF_8 = Charset.forName("UTF-8");
diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/HiveAccumuloHelper.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/HiveAccumuloHelper.java
index 32a4f305c2..5df7b2701a 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/HiveAccumuloHelper.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/HiveAccumuloHelper.java
@@ -341,7 +341,7 @@ public Token<? extends TokenIdentifier> setConnectorInfoForInputAndOutput(Accumu
     AuthenticationToken token = getDelegationToken(conn);
 
     // Make sure the Accumulo token is set in the Configuration (only a stub of the Accumulo
-    // AuthentiationToken is serialized, not the entire token). configureJobConf may be
+    // AuthenticationToken is serialized, not the entire token). configureJobConf may be
     // called multiple times with the same JobConf which results in an error from Accumulo
     // MapReduce API. Catch the error, log a debug message and just keep going
     try {
diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/columns/ColumnMappingFactory.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/columns/ColumnMappingFactory.java
index 63d496ec0a..6b10f0feeb 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/columns/ColumnMappingFactory.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/columns/ColumnMappingFactory.java
@@ -87,7 +87,7 @@ public static ColumnMapping get(String columnSpec, ColumnEncoding defaultEncodin
 
         // Replace any \* that appear in the prefix with a regular *
         if (-1 != cq.indexOf(AccumuloHiveConstants.ESCAPED_ASTERISK)) {
-          cq = cq.replaceAll(AccumuloHiveConstants.ESCAPED_ASERTISK_REGEX,
+          cq = cq.replaceAll(AccumuloHiveConstants.ESCAPED_ASTERISK_REGEX,
               Character.toString(AccumuloHiveConstants.ASTERISK));
         }
 
diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableInputFormat.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableInputFormat.java
index af64eac1fc..e5160d783b 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableInputFormat.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/mr/HiveAccumuloTableInputFormat.java
@@ -121,7 +121,7 @@ public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException
           log.info("Job credential tokens: " + jobConf.getCredentials().getAllTokens());
           AuthenticationToken unwrappedToken = ConfiguratorBase.unwrapAuthenticationToken(jobConf, token);
           log.info("Converted authentication token from Configuration into: " + unwrappedToken);
-          // It's possible that the Job doesn't have the token in its credentials. In this case, unwrapAuthenticatinoToken
+          // It's possible that the Job doesn't have the token in its credentials. In this case, unwrapAuthenticationToken
           // will return back the original token (which we know is insufficient)
           if (unwrappedToken != token) {
             log.info("Creating Accumulo Connector with unwrapped delegation token");
diff --git a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
index 819774d47e..89f57121a0 100644
--- a/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
+++ b/accumulo-handler/src/java/org/apache/hadoop/hive/accumulo/predicate/AccumuloPredicateHandler.java
@@ -242,7 +242,7 @@ public List<Range> getRanges(Configuration conf, ColumnMapper columnMapper)
   }
 
   /**
-   * Encapsulates the traversal over some {@link ExprNodeDesc} tree for the generation of Accumuluo.
+   * Encapsulates the traversal over some {@link ExprNodeDesc} tree for the generation of Accumulo.
    * Ranges using expressions involving the Accumulo rowid-mapped Hive column.
    *
    * @param conf
diff --git a/bin/hive b/bin/hive
index 926d3189a0..b31bc8562e 100755
--- a/bin/hive
+++ b/bin/hive
@@ -155,7 +155,7 @@ for f in ${HIVE_LIB}/*.jar; do
   CLASSPATH=${CLASSPATH}:$f;
 done
 
-# add the auxillary jars such as serdes
+# add the auxiliary jars such as serdes
 if [ -d "${HIVE_AUX_JARS_PATH}" ]; then
   hive_aux_jars_abspath=`cd ${HIVE_AUX_JARS_PATH} && pwd`
   for f in $hive_aux_jars_abspath/*.jar; do
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index ff54593faf..c0325a667b 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -2378,10 +2378,10 @@ public static enum ConfVars {
         "we think the key as a skew join key. "),
     HIVESKEWJOINMAPJOINNUMMAPTASK("hive.skewjoin.mapjoin.map.tasks", 10000,
         "Determine the number of map task used in the follow up map join job for a skew join.\n" +
-        "It should be used together with hive.skewjoin.mapjoin.min.split to perform a fine grained control."),
+        "It should be used together with hive.skewjoin.mapjoin.min.split to perform a fine-grained control."),
     HIVESKEWJOINMAPJOINMINSPLIT("hive.skewjoin.mapjoin.min.split", 33554432L,
         "Determine the number of map task at most used in the follow up map join job for a skew join by specifying \n" +
-        "the minimum split size. It should be used together with hive.skewjoin.mapjoin.map.tasks to perform a fine grained control."),
+        "the minimum split size. It should be used together with hive.skewjoin.mapjoin.map.tasks to perform a fine-grained control."),
 
     HIVESENDHEARTBEAT("hive.heartbeat.interval", 1000,
         "Send a heartbeat after this interval - used by mapjoin and filter operators"),
@@ -2788,7 +2788,7 @@ public static enum ConfVars {
     HIVE_STATS_DEFAULT_PUBLISHER("hive.stats.default.publisher", "",
         "The Java class (implementing the StatsPublisher interface) that is used by default if hive.stats.dbclass is custom type."),
     /**
-     * @deprecated Use MetastoreConf.STATS_DEFAULT_AGGRETATOR
+     * @deprecated Use MetastoreConf.STATS_DEFAULT_AGGREGATOR
      */
     @Deprecated
     HIVE_STATS_DEFAULT_AGGREGATOR("hive.stats.default.aggregator", "",
@@ -2945,7 +2945,7 @@ public static enum ConfVars {
         "queries that read non-orc MM tables with original files will fail. The default in\n" +
         "Hive 3.0 is false."),
     HIVE_LOCK_FILE_MOVE_MODE("hive.lock.file.move.protect", "all", new StringSet("none", "dp", "all"),
-        "During file move operations acqueires a SEMI_SHARED lock at the table level."
+        "During file move operations acquires a SEMI_SHARED lock at the table level."
             + "none:never; dp: only in case of dynamic partitioning operations; all: all table operations"),
 
     // Zookeeper related configs
diff --git a/common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java b/common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java
index 1d477f6a47..9094915f1f 100644
--- a/common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java
+++ b/common/src/test/org/apache/hadoop/hive/common/metrics/TestLegacyMetrics.java
@@ -82,10 +82,10 @@ public void testMetricsMBean() throws Exception {
     mbs.setAttribute(oname, attr);
 
     mBeanInfo = mbs.getMBeanInfo(oname);
-    MBeanAttributeInfo[] attrinuteInfos = mBeanInfo.getAttributes();
-    assertEquals(1, attrinuteInfos.length);
+    MBeanAttributeInfo[] attributeInfos = mBeanInfo.getAttributes();
+    assertEquals(1, attributeInfos.length);
     boolean attrFound = false;
-    for (MBeanAttributeInfo info : attrinuteInfos) {
+    for (MBeanAttributeInfo info : attributeInfos) {
       if ("fooMetric".equals(info.getName())) {
         assertEquals("java.lang.Long", info.getType());
         assertTrue(info.isReadable());
diff --git a/hcatalog/bin/hcat b/hcatalog/bin/hcat
index 2211407d1a..efa7c86740 100644
--- a/hcatalog/bin/hcat
+++ b/hcatalog/bin/hcat
@@ -124,7 +124,7 @@ for jar in ${HIVE_LIB_DIR}/*.jar ; do
 	HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$jar
 done
 
-# add the auxillary jars such as serdes
+# add the auxiliary jars such as serdes
 if [ -d "${HIVE_AUX_JARS_PATH}" ]; then
   for f in ${HIVE_AUX_JARS_PATH}/*.jar; do
     if [[ ! -f $f ]]; then
diff --git a/hcatalog/src/docs/src/documentation/conf/cli.xconf b/hcatalog/src/docs/src/documentation/conf/cli.xconf
index 5c6e245688..c238d97e85 100644
--- a/hcatalog/src/docs/src/documentation/conf/cli.xconf
+++ b/hcatalog/src/docs/src/documentation/conf/cli.xconf
@@ -84,7 +84,7 @@
        |  need to be uploaded to a server, using the timestamp.
        |
        |  The default path is relative to the core webapp directory.
-       |  An asolute path can be used.
+       |  An absolute path can be used.
        +-->
    <!--   <checksums-uri>build/work/checksums</checksums-uri>-->
 
diff --git a/hcatalog/src/docs/src/documentation/content/locationmap.xml b/hcatalog/src/docs/src/documentation/content/locationmap.xml
index 8ac59a9dd6..58422a23fd 100644
--- a/hcatalog/src/docs/src/documentation/content/locationmap.xml
+++ b/hcatalog/src/docs/src/documentation/content/locationmap.xml
@@ -51,7 +51,7 @@
       <select>
         <location src="first-location-attempted"/>
         <location src="second-location-attempted"/>
-        <location src="third-location-attepted"/>
+        <location src="third-location-attempted"/>
       </select>
     </match>
     -->
diff --git a/hcatalog/src/docs/src/documentation/content/xdocs/example.xml b/hcatalog/src/docs/src/documentation/content/xdocs/example.xml
index 8edf6e994f..afc9b0d15c 100644
--- a/hcatalog/src/docs/src/documentation/content/xdocs/example.xml
+++ b/hcatalog/src/docs/src/documentation/content/xdocs/example.xml
@@ -57,7 +57,7 @@ insert overwrite table 20100819events
 select advertiser_id, count(clicks)
 from processedevents
 where date = ‘20100819’
-group by adverstiser_id;
+group by advertiser_id;
 </source>
 
 <p><strong>With Templeton</strong> all these steps can be easily performed programatcally
diff --git a/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/repl/ReplicationTask.java b/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/repl/ReplicationTask.java
index 7c9c5a55a9..70bfd0390a 100644
--- a/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/repl/ReplicationTask.java
+++ b/hcatalog/webhcat/java-client/src/main/java/org/apache/hive/hcatalog/api/repl/ReplicationTask.java
@@ -64,7 +64,7 @@ private static Factory getFactoryInstance(HCatClient client) {
    * a) If a factory has already been instantiated, and is valid, use it.
    * b) If a factoryClassName has been provided, through .resetFactory(), attempt to instantiate that.
    * c) If a hive.repl.task.factory has been set in the default hive conf, use that.
-   * d) If none of the above methods work, instantiate an anoymous factory that will return an error
+   * d) If none of the above methods work, instantiate an anonymous factory that will return an error
    *    whenever called, till a user calls resetFactory.
    */
   private synchronized static void createFactoryInstance(HCatClient client) {
diff --git a/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ExecServiceImpl.java b/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ExecServiceImpl.java
index 5849a1d774..2dae37eb07 100644
--- a/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ExecServiceImpl.java
+++ b/hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/ExecServiceImpl.java
@@ -115,16 +115,16 @@ private ExecServiceImpl() {
   public ExecBean run(String program, List<String> args,
             Map<String, String> env)
     throws NotAuthorizedException, BusyException, ExecuteException, IOException {
-    boolean aquired = false;
+    boolean acquired = false;
     try {
-      aquired = avail.tryAcquire();
-      if (aquired) {
+      acquired = avail.tryAcquire();
+      if (acquired) {
         return runUnlimited(program, args, env);
       } else {
         throw new BusyException();
       }
     } finally {
-      if (aquired) {
+      if (acquired) {
         avail.release();
       }
     }
diff --git a/itests/hive-minikdc/pom.xml b/itests/hive-minikdc/pom.xml
index 22cf244c19..2d6a1804fd 100644
--- a/itests/hive-minikdc/pom.xml
+++ b/itests/hive-minikdc/pom.xml
@@ -36,7 +36,7 @@
   </properties>
 
   <dependencies>
-    <!-- dependencies are always listed in sorted order by groupId, artifectId -->
+    <!-- dependencies are always listed in sorted order by groupId, artifactId -->
     <dependency>
       <groupId>com.google.protobuf</groupId>
       <artifactId>protobuf-java</artifactId>
diff --git a/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/JdbcWithMiniKdcSQLAuthTest.java b/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/JdbcWithMiniKdcSQLAuthTest.java
index 8fc0faf1b4..ca10a45732 100644
--- a/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/JdbcWithMiniKdcSQLAuthTest.java
+++ b/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/JdbcWithMiniKdcSQLAuthTest.java
@@ -90,7 +90,7 @@ public void testAuthorization1() throws Exception {
 
     String tableName1 = "test_jdbc_sql_auth1";
     String tableName2 = "test_jdbc_sql_auth2";
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       // create tables as user1
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
index 35d43d7167..b347552f85 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/BaseReplicationScenariosAcidTables.java
@@ -347,7 +347,7 @@ List<Long> openTxns(int numTxns, TxnStore txnHandler, HiveConf primaryConf) thro
     return txns;
   }
 
-  List<Long> allocateWriteIdsForTablesAndAquireLocks(String primaryDbName, Map<String, Long> tables,
+  List<Long> allocateWriteIdsForTablesAndAcquireLocks(String primaryDbName, Map<String, Long> tables,
                                                      TxnStore txnHandler,
                                                      List<Long> txns, HiveConf primaryConf) throws Throwable {
     AllocateTableWriteIdsRequest rqst = new AllocateTableWriteIdsRequest();
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
index ed7a4f9875..f997f843df 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java
@@ -255,7 +255,7 @@ public void testCompleteFailoverWithReverseBootstrap() throws Throwable {
     Map<String, Long> tablesInSecDb = new HashMap<>();
     tablesInSecDb.put("t1", (long) numTxnsForSecDb);
     tablesInSecDb.put("t2", (long) numTxnsForSecDb);
-    List<Long> lockIdsForSecDb = allocateWriteIdsForTablesAndAquireLocks(primaryDbName + "_extra",
+    List<Long> lockIdsForSecDb = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName + "_extra",
             tablesInSecDb, txnHandler, txnsForSecDb, primaryConf);
 
     //Open 2 txns for Primary Db
@@ -267,7 +267,7 @@ public void testCompleteFailoverWithReverseBootstrap() throws Throwable {
     Map<String, Long> tablesInPrimaryDb = new HashMap<>();
     tablesInPrimaryDb.put("t1", (long) numTxnsForPrimaryDb + 1);
     tablesInPrimaryDb.put("t2", (long) numTxnsForPrimaryDb + 2);
-    List<Long> lockIdsForPrimaryDb = allocateWriteIdsForTablesAndAquireLocks(primaryDbName,
+    List<Long> lockIdsForPrimaryDb = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName,
             tablesInPrimaryDb, txnHandler, txnsForPrimaryDb, primaryConf);
 
     //Open 1 txn with no hive locks acquired
@@ -1104,7 +1104,7 @@ public void testAcidTablesBootstrapWithOpenTxnsTimeout() throws Throwable {
     Map<String, Long> tables = new HashMap<>();
     tables.put("t1", numTxns + 1L);
     tables.put("t2", numTxns + 2L);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
 
     // Bootstrap dump with open txn timeout as 1s.
     List<String> withConfigs = Arrays.asList(
@@ -1223,7 +1223,7 @@ public void testAcidTablesBootstrapWithOpenTxnsDiffDb() throws Throwable {
     Map<String, Long> tablesInSecDb = new HashMap<>();
     tablesInSecDb.put("t1", (long) numTxns);
     tablesInSecDb.put("t2", (long) numTxns);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName + "_extra",
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName + "_extra",
       tablesInSecDb, txnHandler, txns, primaryConf);
 
     // Bootstrap dump with open txn timeout as 300s.
@@ -1319,7 +1319,7 @@ public void testAcidTablesBootstrapWithOpenTxnsWaitingForLock() throws Throwable
     Map<String, Long> tablesInSecDb = new HashMap<>();
     tablesInSecDb.put("t1", (long) numTxns);
     tablesInSecDb.put("t2", (long) numTxns);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName + "_extra",
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName + "_extra",
       tablesInSecDb, txnHandler, txns, primaryConf);
 
     WarehouseInstance.Tuple bootstrapDump  = primary
@@ -1385,14 +1385,14 @@ public void testAcidTablesBootstrapWithOpenTxnsPrimaryAndSecondaryDb() throws Th
     Map<String, Long> tablesInSecDb = new HashMap<>();
     tablesInSecDb.put("t1", (long) numTxns);
     tablesInSecDb.put("t2", (long) numTxns);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName + "_extra",
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName + "_extra",
       tablesInSecDb, txnHandler, txns, primaryConf);
     // Allocate write ids for both tables of primary db for all txns
     // t1=5+1L and t2=5+2L inserts
     Map<String, Long> tablesInPrimDb = new HashMap<>();
     tablesInPrimDb.put("t1", (long) numTxns + 1L);
     tablesInPrimDb.put("t2", (long) numTxns + 2L);
-    lockIds.addAll(allocateWriteIdsForTablesAndAquireLocks(primaryDbName,
+    lockIds.addAll(allocateWriteIdsForTablesAndAcquireLocks(primaryDbName,
       tablesInPrimDb, txnHandler, txnsSameDb, primaryConf));
 
     // Bootstrap dump with open txn timeout as 1s.
@@ -1460,7 +1460,7 @@ public void testAcidTablesBootstrapWithOpenTxnsAbortDisabled() throws Throwable
     Map<String, Long> tables = new HashMap<>();
     tables.put("t1", numTxns + 1L);
     tables.put("t2", numTxns + 2L);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
 
     // Bootstrap dump with open txn timeout as 1s.
     List<String> withConfigs = Arrays.asList(
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
index 145e19a218..e393c3dcd9 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTablesBootstrap.java
@@ -200,7 +200,7 @@ public void testAcidTablesBootstrapDuringIncrementalWithOpenTxnsTimeout() throws
     Map<String, Long> tables = new HashMap<>();
     tables.put("t1", numTxns+2L);
     tables.put("t2", numTxns+6L);
-    List<Long> lockIds = allocateWriteIdsForTablesAndAquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
+    List<Long> lockIds = allocateWriteIdsForTablesAndAcquireLocks(primaryDbName, tables, txnHandler, txns, primaryConf);
 
     // Bootstrap dump with open txn timeout as 1s.
     List<String> withConfigs = new LinkedList<>(dumpWithAcidBootstrapClause);
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
index da1722a0fc..a732af4124 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestTableLevelReplicationScenarios.java
@@ -383,14 +383,14 @@ public void testIncorrectTablePolicyInReplDump() throws Throwable {
     String[] originalTables = new String[] {"t1", "t11", "t2", "t3", "t111"};
     createTables(originalTables, CreateTableType.NON_ACID);
 
-    // Invalid repl policy where abrubtly placed DOT which causes ParseException during REPL dump.
+    // Invalid repl policy where abruptly placed DOT which causes ParseException during REPL dump.
     String[] replicatedTables = new String[] {};
     boolean failed;
     String[] invalidReplPolicies = new String[] {
         primaryDbName + ".t1.t2", // Didn't enclose table pattern within single quotes.
         primaryDbName + ".'t1'.t2", // Table name and include list not allowed.
         primaryDbName + ".t1.'t2'", // Table name and exclude list not allowed.
-        primaryDbName + ".'t1+'.", // Abrubtly ended dot.
+        primaryDbName + ".'t1+'.", // Abruptly ended dot.
         primaryDbName +  ".['t1+'].['t11']", // With square brackets
         primaryDbName + "..''", // Two dots with empty list
         primaryDbName + ".'t1'.'tt2'.'t3'" // More than two list
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMultiAuthorizationPreEventListener.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMultiAuthorizationPreEventListener.java
index 2f22699802..d1e80698ef 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMultiAuthorizationPreEventListener.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestMultiAuthorizationPreEventListener.java
@@ -51,7 +51,7 @@ public static void setUp() throws Exception {
     System.setProperty(HiveConf.ConfVars.METASTORE_PRE_EVENT_LISTENERS.varname,
         AuthorizationPreEventListener.class.getName());
 
-    // Set two dummy classes as authorizatin managers. Two instances should get created.
+    // Set two dummy classes as authorization managers. Two instances should get created.
     System.setProperty(HiveConf.ConfVars.HIVE_METASTORE_AUTHORIZATION_MANAGER.varname,
         DummyHiveMetastoreAuthorizationProvider.class.getName() + ","
             + DummyHiveMetastoreAuthorizationProvider.class.getName());
diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthorization.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthorization.java
index 1e1dbcc605..1f05be10a7 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthorization.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/authorization/TestJdbcWithSQLAuthorization.java
@@ -69,7 +69,7 @@ public void testAuthorization1() throws Exception {
 
     String tableName1 = "test_jdbc_sql_auth1";
     String tableName2 = "test_jdbc_sql_auth2";
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       // create tables as user1
@@ -133,7 +133,7 @@ private Connection getConnection(String userName) throws Exception {
   @Test
   public void testAllowedCommands() throws Exception {
 
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       // create tables as user1
@@ -160,7 +160,7 @@ public void testAllowedCommands() throws Exception {
 
   @Test
   public void testAuthZFailureLlapCachePurge() throws Exception {
-    // using different code blocks so that jdbc variables are not accidently re-used
+    // using different code blocks so that jdbc variables are not accidentally re-used
     // between the actions. Different connection/statement object should be used for each action.
     {
       Connection hs2Conn = getConnection("user1");
diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/StartMiniHS2Cluster.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/StartMiniHS2Cluster.java
index 675cc2a6c0..048400c12d 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/StartMiniHS2Cluster.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/miniHS2/StartMiniHS2Cluster.java
@@ -78,7 +78,7 @@ public void testRunCluster() throws Exception {
     miniHS2.start(confOverlay);
     miniHS2.getDFS().getFileSystem().mkdirs(new Path("/apps_staging_dir/anonymous"));
 
-    System.out.println("JDBC URL avaailable at " + miniHS2.getJdbcURL());
+    System.out.println("JDBC URL available at " + miniHS2.getJdbcURL());
 
     // MiniHS2 cluster is up .. let it run until someone kills the test
     while (true) {
diff --git a/itests/qtest-druid/pom.xml b/itests/qtest-druid/pom.xml
index 842b3af93b..f7d2796364 100644
--- a/itests/qtest-druid/pom.xml
+++ b/itests/qtest-druid/pom.xml
@@ -34,7 +34,7 @@
   <packaging>jar</packaging>
   <name>Hive Integration - QFile Druid Tests</name>
 
-  <!-- dependencies are always listed in sorted order by groupId, artifectId -->
+  <!-- dependencies are always listed in sorted order by groupId, artifactId -->
   <properties>
     <hive.path.to.root>../..</hive.path.to.root>
     <druid.avatica.version>1.15.0</druid.avatica.version>
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCoreBlobstoreCliDriver.java b/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCoreBlobstoreCliDriver.java
index aef0b07006..0d343545e4 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCoreBlobstoreCliDriver.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/cli/control/AbstractCoreBlobstoreCliDriver.java
@@ -96,7 +96,7 @@ public void tearDown() throws Exception {
   public void shutdown() throws Exception {
     qt.shutdown();
     if (System.getenv(QTestUtil.QTEST_LEAVE_FILES) == null) {
-      qt.executeAdhocCommand("dfs -rmdir " + testBlobstorePathUnique);
+      qt.executeAdHocCommand("dfs -rmdir " + testBlobstorePathUnique);
     }
   }
 
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
index e6c16242b2..f3d804a3e4 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java
@@ -680,7 +680,7 @@ private void closeSession(SessionState oldSs) throws IOException {
     }
   }
 
-  public int executeAdhocCommand(String q) throws CommandProcessorException {
+  public int executeAdHocCommand(String q) throws CommandProcessorException {
     if (!q.contains(";")) {
       return -1;
     }
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
index bcefe89b50..40e02d0bde 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/hooks/CheckTableAccessHook.java
@@ -34,7 +34,7 @@
 /*
  * This hook is used for verifying the table access key information
  * that is generated and maintained in the QueryPlan object by the
- * TableAccessAnalyer. All the hook does is print out the table/keys
+ * TableAccessAnalyzer. All the hook does is print out the table/keys
  * per operator recorded in the TableAccessInfo in the QueryPlan.
  */
 public class CheckTableAccessHook implements ExecuteWithHookContext {
diff --git a/itests/util/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java b/itests/util/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
index e201cddfd6..5b72656d48 100644
--- a/itests/util/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
+++ b/itests/util/src/main/java/org/apache/hive/jdbc/miniHS2/MiniHS2.java
@@ -533,7 +533,7 @@ public String getJdbcURL(String dbName) throws Exception {
   /**
    * return connection URL for this server instance
    * @param dbName - DB name to be included in the URL
-   * @param sessionConfExt - Addional string to be appended to sessionConf part of url
+   * @param sessionConfExt - Additional string to be appended to sessionConf part of url
    * @return
    * @throws Exception
    */
@@ -544,7 +544,7 @@ public String getJdbcURL(String dbName, String sessionConfExt) throws Exception
   /**
    * return connection URL for this server instance
    * @param dbName - DB name to be included in the URL
-   * @param sessionConfExt - Addional string to be appended to sessionConf part of url
+   * @param sessionConfExt - Additional string to be appended to sessionConf part of url
    * @param hiveConfExt - Additional string to be appended to HiveConf part of url (excluding the ?)
    * @return
    * @throws Exception
diff --git a/llap-common/src/test/org/apache/hadoop/hive/llap/metrics/TestReadWriteLockMetrics.java b/llap-common/src/test/org/apache/hadoop/hive/llap/metrics/TestReadWriteLockMetrics.java
index f48e1f5417..556e9db58a 100644
--- a/llap-common/src/test/org/apache/hadoop/hive/llap/metrics/TestReadWriteLockMetrics.java
+++ b/llap-common/src/test/org/apache/hadoop/hive/llap/metrics/TestReadWriteLockMetrics.java
@@ -384,7 +384,7 @@ public void testWithoutContention() throws Exception {
                rec.getMetrics().get(ReadLockWaitTimeMax).longValue()
                                     < lhR.getLockMax());
 
-    assertTrue("Max greater or equal to avergae lock time",
+    assertTrue("Max greater or equal to average lock time",
                (rec.getMetrics().get(ReadLockWaitTimeTotal).longValue()
                 / rec.getMetrics().get(ReadLockCount).longValue())
                   <= rec.getMetrics().get(ReadLockWaitTimeMax).longValue());
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/DirWatcher.java b/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/DirWatcher.java
index 83ccc7f154..31722ff3c1 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/DirWatcher.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/DirWatcher.java
@@ -157,7 +157,7 @@ private void registerDir(Path path, WatchedPathInfo watchedPathInfo) {
 
   private void trackWatchForAttempt(WatchedPathInfo watchedPathInfo, WatchKey watchKey) {
     assert watchedPathInfo.pathIdentifier != null;
-    // TODO May be possible to do finer grained locks.
+    // TODO May be possible to do finer-grained locks.
     synchronized (watchesPerAttempt) {
       List<WatchKey> list = watchesPerAttempt.get(watchedPathInfo.pathIdentifier);
       if (list == null) {
@@ -169,7 +169,7 @@ private void trackWatchForAttempt(WatchedPathInfo watchedPathInfo, WatchKey watc
   }
 
   private void cancelWatchesForAttempt(AttemptPathIdentifier pathIdentifier) {
-    // TODO May be possible to do finer grained locks.
+    // TODO May be possible to do finer-grained locks.
     synchronized(watchesPerAttempt) {
       List<WatchKey> list = watchesPerAttempt.remove(pathIdentifier);
       if (list != null) {
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java b/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java
index bc34708805..249b21f6b1 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java
@@ -91,7 +91,7 @@ public long transferTo(WritableByteChannel target, long position)
   /**
    * Since Netty4, deallocate() is called automatically during cleanup, but before the
    * ChannelFutureListeners. Deallocate calls FileChannel.close() and makes the file descriptor
-   * invalid, so every OS cache operation (e.g. posix_fadvice) with the original file descriptor
+   * invalid, so every OS cache operation (e.g. posix_fadvise) with the original file descriptor
    * will fail after this operation, so we need to take care of cleanup operations here (before
    * deallocating) instead of listeners outside.
    */
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java b/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
index 4705d1a636..ef3eaac0d8 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java
@@ -430,7 +430,7 @@ public static ShuffleHandler get() {
 
   /**
    * Serialize the shuffle port into a ByteBuffer for use later on.
-   * @param port the port to be sent to the ApplciationMaster
+   * @param port the port to be sent to the ApplicationMaster
    * @return the serialized form of the port.
    */
   public static ByteBuffer serializeMetaData(int port) throws IOException {
diff --git a/llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/comparator/TestFirstInFirstOutComparator.java b/llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/comparator/TestFirstInFirstOutComparator.java
index dffbb57987..724a9633f4 100644
--- a/llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/comparator/TestFirstInFirstOutComparator.java
+++ b/llap-server/src/test/org/apache/hadoop/hive/llap/daemon/impl/comparator/TestFirstInFirstOutComparator.java
@@ -102,7 +102,7 @@ public void testWaitQueueComparator() throws InterruptedException, IOException {
     assertEquals(r3, queue.peek());
     assertNull(queue.offer(r4, 0));
     assertEquals(r4, queue.peek());
-    // this offer will be accpeted and r1 evicted
+    // this offer will be accepted and r1 evicted
     assertEquals(r1, queue.offer(r5, 0));
     assertEquals(r5, queue.take());
     assertEquals(r4, queue.take());
diff --git a/llap-server/src/test/org/apache/hadoop/hive/llap/shufflehandler/TestShuffleHandler.java b/llap-server/src/test/org/apache/hadoop/hive/llap/shufflehandler/TestShuffleHandler.java
index 591fa2e7d7..de641a9969 100644
--- a/llap-server/src/test/org/apache/hadoop/hive/llap/shufflehandler/TestShuffleHandler.java
+++ b/llap-server/src/test/org/apache/hadoop/hive/llap/shufflehandler/TestShuffleHandler.java
@@ -58,7 +58,7 @@ void setAddress(SocketAddress lastAddress) {
       this.lastAddress = lastAddress;
     }
 
-    SocketAddress getSocketAddres() {
+    SocketAddress getSocketAddress() {
       return lastAddress;
     }
   }
@@ -192,7 +192,7 @@ protected void sendError(ChannelHandlerContext ctx, String message,
     byte[] buffer = new byte[1024];
     while (input.read(buffer) != -1) {
     }
-    SocketAddress firstAddress = lastSocketAddress.getSocketAddres();
+    SocketAddress firstAddress = lastSocketAddress.getSocketAddress();
     input.close();
 
     // For keepAlive via URL
@@ -211,7 +211,7 @@ protected void sendError(ChannelHandlerContext ctx, String message,
     header = new ShuffleHeader();
     header.readFields(input);
     input.close();
-    SocketAddress secondAddress = lastSocketAddress.getSocketAddres();
+    SocketAddress secondAddress = lastSocketAddress.getSocketAddress();
     Assert.assertNotNull("Initial shuffle address should not be null", firstAddress);
     Assert.assertNotNull("Keep-Alive shuffle address should not be null", secondAddress);
     Assert.assertEquals(
diff --git a/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java b/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
index 547d68fcce..a9ae27f25d 100644
--- a/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
+++ b/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java
@@ -375,7 +375,7 @@ public interface OperationCallback<ResultType, CtxType> {
   /**
    * @param node
    * @param callback
-   * @return if it was possible to attemp the registration. Sometimes it's
+   * @return if it was possible to attempt the registration. Sometimes it's
    * not possible because is a dag is not running
    */
   public boolean registerDag(NodeInfo node, final OperationCallback<QueryIdentifierProto, Void> callback) {
diff --git a/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java b/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
index a02aa34aa6..473ab1eb18 100644
--- a/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
+++ b/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java
@@ -2764,7 +2764,7 @@ boolean hadCommFailure() {
       return hadCommFailure;
     }
 
-    boolean _canAccepInternal() {
+    boolean _canAcceptInternal() {
       return !hadCommFailure && !disabled
           &&(numSchedulableTasks == -1 || ((numSchedulableTasks - numScheduledTasks) > 0));
     }
@@ -2773,7 +2773,7 @@ boolean _canAccepInternal() {
     may be running in the system. Also depends upon the capacity usage configuration
      */
     boolean canAcceptTask() {
-      boolean result = _canAccepInternal();
+      boolean result = _canAcceptInternal();
       if (LOG.isTraceEnabled()) {
         LOG.trace(constructCanAcceptLogResult(result));
       }
@@ -2828,7 +2828,7 @@ public String toString() {
 
     private String toShortString() {
       StringBuilder sb = new StringBuilder();
-      sb.append(", canAcceptTask=").append(_canAccepInternal());
+      sb.append(", canAcceptTask=").append(_canAcceptInternal());
       sb.append(", st=").append(numScheduledTasks);
       sb.append(", ac=").append((numSchedulableTasks - numScheduledTasks));
       sb.append(", commF=").append(hadCommFailure);
diff --git a/parser/pom.xml b/parser/pom.xml
index b02476a253..0638ca7375 100644
--- a/parser/pom.xml
+++ b/parser/pom.xml
@@ -32,7 +32,7 @@
   </properties>
 
   <dependencies>
-    <!-- dependencies are always listed in sorted order by groupId, artifectId -->
+    <!-- dependencies are always listed in sorted order by groupId, artifactId -->
     <!-- intra-proect -->
     <dependency>
       <groupId>org.apache.hive</groupId>
@@ -73,7 +73,7 @@
     <sourceDirectory>${basedir}/src/java</sourceDirectory>
     <testSourceDirectory>${basedir}/src/test</testSourceDirectory>
     <plugins>
-      <!-- plugins are always listed in sorted order by groupId, artifectId -->
+      <!-- plugins are always listed in sorted order by groupId, artifactId -->
       <plugin>
         <groupId>org.antlr</groupId>
         <artifactId>antlr3-maven-plugin</artifactId>
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt
index 3569d51036..c884141f93 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMax.txt
@@ -101,17 +101,17 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregrateIndex,
+        int aggregateIndex,
         int row) {
       VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
       return myagg;
     }
 
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       VectorizedRowBatch batch) throws HiveException {
 
       int batchSize = batch.size;
@@ -130,32 +130,32 @@ public class <ClassName> extends VectorAggregateExpression {
       if (inputVector.noNulls) {
         if (inputVector.isRepeating) {
           iterateNoNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             vector[0], batchSize);
         } else {
           if (batch.selectedInUse) {
             iterateNoNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, batch.selected, batchSize);
           } else {
             iterateNoNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, batchSize);
           }
         }
       } else {
         if (inputVector.isRepeating) {
           iterateHasNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             vector[0], batchSize, inputVector.isNull);
         } else {
           if (batch.selectedInUse) {
             iterateHasNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, batchSize, batch.selected, inputVector.isNull);
           } else {
             iterateHasNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, batchSize, inputVector.isNull);
           }
         }
@@ -164,14 +164,14 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       <ValueType> value,
       int batchSize) {
 
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(value);
       }
@@ -179,7 +179,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       <ValueType>[] values,
       int[] selection,
       int batchSize) {
@@ -187,7 +187,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(values[selection[i]]);
       }
@@ -195,13 +195,13 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       <ValueType>[] values,
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(values[i]);
       }
@@ -209,7 +209,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       <ValueType> value,
       int batchSize,
       boolean[] isNull) {
@@ -221,7 +221,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(value);
       }
@@ -229,7 +229,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       <ValueType>[] values,
       int batchSize,
       int[] selection,
@@ -240,7 +240,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             j);
           myagg.minmaxValue(values[i]);
         }
@@ -249,7 +249,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       <ValueType>[] values,
       int batchSize,
       boolean[] isNull) {
@@ -258,7 +258,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
           myagg.minmaxValue(values[i]);
         }
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt
index eb63301b99..4b10e35d6d 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxDecimal.txt
@@ -99,17 +99,17 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregrateIndex,
+        int aggregateIndex,
         int row) {
       VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
       return myagg;
     }
 
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       VectorizedRowBatch batch) throws HiveException {
 
       int batchSize = batch.size;
@@ -129,32 +129,32 @@ public class <ClassName> extends VectorAggregateExpression {
       if (inputVector.noNulls) {
         if (inputVector.isRepeating) {
           iterateNoNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             vector[0], inputVector.scale, batchSize);
         } else {
           if (batch.selectedInUse) {
             iterateNoNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, inputVector.scale, batch.selected, batchSize);
           } else {
             iterateNoNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, inputVector.scale, batchSize);
           }
         }
       } else {
         if (inputVector.isRepeating) {
           iterateHasNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             vector[0], inputVector.scale, batchSize, inputVector.isNull);
         } else {
           if (batch.selectedInUse) {
             iterateHasNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, inputVector.scale, batchSize, batch.selected, inputVector.isNull);
           } else {
             iterateHasNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               vector, inputVector.scale, batchSize, inputVector.isNull);
           }
         }
@@ -163,7 +163,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       HiveDecimalWritable value,
       short scale,
       int batchSize) {
@@ -171,7 +171,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(value, scale);
       }
@@ -179,7 +179,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       HiveDecimalWritable[] vector,
       short scale,
       int[] selection,
@@ -188,7 +188,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(vector[selection[i]], scale);
       }
@@ -196,14 +196,14 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       HiveDecimalWritable[] vector,
       short scale,
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(vector[i], scale);
       }
@@ -211,7 +211,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       HiveDecimalWritable value,
       short scale,
       int batchSize,
@@ -224,7 +224,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(value, scale);
       }
@@ -232,7 +232,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       HiveDecimalWritable[] vector,
       short scale,
       int batchSize,
@@ -244,7 +244,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             j);
           myagg.minmaxValue(vector[i], scale);
         }
@@ -253,7 +253,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       HiveDecimalWritable[] vector,
       short scale,
       int batchSize,
@@ -263,7 +263,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
           myagg.minmaxValue(vector[i], scale);
         }
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
index 9fdf77ceeb..4bc5752a0c 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
@@ -98,17 +98,17 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregrateIndex,
+        int aggregateIndex,
         int row) {
       VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
       return myagg;
     }
 
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       VectorizedRowBatch batch) throws HiveException {
 
       int batchSize = batch.size;
@@ -126,32 +126,32 @@ public class <ClassName> extends VectorAggregateExpression {
       if (inputColVector.noNulls) {
         if (inputColVector.isRepeating) {
           iterateNoNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             inputColVector, batchSize);
         } else {
           if (batch.selectedInUse) {
             iterateNoNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batch.selected, batchSize);
           } else {
             iterateNoNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batchSize);
           }
         }
       } else {
         if (inputColVector.isRepeating) {
           iterateHasNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             inputColVector, batchSize, inputColVector.isNull);
         } else {
           if (batch.selectedInUse) {
             iterateHasNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batchSize, batch.selected, inputColVector.isNull);
           } else {
             iterateHasNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batchSize, inputColVector.isNull);
           }
         }
@@ -160,14 +160,14 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       IntervalDayTimeColumnVector inputColVector,
       int batchSize) {
 
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         // Repeating use index 0.
         myagg.minmaxValue(inputColVector, 0);
@@ -176,7 +176,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       IntervalDayTimeColumnVector inputColVector,
       int[] selection,
       int batchSize) {
@@ -184,7 +184,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(inputColVector, selection[i]);
       }
@@ -192,13 +192,13 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       IntervalDayTimeColumnVector inputColVector,
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(inputColVector, i);
       }
@@ -206,7 +206,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       IntervalDayTimeColumnVector inputColVector,
       int batchSize,
       boolean[] isNull) {
@@ -218,7 +218,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         // Repeating use index 0.
         myagg.minmaxValue(inputColVector, 0);
@@ -227,7 +227,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       IntervalDayTimeColumnVector inputColVector,
       int batchSize,
       int[] selection,
@@ -238,7 +238,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             j);
           myagg.minmaxValue(inputColVector, i);
         }
@@ -247,7 +247,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       IntervalDayTimeColumnVector inputColVector,
       int batchSize,
       boolean[] isNull) {
@@ -256,7 +256,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
           myagg.minmaxValue(inputColVector, i);
         }
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt
index 3387c0d997..a6dd08888b 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxString.txt
@@ -108,17 +108,17 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregrateIndex,
+        int aggregateIndex,
         int row) {
       VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
       return myagg;
     }
 
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       VectorizedRowBatch batch) throws HiveException {
 
       int batchSize = batch.size;
@@ -136,32 +136,32 @@ public class <ClassName> extends VectorAggregateExpression {
       if (inputColumn.noNulls) {
         if (inputColumn.isRepeating) {
           iterateNoNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             inputColumn, batchSize);
         } else {
           if (batch.selectedInUse) {
             iterateNoNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColumn, batch.selected, batchSize);
           } else {
             iterateNoNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColumn, batchSize);
           }
         }
       } else {
         if (inputColumn.isRepeating) {
           iterateHasNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             inputColumn, batchSize, inputColumn.isNull);
         } else {
           if (batch.selectedInUse) {
             iterateHasNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColumn, batchSize, batch.selected);
           } else {
             iterateHasNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColumn, batchSize);
           }
         }
@@ -170,7 +170,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       BytesColumnVector inputColumn,
       int batchSize) {
 
@@ -180,7 +180,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(bytes, start, length);
       }
@@ -188,7 +188,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       BytesColumnVector inputColumn,
       int[] selection,
       int batchSize) {
@@ -197,7 +197,7 @@ public class <ClassName> extends VectorAggregateExpression {
         int row = selection[i];
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(inputColumn.vector[row],
           inputColumn.start[row],
@@ -207,13 +207,13 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       BytesColumnVector inputColumn,
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(inputColumn.vector[i],
           inputColumn.start[i],
@@ -223,7 +223,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       BytesColumnVector inputColumn,
       int batchSize,
       boolean[] isNull) {
@@ -235,7 +235,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         // Repeating use index 0.
         myagg.minmaxValue(inputColumn.vector[0],
@@ -247,7 +247,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       BytesColumnVector inputColumn,
       int batchSize,
       int[] selection) {
@@ -257,7 +257,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!inputColumn.isNull[row]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
           myagg.minmaxValue(inputColumn.vector[row],
             inputColumn.start[row],
@@ -268,7 +268,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       BytesColumnVector inputColumn,
       int batchSize) {
 
@@ -276,7 +276,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!inputColumn.isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
           myagg.minmaxValue(inputColumn.vector[i],
             inputColumn.start[i],
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
index b8d71d6491..d9c623428b 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
@@ -100,17 +100,17 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private Aggregation getCurrentAggregationBuffer(
         VectorAggregationBufferRow[] aggregationBufferSets,
-        int aggregrateIndex,
+        int aggregateIndex,
         int row) {
       VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
       return myagg;
     }
 
     @Override
     public void aggregateInputSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       VectorizedRowBatch batch) throws HiveException {
 
       int batchSize = batch.size;
@@ -128,32 +128,32 @@ public class <ClassName> extends VectorAggregateExpression {
       if (inputColVector.noNulls) {
         if (inputColVector.isRepeating) {
           iterateNoNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             inputColVector, batchSize);
         } else {
           if (batch.selectedInUse) {
             iterateNoNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batch.selected, batchSize);
           } else {
             iterateNoNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batchSize);
           }
         }
       } else {
         if (inputColVector.isRepeating) {
           iterateHasNullsRepeatingWithAggregationSelection(
-            aggregationBufferSets, aggregrateIndex,
+            aggregationBufferSets, aggregateIndex,
             inputColVector, batchSize, inputColVector.isNull);
         } else {
           if (batch.selectedInUse) {
             iterateHasNullsSelectionWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batchSize, batch.selected, inputColVector.isNull);
           } else {
             iterateHasNullsWithAggregationSelection(
-              aggregationBufferSets, aggregrateIndex,
+              aggregationBufferSets, aggregateIndex,
               inputColVector, batchSize, inputColVector.isNull);
           }
         }
@@ -162,14 +162,14 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       TimestampColumnVector inputColVector,
       int batchSize) {
 
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         // Repeating use index 0.
         myagg.minmaxValue(inputColVector, 0);
@@ -178,7 +178,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       TimestampColumnVector inputColVector,
       int[] selection,
       int batchSize) {
@@ -186,7 +186,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(inputColVector, selection[i]);
       }
@@ -194,13 +194,13 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateNoNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       TimestampColumnVector inputColVector,
       int batchSize) {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         myagg.minmaxValue(inputColVector, i);
       }
@@ -208,7 +208,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       TimestampColumnVector inputColVector,
       int batchSize,
       boolean[] isNull) {
@@ -220,7 +220,7 @@ public class <ClassName> extends VectorAggregateExpression {
       for (int i=0; i < batchSize; ++i) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         // Repeating use index 0.
         myagg.minmaxValue(inputColVector, 0);
@@ -229,7 +229,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       TimestampColumnVector inputColVector,
       int batchSize,
       int[] selection,
@@ -240,7 +240,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             j);
           myagg.minmaxValue(inputColVector, i);
         }
@@ -249,7 +249,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
     private void iterateHasNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       TimestampColumnVector inputColVector,
       int batchSize,
       boolean[] isNull) {
@@ -258,7 +258,7 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
           myagg.minmaxValue(inputColVector, i);
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java b/ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
index 0bb06f508c..b931ee5be3 100644
--- a/ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
+++ b/ql/src/java/org/apache/hadoop/hive/llap/WritableByteChannelAdapter.java
@@ -54,7 +54,7 @@ public class WritableByteChannelAdapter implements WritableByteChannel {
   private ChannelFutureListener writeListener = new ChannelFutureListener() {
     @Override
     public void operationComplete(ChannelFuture future) {
-      //Asynch write completed
+      //Async write completed
       //Up the semaphore
       writeResources.release();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AlterTableType.java b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AlterTableType.java
index eb46ed3902..9b1de08956 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AlterTableType.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ddl/table/AlterTableType.java
@@ -60,8 +60,8 @@ public enum AlterTableType {
   TOUCH("touch"),
   RENAME("rename"),
   OWNER("set owner"),
-  ARCHIVE("archieve"),
-  UNARCHIVE("unarchieve"),
+  ARCHIVE("archive"),
+  UNARCHIVE("unarchive"),
   COMPACT("compact"),
   TRUNCATE("truncate"),
   MERGEFILES("merge files"),
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index 37ee66f6c3..935ae77e63 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -122,7 +122,7 @@ public class ReduceSinkOperator extends TerminalOperator<ReduceSinkDesc>
    * If there is no distinct expression, cachedKeys is simply like this.
    * cachedKeys[0] = [col0][col1]
    *
-   * with two distict expression, union(tag:key) is attatched for each distinct expression
+   * with two distict expression, union(tag:key) is attached for each distinct expression
    * cachedKeys[0] = [col0][col1][0:dist1]
    * cachedKeys[1] = [col0][col1][1:dist2]
    *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
index 01d0392910..2565212935 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java
@@ -356,8 +356,8 @@ private int insertKeyIntoHeap(HiveKey key) throws IOException, HiveException {
     hashes[index] = key.hashCode();
     if (null != indexes.store(index)) {
       // it's only for GBY which should forward all values associated with the key in the range
-      // of limit. new value should be attatched with the key but in current implementation,
-      // only one values is allowed. with map-aggreagtion which is true by default,
+      // of limit. new value should be attached with the key but in current implementation,
+      // only one values is allowed. with map-aggregation which is true by default,
       // this is not common case, so just forward new key/value and forget that (todo)
       return FORWARD;
     }
@@ -408,7 +408,7 @@ private interface IndexStore {
 
   /**
    * for order by, same keys are counted (For 1-2-2-3-4, limit 3 is 1-2-2)
-   * MinMaxPriorityQueue is used because it alows duplication and fast access to biggest one
+   * MinMaxPriorityQueue is used because it allows duplication and fast access to biggest one
    */
   private class HashForRow implements IndexStore {
     private final MinMaxPriorityQueue<Integer> indexes = MinMaxPriorityQueue.orderedBy(C).create();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 7474603620..ffb83b407a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -4189,7 +4189,7 @@ public static List<String> getStatsTmpDirs(BaseWork work, Configuration conf) {
     // if its auto-stats gather for inserts or CTAS, stats dir will be in FileSink
     Set<Operator<? extends OperatorDesc>> ops = work.getAllLeafOperators();
     if (work instanceof MapWork) {
-      // if its an anlayze statement, stats dir will be in TableScan
+      // if its an analyze statement, stats dir will be in TableScan
       ops.addAll(work.getAllRootOperators());
     }
     for (Operator<? extends OperatorDesc> op : ops) {
@@ -4673,7 +4673,7 @@ public static void handleDirectInsertTableFinalPath(Path specPath, String unionS
   static List<Path> selectManifestFiles(FileStatus[] manifestFiles) {
     List<Path> manifests = new ArrayList<>();
     if (manifestFiles != null) {
-      Map<String, Integer> fileNameToAttempId = new HashMap<>();
+      Map<String, Integer> fileNameToAttemptId = new HashMap<>();
       Map<String, Path> fileNameToPath = new HashMap<>();
 
       for (FileStatus manifestFile : manifestFiles) {
@@ -4689,13 +4689,13 @@ static List<Path> selectManifestFiles(FileStatus[] manifestFiles) {
           if (matcher.matches()) {
             String taskId = matcher.group(1);
             int attemptId = Integer.parseInt(matcher.group(2));
-            Integer maxAttemptId = fileNameToAttempId.get(taskId);
+            Integer maxAttemptId = fileNameToAttemptId.get(taskId);
             if (maxAttemptId == null) {
-              fileNameToAttempId.put(taskId, attemptId);
+              fileNameToAttemptId.put(taskId, attemptId);
               fileNameToPath.put(taskId, path);
               Utilities.FILE_OP_LOGGER.info("Found manifest file {} with attemptId {}.", path, attemptId);
             } else if (attemptId > maxAttemptId) {
-              fileNameToAttempId.put(taskId, attemptId);
+              fileNameToAttemptId.put(taskId, attemptId);
               fileNameToPath.put(taskId, path);
               Utilities.FILE_OP_LOGGER.info(
                   "Found manifest file {} which has higher attemptId than {}. Ignore the manifest files with attemptId below {}.",
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
index 48aaf42268..3863584936 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
@@ -489,7 +489,7 @@ public static String isEligibleForLocalMode(HiveConf conf,
 
     // ideally we would like to do this check based on the number of splits
     // in the absence of an easy way to get the number of splits - do this
-    // based on the total number of files (pessimistically assumming that
+    // based on the total number of files (pessimistically assuming that
     // splits are equal to number of files in worst case)
     if (inputFileCount > maxInputFiles) {
       return "Number of Input Files (= " + inputFileCount +
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableConf.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableConf.java
index 75423850fd..efb4f591d5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableConf.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableConf.java
@@ -27,7 +27,7 @@
  * among them, which is used in n-way join (multiple small tables are involved).
  */
 public class HybridHashTableConf {
-  private List<HybridHashTableContainer> loadedContainerList; // A list of alrady loaded containers
+  private List<HybridHashTableContainer> loadedContainerList; // A list of already loaded containers
   private int numberOfPartitions = 0; // Number of partitions each table should have
   private int nextSpillPartition = -1;       // The partition to be spilled next
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
index fdb067f310..5713bcb78b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilter.java
@@ -273,14 +273,14 @@ public void aggregateInputSelection(
 
   private void iterateNoNullsRepeatingWithAggregationSelection(
     VectorAggregationBufferRow[] aggregationBufferSets,
-    int aggregrateIndex,
+    int aggregateIndex,
     ColumnVector inputColumn,
     int batchSize) {
 
     for (int i=0; i < batchSize; ++i) {
       Aggregation myagg = getCurrentAggregationBuffer(
         aggregationBufferSets,
-        aggregrateIndex,
+        aggregateIndex,
         i);
       valueProcessor.processValue(myagg, inputColumn, 0);
     }
@@ -288,7 +288,7 @@ private void iterateNoNullsRepeatingWithAggregationSelection(
 
   private void iterateNoNullsSelectionWithAggregationSelection(
     VectorAggregationBufferRow[] aggregationBufferSets,
-    int aggregrateIndex,
+    int aggregateIndex,
     ColumnVector inputColumn,
     int[] selection,
     int batchSize) {
@@ -297,7 +297,7 @@ private void iterateNoNullsSelectionWithAggregationSelection(
       int row = selection[i];
       Aggregation myagg = getCurrentAggregationBuffer(
         aggregationBufferSets,
-        aggregrateIndex,
+        aggregateIndex,
         i);
       valueProcessor.processValue(myagg, inputColumn, row);
     }
@@ -305,13 +305,13 @@ private void iterateNoNullsSelectionWithAggregationSelection(
 
   private void iterateNoNullsWithAggregationSelection(
     VectorAggregationBufferRow[] aggregationBufferSets,
-    int aggregrateIndex,
+    int aggregateIndex,
     ColumnVector inputColumn,
     int batchSize) {
     for (int i=0; i < batchSize; ++i) {
       Aggregation myagg = getCurrentAggregationBuffer(
         aggregationBufferSets,
-        aggregrateIndex,
+        aggregateIndex,
         i);
       valueProcessor.processValue(myagg, inputColumn, i);
     }
@@ -319,7 +319,7 @@ private void iterateNoNullsWithAggregationSelection(
 
   private void iterateHasNullsSelectionWithAggregationSelection(
     VectorAggregationBufferRow[] aggregationBufferSets,
-    int aggregrateIndex,
+    int aggregateIndex,
     ColumnVector inputColumn,
     int batchSize,
     int[] selection) {
@@ -329,7 +329,7 @@ private void iterateHasNullsSelectionWithAggregationSelection(
       if (!inputColumn.isNull[row]) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         valueProcessor.processValue(myagg, inputColumn, i);
       }
@@ -338,7 +338,7 @@ private void iterateHasNullsSelectionWithAggregationSelection(
 
   private void iterateHasNullsWithAggregationSelection(
     VectorAggregationBufferRow[] aggregationBufferSets,
-    int aggregrateIndex,
+    int aggregateIndex,
     ColumnVector inputColumn,
     int batchSize) {
 
@@ -346,7 +346,7 @@ private void iterateHasNullsWithAggregationSelection(
       if (!inputColumn.isNull[i]) {
         Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
         valueProcessor.processValue(myagg, inputColumn, i);
       }
@@ -355,10 +355,10 @@ private void iterateHasNullsWithAggregationSelection(
 
   private Aggregation getCurrentAggregationBuffer(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       int row) {
     VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-    Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+    Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
     return myagg;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java
index 862b0525cb..65d7fec817 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFBloomFilterMerge.java
@@ -455,14 +455,14 @@ public void aggregateInputSelection(
 
   private void iterateNoNullsRepeatingWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       ColumnVector inputColumn,
       int batchSize) {
 
     for (int i=0; i < batchSize; ++i) {
       Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
       processValue(myagg, inputColumn, 0);
     }
@@ -470,7 +470,7 @@ private void iterateNoNullsRepeatingWithAggregationSelection(
 
   private void iterateNoNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       ColumnVector inputColumn,
       int[] selection,
       int batchSize) {
@@ -479,7 +479,7 @@ private void iterateNoNullsSelectionWithAggregationSelection(
       int row = selection[i];
       Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
       processValue(myagg, inputColumn, row);
     }
@@ -487,13 +487,13 @@ private void iterateNoNullsSelectionWithAggregationSelection(
 
   private void iterateNoNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       ColumnVector inputColumn,
       int batchSize) {
     for (int i=0; i < batchSize; ++i) {
       Aggregation myagg = getCurrentAggregationBuffer(
           aggregationBufferSets,
-          aggregrateIndex,
+          aggregateIndex,
           i);
       processValue(myagg, inputColumn, i);
     }
@@ -501,7 +501,7 @@ private void iterateNoNullsWithAggregationSelection(
 
   private void iterateHasNullsSelectionWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       ColumnVector inputColumn,
       int batchSize,
       int[] selection) {
@@ -511,7 +511,7 @@ private void iterateHasNullsSelectionWithAggregationSelection(
       if (!inputColumn.isNull[row]) {
         Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
         processValue(myagg, inputColumn, i);
       }
@@ -520,7 +520,7 @@ private void iterateHasNullsSelectionWithAggregationSelection(
 
   private void iterateHasNullsWithAggregationSelection(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       ColumnVector inputColumn,
       int batchSize) {
 
@@ -528,7 +528,7 @@ private void iterateHasNullsWithAggregationSelection(
       if (!inputColumn.isNull[i]) {
         Aggregation myagg = getCurrentAggregationBuffer(
             aggregationBufferSets,
-            aggregrateIndex,
+            aggregateIndex,
             i);
         processValue(myagg, inputColumn, i);
       }
@@ -537,10 +537,10 @@ private void iterateHasNullsWithAggregationSelection(
 
   private Aggregation getCurrentAggregationBuffer(
       VectorAggregationBufferRow[] aggregationBufferSets,
-      int aggregrateIndex,
+      int aggregateIndex,
       int row) {
     VectorAggregationBufferRow mySet = aggregationBufferSets[row];
-    Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+    Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregateIndex);
     return myagg;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedHashTable.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedHashTable.java
index c6a9323374..13d24f7ab1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedHashTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedHashTable.java
@@ -50,7 +50,7 @@ public abstract class VectorMapJoinOptimizedHashTable
 
   protected final MapJoinTableContainer originalTableContainer;
   protected final MapJoinTableContainerDirectAccess containerDirectAccess;
-  protected final ReusableGetAdaptorDirectAccess adapatorDirectAccess;
+  protected final ReusableGetAdaptorDirectAccess adapterDirectAccess;
 
   public static class SerializedBytes {
     byte[] bytes;
@@ -65,7 +65,7 @@ public VectorMapJoinNonMatchedIterator createNonMatchedIterator(MatchTracker mat
 
   @Override
   public int spillPartitionId() {
-    return adapatorDirectAccess.directSpillPartitionId();
+    return adapterDirectAccess.directSpillPartitionId();
   }
 
   @Override
@@ -94,10 +94,10 @@ public JoinUtil.JoinResult doLookup(byte[] keyBytes, int keyOffset, int keyLengt
     hashTableResult.forget();
 
     JoinUtil.JoinResult joinResult =
-            adapatorDirectAccess.setDirect(keyBytes, keyOffset, keyLength,
+            adapterDirectAccess.setDirect(keyBytes, keyOffset, keyLength,
                 bytesBytesMultiHashMapResult, matchTracker);
     if (joinResult == JoinUtil.JoinResult.SPILL) {
-      hashTableResult.setSpillPartitionId(adapatorDirectAccess.directSpillPartitionId());
+      hashTableResult.setSpillPartitionId(adapterDirectAccess.directSpillPartitionId());
     }
 
     hashTableResult.setJoinResult(joinResult);
@@ -110,7 +110,7 @@ public VectorMapJoinOptimizedHashTable(
 
     this.originalTableContainer = originalTableContainer;
     containerDirectAccess = (MapJoinTableContainerDirectAccess) originalTableContainer;
-    adapatorDirectAccess = (ReusableGetAdaptorDirectAccess) hashMapRowGetter;
+    adapterDirectAccess = (ReusableGetAdaptorDirectAccess) hashMapRowGetter;
   }
 
   @Override
@@ -128,6 +128,6 @@ public long getEstimatedMemorySize() {
 
   @Override
   public MatchTracker createMatchTracker() {
-    return adapatorDirectAccess.createMatchTracker();
+    return adapterDirectAccess.createMatchTracker();
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
index 0034328ed9..222e984d93 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java
@@ -577,7 +577,7 @@ else if (filename.startsWith(BUCKET_PREFIX)) {
   }
 
   /**
-   * If the direct insert is on for ACID tables, the files will contain an "_attempID" postfix.
+   * If the direct insert is on for ACID tables, the files will contain an "_attemptID" postfix.
    * In order to be able to read the files from the delete deltas, we need to know which
    * attemptId belongs to which delta. To make this lookup easy, this method created a map
    * to link the deltas to the attemptId.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/FlatFileInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/FlatFileInputFormat.java
index 7a49121b6c..746bd32b93 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/FlatFileInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/FlatFileInputFormat.java
@@ -106,7 +106,7 @@ public static class SerializationContextFromConf<S> implements
 
     /**
      * Implements configurable so it can use the configuration to find the right
-     * classes Note: ReflectionUtils will automatigically call setConf with the
+     * classes Note: ReflectionUtils will automagically call setConf with the
      * right configuration.
      */
     private Configuration conf;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java b/ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java
index 590319304e..4e71ee8471 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/lock/CompileLock.java
@@ -66,12 +66,12 @@ private boolean tryAcquire(long timeout, TimeUnit unit) {
     try {
       if (underlying.tryLock(0, unit)) {
         LOG.debug(LOCK_ACQUIRED_MSG);
-        return aquired();
+        return acquired();
       }
     } catch (InterruptedException e) {
       Thread.currentThread().interrupt();
       LOG.debug("Interrupted Exception ignored", e);
-      return failedToAquire();
+      return failedToAcquire();
     }
 
     // If the first shot fails, then we log the waiting messages.
@@ -83,26 +83,26 @@ private boolean tryAcquire(long timeout, TimeUnit unit) {
       try {
         if (!underlying.tryLock(timeout, unit)) {
           LOG.error(ErrorMsg.COMPILE_LOCK_TIMED_OUT.getErrorCodedMsg() + ": " + command);
-          return failedToAquire();
+          return failedToAcquire();
         }
       } catch (InterruptedException e) {
         Thread.currentThread().interrupt();
         LOG.debug("Interrupted Exception ignored", e);
-        return failedToAquire();
+        return failedToAcquire();
       }
     } else {
       underlying.lock();
     }
 
     LOG.debug(LOCK_ACQUIRED_MSG);
-    return aquired();
+    return acquired();
   }
 
-  private boolean aquired() {
+  private boolean acquired() {
     return locked(true);
   }
 
-  private boolean failedToAquire() {
+  private boolean failedToAcquire() {
     return locked(false);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
index eec9097039..bf8a6f8a5b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java
@@ -151,7 +151,7 @@ private List<String> getObjectNames(HiveLockObject key) {
    * @param  lockObjects  List of objects and the modes of the locks requested
    * @param  keepAlive    Whether the lock is to be persisted after the statement
    *
-   * Acuire all the locks. Release all the locks and return null if any lock
+   * Acquire all the locks. Release all the locks and return null if any lock
    * could not be acquired.
    **/
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
index 741830975c..30d93fd712 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java
@@ -352,7 +352,7 @@ private boolean getColumnInfo(DynamicListContext ctx, StringBuilder internalColN
     }
     internalColName.append(colExpr.getColumn());
 
-    // fetch table ablias
+    // fetch table alias
     ExprNodeDescUtils.ColumnOrigin columnOrigin =
             ExprNodeDescUtils.findColumnOrigin(exprNodeDesc, ctx.generator);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 7984afcfa2..e9da57df06 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -1959,7 +1959,7 @@ private static boolean isMergeRequiredForMr(HiveConf hconf,
       // If the user has HIVEMERGEMAPREDFILES set to false, the idea was the
       // number of reducers are few, so the number of files anyway are small.
       // However, with this optimization, we are increasing the number of files
-      // possibly by a big margin. So, merge aggresively.
+      // possibly by a big margin. So, merge aggressively.
       return (hconf.getBoolVar(ConfVars.HIVEMERGEMAPFILES) ||
           hconf.getBoolVar(ConfVars.HIVEMERGEMAPREDFILES));
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
index 23c351befa..acfaef7a35 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MapJoinProcessor.java
@@ -194,7 +194,7 @@ private static void genMapJoinLocalWork(MapredWork newWork, MapJoinOperator mapJ
       // set alias to fetch work
       newLocalWork.getAliasToFetchWork().put(alias, fetchWork);
     }
-    // remove small table ailias from aliasToWork;Avoid concurrent modification
+    // remove small table alias from aliasToWork;Avoid concurrent modification
     for (String alias : smallTableAliasList) {
       newWork.getMapWork().getAliasToWork().remove(alias);
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
index 4bcdcc325f..767f7e0c7e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java
@@ -30,7 +30,7 @@ public class CalciteSemanticException extends SemanticException {
   private static final long serialVersionUID = 1L;
 
   public enum UnsupportedFeature {
-    Distinct_without_an_aggreggation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type,
+    Distinct_without_an_aggregation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type,
     Having_clause_without_any_groupby, Invalid_column_reference, Invalid_decimal,
     Less_than_equal_greater_than, Others, Same_name_in_multiple_expressions,
     Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelColumnsAlignment.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelColumnsAlignment.java
index 93d58c26b6..8267e6ed90 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelColumnsAlignment.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRelColumnsAlignment.java
@@ -82,7 +82,7 @@ public HiveRelColumnsAlignment(RelBuilder relBuilder) {
 
   /**
    * Execute the logic in this class. In particular, make a top-down traversal of the tree
-   * and annotate and recreate appropiate operators.
+   * and annotate and recreate appropriate operators.
    */
   public RelNode align(RelNode root) {
     final RelNode newRoot = dispatchAlign(root, ImmutableList.<RelFieldCollation>of());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java
index 71eea91f81..364c0bcfc0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java
@@ -147,7 +147,7 @@ private HiveSubQueryRemoveRule(RelOptRuleOperand operand, String description, Hi
     }
   }
 
-  // given a subquery it checks to see what is the aggegate function
+  // given a subquery it checks to see what is the aggregate function
   /// if COUNT returns true since COUNT produces 0 on empty result set
   private boolean isAggZeroOnEmpty(RexSubQuery e) {
     //as this is corr scalar subquery with agg we expect one aggregate
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java
index 78ecd17e6e..d07bb632f9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTConverter.java
@@ -956,7 +956,7 @@ static class Schema extends ArrayList<ColumnInfo> {
      * Assumption:<br>
      * 1. Project will always be child of Sort.<br>
      * 2. In Calcite every projection in Project is uniquely named
-     * (unambigous) without using table qualifier (table name).<br>
+     * (unambiguous) without using table qualifier (table name).<br>
      *
      * @param order
      *          Hive Sort Node
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
index c681afb00d..9aa1d59415 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java
@@ -244,7 +244,7 @@ private RexNode convert(ExprNodeGenericFuncDesc func) throws SemanticException {
       if (calciteOp.getKind() == SqlKind.CASE) {
         // If it is a case operator, we need to rewrite it
         childRexNodeLst = rewriteCaseChildren(func.getFuncText(), childRexNodeLst, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         childRexNodeLst = adjustCaseBranchTypes(childRexNodeLst, retType, rexBuilder);
       } else if (HiveExtractDate.ALL_FUNCTIONS.contains(calciteOp)) {
         // If it is a extract operator, we need to rewrite it
@@ -273,7 +273,7 @@ private RexNode convert(ExprNodeGenericFuncDesc func) throws SemanticException {
         // This allows to be further reduced to OR, if possible
         calciteOp = SqlStdOperatorTable.CASE;
         childRexNodeLst = rewriteCoalesceChildren(childRexNodeLst, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         childRexNodeLst = adjustCaseBranchTypes(childRexNodeLst, retType, rexBuilder);
       } else if (calciteOp == HiveToDateSqlOperator.INSTANCE) {
         childRexNodeLst = rewriteToDateChildren(childRexNodeLst, rexBuilder);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkDeDuplication.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkDeDuplication.java
index ce44bd38ef..dc5c97d5a3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkDeDuplication.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/correlation/ReduceSinkDeDuplication.java
@@ -141,7 +141,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
     }
   }
 
-  public abstract static class AbsctractReducerReducerProc implements SemanticNodeProcessor {
+  public abstract static class AbstractReducerReducerProc implements SemanticNodeProcessor {
 
     @Override
     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
@@ -175,7 +175,7 @@ protected abstract Object process(ReduceSinkOperator cRS, GroupByOperator cGBY,
         ReduceSinkDeduplicateProcCtx dedupCtx) throws SemanticException;
   }
 
-  static class GroupbyReducerProc extends AbsctractReducerReducerProc {
+  static class GroupbyReducerProc extends AbstractReducerReducerProc {
 
     // given a group by operator this determines if that group by belongs to semi-join branch
     // note that this works only for second last group by in semi-join branch (X-GB-RS-GB-RS)
@@ -245,7 +245,7 @@ public Object process(ReduceSinkOperator cRS, GroupByOperator cGBY,
     }
   }
 
-  static class JoinReducerProc extends AbsctractReducerReducerProc {
+  static class JoinReducerProc extends AbstractReducerReducerProc {
 
     // pRS-pJOIN-cRS
     @Override
@@ -293,7 +293,7 @@ public Object process(ReduceSinkOperator cRS, GroupByOperator cGBY,
     }
   }
 
-  static class ReducerReducerProc extends AbsctractReducerReducerProc {
+  static class ReducerReducerProc extends AbstractReducerReducerProc {
 
     // pRS-cRS
     @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/BucketingSortingInferenceOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/BucketingSortingInferenceOptimizer.java
index 514efdb014..27c38cea81 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/BucketingSortingInferenceOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/BucketingSortingInferenceOptimizer.java
@@ -51,7 +51,7 @@
  *
  * BucketingSortingInferenceOptimizer.
  *
- * For each map reduce task, attmepts to infer bucketing and sorting metadata for the outputs.
+ * For each map reduce task, attempts to infer bucketing and sorting metadata for the outputs.
  *
  * Currently only map reduce tasks which produce final output have there output metadata inferred,
  * but it can be extended to intermediate tasks as well.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
index 5be1329546..9c9dac07a6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/GenMRSkewJoinProcessor.java
@@ -99,7 +99,7 @@ private GenMRSkewJoinProcessor() {
    * </ul>
    * For each table, we launch one mapjoin job, taking the directory containing
    * big keys in this table and corresponding dirs in other tables as input.
-   * (Actally one job for one row in the above.)
+   * (Actually one job for one row in the above.)
    *
    * <p>
    * For more discussions, please check
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
index f88c97adf5..f4705771c9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
@@ -672,7 +672,7 @@ Operator genOPTree(ASTNode ast, PlannerContext plannerCtx) throws SemanticExcept
           }
           this.ctx.setCboInfo(cboMsg);
 
-          // Determine if we should re-throw the exception OR if we try to mark plan as reAnayzeAST to retry
+          // Determine if we should re-throw the exception OR if we try to mark plan as reAnalyzeAST to retry
           // planning as non-CBO.
           if (fallbackStrategy.isFatal(e)) {
             if (e instanceof RuntimeException || e instanceof SemanticException) {
@@ -3643,7 +3643,7 @@ private RelNode genGBRelNode(List<RexNode> gbExprs, List<AggregateInfo> aggInfoL
       }
 
       if (gbChildProjLst.isEmpty()) {
-        // This will happen for count(*), in such cases we arbitarily pick
+        // This will happen for count(*), in such cases we arbitrarily pick
         // first element from srcRel
         gbChildProjLst.add(this.cluster.getRexBuilder().makeInputRef(srcRel, 0));
       }
@@ -4738,7 +4738,7 @@ && isRegex(
               (srcRel.getInputs().size() == 1 && srcRel.getInput(0) instanceof HiveAggregate))) {
             // Likely a malformed query eg, select hash(distinct c1) from t1;
             throw new CalciteSemanticException("Distinct without an aggregation.",
-                    UnsupportedFeature.Distinct_without_an_aggreggation);
+                    UnsupportedFeature.Distinct_without_an_aggregation);
           } else {
             // Case when this is an expression
             TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR, cluster.getRexBuilder());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
index ef4dbb46b4..18cdab5ab9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java
@@ -505,7 +505,7 @@ private static Task<?> loadTable(URI fromURI, Table table, boolean replace, Path
     }
 
     //if Importing into existing table, FileFormat is checked by
-    // ImportSemanticAnalzyer.checked checkTable()
+    // ImportSemanticAnalyzer.checked checkTable()
     Task<?> loadTableTask = TaskFactory.get(moveWork, x.getConf());
     copyTask.addDependentTask(loadTableTask);
     x.getTasks().add(copyTask);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java
index 54fc6a5fcf..4307dceed9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java
@@ -554,7 +554,7 @@ void subqueryRestrictionsCheck(RowResolver parentQueryRR,
           subQueryAST, "SubQuery can contain only 1 item in Select List."));
     }
 
-    boolean hasAggreateExprs = false;
+    boolean hasAggregateExprs = false;
     boolean hasWindowing = false;
 
     // we need to know if aggregate is COUNT since IN corr subq with count aggregate
@@ -566,7 +566,7 @@ void subqueryRestrictionsCheck(RowResolver parentQueryRR,
       int r = SubQueryUtils.checkAggOrWindowing(selectItem);
 
       hasWindowing = hasWindowing | ( r == 3);
-      hasAggreateExprs = hasAggreateExprs | ( r == 1 | r== 2 );
+      hasAggregateExprs = hasAggregateExprs | ( r == 1 | r== 2 );
       hasCount = hasCount | ( r == 2 );
     }
 
@@ -624,7 +624,7 @@ void subqueryRestrictionsCheck(RowResolver parentQueryRR,
       // * IN - always allowed, BUT returns true for cases with aggregate other than COUNT since later in subquery remove
       //        rule we need to know about this case.
       // * NOT IN - always allow, but always return true because later subq remove rule will generate diff plan for this case
-      if (hasAggreateExprs &&
+      if (hasAggregateExprs &&
               !hasExplicitGby) {
 
         if(operator.getType() == SubQueryType.SCALAR) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
index a4e2fe600f..c58a0a076f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/RowResolver.java
@@ -518,8 +518,8 @@ private boolean isAmbiguousReference(String tableAlias, String colAlias) {
         return true;
       }
     } else {
-      for (Map.Entry<String, Map<String, String>> ambigousColsEntry: ambiguousColumns.entrySet()) {
-        Map<String, String> cmap = ambigousColsEntry.getValue();
+      for (Map.Entry<String, Map<String, String>> ambiguousColsEntry: ambiguousColumns.entrySet()) {
+        Map<String, String> cmap = ambiguousColsEntry.getValue();
         for (Map.Entry<String, String> cmapEnt : cmap.entrySet()) {
           if (colAlias.equalsIgnoreCase(cmapEnt.getKey())) {
             return true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index b6a53c3472..346ce318a7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -15181,7 +15181,7 @@ private List<Table> getNonTransactionalTables() {
   /**
    * Check the query results cache to see if the query represented by the lookupInfo can be
    * answered using the results cache. If the cache contains a suitable entry, the semantic analyzer
-   * will be configured to use the found cache entry to anwer the query.
+   * will be configured to use the found cache entry to answer the query.
    */
   private boolean checkResultsCache(QueryResultsCache.LookupInfo lookupInfo, boolean needsReset) {
     if (lookupInfo == null) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/type/HiveFunctionHelper.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/type/HiveFunctionHelper.java
index 51acac269e..ddd28b3f1a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/type/HiveFunctionHelper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/type/HiveFunctionHelper.java
@@ -253,7 +253,7 @@ public RexNode getExpression(String functionText, FunctionInfo fi,
       if (calciteOp.getKind() == SqlKind.CASE) {
         // If it is a case operator, we need to rewrite it
         inputs = RexNodeConverter.rewriteCaseChildren(functionText, inputs, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         inputs = RexNodeConverter.adjustCaseBranchTypes(inputs, returnType, rexBuilder);
         checkForStatefulFunctions(inputs);
       } else if (HiveExtractDate.ALL_FUNCTIONS.contains(calciteOp)) {
@@ -292,7 +292,7 @@ public RexNode getExpression(String functionText, FunctionInfo fi,
         // This allows to be further reduced to OR, if possible
         calciteOp = SqlStdOperatorTable.CASE;
         inputs = RexNodeConverter.rewriteCoalesceChildren(inputs, rexBuilder);
-        // Adjust branch types by inserting explicit casts if the actual is ambigous
+        // Adjust branch types by inserting explicit casts if the actual is ambiguous
         inputs = RexNodeConverter.adjustCaseBranchTypes(inputs, returnType, rexBuilder);
         checkForStatefulFunctions(inputs);
       } else if (calciteOp == HiveToDateSqlOperator.INSTANCE) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
index c173ec392e..a6298e5e01 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/AbstractOperatorDesc.java
@@ -126,8 +126,8 @@ public long getMaxMemoryAvailable() {
   }
 
   @Override
-  public void setMaxMemoryAvailable(final long memoryAvailble) {
-    this.memAvailable = memoryAvailble;
+  public void setMaxMemoryAvailable(final long memoryAvailable) {
+    this.memAvailable = memoryAvailable;
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
index 59d0d96775..c50db7cfa1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/OperatorDesc.java
@@ -32,7 +32,7 @@ public interface OperatorDesc extends Serializable, Cloneable {
   public long getMemoryNeeded();
   public void setMemoryNeeded(long memoryNeeded);
   public long getMaxMemoryAvailable();
-  public void setMaxMemoryAvailable(long memoryAvailble);
+  public void setMaxMemoryAvailable(long memoryAvailable);
   public String getRuntimeStatsTmpDir();
   public void setRuntimeStatsTmpDir(String runtimeStatsTmpDir);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPTFInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPTFInfo.java
index 06cad2cb81..1e09af3b94 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPTFInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorPTFInfo.java
@@ -22,7 +22,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 
 /**
- * VectorGroupByAggregrationInfo.
+ * VectorGroupByAggregationInfo.
  *
  * A convenience data structure that has information needed to vectorize reduce sink.
  *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
index e580214daf..704492371a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/VectorReduceSinkInfo.java
@@ -23,12 +23,12 @@
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
- * VectorGroupByAggregrationInfo.
+ * VectorGroupByAggregationInfo.
  *
  * A convenience data structure that has information needed to vectorize reduce sink.
  *
  * It is created by the Vectorizer when it is determining whether it can specialize so the
- * information doesn't have to be recreated again and agains by the VectorReduceSinkOperator's
+ * information doesn't have to be recreated again and against by the VectorReduceSinkOperator's
  * constructors and later during execution.
  */
 public class VectorReduceSinkInfo {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveMetastoreAuthorizationProvider.java b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveMetastoreAuthorizationProvider.java
index de9b8d1fde..e94d8353c0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveMetastoreAuthorizationProvider.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/HiveMetastoreAuthorizationProvider.java
@@ -25,7 +25,7 @@
 import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePolicyProvider;
 
 /**
- * HiveMetastoreAuthorizationProvider : An extension of HiveAuthorizaytionProvider
+ * HiveMetastoreAuthorizationProvider : An extension of HiveAuthorizationProvider
  * that is intended to be called from the metastore-side. It will be invoked
  * by AuthorizationPreEventListener.
  *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
index defa9b41bb..6a3d719371 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java
@@ -166,7 +166,7 @@ public static StatsUpdater init(CompactionInfo ci, List<String> columnListForSta
     private StatsUpdater(CompactionInfo ci, List<String> columnListForStats,
         HiveConf conf, String userName, String compactionQueueName) {
       this.conf = new HiveConf(conf);
-      //so that Driver doesn't think it's arleady in a transaction
+      //so that Driver doesn't think it's already in a transaction
       this.conf.unset(ValidTxnList.VALID_TXNS_KEY);
       this.userName = userName;
       this.ci = ci;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAssertTrueOOM.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAssertTrueOOM.java
index c25a8adb92..5c72c9893f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAssertTrueOOM.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAssertTrueOOM.java
@@ -66,7 +66,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen
   public Object evaluate(DeferredObject[] arguments) throws HiveException {
     BooleanWritable condition = (BooleanWritable) conditionConverter.convert(arguments[0].get());
     if (condition == null || !condition.get()) {
-      throw new MapJoinMemoryExhaustionError("assert_true_oom: assertation failed; Simulated OOM");
+      throw new MapJoinMemoryExhaustionError("assert_true_oom: assertion failed; Simulated OOM");
     }
     return null;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
index 3b5fffc823..9262920e65 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumericHistogram.java
@@ -172,7 +172,7 @@ public void merge(List other, DoubleObjectInspector doi) {
    */
   public void add(double v) {
     // Binary search to find the closest bucket that v should go into.
-    // 'bin' should be interpreted as the bin to shift right in order to accomodate
+    // 'bin' should be interpreted as the bin to shift right in order to accommodate
     // v. As a result, bin is in the range [0,N], where N means that the value v is
     // greater than all the N bins currently in the histogram. It is also possible that
     // a bucket centered at 'v' already exists, so this must be checked in the next step.
diff --git a/ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java b/ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
index bbd5af07ab..f04642580c 100644
--- a/ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
+++ b/ql/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandler.java
@@ -1563,7 +1563,7 @@ public void run() {
   }
 
   /**
-   * This cannnot be run against Derby (thus in UT) but it can run againt MySQL.
+   * This cannnot be run against Derby (thus in UT) but it can run against MySQL.
    * 1. add to metastore/pom.xml
    *     <dependency>
    *      <groupId>mysql</groupId>
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastHashTable.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastHashTable.java
index d356588a9e..966a858d9a 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastHashTable.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/CheckFastHashTable.java
@@ -764,7 +764,7 @@ public long addRandomExisting(byte[] value, Random r) {
       Preconditions.checkState(count > 0);
       int index = r.nextInt(count);
 
-      // Exists aleady.
+      // Exists already.
 
       return array[index].getKey();
     }
diff --git a/ql/src/test/queries/clientnegative/dyn_part_empty.q.disabled b/ql/src/test/queries/clientnegative/dyn_part_empty.q.disabled
index 3a086d2898..324aa798b0 100644
--- a/ql/src/test/queries/clientnegative/dyn_part_empty.q.disabled
+++ b/ql/src/test/queries/clientnegative/dyn_part_empty.q.disabled
@@ -15,7 +15,7 @@
 -- limitations under the License.
 
 set hive.exec.dynamic.partition=true;
-set hive.stats.autogether=false;
+set hive.stats.autogather=false;
 set hive.error.on.empty.partition=true;
 
 create table dyn_err(key string, value string) partitioned by (ds string);
diff --git a/ql/src/test/results/clientpositive/llap/retry_failure_oom.q.out b/ql/src/test/results/clientpositive/llap/retry_failure_oom.q.out
index bab87ce5fd..8427f6ecaf 100644
--- a/ql/src/test/results/clientpositive/llap/retry_failure_oom.q.out
+++ b/ql/src/test/results/clientpositive/llap/retry_failure_oom.q.out
@@ -21,23 +21,23 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@tx
 #### A masked pattern was here ####
 Status: Failed
-Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
 ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_#ID# [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]
 DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
 ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_#ID# [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
 PREHOOK: query: select assert_true_oom(2 > a) from tx group by a
diff --git a/ql/src/test/results/clientpositive/llap/retry_failure_reorder.q.out b/ql/src/test/results/clientpositive/llap/retry_failure_reorder.q.out
index c9db27d15a..5622db6b71 100644
--- a/ql/src/test/results/clientpositive/llap/retry_failure_reorder.q.out
+++ b/ql/src/test/results/clientpositive/llap/retry_failure_reorder.q.out
@@ -324,23 +324,23 @@ PREHOOK: Input: default@tv
 PREHOOK: Input: default@tw
 #### A masked pattern was here ####
 Status: Failed
-Vertex failed, vertexName=Reducer 4, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Vertex failed, vertexName=Reducer 4, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
 ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_#ID# [Reducer 4] killed/failed due to:OWN_TASK_FAILURE]
 DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 4, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 4, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
 ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_#ID# [Reducer 4] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
 PREHOOK: query: select assert_true_oom(200000 > sum(u*v*w)) from tu
diff --git a/ql/src/test/results/clientpositive/llap/retry_failure_stat_changes.q.out b/ql/src/test/results/clientpositive/llap/retry_failure_stat_changes.q.out
index 20a23f42f9..59a1653da7 100644
--- a/ql/src/test/results/clientpositive/llap/retry_failure_stat_changes.q.out
+++ b/ql/src/test/results/clientpositive/llap/retry_failure_stat_changes.q.out
@@ -280,23 +280,23 @@ PREHOOK: Input: default@px
 PREHOOK: Input: default@tx_n2
 #### A masked pattern was here ####
 Status: Failed
-Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
 ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_#ID# [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]
 DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
-FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 2, vertexId=vertex_#ID#, diagnostics=[Task failed, taskId=task_#ID#, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_#ID#:java.lang.RuntimeException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
-Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertation failed; Simulated OOM
+Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionError: assert_true_oom: assertion failed; Simulated OOM
 #### A masked pattern was here ####
 ]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_#ID# [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
 PREHOOK: query: select assert_true_oom(2000 > sum(u*p)) from tx_n2 join px on (tx_n2.a=px.a) where u<10 and p>2
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/DecimalTypeInfo.java b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/DecimalTypeInfo.java
index 2e76df51e7..3df891b46b 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/DecimalTypeInfo.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/DecimalTypeInfo.java
@@ -107,7 +107,7 @@ public boolean accept(TypeInfo other) {
     }
 
     DecimalTypeInfo dti = (DecimalTypeInfo)other;
-    // Make sure "this" has enough integer room to accomodate other's integer digits.
+    // Make sure "this" has enough integer room to accommodate other's integer digits.
     return this.precision() - this.scale() >= dti.precision() - dti.scale();
   }
 
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
index 2e44074a87..68ba2c25fe 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/binarysortable/TestBinarySortableFast.java
@@ -416,7 +416,7 @@ private void testBinarySortableFastCase(
     Arrays.fill(columnNotNullMarker, BinarySortableSerDe.ONE);
 
     /*
-     * Acending.
+     * Ascending.
      */
     testBinarySortableFast(source, rows,
         columnSortOrderIsDesc, columnNullMarker, columnNotNullMarker,
diff --git a/serde/src/test/org/apache/hadoop/hive/serde2/io/TestTimestampWritableV2.java b/serde/src/test/org/apache/hadoop/hive/serde2/io/TestTimestampWritableV2.java
index 155dc1f58c..360aad0ea6 100644
--- a/serde/src/test/org/apache/hadoop/hive/serde2/io/TestTimestampWritableV2.java
+++ b/serde/src/test/org/apache/hadoop/hive/serde2/io/TestTimestampWritableV2.java
@@ -427,7 +427,7 @@ public void testMaxSize() {
     assertTrue((((double) MAX_ADDITIONAL_SECONDS_BITS + 1) * (1L << 31)) * 1000 >
       Long.MAX_VALUE);
 
-    // This is how many bytes we need to store those additonal bits as a VInt.
+    // This is how many bytes we need to store those additional bits as a VInt.
     assertEquals(4, WritableUtils.getVIntSize(MAX_ADDITIONAL_SECONDS_BITS));
 
     // Therefore, the maximum total size of a serialized timestamp is 4 + 5 + 4 = 13.
diff --git a/service/src/java/org/apache/hive/service/auth/saml/HiveSaml2Client.java b/service/src/java/org/apache/hive/service/auth/saml/HiveSaml2Client.java
index 83dd3ac19c..bff8ba9642 100644
--- a/service/src/java/org/apache/hive/service/auth/saml/HiveSaml2Client.java
+++ b/service/src/java/org/apache/hive/service/auth/saml/HiveSaml2Client.java
@@ -173,7 +173,7 @@ public void setRedirect(HttpServletRequest request, HttpServletResponse response
    * @param request
    * @param response
    * @return the NameId as received in the assertion if the assertion was valid.
-   * @throws HttpSamlAuthenticationException In case the assertition is not present or is
+   * @throws HttpSamlAuthenticationException In case the assertion is not present or is
    *                                         invalid.
    */
   public String validate(HttpServletRequest request, HttpServletResponse response)
diff --git a/service/src/java/org/apache/hive/service/cli/ColumnDescriptor.java b/service/src/java/org/apache/hive/service/cli/ColumnDescriptor.java
index b6e1a683ea..cba7deb7af 100644
--- a/service/src/java/org/apache/hive/service/cli/ColumnDescriptor.java
+++ b/service/src/java/org/apache/hive/service/cli/ColumnDescriptor.java
@@ -50,7 +50,7 @@ public ColumnDescriptor(TColumnDesc tColumnDesc) {
   public static ColumnDescriptor newPrimitiveColumnDescriptor(String name, String comment,
       Type type, int position) {
     // Current usage looks like it's only for metadata columns, but if that changes then
-    // this method may need to require a type qualifiers aruments.
+    // this method may need to require a type qualifiers arguments.
     return new ColumnDescriptor(name, comment, new TypeDescriptor(type), position);
   }
 
diff --git a/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java b/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
index 9a715c1f2d..e0c81fd74f 100644
--- a/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
+++ b/service/src/java/org/apache/hive/service/cli/session/HiveSessionHookContext.java
@@ -22,7 +22,7 @@
 /**
  * HiveSessionHookContext.
  * Interface passed to the HiveServer2 session hook execution. This enables
- * the hook implementation to accesss session config, user and session handle
+ * the hook implementation to access session config, user and session handle
  */
 public interface HiveSessionHookContext {
 
diff --git a/service/src/test/org/apache/hive/service/cli/thrift/ThriftHttpServletTest.java b/service/src/test/org/apache/hive/service/cli/thrift/ThriftHttpServletTest.java
index af3a337715..5695de2382 100644
--- a/service/src/test/org/apache/hive/service/cli/thrift/ThriftHttpServletTest.java
+++ b/service/src/test/org/apache/hive/service/cli/thrift/ThriftHttpServletTest.java
@@ -50,7 +50,7 @@ public void setUp() {
   }
 
   @Test
-  public void testMissingAuthorizatonHeader() throws Exception {
+  public void testMissingAuthorizationHeader() throws Exception {
     HttpServletRequest httpServletRequest = Mockito.mock(HttpServletRequest.class);
     Mockito.when(httpServletRequest.getHeader(HttpAuthUtils.AUTHORIZATION)).thenReturn(null);
 
@@ -61,7 +61,7 @@ public void testMissingAuthorizatonHeader() throws Exception {
   }
 
   @Test
-  public void testEmptyAuthorizatonHeader() throws Exception {
+  public void testEmptyAuthorizationHeader() throws Exception {
     HttpServletRequest httpServletRequest = Mockito.mock(HttpServletRequest.class);
     Mockito.when(httpServletRequest.getHeader(HttpAuthUtils.AUTHORIZATION)).thenReturn("");
 
diff --git a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
index 2c580a5ef4..1be0f89bfc 100644
--- a/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
+++ b/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java
@@ -3451,7 +3451,7 @@ LockResponse checkLock(long lockid)
 
   /**
    * Unlock a set of locks.  This can only be called when the locks are not
-   * assocaited with a transaction.
+   * associated with a transaction.
    * @param lockid lock id returned by
    * {@link #lock(org.apache.hadoop.hive.metastore.api.LockRequest)}
    * @throws NoSuchLockException if the requested lockid does not exist.
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
index 675474199a..c27e481301 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/RawStore.java
@@ -1560,7 +1560,7 @@ List<ColStatsObjWithSourceInfo> getPartitionColStatsForDatabase(String catName,
   /**
    * @param fileIds List of file IDs from the filesystem.
    * @param metadata Metadata buffers corresponding to fileIds in the list.
-   * @param type The type; determines the class that can do additiona processing for metadata.
+   * @param type The type; determines the class that can do additional processing for metadata.
    */
   void putFileMetadata(List<Long> fileIds, List<ByteBuffer> metadata,
       FileMetadataExprType type) throws MetaException;
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
index b77502a685..4a0589fa48 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java
@@ -983,7 +983,7 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str
         List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);
         List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);
         if ((partNames != null) && (partNames.size() > 0)) {
-          Deadline.startTimer("getAggregareStatsForAllPartitions");
+          Deadline.startTimer("getAggregateStatsForAllPartitions");
           AggrStats aggrStatsAllPartitions = rawStore.get_aggr_stats_for(catName, dbName, tblName, partNames, colNames, CacheUtils.HIVE_ENGINE);
           Deadline.stopTimer();
           // Remove default partition from partition names and get aggregate stats again
@@ -997,7 +997,7 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str
           }
           String defaultPartitionName = FileUtils.makePartName(partCols, partVals);
           partNames.remove(defaultPartitionName);
-          Deadline.startTimer("getAggregareStatsForAllPartitionsExceptDefault");
+          Deadline.startTimer("getAggregateStatsForAllPartitionsExceptDefault");
           AggrStats aggrStatsAllButDefaultPartition =
               rawStore.get_aggr_stats_for(catName, dbName, tblName, partNames, colNames, CacheUtils.HIVE_ENGINE);
           Deadline.stopTimer();
diff --git a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java
index 1961041b94..fb36d4c212 100644
--- a/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java
+++ b/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java
@@ -539,7 +539,7 @@ Set<CompactionInfo> findPotentialCompactions(int abortedThreshold, long abortedT
   MutexAPI getMutexAPI();
 
   /**
-   * This is primarily designed to provide coarse grained mutex support to operations running
+   * This is primarily designed to provide coarse-grained mutex support to operations running
    * inside the Metastore (of which there could be several instances).  The initial goal is to
    * ensure that various sub-processes of the Compactor don't step on each other.
    *
diff --git a/standalone-metastore/metastore-server/src/main/scripts/base b/standalone-metastore/metastore-server/src/main/scripts/base
index 0a49b4596e..be90afd6e6 100755
--- a/standalone-metastore/metastore-server/src/main/scripts/base
+++ b/standalone-metastore/metastore-server/src/main/scripts/base
@@ -97,7 +97,7 @@ for f in ${METASTORE_LIB}/*.jar; do
   CLASSPATH=${CLASSPATH}:$f;
 done
 
-# add the auxillary jars such as serdes
+# add the auxiliary jars such as serdes
 if [ -d "${METASTORE_AUX_JARS_PATH}" ]; then
   hive_aux_jars_abspath=`cd ${METASTORE_AUX_JARS_PATH} 2>&1 > /dev/null && pwd`
   for f in $hive_aux_jars_abspath/*.jar; do
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
index 6f02a3dfe0..ec98d2ab5b 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java
@@ -264,7 +264,7 @@ public void setValPreallocated(int elementNum, int length) {
    * @param leftLen length of left argument
    * @param rightSourceBuf container of right argument
    * @param rightStart start of right argument
-   * @param rightLen length of right arugment
+   * @param rightLen length of right argument
    */
   public void setConcat(int elementNum, byte[] leftSourceBuf, int leftStart, int leftLen,
       byte[] rightSourceBuf, int rightStart, int rightLen) {
diff --git a/storage-api/src/test/org/apache/hadoop/hive/common/TestValidCompactorWriteIdList.java b/storage-api/src/test/org/apache/hadoop/hive/common/TestValidCompactorWriteIdList.java
index f378b7b3be..659ea79894 100644
--- a/storage-api/src/test/org/apache/hadoop/hive/common/TestValidCompactorWriteIdList.java
+++ b/storage-api/src/test/org/apache/hadoop/hive/common/TestValidCompactorWriteIdList.java
@@ -78,7 +78,7 @@ public void exceptionsInMidst() {
     Assert.assertEquals(ValidWriteIdList.RangeResponse.NONE, rsp);
   }
   @Test
-  public void exceptionsAbveHighWaterMark() {
+  public void exceptionsAboveHighWaterMark() {
     BitSet bitSet = new BitSet(4);
     bitSet.set(0, 4);
     ValidWriteIdList writeIds = new ValidCompactorWriteIdList(tableName, new long[]{8, 11, 17, 29}, bitSet, 15);
diff --git a/udf/pom.xml b/udf/pom.xml
index cc9662951b..fc587daede 100644
--- a/udf/pom.xml
+++ b/udf/pom.xml
@@ -32,7 +32,7 @@
   </properties>
 
   <dependencies>
-    <!-- dependencies are always listed in sorted order by groupId, artifectId -->
+    <!-- dependencies are always listed in sorted order by groupId, artifactId -->
     <!-- intra-proect -->
     <dependency>
       <groupId>org.apache.hive</groupId>
@@ -60,7 +60,7 @@
     <resources>
     </resources>
     <plugins>
-      <!-- plugins are always listed in sorted order by groupId, artifectId -->
+      <!-- plugins are always listed in sorted order by groupId, artifactId -->
       <plugin>
         <groupId>org.apache.maven.plugins</groupId>
         <artifactId>maven-jar-plugin</artifactId>
