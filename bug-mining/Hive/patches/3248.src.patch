diff --git a/common/src/java/org/apache/hadoop/hive/common/DiskRange.java b/common/src/java/org/apache/hadoop/hive/common/DiskRange.java
new file mode 100644
index 0000000000..0d7f737d98
--- /dev/null
+++ b/common/src/java/org/apache/hadoop/hive/common/DiskRange.java
@@ -0,0 +1,99 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.common;
+
+import java.nio.ByteBuffer;
+
+/**
+ * The sections of a file.
+ */
+public class DiskRange {
+  /** The first address. */
+  protected long offset;
+  /** The address afterwards. */
+  protected long end;
+
+  public DiskRange(long offset, long end) {
+    this.offset = offset;
+    this.end = end;
+    if (end < offset) {
+      throw new IllegalArgumentException("invalid range " + this);
+    }
+  }
+
+  @Override
+  public boolean equals(Object other) {
+    if (other == null || other.getClass() != getClass()) {
+      return false;
+    }
+    DiskRange otherR = (DiskRange) other;
+    return otherR.offset == offset && otherR.end == end;
+  }
+
+  @Override
+  public int hashCode() {
+    return (int)(offset ^ (offset >>> 32)) * 31 + (int)(end ^ (end >>> 32));
+  }
+
+  @Override
+  public String toString() {
+    return "range start: " + offset + " end: " + end;
+  }
+
+  public long getOffset() {
+    return offset;
+  }
+
+  public long getEnd() {
+    return end;
+  }
+
+  public int getLength() {
+    long len = this.end - this.offset;
+    assert len <= Integer.MAX_VALUE;
+    return (int)len;
+  }
+
+  // For subclasses
+  public boolean hasData() {
+    return false;
+  }
+
+  public DiskRange sliceAndShift(long offset, long end, long shiftBy) {
+    // Rather, unexpected usage exception.
+    throw new UnsupportedOperationException();
+  }
+
+  public ByteBuffer getData() {
+    throw new UnsupportedOperationException();
+  }
+
+  protected boolean merge(long otherOffset, long otherEnd) {
+    if (!overlap(offset, end, otherOffset, otherEnd)) return false;
+    offset = Math.min(offset, otherOffset);
+    end = Math.max(end, otherEnd);
+    return true;
+  }
+
+  private static boolean overlap(long leftA, long rightA, long leftB, long rightB) {
+    if (leftA <= leftB) {
+      return rightA >= leftB;
+    }
+    return rightB >= leftA;
+  }
+}
\ No newline at end of file
diff --git a/common/src/java/org/apache/hadoop/hive/common/DiskRangeList.java b/common/src/java/org/apache/hadoop/hive/common/DiskRangeList.java
new file mode 100644
index 0000000000..a4c5a3c8b0
--- /dev/null
+++ b/common/src/java/org/apache/hadoop/hive/common/DiskRangeList.java
@@ -0,0 +1,205 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.common;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+
+import com.google.common.annotations.VisibleForTesting;
+
+/** Java linked list iterator interface is convoluted, and moreover concurrent modifications
+ * of the same list by multiple iterators are impossible. Hence, this.
+ * Java also doesn't support multiple inheritance, so this cannot be done as "aspect"... */
+public class DiskRangeList extends DiskRange {
+  private static final Log LOG = LogFactory.getLog(DiskRangeList.class);
+  public DiskRangeList prev, next;
+
+  public DiskRangeList(long offset, long end) {
+    super(offset, end);
+  }
+
+  /** Replaces this element with another in the list; returns the new element. */
+  public DiskRangeList replaceSelfWith(DiskRangeList other) {
+    other.prev = this.prev;
+    other.next = this.next;
+    if (this.prev != null) {
+      this.prev.next = other;
+    }
+    if (this.next != null) {
+      this.next.prev = other;
+    }
+    this.next = this.prev = null;
+    return other;
+  }
+
+  /**
+   * Inserts an intersecting range before current in the list and adjusts offset accordingly.
+   * @returns the new element.
+   */
+  public DiskRangeList insertPartBefore(DiskRangeList other) {
+    assert other.end >= this.offset;
+    this.offset = other.end;
+    other.prev = this.prev;
+    other.next = this;
+    if (this.prev != null) {
+      this.prev.next = other;
+    }
+    this.prev = other;
+    return other;
+  }
+
+  /**
+   * Inserts an element after current in the list.
+   * @returns the new element.
+   * */
+  public DiskRangeList insertAfter(DiskRangeList other) {
+    other.next = this.next;
+    other.prev = this;
+    if (this.next != null) {
+      this.next.prev = other;
+    }
+    this.next = other;
+    return other;
+  }
+
+  /**
+   * Inserts an intersecting range after current in the list and adjusts offset accordingly.
+   * @returns the new element.
+   */
+  public DiskRangeList insertPartAfter(DiskRangeList other) {
+    assert other.offset <= this.end;
+    this.end = other.offset;
+    return insertAfter(other);
+  }
+
+  /** Removes an element after current from the list. */
+  public void removeAfter() {
+    DiskRangeList other = this.next;
+    this.next = other.next;
+    if (this.next != null) {
+      this.next.prev = this;
+    }
+    other.next = other.prev = null;
+  }
+
+  /** Removes the current element from the list. */
+  public void removeSelf() {
+    if (this.prev != null) {
+      this.prev.next = this.next;
+    }
+    if (this.next != null) {
+      this.next.prev = this.prev;
+    }
+    this.next = this.prev = null;
+  }
+
+  /** Splits current element in the list, using DiskRange::slice */
+  public DiskRangeList split(long cOffset) {
+    insertAfter((DiskRangeList)this.sliceAndShift(cOffset, end, 0));
+    return replaceSelfWith((DiskRangeList)this.sliceAndShift(offset, cOffset, 0));
+  }
+
+  public boolean hasContiguousNext() {
+    return next != null && end == next.offset;
+  }
+
+  @VisibleForTesting
+  public int listSize() {
+    int result = 1;
+    DiskRangeList current = this.next;
+    while (current != null) {
+      ++result;
+      current = current.next;
+    }
+    return result;
+  }
+
+  @VisibleForTesting
+  public DiskRangeList[] listToArray() {
+    DiskRangeList[] result = new DiskRangeList[listSize()];
+    int i = 0;
+    DiskRangeList current = this.next;
+    while (current != null) {
+      result[i] = current;
+      ++i;
+      current = current.next;
+    }
+    return result;
+  }
+
+  public static class DiskRangeListCreateHelper {
+    private DiskRangeList tail = null, head;
+    public DiskRangeListCreateHelper() {
+    }
+
+    public DiskRangeList getTail() {
+      return tail;
+    }
+
+    public void addOrMerge(long offset, long end, boolean doMerge, boolean doLogNew) {
+      if (doMerge && tail != null && tail.merge(offset, end)) return;
+      if (doLogNew) {
+        LOG.info("Creating new range; last range (which can include some previous adds) was "
+            + tail);
+      }
+      DiskRangeList node = new DiskRangeList(offset, end);
+      if (tail == null) {
+        head = tail = node;
+      } else {
+        tail = tail.insertAfter(node);
+      }
+    }
+
+
+    public DiskRangeList get() {
+      return head;
+    }
+
+    public DiskRangeList extract() {
+      DiskRangeList result = head;
+      head = null;
+      return result;
+    }
+  }
+
+  /**
+   * List in-place mutation helper - a bogus first element that is inserted before list head,
+   * and thus remains constant even if head is replaced with some new range via in-place list
+   * mutation. extract() can be used to obtain the modified list.
+   */
+  public static class DiskRangeListMutateHelper extends DiskRangeList {
+    public DiskRangeListMutateHelper(DiskRangeList head) {
+      super(-1, -1);
+      assert head != null;
+      assert head.prev == null;
+      this.next = head;
+      head.prev = this;
+    }
+
+    public DiskRangeList get() {
+      return next;
+    }
+
+    public DiskRangeList extract() {
+      DiskRangeList result = this.next;
+      assert result != null;
+      this.next = result.prev = null;
+      return result;
+    }
+  }
+}
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/CompressionCodec.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/CompressionCodec.java
index 5e2d88083d..ed9d7ac549 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/CompressionCodec.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/CompressionCodec.java
@@ -23,7 +23,7 @@
 
 import javax.annotation.Nullable;
 
-interface CompressionCodec {
+public interface CompressionCodec {
 
   public enum Modifier {
     /* speed/compression tradeoffs */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java
index 9788c16851..f02882f2da 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java
@@ -157,7 +157,7 @@ private static void printMetaData(List<String> files, Configuration conf,
           for (int colIdx : rowIndexCols) {
             sargColumns[colIdx] = true;
           }
-          RecordReaderImpl.Index indices = rows.readRowIndex(stripeIx, sargColumns);
+          RecordReaderImpl.Index indices = rows.readRowIndex(stripeIx, null, sargColumns);
           for (int col : rowIndexCols) {
             StringBuilder buf = new StringBuilder();
             String rowIdxString = getFormattedRowIndices(col, indices.getRowGroupIndex());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java
index 62c6f8d88f..946b1a7c95 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java
@@ -20,28 +20,50 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+import java.util.ListIterator;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.common.DiskRange;
+import org.apache.hadoop.hive.common.DiskRangeList;
+import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BufferChunk;
+import org.apache.hadoop.hive.shims.HadoopShims.ZeroCopyReaderShim;
 
-abstract class InStream extends InputStream {
+import com.google.common.annotations.VisibleForTesting;
+
+public abstract class InStream extends InputStream {
 
   private static final Log LOG = LogFactory.getLog(InStream.class);
 
+  protected final String name;
+  protected final long length;
+
+  public InStream(String name, long length) {
+    this.name = name;
+    this.length = length;
+  }
+
+  public String getStreamName() {
+    return name;
+  }
+
+  public long getStreamLength() {
+    return length;
+  }
+
   private static class UncompressedStream extends InStream {
-    private final String name;
-    private final ByteBuffer[] bytes;
-    private final long[] offsets;
+    private final List<DiskRange> bytes;
     private final long length;
     private long currentOffset;
     private ByteBuffer range;
     private int currentRange;
 
-    public UncompressedStream(String name, ByteBuffer[] input, long[] offsets,
-                              long length) {
-      this.name = name;
+    public UncompressedStream(String name, List<DiskRange> input, long length) {
+      super(name, length);
       this.bytes = input;
-      this.offsets = offsets;
       this.length = length;
       currentRange = 0;
       currentOffset = 0;
@@ -83,12 +105,10 @@ public int available() {
 
     @Override
     public void close() {
-      currentRange = bytes.length;
+      currentRange = bytes.size();
       currentOffset = length;
       // explicit de-ref of bytes[]
-      for(int i = 0; i < bytes.length; i++) {
-        bytes[i] = null;
-      }
+      bytes.clear();
     }
 
     @Override
@@ -97,23 +117,27 @@ public void seek(PositionProvider index) throws IOException {
     }
 
     public void seek(long desired) {
-      for(int i = 0; i < bytes.length; ++i) {
-        if (desired == 0 && bytes[i].remaining() == 0) {
-          if (LOG.isWarnEnabled()) {
-            LOG.warn("Attempting seek into empty stream (" + name + ") Skipping stream.");
-          }
+      if (desired == 0 && bytes.isEmpty()) {
+        logEmptySeek(name);
+        return;
+      }
+      int i = 0;
+      for(DiskRange curRange : bytes) {
+        if (desired == 0 && curRange.getData().remaining() == 0) {
+          logEmptySeek(name);
           return;
         }
-        if (offsets[i] <= desired &&
-            desired - offsets[i] < bytes[i].remaining()) {
+        if (curRange.getOffset() <= desired &&
+            (desired - curRange.getOffset()) < curRange.getLength()) {
           currentOffset = desired;
           currentRange = i;
-          this.range = bytes[i].duplicate();
+          this.range = curRange.getData().duplicate();
           int pos = range.position();
-          pos += (int)(desired - offsets[i]); // this is why we duplicate
+          pos += (int)(desired - curRange.getOffset()); // this is why we duplicate
           this.range.position(pos);
           return;
         }
+        ++i;
       }
       throw new IllegalArgumentException("Seek in " + name + " to " +
         desired + " is outside of the data");
@@ -127,50 +151,40 @@ public String toString() {
     }
   }
 
+  private static ByteBuffer allocateBuffer(int size, boolean isDirect) {
+    // TODO: use the same pool as the ORC readers
+    if (isDirect) {
+      return ByteBuffer.allocateDirect(size);
+    } else {
+      return ByteBuffer.allocate(size);
+    }
+  }
+
   private static class CompressedStream extends InStream {
-    private final String name;
-    private final ByteBuffer[] bytes;
-    private final long[] offsets;
+    private final List<DiskRange> bytes;
     private final int bufferSize;
-    private final long length;
     private ByteBuffer uncompressed;
     private final CompressionCodec codec;
     private ByteBuffer compressed;
     private long currentOffset;
     private int currentRange;
     private boolean isUncompressedOriginal;
-    private boolean isDirect = false;
 
-    public CompressedStream(String name, ByteBuffer[] input,
-                            long[] offsets, long length,
-                            CompressionCodec codec, int bufferSize
-                           ) {
+    public CompressedStream(String name, List<DiskRange> input, long length,
+                            CompressionCodec codec, int bufferSize) {
+      super(name, length);
       this.bytes = input;
-      this.name = name;
       this.codec = codec;
-      this.length = length;
-      if(this.length > 0) {
-        isDirect = this.bytes[0].isDirect();
-      }
-      this.offsets = offsets;
       this.bufferSize = bufferSize;
       currentOffset = 0;
       currentRange = 0;
     }
 
-    private ByteBuffer allocateBuffer(int size) {
-      // TODO: use the same pool as the ORC readers
-      if(isDirect == true) {
-        return ByteBuffer.allocateDirect(size);
-      } else {
-        return ByteBuffer.allocate(size);
-      }
-    }
-
     private void readHeader() throws IOException {
       if (compressed == null || compressed.remaining() <= 0) {
         seek(currentOffset);
       }
+      long originalOffset = currentOffset;
       if (compressed.remaining() > OutStream.HEADER_SIZE) {
         int b0 = compressed.get() & 0xff;
         int b1 = compressed.get() & 0xff;
@@ -193,10 +207,10 @@ private void readHeader() throws IOException {
           isUncompressedOriginal = true;
         } else {
           if (isUncompressedOriginal) {
-            uncompressed = allocateBuffer(bufferSize);
+            uncompressed = allocateBuffer(bufferSize, slice.isDirect());
             isUncompressedOriginal = false;
           } else if (uncompressed == null) {
-            uncompressed = allocateBuffer(bufferSize);
+            uncompressed = allocateBuffer(bufferSize, slice.isDirect());
           } else {
             uncompressed.clear();
           }
@@ -246,11 +260,9 @@ public int available() throws IOException {
     public void close() {
       uncompressed = null;
       compressed = null;
-      currentRange = bytes.length;
+      currentRange = bytes.size();
       currentOffset = length;
-      for(int i = 0; i < bytes.length; i++) {
-        bytes[i] = null;
-      }
+      bytes.clear();
     }
 
     @Override
@@ -267,7 +279,7 @@ public void seek(PositionProvider index) throws IOException {
       }
     }
 
-    /* slices a read only contigous buffer of chunkLength */
+    /* slices a read only contiguous buffer of chunkLength */
     private ByteBuffer slice(int chunkLength) throws IOException {
       int len = chunkLength;
       final long oldOffset = currentOffset;
@@ -279,7 +291,7 @@ private ByteBuffer slice(int chunkLength) throws IOException {
         currentOffset += len;
         compressed.position(compressed.position() + len);
         return slice;
-      } else if (currentRange >= (bytes.length - 1)) {
+      } else if (currentRange >= (bytes.size() - 1)) {
         // nothing has been modified yet
         throw new IOException("EOF in " + this + " while trying to read " +
             chunkLength + " bytes");
@@ -293,16 +305,19 @@ private ByteBuffer slice(int chunkLength) throws IOException {
 
       // we need to consolidate 2 or more buffers into 1
       // first copy out compressed buffers
-      ByteBuffer copy = allocateBuffer(chunkLength);
+      ByteBuffer copy = allocateBuffer(chunkLength, compressed.isDirect());
       currentOffset += compressed.remaining();
       len -= compressed.remaining();
       copy.put(compressed);
+      ListIterator<DiskRange> iter = bytes.listIterator(currentRange);
 
-      while (len > 0 && (++currentRange) < bytes.length) {
+      while (len > 0 && iter.hasNext()) {
+        ++currentRange;
         if (LOG.isDebugEnabled()) {
           LOG.debug(String.format("Read slow-path, >1 cross block reads with %s", this.toString()));
         }
-        compressed = bytes[currentRange].duplicate();
+        DiskRange range = iter.next();
+        compressed = range.getData().duplicate();
         if (compressed.remaining() >= len) {
           slice = compressed.slice();
           slice.limit(len);
@@ -323,40 +338,46 @@ private ByteBuffer slice(int chunkLength) throws IOException {
     }
 
     private void seek(long desired) throws IOException {
-      for(int i = 0; i < bytes.length; ++i) {
-        if (offsets[i] <= desired &&
-            desired - offsets[i] < bytes[i].remaining()) {
+      if (desired == 0 && bytes.isEmpty()) {
+        logEmptySeek(name);
+        return;
+      }
+      int i = 0;
+      for (DiskRange range : bytes) {
+        if (range.getOffset() <= desired && desired < range.getEnd()) {
           currentRange = i;
-          compressed = bytes[i].duplicate();
+          compressed = range.getData().duplicate();
           int pos = compressed.position();
-          pos += (int)(desired - offsets[i]);
+          pos += (int)(desired - range.getOffset());
           compressed.position(pos);
           currentOffset = desired;
           return;
         }
+        ++i;
       }
       // if they are seeking to the precise end, go ahead and let them go there
-      int segments = bytes.length;
-      if (segments != 0 &&
-          desired == offsets[segments - 1] + bytes[segments - 1].remaining()) {
+      int segments = bytes.size();
+      if (segments != 0 && desired == bytes.get(segments - 1).getEnd()) {
+        DiskRange range = bytes.get(segments - 1);
         currentRange = segments - 1;
-        compressed = bytes[currentRange].duplicate();
+        compressed = range.getData().duplicate();
         compressed.position(compressed.limit());
         currentOffset = desired;
         return;
       }
-      throw new IOException("Seek outside of data in " + this + " to " +
-        desired);
+      throw new IOException("Seek outside of data in " + this + " to " + desired);
     }
 
     private String rangeString() {
       StringBuilder builder = new StringBuilder();
-      for(int i=0; i < offsets.length; ++i) {
+      int i = 0;
+      for (DiskRange range : bytes) {
         if (i != 0) {
           builder.append("; ");
         }
-        builder.append(" range " + i + " = " + offsets[i] + " to " +
-            bytes[i].remaining());
+        builder.append(" range " + i + " = " + range.getOffset()
+            + " to " + (range.getEnd() - range.getOffset()));
+        ++i;
       }
       return builder.toString();
     }
@@ -375,10 +396,16 @@ public String toString() {
 
   public abstract void seek(PositionProvider index) throws IOException;
 
+  private static void logEmptySeek(String name) {
+    if (LOG.isWarnEnabled()) {
+      LOG.warn("Attempting seek into empty stream (" + name + ") Skipping stream.");
+    }
+  }
+
   /**
    * Create an input stream from a list of buffers.
-   * @param name the name of the stream
-   * @param input the list of ranges of bytes for the stream
+   * @param streamName the name of the stream
+   * @param buffers the list of ranges of bytes for the stream
    * @param offsets a list of offsets (the same length as input) that must
    *                contain the first offset of the each set of bytes in input
    * @param length the length in bytes of the stream
@@ -387,17 +414,40 @@ public String toString() {
    * @return an input stream
    * @throws IOException
    */
-  public static InStream create(String name,
-                                ByteBuffer[] input,
+  @VisibleForTesting
+  @Deprecated
+  public static InStream create(String streamName,
+                                ByteBuffer[] buffers,
                                 long[] offsets,
                                 long length,
                                 CompressionCodec codec,
                                 int bufferSize) throws IOException {
+    List<DiskRange> input = new ArrayList<DiskRange>(buffers.length);
+    for (int i = 0; i < buffers.length; ++i) {
+      input.add(new BufferChunk(buffers[i], offsets[i]));
+    }
+    return create(streamName, input, length, codec, bufferSize);
+  }
+
+  /**
+   * Create an input stream from a list of disk ranges with data.
+   * @param name the name of the stream
+   * @param input the list of ranges of bytes for the stream; from disk or cache
+   * @param length the length in bytes of the stream
+   * @param codec the compression codec
+   * @param bufferSize the compression buffer size
+   * @return an input stream
+   * @throws IOException
+   */
+  public static InStream create(String name,
+                                List<DiskRange> input,
+                                long length,
+                                CompressionCodec codec,
+                                int bufferSize) throws IOException {
     if (codec == null) {
-      return new UncompressedStream(name, input, offsets, length);
+      return new UncompressedStream(name, input, length);
     } else {
-      return new CompressedStream(name, input, offsets, length, codec,
-          bufferSize);
+      return new CompressedStream(name, input, length, codec, bufferSize);
     }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java
new file mode 100644
index 0000000000..b76d26e489
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/MetadataReader.java
@@ -0,0 +1,122 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.orc;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.List;
+
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.DiskRange;
+import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BufferChunk;
+
+import com.google.common.collect.Lists;
+
+public class MetadataReader {
+  private final FSDataInputStream file;
+  private final CompressionCodec codec;
+  private final int bufferSize;
+  private final int typeCount;
+
+  public MetadataReader(FileSystem fileSystem, Path path,
+      CompressionCodec codec, int bufferSize, int typeCount) throws IOException {
+    this(fileSystem.open(path), codec, bufferSize, typeCount);
+  }
+
+  public MetadataReader(FSDataInputStream file,
+      CompressionCodec codec, int bufferSize, int typeCount) {
+    this.file = file;
+    this.codec = codec;
+    this.bufferSize = bufferSize;
+    this.typeCount = typeCount;
+  }
+
+  public RecordReaderImpl.Index readRowIndex(StripeInformation stripe, OrcProto.StripeFooter footer,
+      boolean[] included, OrcProto.RowIndex[] indexes, boolean[] sargColumns,
+      OrcProto.BloomFilterIndex[] bloomFilterIndices) throws IOException {
+    if (footer == null) {
+      footer = readStripeFooter(stripe);
+    }
+    if (indexes == null) {
+      indexes = new OrcProto.RowIndex[typeCount];
+    }
+    if (bloomFilterIndices == null) {
+      bloomFilterIndices = new OrcProto.BloomFilterIndex[typeCount];
+    }
+    long offset = stripe.getOffset();
+    List<OrcProto.Stream> streams = footer.getStreamsList();
+    for (int i = 0; i < streams.size(); i++) {
+      OrcProto.Stream stream = streams.get(i);
+      OrcProto.Stream nextStream = null;
+      if (i < streams.size() - 1) {
+        nextStream = streams.get(i+1);
+      }
+      int col = stream.getColumn();
+      int len = (int) stream.getLength();
+      // row index stream and bloom filter are interlaced, check if the sarg column contains bloom
+      // filter and combine the io to read row index and bloom filters for that column together
+      if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.ROW_INDEX)) {
+        boolean readBloomFilter = false;
+        // TODO#: HERE
+        if (sargColumns != null && sargColumns[col] &&
+            nextStream.getKind() == OrcProto.Stream.Kind.BLOOM_FILTER) {
+          len += nextStream.getLength();
+          i += 1;
+          readBloomFilter = true;
+        }
+        if ((included == null || included[col]) && indexes[col] == null) {
+          byte[] buffer = new byte[len];
+          file.seek(offset);
+          file.readFully(buffer);
+          ByteBuffer[] bb = new ByteBuffer[] {ByteBuffer.wrap(buffer)};
+          indexes[col] = OrcProto.RowIndex.parseFrom(InStream.create("index",
+              bb, new long[]{0}, stream.getLength(), codec, bufferSize));
+          if (readBloomFilter) {
+            bb[0].position((int) stream.getLength());
+            bloomFilterIndices[col] = OrcProto.BloomFilterIndex.parseFrom(
+                InStream.create("bloom_filter", bb, new long[]{0}, nextStream.getLength(),
+                    codec, bufferSize));
+          }
+        }
+      }
+      offset += len;
+    }
+
+    RecordReaderImpl.Index index = new RecordReaderImpl.Index(indexes, bloomFilterIndices);
+    return index;
+  }
+
+  public OrcProto.StripeFooter readStripeFooter(StripeInformation stripe) throws IOException {
+    long offset = stripe.getOffset() + stripe.getIndexLength() + stripe.getDataLength();
+    int tailLength = (int) stripe.getFooterLength();
+
+    // read the footer
+    ByteBuffer tailBuf = ByteBuffer.allocate(tailLength);
+    file.seek(offset);
+    file.readFully(tailBuf.array(), tailBuf.arrayOffset(), tailLength);
+    return OrcProto.StripeFooter.parseFrom(InStream.create("footer",
+        Lists.<DiskRange>newArrayList(new BufferChunk(tailBuf, 0)),
+        tailLength, codec, bufferSize));
+  }
+
+  public void close() throws IOException {
+    file.close();
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
index 25bb15a1ee..49a8e8028b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
@@ -193,7 +193,7 @@ public static class ReaderOptions {
     private ReaderImpl.FileMetaInfo fileMetaInfo;
     private long maxLength = Long.MAX_VALUE;
 
-    ReaderOptions(Configuration conf) {
+    public ReaderOptions(Configuration conf) {
       this.conf = conf;
     }
     ReaderOptions fileMetaInfo(ReaderImpl.FileMetaInfo info) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index 498ee14459..c38867db20 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -44,6 +44,9 @@
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedInputFormatInterface;
 import org.apache.hadoop.hive.ql.io.AcidInputFormat;
 import org.apache.hadoop.hive.ql.io.AcidOutputFormat;
@@ -102,8 +105,7 @@
  */
 public class OrcInputFormat  implements InputFormat<NullWritable, OrcStruct>,
   InputFormatChecker, VectorizedInputFormatInterface,
-    AcidInputFormat<NullWritable, OrcStruct>, 
-    CombineHiveInputFormat.AvoidSplitCombination {
+    AcidInputFormat<NullWritable, OrcStruct>, CombineHiveInputFormat.AvoidSplitCombination {
 
   private static final Log LOG = LogFactory.getLog(OrcInputFormat.class);
   static final HadoopShims SHIMS = ShimLoader.getHadoopShims();
@@ -111,7 +113,6 @@ public class OrcInputFormat  implements InputFormat<NullWritable, OrcStruct>,
       SHIMS.getHadoopConfNames().get("MAPREDMINSPLITSIZE");
   static final String MAX_SPLIT_SIZE =
       SHIMS.getHadoopConfNames().get("MAPREDMAXSPLITSIZE");
-  static final String SARG_PUSHDOWN = "sarg.pushdown";
 
   private static final long DEFAULT_MIN_SPLIT_SIZE = 16 * 1024 * 1024;
   private static final long DEFAULT_MAX_SPLIT_SIZE = 256 * 1024 * 1024;
@@ -217,14 +218,17 @@ public static RecordReader createReaderFromFile(Reader file,
                                                   long offset, long length
                                                   ) throws IOException {
     Reader.Options options = new Reader.Options().range(offset, length);
-    boolean isOriginal =
-        !file.hasMetadataValue(OrcRecordUpdater.ACID_KEY_INDEX_NAME);
+    boolean isOriginal = isOriginal(file);
     List<OrcProto.Type> types = file.getTypes();
-    setIncludedColumns(options, types, conf, isOriginal);
+    options.include(genIncludedColumns(types, conf, isOriginal));
     setSearchArgument(options, types, conf, isOriginal);
     return file.rowsOptions(options);
   }
 
+  public static boolean isOriginal(Reader file) {
+    return !file.hasMetadataValue(OrcRecordUpdater.ACID_KEY_INDEX_NAME);
+  }
+
   /**
    * Recurse down into a type subtree turning on all of the sub-columns.
    * @param types the types of the file
@@ -244,6 +248,21 @@ private static void includeColumnRecursive(List<OrcProto.Type> types,
     }
   }
 
+  public static boolean[] genIncludedColumns(
+      List<OrcProto.Type> types, List<Integer> included, boolean isOriginal) {
+    int rootColumn = getRootColumn(isOriginal);
+    int numColumns = types.size() - rootColumn;
+    boolean[] result = new boolean[numColumns];
+    result[0] = true;
+    OrcProto.Type root = types.get(rootColumn);
+    for(int i=0; i < root.getSubtypesCount(); ++i) {
+      if (included.contains(i)) {
+        includeColumnRecursive(types, result, root.getSubtypes(i),
+            rootColumn);
+      }
+    }
+    return result;
+  }
   /**
    * Take the configuration and figure out which columns we need to include.
    * @param options the options to update
@@ -251,64 +270,51 @@ private static void includeColumnRecursive(List<OrcProto.Type> types,
    * @param conf the configuration
    * @param isOriginal is the file in the original format?
    */
-  static void setIncludedColumns(Reader.Options options,
-                                 List<OrcProto.Type> types,
-                                 Configuration conf,
-                                 boolean isOriginal) {
-    int rootColumn = getRootColumn(isOriginal);
-    if (!ColumnProjectionUtils.isReadAllColumns(conf)) {
-      int numColumns = types.size() - rootColumn;
-      boolean[] result = new boolean[numColumns];
-      result[0] = true;
-      OrcProto.Type root = types.get(rootColumn);
+  public static boolean[] genIncludedColumns(
+      List<OrcProto.Type> types, Configuration conf, boolean isOriginal) {
+     if (!ColumnProjectionUtils.isReadAllColumns(conf)) {
       List<Integer> included = ColumnProjectionUtils.getReadColumnIDs(conf);
-      for(int i=0; i < root.getSubtypesCount(); ++i) {
-        if (included.contains(i)) {
-          includeColumnRecursive(types, result, root.getSubtypes(i),
-              rootColumn);
-        }
-      }
-      options.include(result);
+      return genIncludedColumns(types, included, isOriginal);
     } else {
-      options.include(null);
+      return null;
     }
   }
 
+  public static String[] getSargColumnNames(String[] originalColumnNames,
+      List<OrcProto.Type> types, boolean[] includedColumns, boolean isOriginal) {
+    int rootColumn = getRootColumn(isOriginal);
+    String[] columnNames = new String[types.size() - rootColumn];
+    int i = 0;
+    for(int columnId: types.get(rootColumn).getSubtypesList()) {
+      if (includedColumns == null || includedColumns[columnId - rootColumn]) {
+        // this is guaranteed to be positive because types only have children
+        // ids greater than their own id.
+        columnNames[columnId - rootColumn] = originalColumnNames[i++];
+      }
+    }
+    return columnNames;
+  }
+
   static void setSearchArgument(Reader.Options options,
                                 List<OrcProto.Type> types,
                                 Configuration conf,
                                 boolean isOriginal) {
-    int rootColumn = getRootColumn(isOriginal);
-    String serializedPushdown = conf.get(TableScanDesc.FILTER_EXPR_CONF_STR);
-    String sargPushdown = conf.get(SARG_PUSHDOWN);
-    String columnNamesString =
-        conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR);
-    if ((sargPushdown == null && serializedPushdown == null)
-        || columnNamesString == null) {
+    String columnNamesString = conf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR);
+    if (columnNamesString == null) {
+      LOG.debug("No ORC pushdown predicate - no column names");
+      options.searchArgument(null, null);
+      return;
+    }
+    SearchArgument sarg = SearchArgumentFactory.createFromConf(conf);
+    if (sarg == null) {
       LOG.debug("No ORC pushdown predicate");
       options.searchArgument(null, null);
-    } else {
-      SearchArgument sarg;
-      if (serializedPushdown != null) {
-        sarg = SearchArgumentFactory.create
-            (Utilities.deserializeExpression(serializedPushdown));
-      } else {
-        sarg = SearchArgumentFactory.create(sargPushdown);
-      }
-      LOG.info("ORC pushdown predicate: " + sarg);
-      String[] neededColumnNames = columnNamesString.split(",");
-      String[] columnNames = new String[types.size() - rootColumn];
-      boolean[] includedColumns = options.getInclude();
-      int i = 0;
-      for(int columnId: types.get(rootColumn).getSubtypesList()) {
-        if (includedColumns == null || includedColumns[columnId - rootColumn]) {
-          // this is guaranteed to be positive because types only have children
-          // ids greater than their own id.
-          columnNames[columnId - rootColumn] = neededColumnNames[i++];
-        }
-      }
-      options.searchArgument(sarg, columnNames);
+      return;
     }
+
+    LOG.info("ORC pushdown predicate: " + sarg);
+    options.searchArgument(sarg, getSargColumnNames(
+        columnNamesString.split(","), types, options.getInclude(), isOriginal));
   }
 
   @Override
@@ -776,7 +782,7 @@ public void run() {
         // deltas may change the rows making them match the predicate.
         if (deltas.isEmpty()) {
           Reader.Options options = new Reader.Options();
-          setIncludedColumns(options, types, context.conf, isOriginal);
+          options.include(genIncludedColumns(types, context.conf, isOriginal));
           setSearchArgument(options, types, context.conf, isOriginal);
           // only do split pruning if HIVE-8732 has been fixed in the writer
           if (options.getSearchArgument() != null &&
@@ -1124,7 +1130,7 @@ public RowReader<OrcStruct> getReader(InputSplit inputSplit,
           .getBucket();
       reader = OrcFile.createReader(path, OrcFile.readerOptions(conf));
       final List<OrcProto.Type> types = reader.getTypes();
-      setIncludedColumns(readOptions, types, conf, split.isOriginal());
+      readOptions.include(genIncludedColumns(types, conf, split.isOriginal()));
       setSearchArgument(readOptions, types, conf, split.isOriginal());
     } else {
       bucket = (int) split.getStart();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/PositionProvider.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/PositionProvider.java
index 3daa9ba4bc..04d81cc0c9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/PositionProvider.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/PositionProvider.java
@@ -21,6 +21,6 @@
 /**
  * An interface used for seeking to a row index.
  */
-interface PositionProvider {
+public interface PositionProvider {
   long getNext();
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/Reader.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/Reader.java
index f85c21b55e..1f290851cb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/Reader.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/Reader.java
@@ -318,4 +318,5 @@ RecordReader rows(long offset, long length,
                     boolean[] include, SearchArgument sarg,
                     String[] neededColumns) throws IOException;
 
+  MetadataReader metadata() throws IOException;
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
index 03f808556f..50f417b20a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.io.orc;
 
+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVE_ORC_ZEROCOPY;
+
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.ByteBuffer;
@@ -33,34 +35,37 @@
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.DiskRange;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.io.orc.OrcProto.Type;
-import org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
+import org.apache.hadoop.hive.ql.io.orc.OrcProto.UserMetadataItem;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.Text;
+import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BufferChunk;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Sets;
 import com.google.protobuf.CodedInputStream;
 
-final class ReaderImpl implements Reader {
+public class ReaderImpl implements Reader {
 
   private static final Log LOG = LogFactory.getLog(ReaderImpl.class);
 
   private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;
 
-  private final FileSystem fileSystem;
-  private final Path path;
-  private final CompressionKind compressionKind;
-  private final CompressionCodec codec;
-  private final int bufferSize;
+  protected final FileSystem fileSystem;
+  protected final Path path;
+  protected final CompressionKind compressionKind;
+  protected final CompressionCodec codec;
+  protected final int bufferSize;
   private OrcProto.Metadata metadata = null;
   private final int metadataSize;
-  private final OrcProto.Footer footer;
+  protected final OrcProto.Footer footer;
   private final ObjectInspector inspector;
   private long deserializedSize = -1;
-  private final Configuration conf;
+  protected final Configuration conf;
   private final List<Integer> versionList;
   private final OrcFile.WriterVersion writerVersion;
 
@@ -295,7 +300,7 @@ static void checkOrcVersion(Log log, Path path, List<Integer> version) {
    * @param options options for reading
    * @throws IOException
    */
-  ReaderImpl(Path path, OrcFile.ReaderOptions options) throws IOException {
+  public ReaderImpl(Path path, OrcFile.ReaderOptions options) throws IOException {
     FileSystem fs = options.getFilesystem();
     if (fs == null) {
       fs = path.getFileSystem(options.getConfiguration());
@@ -463,14 +468,14 @@ private static class MetaInfoObjExtractor{
       int footerBufferSize = footerBuffer.limit() - footerBuffer.position() - metadataSize;
       footerBuffer.limit(position + metadataSize);
 
-      InputStream instream = InStream.create("metadata", new ByteBuffer[]{footerBuffer},
-          new long[]{0L}, metadataSize, codec, bufferSize);
+      InputStream instream = InStream.create("metadata", Lists.<DiskRange>newArrayList(
+          new BufferChunk(footerBuffer, 0)), metadataSize, codec, bufferSize);
       this.metadata = OrcProto.Metadata.parseFrom(instream);
 
       footerBuffer.position(position + metadataSize);
       footerBuffer.limit(position + metadataSize + footerBufferSize);
-      instream = InStream.create("footer", new ByteBuffer[]{footerBuffer},
-          new long[]{0L}, footerBufferSize, codec, bufferSize);
+      instream = InStream.create("footer", Lists.<DiskRange>newArrayList(
+          new BufferChunk(footerBuffer, 0)), footerBufferSize, codec, bufferSize);
       this.footer = OrcProto.Footer.parseFrom(instream);
 
       footerBuffer.position(position);
@@ -690,4 +695,9 @@ List<OrcProto.StripeStatistics> getOrcProtoStripeStatistics() {
   public List<UserMetadataItem> getOrcProtoUserMetadata() {
     return footer.getMetadataList();
   }
+
+  @Override
+  public MetadataReader metadata() throws IOException {
+    return new MetadataReader(fileSystem, path, codec, bufferSize, footer.getTypesCount());
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
index 458ad21733..4989b2797b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java
@@ -30,12 +30,11 @@
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.LinkedHashMap;
+import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
-import java.util.TreeMap;
 
 import org.apache.commons.lang.StringUtils;
-import org.apache.commons.lang.builder.HashCodeBuilder;
 import org.apache.commons.lang3.exception.ExceptionUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -43,6 +42,9 @@
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.DiskRange;
+import org.apache.hadoop.hive.common.DiskRangeList;
+import org.apache.hadoop.hive.common.DiskRangeList.DiskRangeListCreateHelper;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
@@ -53,6 +55,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.TimestampUtils;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
+import org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.ByteBufferAllocatorPool;
 import org.apache.hadoop.hive.ql.io.filters.BloomFilter;
 import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
@@ -67,9 +70,7 @@
 import org.apache.hadoop.hive.serde2.io.ShortWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;
-import org.apache.hadoop.hive.shims.HadoopShims.ByteBufferPoolShim;
 import org.apache.hadoop.hive.shims.HadoopShims.ZeroCopyReaderShim;
-import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.BooleanWritable;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.FloatWritable;
@@ -77,11 +78,11 @@
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 
-import com.google.common.collect.ComparisonChain;
+import com.google.common.collect.Lists;
 
 class RecordReaderImpl implements RecordReader {
 
-  private static final Log LOG = LogFactory.getLog(RecordReaderImpl.class);
+  static final Log LOG = LogFactory.getLog(RecordReaderImpl.class);
   private static final boolean isLogTraceEnabled = LOG.isTraceEnabled();
   private static final boolean isLogDebugEnabled = LOG.isDebugEnabled();
 
@@ -103,20 +104,15 @@ class RecordReaderImpl implements RecordReader {
   private long rowCountInStripe = 0;
   private final Map<StreamName, InStream> streams =
       new HashMap<StreamName, InStream>();
-  List<BufferChunk> bufferChunks = new ArrayList<BufferChunk>(0);
+  DiskRangeList bufferChunks = null;
   private final TreeReader reader;
   private final OrcProto.RowIndex[] indexes;
   private final OrcProto.BloomFilterIndex[] bloomFilterIndices;
-  private final SearchArgument sarg;
-  // the leaf predicates for the sarg
-  private final List<PredicateLeaf> sargLeaves;
-  // an array the same length as the sargLeaves that map them to column ids
-  private final int[] filterColumns;
-  // same as the above array, but indices are set to true
-  private final boolean[] sargColumns;
+  private final SargApplier sargApp;
   // an array about which row groups aren't skipped
   private boolean[] includedRowGroups = null;
   private final Configuration conf;
+  private final MetadataReader metadata;
 
   private final ByteBufferAllocatorPool pool = new ByteBufferAllocatorPool();
   private final ZeroCopyReaderShim zcr;
@@ -137,85 +133,9 @@ public OrcProto.RowIndex[] getRowGroupIndex() {
     public OrcProto.BloomFilterIndex[] getBloomFilterIndex() {
       return bloomFilterIndex;
     }
-  }
-
-  // this is an implementation copied from ElasticByteBufferPool in hadoop-2,
-  // which lacks a clear()/clean() operation
-  public final static class ByteBufferAllocatorPool implements ByteBufferPoolShim {
-    private static final class Key implements Comparable<Key> {
-      private final int capacity;
-      private final long insertionGeneration;
-
-      Key(int capacity, long insertionGeneration) {
-        this.capacity = capacity;
-        this.insertionGeneration = insertionGeneration;
-      }
-
-      @Override
-      public int compareTo(Key other) {
-        return ComparisonChain.start().compare(capacity, other.capacity)
-            .compare(insertionGeneration, other.insertionGeneration).result();
-      }
-
-      @Override
-      public boolean equals(Object rhs) {
-        if (rhs == null) {
-          return false;
-        }
-        try {
-          Key o = (Key) rhs;
-          return (compareTo(o) == 0);
-        } catch (ClassCastException e) {
-          return false;
-        }
-      }
-
-      @Override
-      public int hashCode() {
-        return new HashCodeBuilder().append(capacity).append(insertionGeneration)
-            .toHashCode();
-      }
-    }
 
-    private final TreeMap<Key, ByteBuffer> buffers = new TreeMap<Key, ByteBuffer>();
-
-    private final TreeMap<Key, ByteBuffer> directBuffers = new TreeMap<Key, ByteBuffer>();
-
-    private long currentGeneration = 0;
-
-    private final TreeMap<Key, ByteBuffer> getBufferTree(boolean direct) {
-      return direct ? directBuffers : buffers;
-    }
-
-    public void clear() {
-      buffers.clear();
-      directBuffers.clear();
-    }
-
-    @Override
-    public ByteBuffer getBuffer(boolean direct, int length) {
-      TreeMap<Key, ByteBuffer> tree = getBufferTree(direct);
-      Map.Entry<Key, ByteBuffer> entry = tree.ceilingEntry(new Key(length, 0));
-      if (entry == null) {
-        return direct ? ByteBuffer.allocateDirect(length) : ByteBuffer
-            .allocate(length);
-      }
-      tree.remove(entry.getKey());
-      return entry.getValue();
-    }
-
-    @Override
-    public void putBuffer(ByteBuffer buffer) {
-      TreeMap<Key, ByteBuffer> tree = getBufferTree(buffer.isDirect());
-      while (true) {
-        Key key = new Key(buffer.capacity(), currentGeneration++);
-        if (!tree.containsKey(key)) {
-          tree.put(key, buffer);
-          return;
-        }
-        // Buffers are indexed by (capacity, generation).
-        // If our key is not unique on the first try, we try again
-      }
+    public void setRowGroupIndex(OrcProto.RowIndex[] rowGroupIndex) {
+      this.rowGroupIndex = rowGroupIndex;
     }
   }
 
@@ -245,7 +165,7 @@ static int findColumns(String[] columnNames,
    *                   result
    * @return an array mapping the sarg leaves to concrete column numbers
    */
-  static int[] mapSargColumns(List<PredicateLeaf> sargLeaves,
+  public static int[] mapSargColumns(List<PredicateLeaf> sargLeaves,
                              String[] columnNames,
                              int rootColumn) {
     int[] result = new int[sargLeaves.size()];
@@ -257,7 +177,7 @@ static int[] mapSargColumns(List<PredicateLeaf> sargLeaves,
     return result;
   }
 
-  RecordReaderImpl(List<StripeInformation> stripes,
+  protected RecordReaderImpl(List<StripeInformation> stripes,
                    FileSystem fileSystem,
                    Path path,
                    Reader.Options options,
@@ -274,22 +194,14 @@ static int[] mapSargColumns(List<PredicateLeaf> sargLeaves,
     this.bufferSize = bufferSize;
     this.included = options.getInclude();
     this.conf = conf;
-    this.sarg = options.getSearchArgument();
-    if (sarg != null) {
-      sargLeaves = sarg.getLeaves();
-      filterColumns = mapSargColumns(sargLeaves, options.getColumnNames(), 0);
-      // included will not be null, row options will fill the array with trues if null
-      sargColumns = new boolean[included.length];
-      for (int i : filterColumns) {
-        // filter columns may have -1 as index which could be partition column in SARG.
-        if (i > 0) {
-          sargColumns[i] = true;
-        }
-      }
+    this.rowIndexStride = strideRate;
+    this.metadata = new MetadataReader(file, codec, bufferSize, types.size());
+    SearchArgument sarg = options.getSearchArgument();
+    if (sarg != null && strideRate != 0) {
+      sargApp = new SargApplier(
+          sarg, options.getColumnNames(), strideRate, types, included.length);
     } else {
-      sargLeaves = null;
-      filterColumns = null;
-      sargColumns = null;
+      sargApp = null;
     }
     long rows = 0;
     long skippedRows = 0;
@@ -307,31 +219,28 @@ static int[] mapSargColumns(List<PredicateLeaf> sargLeaves,
 
     final boolean zeroCopy = (conf != null)
         && (HiveConf.getBoolVar(conf, HIVE_ORC_ZEROCOPY));
-
-    if (zeroCopy
-        && (codec == null || ((codec instanceof DirectDecompressionCodec)
-            && ((DirectDecompressionCodec) codec).isAvailable()))) {
-      /* codec is null or is available */
-      this.zcr = ShimLoader.getHadoopShims().getZeroCopyReader(file, pool);
-    } else {
-      this.zcr = null;
-    }
+    zcr = zeroCopy ? RecordReaderUtils.createZeroCopyShim(file, codec, pool) : null;
 
     firstRow = skippedRows;
     totalRowCount = rows;
-    reader = createTreeReader(path, 0, types, included, conf);
+    boolean skipCorrupt = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_ORC_SKIP_CORRUPT_DATA);
+    reader = createTreeReader(0, types, included, skipCorrupt);
     indexes = new OrcProto.RowIndex[types.size()];
     bloomFilterIndices = new OrcProto.BloomFilterIndex[types.size()];
-    rowIndexStride = strideRate;
-    advanceToNextRow(0L);
+    advanceToNextRow(reader, 0L, true);
   }
 
-  private static final class PositionProviderImpl implements PositionProvider {
+  public static final class PositionProviderImpl implements PositionProvider {
     private final OrcProto.RowIndexEntry entry;
-    private int index = 0;
+    private int index;
 
-    PositionProviderImpl(OrcProto.RowIndexEntry entry) {
+    public PositionProviderImpl(OrcProto.RowIndexEntry entry) {
+      this(entry, 0);
+    }
+
+    public PositionProviderImpl(OrcProto.RowIndexEntry entry, int startPos) {
       this.entry = entry;
+      this.index = startPos;
     }
 
     @Override
@@ -340,33 +249,39 @@ public long getNext() {
     }
   }
 
-  private abstract static class TreeReader {
-    protected final Path path;
+  public abstract static class TreeReader {
     protected final int columnId;
-    private BitFieldReader present = null;
+    public BitFieldReader present = null;
     protected boolean valuePresent = false;
-    protected final Configuration conf;
 
-    TreeReader(Path path, int columnId, Configuration conf) {
-      this.path = path;
+    public TreeReader(int columnId) throws IOException {
+      this(columnId, null);
+    }
+
+    public TreeReader(int columnId, InStream in) throws IOException {
       this.columnId = columnId;
-      this.conf = conf;
+      if (in == null) {
+        present = null;
+        valuePresent = true;
+      } else {
+        present = new BitFieldReader(in, 1);
+      }
     }
 
     void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
     IntegerReader createIntegerReader(OrcProto.ColumnEncoding.Kind kind,
         InStream in,
-        boolean signed) throws IOException {
+        boolean signed, boolean skipCorrupt) throws IOException {
       switch (kind) {
       case DIRECT_V2:
       case DICTIONARY_V2:
-        return new RunLengthIntegerReaderV2(in, signed, conf);
+        return new RunLengthIntegerReaderV2(in, signed, skipCorrupt);
       case DIRECT:
       case DICTIONARY:
         return new RunLengthIntegerReader(in, signed);
@@ -395,8 +310,12 @@ void startStripe(Map<StreamName, InStream> streams,
      * @throws IOException
      */
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    public void seek(PositionProvider index) throws IOException {
       if (present != null) {
-        present.seek(index[columnId]);
+        present.seek(index);
       }
     }
 
@@ -431,8 +350,7 @@ Object next(Object previous) throws IOException {
      * @return
      * @throws IOException
      */
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
-
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       ColumnVector result = (ColumnVector) previousVector;
       if (present != null) {
         // Set noNulls and isNull vector of the ColumnVector based on
@@ -456,11 +374,18 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
     }
   }
 
-  private static class BooleanTreeReader extends TreeReader{
-    private BitFieldReader reader = null;
+  public static class BooleanTreeReader extends TreeReader {
+    protected BitFieldReader reader = null;
 
-    BooleanTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    public BooleanTreeReader(int columnId) throws IOException {
+      this(columnId, null, null);
+    }
+
+    public BooleanTreeReader(int columnId, InStream present, InStream data) throws IOException {
+      super(columnId, present);
+      if (data != null) {
+        reader = new BitFieldReader(data, 1);
+      }
     }
 
     @Override
@@ -474,8 +399,13 @@ void startStripe(Map<StreamName, InStream> streams,
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -499,7 +429,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -516,11 +446,16 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
     }
   }
 
-  private static class ByteTreeReader extends TreeReader{
-    private RunLengthByteReader reader = null;
+  public static class ByteTreeReader extends TreeReader{
+    protected RunLengthByteReader reader = null;
 
-    ByteTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    ByteTreeReader(int columnId) throws IOException {
+      this(columnId, null, null);
+    }
+
+    public ByteTreeReader(int columnId, InStream present, InStream data) throws IOException {
+      super(columnId, present);
+      this.reader = new RunLengthByteReader(data);
     }
 
     @Override
@@ -534,8 +469,13 @@ void startStripe(Map<StreamName, InStream> streams,
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -554,7 +494,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -576,11 +516,21 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class ShortTreeReader extends TreeReader{
-    private IntegerReader reader = null;
+  public static class ShortTreeReader extends TreeReader{
+    protected IntegerReader reader = null;
 
-    ShortTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    public ShortTreeReader(int columnId) throws IOException {
+      this(columnId, null, null, null);
+    }
+
+    public ShortTreeReader(int columnId, InStream present, InStream data,
+        OrcProto.ColumnEncoding encoding)
+        throws IOException {
+      super(columnId, present);
+      if (data != null && encoding != null) {
+        checkEncoding(encoding);
+        this.reader = createIntegerReader(encoding.getKind(), data, true, false);
+      }
     }
 
     @Override
@@ -588,7 +538,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -599,13 +549,19 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       StreamName name = new StreamName(columnId,
           OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true);
+      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true,
+          false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -624,7 +580,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -646,11 +602,21 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class IntTreeReader extends TreeReader{
-    private IntegerReader reader = null;
+  public static class IntTreeReader extends TreeReader{
+    protected IntegerReader reader = null;
+
+    public IntTreeReader(int columnId) throws IOException {
+      this(columnId, null, null, null);
+    }
 
-    IntTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    public IntTreeReader(int columnId, InStream present, InStream data,
+        OrcProto.ColumnEncoding encoding)
+        throws IOException {
+      super(columnId, present);
+      if (data != null && encoding != null) {
+        checkEncoding(encoding);
+        this.reader = createIntegerReader(encoding.getKind(), data, true, false);
+      }
     }
 
     @Override
@@ -658,7 +624,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -669,13 +635,19 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       StreamName name = new StreamName(columnId,
           OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true);
+      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true,
+          false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -694,7 +666,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -716,11 +688,22 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class LongTreeReader extends TreeReader{
-    private IntegerReader reader = null;
+  public static class LongTreeReader extends TreeReader{
+    protected IntegerReader reader = null;
 
-    LongTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    LongTreeReader(int columnId, boolean skipCorrupt) throws IOException {
+      this(columnId, null, null, null, skipCorrupt);
+    }
+
+    public LongTreeReader(int columnId, InStream present, InStream data,
+        OrcProto.ColumnEncoding encoding,
+        boolean skipCorrupt)
+        throws IOException {
+      super(columnId, present);
+      if (data != null && encoding != null) {
+        checkEncoding(encoding);
+        this.reader = createIntegerReader(encoding.getKind(), data, true, skipCorrupt);
+      }
     }
 
     @Override
@@ -728,7 +711,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -739,13 +722,19 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       StreamName name = new StreamName(columnId,
           OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true);
+      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true,
+          false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -764,7 +753,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -786,13 +775,18 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class FloatTreeReader extends TreeReader{
-    private InStream stream;
+  public static class FloatTreeReader extends TreeReader{
+    protected InStream stream;
     private final SerializationUtils utils;
 
-    FloatTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    public FloatTreeReader(int columnId) throws IOException {
+      this(columnId, null, null);
+    }
+
+    public FloatTreeReader(int columnId, InStream present, InStream data) throws IOException {
+      super(columnId, present);
       this.utils = new SerializationUtils();
+      this.stream = data;
     }
 
     @Override
@@ -807,8 +801,13 @@ void startStripe(Map<StreamName, InStream> streams,
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      stream.seek(index[columnId]);
+      stream.seek(index);
     }
 
     @Override
@@ -827,7 +826,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       DoubleColumnVector result = null;
       if (previousVector == null) {
         result = new DoubleColumnVector();
@@ -860,7 +859,7 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
     }
 
     @Override
-    void skipRows(long items) throws IOException {
+    protected void skipRows(long items) throws IOException {
       items = countNonNulls(items);
       for(int i=0; i < items; ++i) {
         utils.readFloat(stream);
@@ -868,13 +867,18 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class DoubleTreeReader extends TreeReader{
-    private InStream stream;
+  public static class DoubleTreeReader extends TreeReader{
+    protected InStream stream;
     private final SerializationUtils utils;
 
-    DoubleTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    public DoubleTreeReader(int columnId) throws IOException {
+      this(columnId, null, null);
+    }
+
+    public DoubleTreeReader(int columnId, InStream present, InStream data) throws IOException {
+      super(columnId, present);
       this.utils = new SerializationUtils();
+      this.stream = data;
     }
 
     @Override
@@ -890,8 +894,13 @@ void startStripe(Map<StreamName, InStream> streams,
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      stream.seek(index[columnId]);
+      stream.seek(index);
     }
 
     @Override
@@ -910,7 +919,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       DoubleColumnVector result = null;
       if (previousVector == null) {
         result = new DoubleColumnVector();
@@ -948,15 +957,25 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class BinaryTreeReader extends TreeReader{
+  public static class BinaryTreeReader extends TreeReader{
     protected InStream stream;
     protected IntegerReader lengths = null;
 
     protected final LongColumnVector scratchlcv;
 
-    BinaryTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    BinaryTreeReader(int columnId) throws IOException {
+      this(columnId, null, null, null, null);
+    }
+
+    public BinaryTreeReader(int columnId, InStream present, InStream data, InStream length,
+        OrcProto.ColumnEncoding encoding) throws IOException {
+      super(columnId, present);
       scratchlcv = new LongColumnVector();
+      this.stream = data;
+      if (length != null && encoding != null) {
+        checkEncoding(encoding);
+        this.lengths = createIntegerReader(encoding.getKind(), length, false, false);
+      }
     }
 
     @Override
@@ -964,7 +983,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -977,14 +996,19 @@ void startStripe(Map<StreamName, InStream> streams,
           OrcProto.Stream.Kind.DATA);
       stream = streams.get(name);
       lengths = createIntegerReader(encodings.get(columnId).getKind(), streams.get(new
-          StreamName(columnId, OrcProto.Stream.Kind.LENGTH)), false);
+          StreamName(columnId, OrcProto.Stream.Kind.LENGTH)), false, false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      stream.seek(index[columnId]);
-      lengths.seek(index[columnId]);
+      stream.seek(index);
+      lengths.seek(index);
     }
 
     @Override
@@ -1013,7 +1037,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       BytesColumnVector result = null;
       if (previousVector == null) {
         result = new BytesColumnVector();
@@ -1039,13 +1063,31 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class TimestampTreeReader extends TreeReader{
-    private IntegerReader data = null;
-    private IntegerReader nanos = null;
-    private final LongColumnVector nanoVector = new LongColumnVector();
+  public static class TimestampTreeReader extends TreeReader {
+    protected IntegerReader data = null;
+    protected IntegerReader nanos = null;
+    private final boolean skipCorrupt;
 
-    TimestampTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    TimestampTreeReader(int columnId, boolean skipCorrupt) throws IOException {
+      this(columnId, null, null, null, null, skipCorrupt);
+    }
+
+    public TimestampTreeReader(int columnId, InStream presentStream, InStream dataStream,
+        InStream nanosStream, OrcProto.ColumnEncoding encoding, boolean skipCorrupt)
+        throws IOException {
+      super(columnId, presentStream);
+      this.skipCorrupt = skipCorrupt;
+      if (encoding != null) {
+        checkEncoding(encoding);
+
+        if (dataStream != null) {
+          this.data = createIntegerReader(encoding.getKind(), dataStream, true, skipCorrupt);
+        }
+
+        if (nanosStream != null) {
+          this.nanos = createIntegerReader(encoding.getKind(), nanosStream, false, skipCorrupt);
+        }
+      }
     }
 
     @Override
@@ -1053,7 +1095,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -1064,17 +1106,22 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       data = createIntegerReader(encodings.get(columnId).getKind(),
           streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.DATA)), true);
+              OrcProto.Stream.Kind.DATA)), true, skipCorrupt);
       nanos = createIntegerReader(encodings.get(columnId).getKind(),
           streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.SECONDARY)), false);
+              OrcProto.Stream.Kind.SECONDARY)), false, skipCorrupt);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      data.seek(index[columnId]);
-      nanos.seek(index[columnId]);
+      data.seek(index);
+      nanos.seek(index);
     }
 
     @Override
@@ -1105,7 +1152,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -1149,11 +1196,20 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class DateTreeReader extends TreeReader{
-    private IntegerReader reader = null;
+  public static class DateTreeReader extends TreeReader{
+    protected IntegerReader reader = null;
 
-    DateTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    DateTreeReader(int columnId) throws IOException {
+      this(columnId, null, null, null);
+    }
+
+    public DateTreeReader(int columnId, InStream present, InStream data,
+        OrcProto.ColumnEncoding encoding) throws IOException {
+      super(columnId, present);
+      if (data != null && encoding != null) {
+        checkEncoding(encoding);
+        reader = createIntegerReader(encoding.getKind(), data, true, false);
+      }
     }
 
     @Override
@@ -1161,7 +1217,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -1172,13 +1228,18 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       StreamName name = new StreamName(columnId,
           OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true);
+      reader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(name), true, false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -1197,7 +1258,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       LongColumnVector result = null;
       if (previousVector == null) {
         result = new LongColumnVector();
@@ -1219,18 +1280,30 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class DecimalTreeReader extends TreeReader{
-    private InStream valueStream;
-    private IntegerReader scaleStream = null;
-    private LongColumnVector scratchScaleVector = new LongColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
+  public static class DecimalTreeReader extends TreeReader{
+    protected InStream valueStream;
+    protected IntegerReader scaleReader = null;
+    private LongColumnVector scratchScaleVector;
 
     private final int precision;
     private final int scale;
 
-    DecimalTreeReader(Path path, int columnId, int precision, int scale, Configuration conf) {
-      super(path, columnId, conf);
+    DecimalTreeReader(int columnId, int precision, int scale) throws IOException {
+      this(columnId, precision, scale, null, null, null, null);
+    }
+
+    public DecimalTreeReader(int columnId, int precision, int scale, InStream present,
+        InStream valueStream, InStream scaleStream, OrcProto.ColumnEncoding encoding)
+        throws IOException {
+      super(columnId, present);
       this.precision = precision;
       this.scale = scale;
+      this.scratchScaleVector = new LongColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
+      this.valueStream = valueStream;
+      if (scaleStream != null && encoding != null) {
+        checkEncoding(encoding);
+        this.scaleReader = createIntegerReader(encoding.getKind(), scaleStream, true, false);
+      }
     }
 
     @Override
@@ -1238,7 +1311,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -1249,15 +1322,20 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       valueStream = streams.get(new StreamName(columnId,
           OrcProto.Stream.Kind.DATA));
-      scaleStream = createIntegerReader(encodings.get(columnId).getKind(), streams.get(
-          new StreamName(columnId, OrcProto.Stream.Kind.SECONDARY)), true);
+      scaleReader = createIntegerReader(encodings.get(columnId).getKind(), streams.get(
+          new StreamName(columnId, OrcProto.Stream.Kind.SECONDARY)), true, false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      valueStream.seek(index[columnId]);
-      scaleStream.seek(index[columnId]);
+      valueStream.seek(index);
+      scaleReader.seek(index);
     }
 
     @Override
@@ -1271,14 +1349,14 @@ Object next(Object previous) throws IOException {
           result = (HiveDecimalWritable) previous;
         }
         result.set(HiveDecimal.create(SerializationUtils.readBigInteger(valueStream),
-            (int) scaleStream.next()));
+            (int) scaleReader.next()));
         return HiveDecimalUtils.enforcePrecisionScale(result, precision, scale);
       }
       return null;
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       DecimalColumnVector result = null;
       if (previousVector == null) {
         result = new DecimalColumnVector(precision, scale);
@@ -1296,7 +1374,7 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
       if (result.isRepeating) {
         if (!result.isNull[0]) {
           BigInteger bInt = SerializationUtils.readBigInteger(valueStream);
-          short scaleInData = (short) scaleStream.next();
+          short scaleInData = (short) scaleReader.next();
           HiveDecimal dec = HiveDecimal.create(bInt, scaleInData);
           dec = HiveDecimalUtils.enforcePrecisionScale(dec, precision, scale);
           result.set(0, dec);
@@ -1304,7 +1382,7 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
       } else {
         // result vector has isNull values set, use the same to read scale vector.
         scratchScaleVector.isNull = result.isNull;
-        scaleStream.nextVector(scratchScaleVector, batchSize);
+        scaleReader.nextVector(scratchScaleVector, batchSize);
         for (int i = 0; i < batchSize; i++) {
           if (!result.isNull[i]) {
             BigInteger bInt = SerializationUtils.readBigInteger(valueStream);
@@ -1326,7 +1404,7 @@ void skipRows(long items) throws IOException {
       for(int i=0; i < items; i++) {
         SerializationUtils.readBigInteger(valueStream);
       }
-      scaleStream.skip(items);
+      scaleReader.skip(items);
     }
   }
 
@@ -1335,11 +1413,33 @@ void skipRows(long items) throws IOException {
    * stripe, it creates an internal reader based on whether a direct or
    * dictionary encoding was used.
    */
-  private static class StringTreeReader extends TreeReader {
-    private TreeReader reader;
+  public static class StringTreeReader extends TreeReader {
+    protected TreeReader reader;
 
-    StringTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    public StringTreeReader(int columnId) throws IOException {
+      super(columnId);
+    }
+
+    public StringTreeReader(int columnId, InStream present, InStream data, InStream length,
+        InStream dictionary, OrcProto.ColumnEncoding encoding) throws IOException {
+      super(columnId, present);
+      if (encoding != null) {
+        switch (encoding.getKind()) {
+          case DIRECT:
+          case DIRECT_V2:
+            reader = new StringDirectTreeReader(columnId, present, data, length,
+                encoding.getKind());
+            break;
+          case DICTIONARY:
+          case DICTIONARY_V2:
+            reader = new StringDictionaryTreeReader(columnId, present, data, length, dictionary,
+                encoding);
+            break;
+          default:
+            throw new IllegalArgumentException("Unsupported encoding " +
+                encoding.getKind());
+        }
+      }
     }
 
     @Override
@@ -1356,11 +1456,11 @@ void startStripe(Map<StreamName, InStream> streams,
       switch (encodings.get(columnId).getKind()) {
         case DIRECT:
         case DIRECT_V2:
-          reader = new StringDirectTreeReader(path, columnId, conf);
+          reader = new StringDirectTreeReader(columnId);
           break;
         case DICTIONARY:
         case DICTIONARY_V2:
-          reader = new StringDictionaryTreeReader(path, columnId, conf);
+          reader = new StringDictionaryTreeReader(columnId);
           break;
         default:
           throw new IllegalArgumentException("Unsupported encoding " +
@@ -1374,13 +1474,18 @@ void seek(PositionProvider[] index) throws IOException {
       reader.seek(index);
     }
 
+    @Override
+    public void seek(PositionProvider index) throws IOException {
+      reader.seek(index);
+    }
+
     @Override
     Object next(Object previous) throws IOException {
       return reader.next(previous);
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       return reader.nextVector(previousVector, batchSize);
     }
 
@@ -1393,7 +1498,7 @@ void skipRows(long items) throws IOException {
   // This class collects together very similar methods for reading an ORC vector of byte arrays and
   // creating the BytesColumnVector.
   //
-  private static class BytesColumnVectorUtil {
+   public static class BytesColumnVectorUtil {
 
     private static byte[] commonReadByteArrays(InStream stream, IntegerReader lengths, LongColumnVector scratchlcv,
             BytesColumnVector result, long batchSize) throws IOException {
@@ -1464,15 +1569,23 @@ public static void readOrcByteArrays(InStream stream, IntegerReader lengths, Lon
    * A reader for string columns that are direct encoded in the current
    * stripe.
    */
-  private static class StringDirectTreeReader extends TreeReader {
-    private InStream stream;
-    private IntegerReader lengths;
-
+  public static class StringDirectTreeReader extends TreeReader {
+    public InStream stream;
+    public IntegerReader lengths;
     private final LongColumnVector scratchlcv;
 
-    StringDirectTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
-      scratchlcv = new LongColumnVector();
+    StringDirectTreeReader(int columnId) throws IOException {
+      this(columnId, null, null, null, null);
+    }
+
+    public StringDirectTreeReader(int columnId, InStream present, InStream data, InStream length,
+        OrcProto.ColumnEncoding.Kind encoding) throws IOException {
+      super(columnId, present);
+      this.scratchlcv = new LongColumnVector();
+      this.stream = data;
+      if (length != null && encoding != null) {
+        this.lengths = createIntegerReader(encoding, length, false, false);
+      }
     }
 
     @Override
@@ -1480,7 +1593,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT &&
           encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -1494,14 +1607,19 @@ void startStripe(Map<StreamName, InStream> streams,
       stream = streams.get(name);
       lengths = createIntegerReader(encodings.get(columnId).getKind(),
           streams.get(new StreamName(columnId, OrcProto.Stream.Kind.LENGTH)),
-          false);
+          false, false);
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      stream.seek(index[columnId]);
-      lengths.seek(index[columnId]);
+      stream.seek(index);
+      lengths.seek(index);
     }
 
     @Override
@@ -1531,7 +1649,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       BytesColumnVector result = null;
       if (previousVector == null) {
         result = new BytesColumnVector();
@@ -1561,17 +1679,34 @@ void skipRows(long items) throws IOException {
    * A reader for string columns that are dictionary encoded in the current
    * stripe.
    */
-  private static class StringDictionaryTreeReader extends TreeReader {
+  public static class StringDictionaryTreeReader extends TreeReader {
     private DynamicByteArray dictionaryBuffer;
     private int[] dictionaryOffsets;
-    private IntegerReader reader;
+    public IntegerReader reader;
 
     private byte[] dictionaryBufferInBytesCache = null;
     private final LongColumnVector scratchlcv;
 
-    StringDictionaryTreeReader(Path path, int columnId, Configuration conf) {
-      super(path, columnId, conf);
+    StringDictionaryTreeReader(int columnId) throws IOException {
+      this(columnId, null, null, null, null, null);
+    }
+
+    public StringDictionaryTreeReader(int columnId, InStream present, InStream data,
+        InStream length, InStream dictionary, OrcProto.ColumnEncoding encoding)
+        throws IOException{
+      super(columnId, present);
       scratchlcv = new LongColumnVector();
+      if (data != null && encoding != null) {
+        this.reader = createIntegerReader(encoding.getKind(), data, false, false);
+      }
+
+      if (dictionary != null && encoding != null) {
+        readDictionaryStream(dictionary);
+      }
+
+      if (length != null && encoding != null) {
+        readDictionaryLengthStream(length, encoding);
+      }
     }
 
     @Override
@@ -1579,7 +1714,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DICTIONARY &&
           encoding.getKind() != OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -1590,28 +1725,27 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
 
       // read the dictionary blob
-      int dictionarySize = encodings.get(columnId).getDictionarySize();
       StreamName name = new StreamName(columnId,
           OrcProto.Stream.Kind.DICTIONARY_DATA);
       InStream in = streams.get(name);
-      if (in != null) { // Guard against empty dictionary stream.
-        if (in.available() > 0) {
-          dictionaryBuffer = new DynamicByteArray(64, in.available());
-          dictionaryBuffer.readAll(in);
-          // Since its start of strip invalidate the cache.
-          dictionaryBufferInBytesCache = null;
-        }
-        in.close();
-      } else {
-        dictionaryBuffer = null;
-      }
+      readDictionaryStream(in);
 
       // read the lengths
       name = new StreamName(columnId, OrcProto.Stream.Kind.LENGTH);
       in = streams.get(name);
+      readDictionaryLengthStream(in, encodings.get(columnId));
+
+      // set up the row reader
+      name = new StreamName(columnId, OrcProto.Stream.Kind.DATA);
+      reader = createIntegerReader(encodings.get(columnId).getKind(),
+          streams.get(name), false, false);
+    }
+
+    private void readDictionaryLengthStream(InStream in, OrcProto.ColumnEncoding encoding)
+        throws IOException {
+      int dictionarySize = encoding.getDictionarySize();
       if (in != null) { // Guard against empty LENGTH stream.
-        IntegerReader lenReader = createIntegerReader(encodings.get(columnId)
-            .getKind(), in, false);
+        IntegerReader lenReader = createIntegerReader(encoding.getKind(), in, false, false);
         int offset = 0;
         if (dictionaryOffsets == null ||
             dictionaryOffsets.length < dictionarySize + 1) {
@@ -1625,16 +1759,31 @@ void startStripe(Map<StreamName, InStream> streams,
         in.close();
       }
 
-      // set up the row reader
-      name = new StreamName(columnId, OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(encodings.get(columnId).getKind(),
-          streams.get(name), false);
+    }
+
+    private void readDictionaryStream(InStream in) throws IOException {
+      if (in != null) { // Guard against empty dictionary stream.
+        if (in.available() > 0) {
+          dictionaryBuffer = new DynamicByteArray(64, in.available());
+          dictionaryBuffer.readAll(in);
+          // Since its start of strip invalidate the cache.
+          dictionaryBufferInBytesCache = null;
+        }
+        in.close();
+      } else {
+        dictionaryBuffer = null;
+      }
     }
 
     @Override
     void seek(PositionProvider[] index) throws IOException {
+      seek(index[columnId]);
+    }
+
+    @Override
+    public void seek(PositionProvider index) throws IOException {
       super.seek(index);
-      reader.seek(index[columnId]);
+      reader.seek(index);
     }
 
     @Override
@@ -1663,7 +1812,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       BytesColumnVector result = null;
       int offset = 0, length = 0;
       if (previousVector == null) {
@@ -1736,11 +1885,16 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static class CharTreeReader extends StringTreeReader {
+  public static class CharTreeReader extends StringTreeReader {
     int maxLength;
 
-    CharTreeReader(Path path, int columnId, int maxLength, Configuration conf) {
-      super(path, columnId, conf);
+    public CharTreeReader(int columnId, int maxLength) throws IOException {
+      this(columnId, maxLength, null, null, null, null, null);
+    }
+
+    public CharTreeReader(int columnId, int maxLength, InStream present, InStream data,
+        InStream length, InStream dictionary, OrcProto.ColumnEncoding encoding) throws IOException {
+      super(columnId, present, data, length, dictionary, encoding);
       this.maxLength = maxLength;
     }
 
@@ -1764,7 +1918,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       // Get the vector of strings from StringTreeReader, then make a 2nd pass to
       // adjust down the length (right trim and truncate) if necessary.
       BytesColumnVector result = (BytesColumnVector) super.nextVector(previousVector, batchSize);
@@ -1800,11 +1954,16 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
     }
   }
 
-  private static class VarcharTreeReader extends StringTreeReader {
+  public static class VarcharTreeReader extends StringTreeReader {
     int maxLength;
 
-    VarcharTreeReader(Path path, int columnId, int maxLength, Configuration conf) {
-      super(path, columnId, conf);
+    public VarcharTreeReader(int columnId, int maxLength) throws IOException {
+      this(columnId, maxLength, null, null, null, null, null);
+    }
+
+    public VarcharTreeReader(int columnId, int maxLength, InStream present, InStream data,
+        InStream length, InStream dictionary, OrcProto.ColumnEncoding encoding) throws IOException {
+      super(columnId, present, data, length, dictionary, encoding);
       this.maxLength = maxLength;
     }
 
@@ -1828,7 +1987,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       // Get the vector of strings from StringTreeReader, then make a 2nd pass to
       // adjust down the length (truncate) if necessary.
       BytesColumnVector result = (BytesColumnVector) super.nextVector(previousVector, batchSize);
@@ -1867,19 +2026,23 @@ Object nextVector(Object previousVector, long batchSize) throws IOException {
   private static class StructTreeReader extends TreeReader {
     private final TreeReader[] fields;
     private final String[] fieldNames;
+    private final List<TreeReader> readers;
 
-    StructTreeReader(Path path, int columnId,
+    StructTreeReader(int columnId,
                      List<OrcProto.Type> types,
-                     boolean[] included, Configuration conf) throws IOException {
-      super(path, columnId, conf);
+                     boolean[] included,
+                     boolean skipCorrupt) throws IOException {
+      super(columnId);
       OrcProto.Type type = types.get(columnId);
       int fieldCount = type.getFieldNamesCount();
       this.fields = new TreeReader[fieldCount];
       this.fieldNames = new String[fieldCount];
+      this.readers = new ArrayList<TreeReader>();
       for(int i=0; i < fieldCount; ++i) {
         int subtype = type.getSubtypes(i);
         if (included == null || included[subtype]) {
-          this.fields[i] = createTreeReader(path, subtype, types, included, conf);
+          this.fields[i] = createTreeReader(subtype, types, included, skipCorrupt);
+          readers.add(this.fields[i]);
         }
         this.fieldNames[i] = type.getFieldNames(i);
       }
@@ -1922,7 +2085,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       ColumnVector[] result = null;
       if (previousVector == null) {
         result = new ColumnVector[fields.length];
@@ -1970,17 +2133,18 @@ private static class UnionTreeReader extends TreeReader {
     private final TreeReader[] fields;
     private RunLengthByteReader tags;
 
-    UnionTreeReader(Path path, int columnId,
+    UnionTreeReader(int columnId,
                     List<OrcProto.Type> types,
-                    boolean[] included, Configuration conf) throws IOException {
-      super(path, columnId, conf);
+                    boolean[] included,
+                    boolean skipCorrupt) throws IOException {
+      super(columnId);
       OrcProto.Type type = types.get(columnId);
       int fieldCount = type.getSubtypesCount();
       this.fields = new TreeReader[fieldCount];
       for(int i=0; i < fieldCount; ++i) {
         int subtype = type.getSubtypes(i);
         if (included == null || included[subtype]) {
-          this.fields[i] = createTreeReader(path, subtype, types, included, conf);
+          this.fields[i] = createTreeReader(subtype, types, included, skipCorrupt);
         }
       }
     }
@@ -2013,7 +2177,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previousVector, long batchSize) throws IOException {
+    public Object nextVector(Object previousVector, long batchSize) throws IOException {
       throw new UnsupportedOperationException(
           "NextVector is not supported operation for Union type");
     }
@@ -2049,13 +2213,13 @@ private static class ListTreeReader extends TreeReader {
     private final TreeReader elementReader;
     private IntegerReader lengths = null;
 
-    ListTreeReader(Path path, int columnId,
+    ListTreeReader(int columnId,
                    List<OrcProto.Type> types,
-                   boolean[] included, Configuration conf) throws IOException {
-      super(path, columnId, conf);
+                   boolean[] included,
+                   boolean skipCorrupt) throws IOException {
+      super(columnId);
       OrcProto.Type type = types.get(columnId);
-      elementReader = createTreeReader(path, type.getSubtypes(0), types,
-          included, conf);
+      elementReader = createTreeReader(type.getSubtypes(0), types, included, skipCorrupt);
     }
 
     @Override
@@ -2096,7 +2260,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previous, long batchSize) throws IOException {
+    public Object nextVector(Object previous, long batchSize) throws IOException {
       throw new UnsupportedOperationException(
           "NextVector is not supported operation for List type");
     }
@@ -2106,7 +2270,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -2117,7 +2281,7 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       lengths = createIntegerReader(encodings.get(columnId).getKind(),
           streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.LENGTH)), false);
+              OrcProto.Stream.Kind.LENGTH)), false, false);
       if (elementReader != null) {
         elementReader.startStripe(streams, encodings);
       }
@@ -2139,21 +2303,21 @@ private static class MapTreeReader extends TreeReader {
     private final TreeReader valueReader;
     private IntegerReader lengths = null;
 
-    MapTreeReader(Path path,
-                  int columnId,
+    MapTreeReader(int columnId,
                   List<OrcProto.Type> types,
-                  boolean[] included, Configuration conf) throws IOException {
-      super(path, columnId, conf);
+                  boolean[] included,
+                  boolean skipCorrupt) throws IOException {
+      super(columnId);
       OrcProto.Type type = types.get(columnId);
       int keyColumn = type.getSubtypes(0);
       int valueColumn = type.getSubtypes(1);
       if (included == null || included[keyColumn]) {
-        keyReader = createTreeReader(path, keyColumn, types, included, conf);
+        keyReader = createTreeReader(keyColumn, types, included, skipCorrupt);
       } else {
         keyReader = null;
       }
       if (included == null || included[valueColumn]) {
-        valueReader = createTreeReader(path, valueColumn, types, included, conf);
+        valueReader = createTreeReader(valueColumn, types, included, skipCorrupt);
       } else {
         valueReader = null;
       }
@@ -2190,7 +2354,7 @@ Object next(Object previous) throws IOException {
     }
 
     @Override
-    Object nextVector(Object previous, long batchSize) throws IOException {
+    public Object nextVector(Object previous, long batchSize) throws IOException {
       throw new UnsupportedOperationException(
           "NextVector is not supported operation for Map type");
     }
@@ -2200,7 +2364,7 @@ void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
       if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
           (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
         throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId + " of " + path);
+            columnId);
       }
     }
 
@@ -2211,7 +2375,7 @@ void startStripe(Map<StreamName, InStream> streams,
       super.startStripe(streams, encodings);
       lengths = createIntegerReader(encodings.get(columnId).getKind(),
           streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.LENGTH)), false);
+              OrcProto.Stream.Kind.LENGTH)), false, false);
       if (keyReader != null) {
         keyReader.startStripe(streams, encodings);
       }
@@ -2232,77 +2396,65 @@ void skipRows(long items) throws IOException {
     }
   }
 
-  private static TreeReader createTreeReader(Path path,
-                                             int columnId,
+  private static TreeReader createTreeReader(int columnId,
                                              List<OrcProto.Type> types,
                                              boolean[] included,
-                                             Configuration conf
+                                             boolean skipCorrupt
                                             ) throws IOException {
     OrcProto.Type type = types.get(columnId);
     switch (type.getKind()) {
       case BOOLEAN:
-        return new BooleanTreeReader(path, columnId, conf);
+        return new BooleanTreeReader(columnId);
       case BYTE:
-        return new ByteTreeReader(path, columnId, conf);
+        return new ByteTreeReader(columnId);
       case DOUBLE:
-        return new DoubleTreeReader(path, columnId, conf);
+        return new DoubleTreeReader(columnId);
       case FLOAT:
-        return new FloatTreeReader(path, columnId, conf);
+        return new FloatTreeReader(columnId);
       case SHORT:
-        return new ShortTreeReader(path, columnId, conf);
+        return new ShortTreeReader(columnId);
       case INT:
-        return new IntTreeReader(path, columnId, conf);
+        return new IntTreeReader(columnId);
       case LONG:
-        return new LongTreeReader(path, columnId, conf);
+        return new LongTreeReader(columnId, skipCorrupt);
       case STRING:
-        return new StringTreeReader(path, columnId, conf);
+        return new StringTreeReader(columnId);
       case CHAR:
         if (!type.hasMaximumLength()) {
           throw new IllegalArgumentException("ORC char type has no length specified");
         }
-        return new CharTreeReader(path, columnId, type.getMaximumLength(), conf);
+        return new CharTreeReader(columnId, type.getMaximumLength());
       case VARCHAR:
         if (!type.hasMaximumLength()) {
           throw new IllegalArgumentException("ORC varchar type has no length specified");
         }
-        return new VarcharTreeReader(path, columnId, type.getMaximumLength(), conf);
+        return new VarcharTreeReader(columnId, type.getMaximumLength());
       case BINARY:
-        return new BinaryTreeReader(path, columnId, conf);
+        return new BinaryTreeReader(columnId);
       case TIMESTAMP:
-        return new TimestampTreeReader(path, columnId, conf);
+        return new TimestampTreeReader(columnId, skipCorrupt);
       case DATE:
-        return new DateTreeReader(path, columnId, conf);
+        return new DateTreeReader(columnId);
       case DECIMAL:
         int precision = type.hasPrecision() ? type.getPrecision() : HiveDecimal.SYSTEM_DEFAULT_PRECISION;
         int scale =  type.hasScale()? type.getScale() : HiveDecimal.SYSTEM_DEFAULT_SCALE;
-        return new DecimalTreeReader(path, columnId, precision, scale, conf);
+        return new DecimalTreeReader(columnId, precision, scale);
       case STRUCT:
-        return new StructTreeReader(path, columnId, types, included, conf);
+        return new StructTreeReader(columnId, types, included, skipCorrupt);
       case LIST:
-        return new ListTreeReader(path, columnId, types, included, conf);
+        return new ListTreeReader(columnId, types, included, skipCorrupt);
       case MAP:
-        return new MapTreeReader(path, columnId, types, included, conf);
+        return new MapTreeReader(columnId, types, included, skipCorrupt);
       case UNION:
-        return new UnionTreeReader(path, columnId, types, included, conf);
+        return new UnionTreeReader(columnId, types, included, skipCorrupt);
       default:
         throw new IllegalArgumentException("Unsupported type " +
           type.getKind());
     }
   }
 
-  OrcProto.StripeFooter readStripeFooter(StripeInformation stripe
-                                         ) throws IOException {
-    long offset = stripe.getOffset() + stripe.getIndexLength() +
-        stripe.getDataLength();
-    int tailLength = (int) stripe.getFooterLength();
-
-    // read the footer
-    ByteBuffer tailBuf = ByteBuffer.allocate(tailLength);
-    file.seek(offset);
-    file.readFully(tailBuf.array(), tailBuf.arrayOffset(), tailLength);
-    return OrcProto.StripeFooter.parseFrom(InStream.create("footer",
-        new ByteBuffer[]{tailBuf}, new long[]{0}, tailLength, codec,
-        bufferSize));
+  OrcProto.StripeFooter readStripeFooter(StripeInformation stripe) throws IOException {
+    return metadata.readStripeFooter(stripe);
   }
 
   static enum Location {
@@ -2692,58 +2844,95 @@ private static Object getConvertedStatsObj(Object statsObj, Object predObj) {
     return statsObj;
   }
 
+  public static class SargApplier {
+    private final SearchArgument sarg;
+    private final List<PredicateLeaf> sargLeaves;
+    private final int[] filterColumns;
+    private final long rowIndexStride;
+    private final OrcProto.BloomFilterIndex[] bloomFilterIndices;
+    // same as the above array, but indices are set to true
+    private final boolean[] sargColumns;
+    public SargApplier(SearchArgument sarg, String[] columnNames, long rowIndexStride,
+        List<OrcProto.Type> types, int includedCount) {
+      this.sarg = sarg;
+      sargLeaves = sarg.getLeaves();
+      filterColumns = mapSargColumns(sargLeaves, columnNames, 0);
+      bloomFilterIndices = new OrcProto.BloomFilterIndex[types.size()];
+      this.rowIndexStride = rowIndexStride;
+      // included will not be null, row options will fill the array with trues if null
+      sargColumns = new boolean[includedCount];
+      for (int i : filterColumns) {
+        // filter columns may have -1 as index which could be partition column in SARG.
+        if (i > 0) {
+          sargColumns[i] = true;
+        }
+      }
+    }
+
+    /**
+     * Pick the row groups that we need to load from the current stripe.
+     *
+     * @return an array with a boolean for each row group or null if all of the
+     * row groups must be read.
+     * @throws IOException
+     */
+    public boolean[] pickRowGroups(
+        StripeInformation stripe, OrcProto.RowIndex[] indexes) throws IOException {
+      long rowsInStripe = stripe.getNumberOfRows();
+      int groupsInStripe = (int) ((rowsInStripe + rowIndexStride - 1) / rowIndexStride);
+      boolean[] result = new boolean[groupsInStripe]; // TODO: avoid alloc?
+      TruthValue[] leafValues = new TruthValue[sargLeaves.size()];
+      for (int rowGroup = 0; rowGroup < result.length; ++rowGroup) {
+        for (int pred = 0; pred < leafValues.length; ++pred) {
+          if (filterColumns[pred] != -1) {
+            OrcProto.ColumnStatistics stats =
+                indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();
+            OrcProto.BloomFilter bf = null;
+            if (bloomFilterIndices[filterColumns[pred]] != null) {
+              bf = bloomFilterIndices[filterColumns[pred]].getBloomFilter(rowGroup);
+            }
+            leafValues[pred] = evaluatePredicateProto(stats, sargLeaves.get(pred), bf);
+            if (LOG.isDebugEnabled()) {
+              LOG.debug("Stats = " + stats);
+              LOG.debug("Setting " + sargLeaves.get(pred) + " to " +
+                  leafValues[pred]);
+            }
+          } else {
+            // the column is a virtual column
+            leafValues[pred] = TruthValue.YES_NO_NULL;
+          }
+        }
+        result[rowGroup] = sarg.evaluate(leafValues).isNeeded();
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Row group " + (rowIndexStride * rowGroup) + " to " +
+              (rowIndexStride * (rowGroup + 1) - 1) + " is " +
+              (result[rowGroup] ? "" : "not ") + "included.");
+        }
+      }
+
+      // if we found something to skip, use the array. otherwise, return null.
+      for (boolean b : result) {
+        if (!b) {
+          return result;
+        }
+      }
+      return null;
+    }
+  }
+
   /**
    * Pick the row groups that we need to load from the current stripe.
    * @return an array with a boolean for each row group or null if all of the
    *    row groups must be read.
    * @throws IOException
    */
-  private boolean[] pickRowGroups() throws IOException {
+  protected boolean[] pickRowGroups() throws IOException {
     // if we don't have a sarg or indexes, we read everything
-    if (sarg == null || rowIndexStride == 0) {
+    if (sargApp == null) {
       return null;
     }
-    readRowIndex(currentStripe, sargColumns);
-    long rowsInStripe = stripes.get(currentStripe).getNumberOfRows();
-    int groupsInStripe = (int) ((rowsInStripe + rowIndexStride - 1) /
-        rowIndexStride);
-    boolean[] result = new boolean[groupsInStripe];
-    TruthValue[] leafValues = new TruthValue[sargLeaves.size()];
-    for(int rowGroup=0; rowGroup < result.length; ++rowGroup) {
-      for(int pred=0; pred < leafValues.length; ++pred) {
-        if (filterColumns[pred] != -1) {
-          OrcProto.ColumnStatistics stats =
-              indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();
-          OrcProto.BloomFilter bf = null;
-          if (bloomFilterIndices[filterColumns[pred]] != null) {
-            bf = bloomFilterIndices[filterColumns[pred]].getBloomFilter(rowGroup);
-          }
-          leafValues[pred] = evaluatePredicateProto(stats, sargLeaves.get(pred), bf);
-          if (LOG.isDebugEnabled()) {
-            LOG.debug("Stats = " + stats);
-            LOG.debug("Setting " + sargLeaves.get(pred) + " to " +
-                leafValues[pred]);
-          }
-        } else {
-          // the column is a virtual column
-          leafValues[pred] = TruthValue.YES_NO_NULL;
-        }
-      }
-      result[rowGroup] = sarg.evaluate(leafValues).isNeeded();
-      if (LOG.isDebugEnabled()) {
-        LOG.debug("Row group " + (rowIndexStride * rowGroup) + " to " +
-            (rowIndexStride * (rowGroup+1) - 1) + " is " +
-            (result[rowGroup] ? "" : "not ") + "included.");
-      }
-    }
-
-    // if we found something to skip, use the array. otherwise, return null.
-    for(boolean b: result) {
-      if (!b) {
-        return result;
-      }
-    }
-    return null;
+    readRowIndex(currentStripe, included, sargApp.sargColumns);
+    return sargApp.pickRowGroups(stripes.get(currentStripe), indexes);
   }
 
   private void clearStreams() throws IOException {
@@ -2751,13 +2940,14 @@ private void clearStreams() throws IOException {
     for(InStream is: streams.values()) {
       is.close();
     }
-    if(bufferChunks != null) {
-      if(zcr != null) {
-        for (BufferChunk bufChunk : bufferChunks) {
-          zcr.releaseBuffer(bufChunk.chunk);
+    if (bufferChunks != null) {
+      if (zcr != null) {
+        for (DiskRangeList range = bufferChunks; range != null; range = range.next) {
+          if (!(range instanceof BufferChunk)) continue;
+          zcr.releaseBuffer(((BufferChunk)range).chunk);
         }
       }
-      bufferChunks.clear();
+      bufferChunks = null;
     }
     streams.clear();
   }
@@ -2767,20 +2957,7 @@ private void clearStreams() throws IOException {
    * @throws IOException
    */
   private void readStripe() throws IOException {
-    StripeInformation stripe = stripes.get(currentStripe);
-    stripeFooter = readStripeFooter(stripe);
-    clearStreams();
-    // setup the position in the stripe
-    rowCountInStripe = stripe.getNumberOfRows();
-    rowInStripe = 0;
-    rowBaseInStripe = 0;
-    for(int i=0; i < currentStripe; ++i) {
-      rowBaseInStripe += stripes.get(i).getNumberOfRows();
-    }
-    // reset all of the indexes
-    for(int i=0; i < indexes.length; ++i) {
-      indexes[i] = null;
-    }
+    StripeInformation stripe = beginReadStripe();
     includedRowGroups = pickRowGroups();
 
     // move forward to the first unskipped row
@@ -2802,167 +2979,88 @@ private void readStripe() throws IOException {
       reader.startStripe(streams, stripeFooter.getColumnsList());
       // if we skipped the first row group, move the pointers forward
       if (rowInStripe != 0) {
-        seekToRowEntry((int) (rowInStripe / rowIndexStride));
+        seekToRowEntry(reader, (int) (rowInStripe / rowIndexStride));
       }
     }
   }
 
-  private void readAllDataStreams(StripeInformation stripe
-                                  ) throws IOException {
+  private StripeInformation beginReadStripe() throws IOException {
+    StripeInformation stripe = stripes.get(currentStripe);
+    stripeFooter = readStripeFooter(stripe);
+    clearStreams();
+    // setup the position in the stripe
+    rowCountInStripe = stripe.getNumberOfRows();
+    rowInStripe = 0;
+    rowBaseInStripe = 0;
+    for(int i=0; i < currentStripe; ++i) {
+      rowBaseInStripe += stripes.get(i).getNumberOfRows();
+    }
+    // reset all of the indexes
+    for(int i=0; i < indexes.length; ++i) {
+      indexes[i] = null;
+    }
+    return stripe;
+  }
+
+  private void readAllDataStreams(StripeInformation stripe) throws IOException {
     long start = stripe.getIndexLength();
     long end = start + stripe.getDataLength();
     // explicitly trigger 1 big read
-    DiskRange[] ranges = new DiskRange[]{new DiskRange(start, end)};
-    bufferChunks = readDiskRanges(file, stripe.getOffset(), Arrays.asList(ranges));
+    DiskRangeList toRead = new DiskRangeList(start, end);
+    bufferChunks = RecordReaderUtils.readDiskRanges(file, zcr, stripe.getOffset(), toRead, false);
     List<OrcProto.Stream> streamDescriptions = stripeFooter.getStreamsList();
-    createStreams(streamDescriptions, bufferChunks, null, codec, bufferSize, streams);
-  }
-
-  /**
-   * The sections of stripe that we need to read.
-   */
-  static class DiskRange {
-    /** the first address we need to read. */
-    long offset;
-    /** the first address afterwards. */
-    long end;
-
-    DiskRange(long offset, long end) {
-      this.offset = offset;
-      this.end = end;
-      if (end < offset) {
-        throw new IllegalArgumentException("invalid range " + this);
-      }
-    }
-
-    @Override
-    public boolean equals(Object other) {
-      if (other == null || other.getClass() != getClass()) {
-        return false;
-      }
-      DiskRange otherR = (DiskRange) other;
-      return otherR.offset == offset && otherR.end == end;
-    }
-
-    @Override
-    public String toString() {
-      return "range start: " + offset + " end: " + end;
-    }
+    createStreams(
+        streamDescriptions, bufferChunks, null, codec, bufferSize, streams);
   }
 
   /**
    * The sections of stripe that we have read.
    * This might not match diskRange - 1 disk range can be multiple buffer chunks, depending on DFS block boundaries.
    */
-  static class BufferChunk {
+  public static class BufferChunk extends DiskRangeList {
     final ByteBuffer chunk;
-    /** the first address we need to read. */
-    final long offset;
-    /** end of the buffer **/
-    final long end;
 
     BufferChunk(ByteBuffer chunk, long offset) {
-      this.offset = offset;
+      super(offset, offset + chunk.remaining());
       this.chunk = chunk;
-      end = offset + chunk.remaining();
     }
 
     @Override
-    public final String toString() {
-      return "range start: " + offset + " size: " + chunk.remaining() + " type: "
-          + (chunk.isDirect() ? "direct" : "array-backed");
+    public boolean hasData() {
+      return chunk != null;
     }
-  }
 
-  private static final int BYTE_STREAM_POSITIONS = 1;
-  private static final int RUN_LENGTH_BYTE_POSITIONS =
-      BYTE_STREAM_POSITIONS + 1;
-  private static final int BITFIELD_POSITIONS = RUN_LENGTH_BYTE_POSITIONS + 1;
-  private static final int RUN_LENGTH_INT_POSITIONS =
-    BYTE_STREAM_POSITIONS + 1;
-
-  /**
-   * Get the offset in the index positions for the column that the given
-   * stream starts.
-   * @param encoding the encoding of the column
-   * @param type the type of the column
-   * @param stream the kind of the stream
-   * @param isCompressed is the file compressed
-   * @param hasNulls does the column have a PRESENT stream?
-   * @return the number of positions that will be used for that stream
-   */
-  static int getIndexPosition(OrcProto.ColumnEncoding.Kind encoding,
-                              OrcProto.Type.Kind type,
-                              OrcProto.Stream.Kind stream,
-                              boolean isCompressed,
-                              boolean hasNulls) {
-    if (stream == OrcProto.Stream.Kind.PRESENT) {
-      return 0;
-    }
-    int compressionValue = isCompressed ? 1 : 0;
-    int base = hasNulls ? (BITFIELD_POSITIONS + compressionValue) : 0;
-    switch (type) {
-      case BOOLEAN:
-      case BYTE:
-      case SHORT:
-      case INT:
-      case LONG:
-      case FLOAT:
-      case DOUBLE:
-      case DATE:
-      case STRUCT:
-      case MAP:
-      case LIST:
-      case UNION:
-        return base;
-      case CHAR:
-      case VARCHAR:
-      case STRING:
-        if (encoding == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
-            encoding == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
-          return base;
-        } else {
-          if (stream == OrcProto.Stream.Kind.DATA) {
-            return base;
-          } else {
-            return base + BYTE_STREAM_POSITIONS + compressionValue;
-          }
-        }
-      case BINARY:
-        if (stream == OrcProto.Stream.Kind.DATA) {
-          return base;
-        }
-        return base + BYTE_STREAM_POSITIONS + compressionValue;
-      case DECIMAL:
-        if (stream == OrcProto.Stream.Kind.DATA) {
-          return base;
-        }
-        return base + BYTE_STREAM_POSITIONS + compressionValue;
-      case TIMESTAMP:
-        if (stream == OrcProto.Stream.Kind.DATA) {
-          return base;
-        }
-        return base + RUN_LENGTH_INT_POSITIONS + compressionValue;
-      default:
-        throw new IllegalArgumentException("Unknown type " + type);
+    @Override
+    public final String toString() {
+      boolean makesSense = chunk.remaining() == (end - offset);
+      return "data range [" + offset + ", " + end + "), size: " + chunk.remaining()
+          + (makesSense ? "" : "(!)") + " type: " + (chunk.isDirect() ? "direct" : "array-backed");
     }
-  }
 
-  // for uncompressed streams, what is the most overlap with the following set
-  // of rows (long vint literal group).
-  static final int WORST_UNCOMPRESSED_SLOP = 2 + 8 * 512;
+    @Override
+    public DiskRange sliceAndShift(long offset, long end, long shiftBy) {
+      assert offset <= end && offset >= this.offset && end <= this.end;
+      assert offset + shiftBy >= 0;
+      ByteBuffer sliceBuf = chunk.slice();
+      int newPos = (int)(offset - this.offset);
+      int newLimit = newPos + (int)(end - offset);
+      try {
+        sliceBuf.position(newPos);
+        sliceBuf.limit(newLimit);
+      } catch (Throwable t) {
+        LOG.error("Failed to slice buffer chunk with range" + " [" + this.offset + ", " + this.end
+            + "), position: " + chunk.position() + " limit: " + chunk.limit() + ", "
+            + (chunk.isDirect() ? "direct" : "array") + "; to [" + offset + ", " + end + ") "
+            + t.getClass());
+        throw new RuntimeException(t);
+      }
+      return new BufferChunk(sliceBuf, offset + shiftBy);
+    }
 
-  /**
-   * Is this stream part of a dictionary?
-   * @return is this part of a dictionary?
-   */
-  static boolean isDictionary(OrcProto.Stream.Kind kind,
-                              OrcProto.ColumnEncoding encoding) {
-    OrcProto.ColumnEncoding.Kind encodingKind = encoding.getKind();
-    return kind == OrcProto.Stream.Kind.DICTIONARY_DATA ||
-      (kind == OrcProto.Stream.Kind.LENGTH &&
-       (encodingKind == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
-        encodingKind == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2));
+    @Override
+    public ByteBuffer getData() {
+      return chunk;
+    }
   }
 
   /**
@@ -2978,7 +3076,7 @@ static boolean isDictionary(OrcProto.Stream.Kind kind,
    * @param compressionSize the compression block size
    * @return the list of disk ranges that will be loaded
    */
-  static List<DiskRange> planReadPartialDataStreams
+  static DiskRangeList planReadPartialDataStreams
       (List<OrcProto.Stream> streamList,
        OrcProto.RowIndex[] indexes,
        boolean[] includedColumns,
@@ -2986,17 +3084,13 @@ static boolean isDictionary(OrcProto.Stream.Kind kind,
        boolean isCompressed,
        List<OrcProto.ColumnEncoding> encodings,
        List<OrcProto.Type> types,
-       int compressionSize) {
-    List<DiskRange> result = new ArrayList<DiskRange>();
+       int compressionSize,
+       boolean doMergeBuffers) {
     long offset = 0;
     // figure out which columns have a present stream
-    boolean[] hasNull = new boolean[types.size()];
-    for(OrcProto.Stream stream: streamList) {
-      if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.PRESENT)) {
-        hasNull[stream.getColumn()] = true;
-      }
-    }
-    for(OrcProto.Stream stream: streamList) {
+    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);
+    DiskRangeListCreateHelper list = new DiskRangeListCreateHelper();
+    for (OrcProto.Stream stream : streamList) {
       long length = stream.getLength();
       int column = stream.getColumn();
       OrcProto.Stream.Kind streamKind = stream.getKind();
@@ -3005,203 +3099,58 @@ static boolean isDictionary(OrcProto.Stream.Kind kind,
           (StreamName.getArea(streamKind) == StreamName.Area.DATA) &&
           includedColumns[column]) {
         // if we aren't filtering or it is a dictionary, load it.
-        if (includedRowGroups == null ||
-            isDictionary(streamKind, encodings.get(column))) {
-          result.add(new DiskRange(offset, offset + length));
+        if (includedRowGroups == null
+            || RecordReaderUtils.isDictionary(streamKind, encodings.get(column))) {
+          RecordReaderUtils.addEntireStreamToRanges(offset, length, list, doMergeBuffers);
         } else {
-          for(int group=0; group < includedRowGroups.length; ++group) {
-            if (includedRowGroups[group]) {
-              int posn = getIndexPosition(encodings.get(column).getKind(),
-                  types.get(column).getKind(), stream.getKind(), isCompressed,
-                  hasNull[column]);
-              long start = indexes[column].getEntry(group).getPositions(posn);
-              final long nextGroupOffset;
-              if (group < includedRowGroups.length - 1) {
-                nextGroupOffset = indexes[column].getEntry(group + 1).getPositions(posn);
-              } else {
-                nextGroupOffset = length;
-              }
-
-              // figure out the worst case last location
-
-              // if adjacent groups have the same compressed block offset then stretch the slop
-              // by factor of 2 to safely accommodate the next compression block.
-              // One for the current compression block and another for the next compression block.
-              final long slop = isCompressed ? 2 * (OutStream.HEADER_SIZE + compressionSize)
-                  : WORST_UNCOMPRESSED_SLOP;
-              long end = (group == includedRowGroups.length - 1) ? length : Math.min(length,
-                  nextGroupOffset + slop);
-              result.add(new DiskRange(offset + start, offset + end));
-            }
-          }
+          RecordReaderUtils.addRgFilteredStreamToRanges(stream, includedRowGroups,
+              isCompressed, indexes[column], encodings.get(column), types.get(column),
+              compressionSize, hasNull[column], offset, length, list, doMergeBuffers);
         }
       }
       offset += length;
     }
-    return result;
-  }
-
-  /**
-   * Update the disk ranges to collapse adjacent or overlapping ranges. It
-   * assumes that the ranges are sorted.
-   * @param ranges the list of disk ranges to merge
-   */
-  static void mergeDiskRanges(List<DiskRange> ranges) {
-    DiskRange prev = null;
-    for(int i=0; i < ranges.size(); ++i) {
-      DiskRange current = ranges.get(i);
-      if (prev != null && overlap(prev.offset, prev.end,
-          current.offset, current.end)) {
-        prev.offset = Math.min(prev.offset, current.offset);
-        prev.end = Math.max(prev.end, current.end);
-        ranges.remove(i);
-        i -= 1;
-      } else {
-        prev = current;
-      }
-    }
-  }
-
-  /**
-   * Read the list of ranges from the file.
-   * @param file the file to read
-   * @param base the base of the stripe
-   * @param ranges the disk ranges within the stripe to read
-   * @return the bytes read for each disk range, which is the same length as
-   *    ranges
-   * @throws IOException
-   */
-  List<BufferChunk> readDiskRanges(FSDataInputStream file,
-                                 long base,
-                                 List<DiskRange> ranges) throws IOException {
-    ArrayList<BufferChunk> result = new ArrayList<RecordReaderImpl.BufferChunk>(ranges.size());
-    for(DiskRange range: ranges) {
-      int len = (int) (range.end - range.offset);
-      long off = range.offset;
-      file.seek(base + off);
-      if(zcr != null) {
-        while(len > 0) {
-          ByteBuffer partial = zcr.readBuffer(len, false);
-          result.add(new BufferChunk(partial, off));
-          int read = partial.remaining();
-          len -= read;
-          off += read;
-        }
-      } else {
-        byte[] buffer = new byte[len];
-        file.readFully(buffer, 0, buffer.length);
-        result.add(new BufferChunk(ByteBuffer.wrap(buffer), range.offset));
-      }
-    }
-    return result;
-  }
-
-  /**
-   * Does region A overlap region B? The end points are inclusive on both sides.
-   * @param leftA A's left point
-   * @param rightA A's right point
-   * @param leftB B's left point
-   * @param rightB B's right point
-   * @return Does region A overlap region B?
-   */
-  static boolean overlap(long leftA, long rightA, long leftB, long rightB) {
-    if (leftA <= leftB) {
-      return rightA >= leftB;
-    }
-    return rightB >= leftA;
-  }
-
-  /**
-   * Build a string representation of a list of disk ranges.
-   * @param ranges ranges to stringify
-   * @return the resulting string
-   */
-  static String stringifyDiskRanges(List<DiskRange> ranges) {
-    StringBuilder buffer = new StringBuilder();
-    buffer.append("[");
-    for(int i=0; i < ranges.size(); ++i) {
-      if (i != 0) {
-        buffer.append(", ");
-      }
-      buffer.append(ranges.get(i).toString());
-    }
-    buffer.append("]");
-    return buffer.toString();
+    return list.extract();
   }
 
-  static void createStreams(List<OrcProto.Stream> streamDescriptions,
-                            List<BufferChunk> ranges,
+  void createStreams(List<OrcProto.Stream> streamDescriptions,
+                            DiskRangeList ranges,
                             boolean[] includeColumn,
                             CompressionCodec codec,
                             int bufferSize,
-                            Map<StreamName, InStream> streams
-                           ) throws IOException {
-    long offset = 0;
-    for(OrcProto.Stream streamDesc: streamDescriptions) {
+                            Map<StreamName, InStream> streams) throws IOException {
+    long streamOffset = 0;
+    for (OrcProto.Stream streamDesc: streamDescriptions) {
       int column = streamDesc.getColumn();
-      // do not create stream if stream kind does not exist
-      if ((includeColumn == null || includeColumn[column]) &&
+      if ((includeColumn != null && !includeColumn[column]) ||
           streamDesc.hasKind() &&
-          (StreamName.getArea(streamDesc.getKind()) == StreamName.Area.DATA)) {
-        long length = streamDesc.getLength();
-        int first = -1;
-        int last = -2;
-        for(int i=0; i < ranges.size(); ++i) {
-          BufferChunk range = ranges.get(i);
-          if (overlap(offset, offset+length, range.offset, range.end)) {
-            if (first == -1) {
-              first = i;
-            }
-            last = i;
-          }
-        }
-        ByteBuffer[] buffers = new ByteBuffer[last - first + 1];
-        long[] offsets = new long[last - first + 1];
-        for(int i=0; i < buffers.length; ++i) {
-          BufferChunk range = ranges.get(i + first);
-          long start = Math.max(range.offset, offset);
-          long end = Math.min(range.end, offset+length);
-          buffers[i] = range.chunk.slice();
-          assert range.chunk.position() == 0; // otherwise we'll mix up positions
-          /*
-           * buffers are positioned in-wards if the offset > range.offset
-           * offsets[i] == range.offset - offset, except if offset > range.offset
-           */
-          if(offset > range.offset) {
-            buffers[i].position((int)(offset - range.offset));
-            buffers[i].limit((int)(end - range.offset));
-            offsets[i] = 0;
-          } else {
-            buffers[i].position(0);
-            buffers[i].limit((int)(end - range.offset));
-            offsets[i] = (range.offset - offset);
-          }
-        }
-        StreamName name = new StreamName(column, streamDesc.getKind());
-        streams.put(name, InStream.create(name.toString(), buffers, offsets,
-            length, codec, bufferSize));
+          (StreamName.getArea(streamDesc.getKind()) != StreamName.Area.DATA)) {
+        streamOffset += streamDesc.getLength();
+        continue;
       }
-      offset += streamDesc.getLength();
+      List<DiskRange> buffers = RecordReaderUtils.getStreamBuffers(
+          ranges, streamOffset, streamDesc.getLength());
+      StreamName name = new StreamName(column, streamDesc.getKind());
+      streams.put(name, InStream.create(name.toString(), buffers,
+          streamDesc.getLength(), codec, bufferSize));
+      streamOffset += streamDesc.getLength();
     }
   }
 
-  private void readPartialDataStreams(StripeInformation stripe
-                                      ) throws IOException {
+  private void readPartialDataStreams(StripeInformation stripe) throws IOException {
     List<OrcProto.Stream> streamList = stripeFooter.getStreamsList();
-    List<DiskRange> chunks =
-        planReadPartialDataStreams(streamList,
+    DiskRangeList toRead = planReadPartialDataStreams(streamList,
             indexes, included, includedRowGroups, codec != null,
-            stripeFooter.getColumnsList(), types, bufferSize);
+            stripeFooter.getColumnsList(), types, bufferSize, true);
     if (LOG.isDebugEnabled()) {
-      LOG.debug("chunks = " + stringifyDiskRanges(chunks));
+      LOG.debug("chunks = " + RecordReaderUtils.stringifyDiskRanges(toRead));
     }
-    mergeDiskRanges(chunks);
+    bufferChunks = RecordReaderUtils.readDiskRanges(file, zcr, stripe.getOffset(), toRead, false);
     if (LOG.isDebugEnabled()) {
-      LOG.debug("merge = " + stringifyDiskRanges(chunks));
+      LOG.debug("merge = " + RecordReaderUtils.stringifyDiskRanges(bufferChunks));
     }
-    bufferChunks = readDiskRanges(file, stripe.getOffset(), chunks);
-    createStreams(streamList, bufferChunks, included, codec, bufferSize,
-        streams);
+
+    createStreams(streamList, bufferChunks, included, codec, bufferSize, streams);
   }
 
   @Override
@@ -3228,7 +3177,8 @@ private void advanceStripe() throws IOException {
    * @param nextRow the row we want to go to
    * @throws IOException
    */
-  private void advanceToNextRow(long nextRow) throws IOException {
+  private boolean advanceToNextRow(
+      TreeReader reader, long nextRow, boolean canAdvanceStripe) throws IOException {
     long nextRowInStripe = nextRow - rowBaseInStripe;
     // check for row skipping
     if (rowIndexStride != 0 &&
@@ -3236,32 +3186,35 @@ private void advanceToNextRow(long nextRow) throws IOException {
         nextRowInStripe < rowCountInStripe) {
       int rowGroup = (int) (nextRowInStripe / rowIndexStride);
       if (!includedRowGroups[rowGroup]) {
-        while (rowGroup < includedRowGroups.length &&
-               !includedRowGroups[rowGroup]) {
+        while (rowGroup < includedRowGroups.length && !includedRowGroups[rowGroup]) {
           rowGroup += 1;
         }
-        // if we are off the end of the stripe, just move stripes
         if (rowGroup >= includedRowGroups.length) {
-          advanceStripe();
-          return;
+          if (canAdvanceStripe) {
+            advanceStripe();
+          }
+          return canAdvanceStripe;
         }
         nextRowInStripe = Math.min(rowCountInStripe, rowGroup * rowIndexStride);
       }
     }
-    if (nextRowInStripe < rowCountInStripe) {
-      if (nextRowInStripe != rowInStripe) {
-        if (rowIndexStride != 0) {
-          int rowGroup = (int) (nextRowInStripe / rowIndexStride);
-          seekToRowEntry(rowGroup);
-          reader.skipRows(nextRowInStripe - rowGroup * rowIndexStride);
-        } else {
-          reader.skipRows(nextRowInStripe - rowInStripe);
-        }
-        rowInStripe = nextRowInStripe;
+    if (nextRowInStripe >= rowCountInStripe) {
+      if (canAdvanceStripe) {
+        advanceStripe();
       }
-    } else {
-      advanceStripe();
+      return canAdvanceStripe;
+    }
+    if (nextRowInStripe != rowInStripe) {
+      if (rowIndexStride != 0) {
+        int rowGroup = (int) (nextRowInStripe / rowIndexStride);
+        seekToRowEntry(reader, rowGroup);
+        reader.skipRows(nextRowInStripe - rowGroup * rowIndexStride);
+      } else {
+        reader.skipRows(nextRowInStripe - rowInStripe);
+      }
+      rowInStripe = nextRowInStripe;
     }
+    return true;
   }
 
   @Override
@@ -3270,11 +3223,7 @@ public Object next(Object previous) throws IOException {
       final Object result = reader.next(previous);
       // find the next row
       rowInStripe += 1;
-      advanceToNextRow(rowInStripe + rowBaseInStripe);
-      if (isLogTraceEnabled) {
-        LOG.trace("row from " + reader.path);
-        LOG.trace("orc row = " + result);
-      }
+      advanceToNextRow(reader, rowInStripe + rowBaseInStripe, true);
       return result;
     } catch (IOException e) {
       // Rethrow exception with file name in log message
@@ -3291,37 +3240,7 @@ public VectorizedRowBatch nextBatch(VectorizedRowBatch previous) throws IOExcept
         readStripe();
       }
 
-      long batchSize = 0;
-
-      // In case of PPD, batch size should be aware of row group boundaries. If only a subset of row
-      // groups are selected then marker position is set to the end of range (subset of row groups
-      // within strip). Batch size computed out of marker position makes sure that batch size is
-      // aware of row group boundary and will not cause overflow when reading rows
-      // illustration of this case is here https://issues.apache.org/jira/browse/HIVE-6287
-      if (rowIndexStride != 0 && includedRowGroups != null && rowInStripe < rowCountInStripe) {
-        int startRowGroup = (int) (rowInStripe / rowIndexStride);
-        if (!includedRowGroups[startRowGroup]) {
-          while (startRowGroup < includedRowGroups.length && !includedRowGroups[startRowGroup]) {
-            startRowGroup += 1;
-          }
-        }
-
-        int endRowGroup = startRowGroup;
-        while (endRowGroup < includedRowGroups.length && includedRowGroups[endRowGroup]) {
-          endRowGroup += 1;
-        }
-
-        final long markerPosition =
-            (endRowGroup * rowIndexStride) < rowCountInStripe ? (endRowGroup * rowIndexStride)
-                : rowCountInStripe;
-        batchSize = Math.min(VectorizedRowBatch.DEFAULT_SIZE, (markerPosition - rowInStripe));
-
-        if (isLogDebugEnabled && batchSize < VectorizedRowBatch.DEFAULT_SIZE) {
-          LOG.debug("markerPosition: " + markerPosition + " batchSize: " + batchSize);
-        }
-      } else {
-        batchSize = Math.min(VectorizedRowBatch.DEFAULT_SIZE, (rowCountInStripe - rowInStripe));
-      }
+      long batchSize = computeBatchSize(VectorizedRowBatch.DEFAULT_SIZE);
 
       rowInStripe += batchSize;
       if (previous == null) {
@@ -3329,13 +3248,13 @@ public VectorizedRowBatch nextBatch(VectorizedRowBatch previous) throws IOExcept
         result = new VectorizedRowBatch(cols.length);
         result.cols = cols;
       } else {
-        result = previous;
+        result = (VectorizedRowBatch) previous;
         result.selectedInUse = false;
         reader.nextVector(result.cols, (int) batchSize);
       }
 
       result.size = (int) batchSize;
-      advanceToNextRow(rowInStripe + rowBaseInStripe);
+      advanceToNextRow(reader, rowInStripe + rowBaseInStripe, true);
       return result;
     } catch (IOException e) {
       // Rethrow exception with file name in log message
@@ -3343,6 +3262,41 @@ public VectorizedRowBatch nextBatch(VectorizedRowBatch previous) throws IOExcept
     }
   }
 
+  private long computeBatchSize(long targetBatchSize) {
+    long batchSize = 0;
+    // In case of PPD, batch size should be aware of row group boundaries. If only a subset of row
+    // groups are selected then marker position is set to the end of range (subset of row groups
+    // within strip). Batch size computed out of marker position makes sure that batch size is
+    // aware of row group boundary and will not cause overflow when reading rows
+    // illustration of this case is here https://issues.apache.org/jira/browse/HIVE-6287
+    if (rowIndexStride != 0 && includedRowGroups != null && rowInStripe < rowCountInStripe) {
+      int startRowGroup = (int) (rowInStripe / rowIndexStride);
+      if (!includedRowGroups[startRowGroup]) {
+        while (startRowGroup < includedRowGroups.length && !includedRowGroups[startRowGroup]) {
+          startRowGroup += 1;
+        }
+      }
+
+      int endRowGroup = startRowGroup;
+      while (endRowGroup < includedRowGroups.length && includedRowGroups[endRowGroup]) {
+        endRowGroup += 1;
+      }
+
+      final long markerPosition =
+          (endRowGroup * rowIndexStride) < rowCountInStripe ? (endRowGroup * rowIndexStride)
+              : rowCountInStripe;
+      batchSize = Math.min(targetBatchSize, (markerPosition - rowInStripe));
+
+      if (isLogDebugEnabled && batchSize < targetBatchSize) {
+        LOG.debug("markerPosition: " + markerPosition + " batchSize: " + batchSize);
+      }
+    } else {
+      batchSize = Math.min(targetBatchSize, (rowCountInStripe - rowInStripe));
+    }
+    return batchSize;
+  }
+
+
   @Override
   public void close() throws IOException {
     clearStreams();
@@ -3376,65 +3330,32 @@ private int findStripe(long rowNumber) {
     throw new IllegalArgumentException("Seek after the end of reader range");
   }
 
-  Index readRowIndex(int stripeIndex, boolean[] sargColumns) throws IOException {
-    long offset = stripes.get(stripeIndex).getOffset();
-    OrcProto.StripeFooter stripeFooter;
-    OrcProto.RowIndex[] indexes;
+  Index readRowIndex(
+      int stripeIndex, boolean[] included, boolean[] sargColumns) throws IOException {
+    return readRowIndex(stripeIndex, included, null, null, sargColumns);
+  }
+
+  Index readRowIndex(int stripeIndex, boolean[] included, OrcProto.RowIndex[] indexes,
+      OrcProto.BloomFilterIndex[] bloomFilterIndex, boolean[] sargColumns) throws IOException {
+    StripeInformation stripe = stripes.get(stripeIndex);
+    OrcProto.StripeFooter stripeFooter = null;
     // if this is the current stripe, use the cached objects.
     if (stripeIndex == currentStripe) {
       stripeFooter = this.stripeFooter;
-      indexes = this.indexes;
-    } else {
-      stripeFooter = readStripeFooter(stripes.get(stripeIndex));
-      indexes = new OrcProto.RowIndex[this.indexes.length];
-    }
-    List<OrcProto.Stream> streams = stripeFooter.getStreamsList();
-    for (int i = 0; i < streams.size(); i++) {
-      OrcProto.Stream stream = streams.get(i);
-      OrcProto.Stream nextStream = null;
-      if (i < streams.size() - 1) {
-        nextStream = streams.get(i+1);
-      }
-      int col = stream.getColumn();
-      int len = (int) stream.getLength();
-      // row index stream and bloom filter are interlaced, check if the sarg column contains bloom
-      // filter and combine the io to read row index and bloom filters for that column together
-      if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.ROW_INDEX)) {
-        boolean readBloomFilter = false;
-        if (sargColumns != null && sargColumns[col] &&
-            nextStream.getKind() == OrcProto.Stream.Kind.BLOOM_FILTER) {
-          len += nextStream.getLength();
-          i += 1;
-          readBloomFilter = true;
-        }
-        if ((included == null || included[col]) && indexes[col] == null) {
-          byte[] buffer = new byte[len];
-          file.seek(offset);
-          file.readFully(buffer);
-          ByteBuffer[] bb = new ByteBuffer[] {ByteBuffer.wrap(buffer)};
-          indexes[col] = OrcProto.RowIndex.parseFrom(InStream.create("index",
-              bb, new long[]{0}, stream.getLength(), codec, bufferSize));
-          if (readBloomFilter) {
-            bb[0].position((int) stream.getLength());
-            bloomFilterIndices[col] = OrcProto.BloomFilterIndex.parseFrom(
-                InStream.create("bloom_filter", bb, new long[]{0}, nextStream.getLength(),
-                    codec, bufferSize));
-          }
-        }
-      }
-      offset += len;
+      indexes = indexes == null ? this.indexes : indexes;
+      bloomFilterIndex = bloomFilterIndex == null ? this.bloomFilterIndices : bloomFilterIndex;
+      sargColumns = sargColumns == null ?
+          (sargApp == null ? null : sargApp.sargColumns) : sargColumns;
     }
-
-    Index index = new Index(indexes, bloomFilterIndices);
-    return index;
+    return metadata.readRowIndex(
+        stripe, stripeFooter, included, indexes, sargColumns, bloomFilterIndex);
   }
 
-  private void seekToRowEntry(int rowEntry) throws IOException {
+  private void seekToRowEntry(TreeReader reader, int rowEntry) throws IOException {
     PositionProvider[] index = new PositionProvider[indexes.length];
-    for(int i=0; i < indexes.length; ++i) {
+    for (int i = 0; i < indexes.length; ++i) {
       if (indexes[i] != null) {
-        index[i]=
-            new PositionProviderImpl(indexes[i].getEntry(rowEntry));
+        index[i] = new PositionProviderImpl(indexes[i].getEntry(rowEntry));
       }
     }
     reader.seek(index);
@@ -3444,10 +3365,10 @@ private void seekToRowEntry(int rowEntry) throws IOException {
   public void seekToRow(long rowNumber) throws IOException {
     if (rowNumber < 0) {
       throw new IllegalArgumentException("Seek to a negative row number " +
-                                         rowNumber);
+          rowNumber);
     } else if (rowNumber < firstRow) {
       throw new IllegalArgumentException("Seek before reader range " +
-                                         rowNumber);
+          rowNumber);
     }
     // convert to our internal form (rows from the beginning of slice)
     rowNumber -= firstRow;
@@ -3458,9 +3379,9 @@ public void seekToRow(long rowNumber) throws IOException {
       currentStripe = rightStripe;
       readStripe();
     }
-    readRowIndex(currentStripe, sargColumns);
+    readRowIndex(currentStripe, included, sargApp == null ? null : sargApp.sargColumns);
 
-    // if we aren't to the right row yet, advanance in the stripe.
-    advanceToNextRow(rowNumber);
+    // if we aren't to the right row yet, advance in the stripe.
+    advanceToNextRow(reader, rowNumber, true);
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderUtils.java
new file mode 100644
index 0000000000..01b07f2cf7
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderUtils.java
@@ -0,0 +1,434 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.io.orc;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.commons.lang.builder.HashCodeBuilder;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.DFSClient;
+import org.apache.hadoop.hdfs.DistributedFileSystem;
+import org.apache.hadoop.hive.common.DiskRange;
+import org.apache.hadoop.hive.common.DiskRangeList;
+import org.apache.hadoop.hive.common.DiskRangeList.DiskRangeListCreateHelper;
+import org.apache.hadoop.hive.common.DiskRangeList.DiskRangeListMutateHelper;
+import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.BufferChunk;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hadoop.hive.shims.HadoopShims.ByteBufferPoolShim;
+import org.apache.hadoop.hive.shims.HadoopShims.ZeroCopyReaderShim;
+
+import com.google.common.collect.ComparisonChain;
+
+/**
+ * Stateless methods shared between RecordReaderImpl and EncodedReaderImpl.
+ */
+public class RecordReaderUtils {
+  static boolean[] findPresentStreamsByColumn(
+      List<OrcProto.Stream> streamList, List<OrcProto.Type> types) {
+    boolean[] hasNull = new boolean[types.size()];
+    for(OrcProto.Stream stream: streamList) {
+      if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.PRESENT)) {
+        hasNull[stream.getColumn()] = true;
+      }
+    }
+    return hasNull;
+  }
+
+  /**
+   * Does region A overlap region B? The end points are inclusive on both sides.
+   * @param leftA A's left point
+   * @param rightA A's right point
+   * @param leftB B's left point
+   * @param rightB B's right point
+   * @return Does region A overlap region B?
+   */
+  static boolean overlap(long leftA, long rightA, long leftB, long rightB) {
+    if (leftA <= leftB) {
+      return rightA >= leftB;
+    }
+    return rightB >= leftA;
+  }
+
+  static void addEntireStreamToRanges(
+      long offset, long length, DiskRangeListCreateHelper list, boolean doMergeBuffers) {
+    list.addOrMerge(offset, offset + length, doMergeBuffers, false);
+  }
+
+  static void addRgFilteredStreamToRanges(OrcProto.Stream stream,
+      boolean[] includedRowGroups, boolean isCompressed, OrcProto.RowIndex index,
+      OrcProto.ColumnEncoding encoding, OrcProto.Type type, int compressionSize, boolean hasNull,
+      long offset, long length, DiskRangeListCreateHelper list, boolean doMergeBuffers) {
+    for (int group = 0; group < includedRowGroups.length; ++group) {
+      if (!includedRowGroups[group]) continue;
+      int posn = getIndexPosition(
+          encoding.getKind(), type.getKind(), stream.getKind(), isCompressed, hasNull);
+      long start = index.getEntry(group).getPositions(posn);
+      final long nextGroupOffset;
+      boolean isLast = group == (includedRowGroups.length - 1);
+      nextGroupOffset = isLast ? length : index.getEntry(group + 1).getPositions(posn);
+
+      start += offset;
+      long end = offset + estimateRgEndOffset(
+          isCompressed, isLast, nextGroupOffset, length, compressionSize);
+      list.addOrMerge(start, end, doMergeBuffers, true);
+    }
+  }
+
+  static long estimateRgEndOffset(boolean isCompressed, boolean isLast,
+      long nextGroupOffset, long streamLength, int bufferSize) {
+    // figure out the worst case last location
+    // if adjacent groups have the same compressed block offset then stretch the slop
+    // by factor of 2 to safely accommodate the next compression block.
+    // One for the current compression block and another for the next compression block.
+    long slop = isCompressed ? 2 * (OutStream.HEADER_SIZE + bufferSize) : WORST_UNCOMPRESSED_SLOP;
+    return isLast ? streamLength : Math.min(streamLength, nextGroupOffset + slop);
+  }
+
+  private static final int BYTE_STREAM_POSITIONS = 1;
+  private static final int RUN_LENGTH_BYTE_POSITIONS = BYTE_STREAM_POSITIONS + 1;
+  private static final int BITFIELD_POSITIONS = RUN_LENGTH_BYTE_POSITIONS + 1;
+  private static final int RUN_LENGTH_INT_POSITIONS = BYTE_STREAM_POSITIONS + 1;
+
+  /**
+   * Get the offset in the index positions for the column that the given
+   * stream starts.
+   * @param columnEncoding the encoding of the column
+   * @param columnType the type of the column
+   * @param streamType the kind of the stream
+   * @param isCompressed is the file compressed
+   * @param hasNulls does the column have a PRESENT stream?
+   * @return the number of positions that will be used for that stream
+   */
+  public static int getIndexPosition(OrcProto.ColumnEncoding.Kind columnEncoding,
+                              OrcProto.Type.Kind columnType,
+                              OrcProto.Stream.Kind streamType,
+                              boolean isCompressed,
+                              boolean hasNulls) {
+    if (streamType == OrcProto.Stream.Kind.PRESENT) {
+      return 0;
+    }
+    int compressionValue = isCompressed ? 1 : 0;
+    int base = hasNulls ? (BITFIELD_POSITIONS + compressionValue) : 0;
+    switch (columnType) {
+      case BOOLEAN:
+      case BYTE:
+      case SHORT:
+      case INT:
+      case LONG:
+      case FLOAT:
+      case DOUBLE:
+      case DATE:
+      case STRUCT:
+      case MAP:
+      case LIST:
+      case UNION:
+        return base;
+      case CHAR:
+      case VARCHAR:
+      case STRING:
+        if (columnEncoding == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
+            columnEncoding == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
+          return base;
+        } else {
+          if (streamType == OrcProto.Stream.Kind.DATA) {
+            return base;
+          } else {
+            return base + BYTE_STREAM_POSITIONS + compressionValue;
+          }
+        }
+      case BINARY:
+        if (streamType == OrcProto.Stream.Kind.DATA) {
+          return base;
+        }
+        return base + BYTE_STREAM_POSITIONS + compressionValue;
+      case DECIMAL:
+        if (streamType == OrcProto.Stream.Kind.DATA) {
+          return base;
+        }
+        return base + BYTE_STREAM_POSITIONS + compressionValue;
+      case TIMESTAMP:
+        if (streamType == OrcProto.Stream.Kind.DATA) {
+          return base;
+        }
+        return base + RUN_LENGTH_INT_POSITIONS + compressionValue;
+      default:
+        throw new IllegalArgumentException("Unknown type " + columnType);
+    }
+  }
+
+  // for uncompressed streams, what is the most overlap with the following set
+  // of rows (long vint literal group).
+  static final int WORST_UNCOMPRESSED_SLOP = 2 + 8 * 512;
+
+  /**
+   * Is this stream part of a dictionary?
+   * @return is this part of a dictionary?
+   */
+  static boolean isDictionary(OrcProto.Stream.Kind kind,
+                              OrcProto.ColumnEncoding encoding) {
+    assert kind != OrcProto.Stream.Kind.DICTIONARY_COUNT;
+    OrcProto.ColumnEncoding.Kind encodingKind = encoding.getKind();
+    return kind == OrcProto.Stream.Kind.DICTIONARY_DATA ||
+      (kind == OrcProto.Stream.Kind.LENGTH &&
+       (encodingKind == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
+        encodingKind == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2));
+  }
+
+  /**
+   * Build a string representation of a list of disk ranges.
+   * @param ranges ranges to stringify
+   * @return the resulting string
+   */
+  static String stringifyDiskRanges(DiskRangeList range) {
+    StringBuilder buffer = new StringBuilder();
+    buffer.append("[");
+    boolean isFirst = true;
+    while (range != null) {
+      if (!isFirst) {
+        buffer.append(", ");
+      }
+      isFirst = false;
+      buffer.append(range.toString());
+      range = range.next;
+    }
+    buffer.append("]");
+    return buffer.toString();
+  }
+
+  /**
+   * Read the list of ranges from the file.
+   * @param file the file to read
+   * @param base the base of the stripe
+   * @param ranges the disk ranges within the stripe to read
+   * @return the bytes read for each disk range, which is the same length as
+   *    ranges
+   * @throws IOException
+   */
+  static DiskRangeList readDiskRanges(FSDataInputStream file,
+                                 ZeroCopyReaderShim zcr,
+                                 long base,
+                                 DiskRangeList range,
+                                 boolean doForceDirect) throws IOException {
+    if (range == null) return null;
+    DiskRangeList prev = range.prev;
+    if (prev == null) {
+      prev = new DiskRangeListMutateHelper(range);
+    }
+    while (range != null) {
+      if (range.hasData()) {
+        range = range.next;
+        continue;
+      }
+      int len = (int) (range.getEnd() - range.getOffset());
+      long off = range.getOffset();
+      file.seek(base + off);
+      if (zcr != null) {
+        boolean hasReplaced = false;
+        while (len > 0) {
+          ByteBuffer partial = zcr.readBuffer(len, false);
+          BufferChunk bc = new BufferChunk(partial, off);
+          if (!hasReplaced) {
+            range.replaceSelfWith(bc);
+            hasReplaced = true;
+          } else {
+            range.insertAfter(bc);
+          }
+          range = bc;
+          int read = partial.remaining();
+          len -= read;
+          off += read;
+        }
+      } else if (doForceDirect) {
+        ByteBuffer directBuf = ByteBuffer.allocateDirect(len);
+        readDirect(file, len, directBuf, true);
+        range = range.replaceSelfWith(new BufferChunk(directBuf, range.getOffset()));
+      } else {
+        byte[] buffer = new byte[len];
+        file.readFully(buffer, 0, buffer.length);
+        range = range.replaceSelfWith(new BufferChunk(ByteBuffer.wrap(buffer), range.getOffset()));
+      }
+      range = range.next;
+    }
+    return prev.next;
+  }
+
+  public static void readDirect(FSDataInputStream file,
+      int len, ByteBuffer directBuf, boolean doSetLimit) throws IOException {
+    // TODO: HDFS API is a mess, so handle all kinds of cases.
+    // Before 2.7, read() also doesn't adjust position correctly, so track it separately.
+    int pos = directBuf.position(), startPos = pos, endPos = pos + len;
+    try {
+      while (pos < endPos) {
+        int count = file.read(directBuf);
+        if (count < 0) throw new EOFException();
+        assert count != 0 : "0-length read: " + (endPos - pos) + "@" + (pos - startPos);
+        pos += count;
+        assert pos <= endPos : "Position " + pos + " > " + endPos + " after reading " + count;
+        directBuf.position(pos);
+      }
+    } catch (UnsupportedOperationException ex) {
+      assert pos == startPos;
+      // Happens in q files and such.
+      RecordReaderImpl.LOG.error("Stream does not support direct read; we will copy.");
+      byte[] buffer = new byte[len];
+      file.readFully(buffer, 0, buffer.length);
+      directBuf.put(buffer);
+    }
+    directBuf.position(startPos);
+    if (doSetLimit) {
+      directBuf.limit(startPos + len);
+    }
+  }
+
+
+  static List<DiskRange> getStreamBuffers(DiskRangeList range, long offset, long length) {
+    // This assumes sorted ranges (as do many other parts of ORC code.
+    ArrayList<DiskRange> buffers = new ArrayList<DiskRange>();
+    if (length == 0) return buffers;
+    long streamEnd = offset + length;
+    boolean inRange = false;
+    while (range != null) {
+      if (!inRange) {
+        if (range.getEnd() <= offset) {
+          range = range.next;
+          continue; // Skip until we are in range.
+        }
+        inRange = true;
+        if (range.getOffset() < offset) {
+          // Partial first buffer, add a slice of it.
+          buffers.add(range.sliceAndShift(offset, Math.min(streamEnd, range.getEnd()), -offset));
+          if (range.getEnd() >= streamEnd) break; // Partial first buffer is also partial last buffer.
+          range = range.next;
+          continue;
+        }
+      } else if (range.getOffset() >= streamEnd) {
+        break;
+      }
+      if (range.getEnd() > streamEnd) {
+        // Partial last buffer (may also be the first buffer), add a slice of it.
+        buffers.add(range.sliceAndShift(range.getOffset(), streamEnd, -offset));
+        break;
+      }
+      // Buffer that belongs entirely to one stream.
+      // TODO: ideally we would want to reuse the object and remove it from the list, but we cannot
+      //       because bufferChunks is also used by clearStreams for zcr. Create a useless dup.
+      buffers.add(range.sliceAndShift(range.getOffset(), range.getEnd(), -offset));
+      if (range.getEnd() == streamEnd) break;
+      range = range.next;
+    }
+    return buffers;
+  }
+
+  static ZeroCopyReaderShim createZeroCopyShim(FSDataInputStream file,
+      CompressionCodec codec, ByteBufferAllocatorPool pool) throws IOException {
+    if ((codec == null || ((codec instanceof DirectDecompressionCodec)
+            && ((DirectDecompressionCodec) codec).isAvailable()))) {
+      /* codec is null or is available */
+      return ShimLoader.getHadoopShims().getZeroCopyReader(file, pool);
+    }
+    return null;
+  }
+
+  // this is an implementation copied from ElasticByteBufferPool in hadoop-2,
+  // which lacks a clear()/clean() operation
+  public final static class ByteBufferAllocatorPool implements ByteBufferPoolShim {
+    private static final class Key implements Comparable<Key> {
+      private final int capacity;
+      private final long insertionGeneration;
+
+      Key(int capacity, long insertionGeneration) {
+        this.capacity = capacity;
+        this.insertionGeneration = insertionGeneration;
+      }
+
+      @Override
+      public int compareTo(Key other) {
+        return ComparisonChain.start().compare(capacity, other.capacity)
+            .compare(insertionGeneration, other.insertionGeneration).result();
+      }
+
+      @Override
+      public boolean equals(Object rhs) {
+        if (rhs == null) {
+          return false;
+        }
+        try {
+          Key o = (Key) rhs;
+          return (compareTo(o) == 0);
+        } catch (ClassCastException e) {
+          return false;
+        }
+      }
+
+      @Override
+      public int hashCode() {
+        return new HashCodeBuilder().append(capacity).append(insertionGeneration)
+            .toHashCode();
+      }
+    }
+
+    private final TreeMap<Key, ByteBuffer> buffers = new TreeMap<Key, ByteBuffer>();
+
+    private final TreeMap<Key, ByteBuffer> directBuffers = new TreeMap<Key, ByteBuffer>();
+
+    private long currentGeneration = 0;
+
+    private final TreeMap<Key, ByteBuffer> getBufferTree(boolean direct) {
+      return direct ? directBuffers : buffers;
+    }
+
+    public void clear() {
+      buffers.clear();
+      directBuffers.clear();
+    }
+
+    @Override
+    public ByteBuffer getBuffer(boolean direct, int length) {
+      TreeMap<Key, ByteBuffer> tree = getBufferTree(direct);
+      Map.Entry<Key, ByteBuffer> entry = tree.ceilingEntry(new Key(length, 0));
+      if (entry == null) {
+        return direct ? ByteBuffer.allocateDirect(length) : ByteBuffer
+            .allocate(length);
+      }
+      tree.remove(entry.getKey());
+      return entry.getValue();
+    }
+
+    @Override
+    public void putBuffer(ByteBuffer buffer) {
+      TreeMap<Key, ByteBuffer> tree = getBufferTree(buffer.isDirect());
+      while (true) {
+        Key key = new Key(buffer.capacity(), currentGeneration++);
+        if (!tree.containsKey(key)) {
+          tree.put(key, buffer);
+          return;
+        }
+        // Buffers are indexed by (capacity, generation).
+        // If our key is not unique on the first try, we try again
+      }
+    }
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RunLengthIntegerReaderV2.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RunLengthIntegerReaderV2.java
index 4057036e7d..f406e1721c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RunLengthIntegerReaderV2.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RunLengthIntegerReaderV2.java
@@ -42,10 +42,10 @@ class RunLengthIntegerReaderV2 implements IntegerReader {
   private final SerializationUtils utils;
 
   RunLengthIntegerReaderV2(InStream input, boolean signed,
-      Configuration conf) throws IOException {
+      boolean skipCorrupt) throws IOException {
     this.input = input;
     this.signed = signed;
-    this.skipCorrupt = HiveConf.getBoolVar(conf, ConfVars.HIVE_ORC_SKIP_CORRUPT_DATA);
+    this.skipCorrupt = skipCorrupt;
     this.utils = new SerializationUtils();
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
index 23e5f2754e..3992d8ce85 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java
@@ -27,6 +27,9 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedInputFormatInterface;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
@@ -62,7 +65,7 @@ static class VectorizedOrcRecordReader
       this.offset = fileSplit.getStart();
       this.length = fileSplit.getLength();
       options.range(offset, length);
-      OrcInputFormat.setIncludedColumns(options, types, conf, true);
+      options.include(OrcInputFormat.genIncludedColumns(types, conf, true));
       OrcInputFormat.setSearchArgument(options, types, conf, true);
 
       this.reader = file.rowsOptions(options);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
index 79dc5a1676..8e355a247c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
@@ -99,7 +99,7 @@
  * particular, because the MemoryManager is shared between writers, this class
  * assumes that checkMemory may be called from a separate thread.
  */
-class WriterImpl implements Writer, MemoryManager.Callback {
+public class WriterImpl implements Writer, MemoryManager.Callback {
 
   private static final Log LOG = LogFactory.getLog(WriterImpl.class);
 
@@ -313,7 +313,7 @@ private long getMemoryAvailableForORC() {
     return totalMemoryPool;
   }
 
-  static CompressionCodec createCodec(CompressionKind kind) {
+  public static CompressionCodec createCodec(CompressionKind kind) {
     switch (kind) {
       case NONE:
         return null;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentFactory.java
index f4a2e65294..c75e820699 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentFactory.java
@@ -18,14 +18,20 @@
 
 package org.apache.hadoop.hive.ql.io.sarg;
 
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
 import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.Builder;
+import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;
+import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 
 /**
  * A factory for creating SearchArguments.
  */
 public class SearchArgumentFactory {
+  public static final String SARG_PUSHDOWN = "sarg.pushdown";
+
   public static SearchArgument create(ExprNodeGenericFuncDesc expression) {
     return new SearchArgumentImpl(expression);
   }
@@ -37,4 +43,14 @@ public static Builder newBuilder() {
   public static SearchArgument create(String kryo) {
     return SearchArgumentImpl.fromKryo(kryo);
   }
+
+  public static SearchArgument createFromConf(Configuration conf) {
+    String sargString = null;
+    if ((sargString = conf.get(TableScanDesc.FILTER_EXPR_CONF_STR)) != null) {
+      return create(Utilities.deserializeExpression(sargString));
+    } else if ((sargString = conf.get(SARG_PUSHDOWN)) != null) {
+      return create(sargString);
+    }
+    return null;
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInStream.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInStream.java
index 0ea4a7bf32..4c3ddfcf86 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInStream.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInStream.java
@@ -18,7 +18,8 @@
 
 package org.apache.hadoop.hive.ql.io.orc;
 
-import org.junit.Test;
+import static junit.framework.Assert.assertEquals;
+import static junit.framework.Assert.fail;
 
 import java.io.DataInputStream;
 import java.io.DataOutput;
@@ -28,8 +29,7 @@
 import java.util.ArrayList;
 import java.util.List;
 
-import static junit.framework.Assert.assertEquals;
-import static junit.framework.Assert.fail;
+import org.junit.Test;
 
 public class TestInStream {
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
index 2cc3d7a3ff..c4eb84b27a 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
@@ -1687,7 +1687,7 @@ public void testSetSearchArgument() throws Exception {
     types.add(builder.build());
     SearchArgument isNull = SearchArgumentFactory.newBuilder()
         .startAnd().isNull("cost").end().build();
-    conf.set(OrcInputFormat.SARG_PUSHDOWN, isNull.toKryo());
+    conf.set(SearchArgumentFactory.SARG_PUSHDOWN, isNull.toKryo());
     conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR,
         "url,cost");
     options.include(new boolean[]{true, true, false, true, false});
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestIntegerCompressionReader.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestIntegerCompressionReader.java
index 591ec3fe24..fe07118e31 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestIntegerCompressionReader.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestIntegerCompressionReader.java
@@ -58,8 +58,7 @@ public void runSeekTest(CompressionCodec codec) throws Exception {
       new RunLengthIntegerReaderV2(InStream.create
                                    ("test", new ByteBuffer[]{inBuf},
                                     new long[]{0}, inBuf.remaining(),
-                                    codec, 1000), true,
-                                    new Configuration());
+                                    codec, 1000), true, false);
     for(int i=0; i < 2048; ++i) {
       int x = (int) in.next();
       if (i < 1024) {
@@ -114,8 +113,7 @@ public void testSkips() throws Exception {
                                                    new ByteBuffer[]{inBuf},
                                                    new long[]{0},
                                                    inBuf.remaining(),
-                                                   null, 100), true,
-                                                   new Configuration());
+                                                   null, 100), true, false);
     for(int i=0; i < 2048; i += 10) {
       int x = (int) in.next();
       if (i < 1024) {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
index cd1d645a44..7f9b615dc5 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
@@ -695,7 +695,7 @@ public void testStripeLevelStats() throws Exception {
     assertEquals(5000, ((StringColumnStatistics)ss3.getColumnStatistics()[2]).getSum());
 
     RecordReaderImpl recordReader = (RecordReaderImpl) reader.rows();
-    OrcProto.RowIndex[] index = recordReader.readRowIndex(0, null).getRowGroupIndex();
+    OrcProto.RowIndex[] index = recordReader.readRowIndex(0, null, null).getRowGroupIndex();
     assertEquals(3, index.length);
     List<OrcProto.RowIndexEntry> items = index[1].getEntryList();
     assertEquals(1, items.size());
@@ -705,7 +705,7 @@ public void testStripeLevelStats() throws Exception {
     assertEquals(0, items.get(0).getPositions(2));
     assertEquals(1, 
                  items.get(0).getStatistics().getIntStatistics().getMinimum());
-    index = recordReader.readRowIndex(1, null).getRowGroupIndex();
+    index = recordReader.readRowIndex(1, null, null).getRowGroupIndex();
     assertEquals(3, index.length);
     items = index[1].getEntryList();
     assertEquals(2, 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java
index 326dde466c..d0f3a5ec81 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java
@@ -20,8 +20,7 @@
 
 import static junit.framework.Assert.assertEquals;
 import static org.hamcrest.core.Is.is;
-import static org.junit.Assert.assertThat;
-import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.*;
 
 import java.io.IOException;
 import java.io.InputStream;
@@ -36,6 +35,7 @@
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PositionedReadable;
 import org.apache.hadoop.fs.Seekable;
+import org.apache.hadoop.hive.common.DiskRangeList;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.io.filters.BloomFilter;
 import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.Location;
@@ -776,109 +776,88 @@ public void testIsNullWithNullInStats() throws Exception {
 
   @Test
   public void testOverlap() throws Exception {
-    assertTrue(!RecordReaderImpl.overlap(0, 10, -10, -1));
-    assertTrue(RecordReaderImpl.overlap(0, 10, -1, 0));
-    assertTrue(RecordReaderImpl.overlap(0, 10, -1, 1));
-    assertTrue(RecordReaderImpl.overlap(0, 10, 2, 8));
-    assertTrue(RecordReaderImpl.overlap(0, 10, 5, 10));
-    assertTrue(RecordReaderImpl.overlap(0, 10, 10, 11));
-    assertTrue(RecordReaderImpl.overlap(0, 10, 0, 10));
-    assertTrue(RecordReaderImpl.overlap(0, 10, -1, 11));
-    assertTrue(!RecordReaderImpl.overlap(0, 10, 11, 12));
+    assertTrue(!RecordReaderUtils.overlap(0, 10, -10, -1));
+    assertTrue(RecordReaderUtils.overlap(0, 10, -1, 0));
+    assertTrue(RecordReaderUtils.overlap(0, 10, -1, 1));
+    assertTrue(RecordReaderUtils.overlap(0, 10, 2, 8));
+    assertTrue(RecordReaderUtils.overlap(0, 10, 5, 10));
+    assertTrue(RecordReaderUtils.overlap(0, 10, 10, 11));
+    assertTrue(RecordReaderUtils.overlap(0, 10, 0, 10));
+    assertTrue(RecordReaderUtils.overlap(0, 10, -1, 11));
+    assertTrue(!RecordReaderUtils.overlap(0, 10, 11, 12));
   }
 
-  private static List<RecordReaderImpl.DiskRange> diskRanges(Integer... points) {
-    List<RecordReaderImpl.DiskRange> result =
-        new ArrayList<RecordReaderImpl.DiskRange>();
-    for(int i=0; i < points.length; i += 2) {
-      result.add(new RecordReaderImpl.DiskRange(points[i], points[i+1]));
+  private static DiskRangeList diskRanges(Integer... points) {
+    DiskRangeList head = null, tail = null;
+    for(int i = 0; i < points.length; i += 2) {
+      DiskRangeList range = new DiskRangeList(points[i], points[i+1]);
+      if (tail == null) {
+        head = tail = range;
+      } else {
+        tail = tail.insertAfter(range);
+      }
     }
-    return result;
-  }
-
-  @Test
-  public void testMergeDiskRanges() throws Exception {
-    List<RecordReaderImpl.DiskRange> list = diskRanges();
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges()));
-    list = diskRanges(100, 200, 300, 400, 500, 600);
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges(100, 200, 300, 400, 500, 600)));
-    list = diskRanges(100, 200, 150, 300, 400, 500);
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges(100, 300, 400, 500)));
-    list = diskRanges(100, 200, 300, 400, 400, 500);
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges(100, 200, 300, 500)));
-    list = diskRanges(100, 200, 0, 300);
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges(0, 300)));
-    list = diskRanges(0, 500, 200, 400);
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges(0, 500)));
-    list = diskRanges(0, 100, 100, 200, 200, 300, 300, 400);
-    RecordReaderImpl.mergeDiskRanges(list);
-    assertThat(list, is(diskRanges(0, 400)));
+    return head;
   }
 
   @Test
   public void testGetIndexPosition() throws Exception {
-    assertEquals(0, RecordReaderImpl.getIndexPosition
+    assertEquals(0, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
             OrcProto.Stream.Kind.PRESENT, true, true));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
             OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderImpl.getIndexPosition
+    assertEquals(3, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
             OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(0, RecordReaderImpl.getIndexPosition
+    assertEquals(0, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
             OrcProto.Stream.Kind.DATA, true, false));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DICTIONARY, OrcProto.Type.Kind.STRING,
             OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
             OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderImpl.getIndexPosition
+    assertEquals(3, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
             OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(6, RecordReaderImpl.getIndexPosition
+    assertEquals(6, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
             OrcProto.Stream.Kind.LENGTH, true, true));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
             OrcProto.Stream.Kind.LENGTH, false, true));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
             OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderImpl.getIndexPosition
+    assertEquals(3, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
             OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(6, RecordReaderImpl.getIndexPosition
+    assertEquals(6, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
             OrcProto.Stream.Kind.SECONDARY, true, true));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
             OrcProto.Stream.Kind.SECONDARY, false, true));
-    assertEquals(4, RecordReaderImpl.getIndexPosition
+    assertEquals(4, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
             OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderImpl.getIndexPosition
+    assertEquals(3, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
             OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(7, RecordReaderImpl.getIndexPosition
+    assertEquals(7, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
             OrcProto.Stream.Kind.SECONDARY, true, true));
-    assertEquals(5, RecordReaderImpl.getIndexPosition
+    assertEquals(5, RecordReaderUtils.getIndexPosition
         (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
             OrcProto.Stream.Kind.SECONDARY, false, true));
   }
 
   @Test
   public void testPartialPlan() throws Exception {
-    List<RecordReaderImpl.DiskRange> result;
+    DiskRangeList result;
 
     // set the streams
     List<OrcProto.Stream> streams = new ArrayList<OrcProto.Stream>();
@@ -947,38 +926,53 @@ public void testPartialPlan() throws Exception {
 
     // filter by rows and groups
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768);
+        columns, rowGroups, false, encodings, types, 32768, false);
     assertThat(result, is(diskRanges(0, 1000, 100, 1000, 400, 1000,
-        1000, 11000 + RecordReaderImpl.WORST_UNCOMPRESSED_SLOP,
-        11000, 21000 + RecordReaderImpl.WORST_UNCOMPRESSED_SLOP,
-        41000, 51000 + RecordReaderImpl.WORST_UNCOMPRESSED_SLOP)));
+        1000, 11000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
+        11000, 21000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
+        41000, 51000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
+    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
+        columns, rowGroups, false, encodings, types, 32768, true);
+    assertThat(result, is(diskRanges(0, 21000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
+        41000, 51000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
 
     // if we read no rows, don't read any bytes
     rowGroups = new boolean[]{false, false, false, false, false, false};
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768);
-    assertThat(result, is(diskRanges()));
+        columns, rowGroups, false, encodings, types, 32768, false);
+    assertNull(result);
 
     // all rows, but only columns 0 and 2.
     rowGroups = null;
     columns = new boolean[]{true, false, true};
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, null, false, encodings, types, 32768);
+        columns, null, false, encodings, types, 32768, false);
     assertThat(result, is(diskRanges(100000, 102000, 102000, 200000)));
+    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
+        columns, null, false, encodings, types, 32768, true);
+    assertThat(result, is(diskRanges(100000, 200000)));
 
     rowGroups = new boolean[]{false, true, false, false, false, false};
     indexes[2] = indexes[1];
     indexes[1] = null;
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768);
+        columns, rowGroups, false, encodings, types, 32768, false);
+    assertThat(result, is(diskRanges(100100, 102000,
+        112000, 122000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
+    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
+        columns, rowGroups, false, encodings, types, 32768, true);
     assertThat(result, is(diskRanges(100100, 102000,
-        112000, 122000 + RecordReaderImpl.WORST_UNCOMPRESSED_SLOP)));
+        112000, 122000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
 
     rowGroups = new boolean[]{false, false, false, false, false, true};
     indexes[1] = indexes[2];
     columns = new boolean[]{true, true, true};
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768);
+        columns, rowGroups, false, encodings, types, 32768, false);
+    assertThat(result, is(diskRanges(500, 1000, 51000, 100000, 100500, 102000,
+        152000, 200000)));
+    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
+        columns, rowGroups, false, encodings, types, 32768, true);
     assertThat(result, is(diskRanges(500, 1000, 51000, 100000, 100500, 102000,
         152000, 200000)));
   }
@@ -986,7 +980,7 @@ public void testPartialPlan() throws Exception {
 
   @Test
   public void testPartialPlanCompressed() throws Exception {
-    List<RecordReaderImpl.DiskRange> result;
+    DiskRangeList result;
 
     // set the streams
     List<OrcProto.Stream> streams = new ArrayList<OrcProto.Stream>();
@@ -1055,20 +1049,20 @@ public void testPartialPlanCompressed() throws Exception {
 
     // filter by rows and groups
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, true, encodings, types, 32768);
+        columns, rowGroups, true, encodings, types, 32768, false);
     assertThat(result, is(diskRanges(0, 1000, 100, 1000,
         400, 1000, 1000, 11000+(2*32771),
         11000, 21000+(2*32771), 41000, 100000)));
 
     rowGroups = new boolean[]{false, false, false, false, false, true};
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, true, encodings, types, 32768);
+        columns, rowGroups, true, encodings, types, 32768, false);
     assertThat(result, is(diskRanges(500, 1000, 51000, 100000)));
   }
 
   @Test
   public void testPartialPlanString() throws Exception {
-    List<RecordReaderImpl.DiskRange> result;
+    DiskRangeList result;
 
     // set the streams
     List<OrcProto.Stream> streams = new ArrayList<OrcProto.Stream>();
@@ -1143,10 +1137,10 @@ public void testPartialPlanString() throws Exception {
 
     // filter by rows and groups
     result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768);
+        columns, rowGroups, false, encodings, types, 32768, false);
     assertThat(result, is(diskRanges(100, 1000, 400, 1000, 500, 1000,
-        11000, 21000 + RecordReaderImpl.WORST_UNCOMPRESSED_SLOP,
-        41000, 51000 + RecordReaderImpl.WORST_UNCOMPRESSED_SLOP,
+        11000, 21000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
+        41000, 51000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
         51000, 95000, 95000, 97000, 97000, 100000)));
   }
 
