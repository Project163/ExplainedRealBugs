diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 593c566141..abff8682d0 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -1261,14 +1261,14 @@ public static enum ConfVars {
 
     HIVEOUTERJOINSUPPORTSFILTERS("hive.outerjoin.supports.filters", true, ""),
 
-    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "minimal", new StringSet("minimal", "more"),
+    HIVEFETCHTASKCONVERSION("hive.fetch.task.conversion", "more", new StringSet("minimal", "more"),
         "Some select queries can be converted to single FETCH task minimizing latency.\n" +
         "Currently the query should be single sourced not having any subquery and should not have\n" +
         "any aggregations or distincts (which incurs RS), lateral views and joins.\n" +
         "1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only\n" +
         "2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)\n"
     ),
-    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", -1l,
+    HIVEFETCHTASKCONVERSIONTHRESHOLD("hive.fetch.task.conversion.threshold", 1073741824L,
         "Input threshold for applying hive.fetch.task.conversion. If target table is native, input length\n" +
         "is calculated by summation of file lengths. If it's not native, storage handler for the table\n" +
         "can optionally implement org.apache.hadoop.hive.ql.metadata.InputEstimator interface."),
diff --git a/conf/hive-default.xml.template b/conf/hive-default.xml.template
index ba922d030e..bc68ccf0d0 100644
--- a/conf/hive-default.xml.template
+++ b/conf/hive-default.xml.template
@@ -1,7 +1,5 @@
 <?xml version="1.0" encoding="UTF-8" standalone="no"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-
-<!--
+<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
    this work for additional information regarding copyright ownership.
@@ -16,8 +14,7 @@
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
--->
-<configuration>
+--><configuration>
   <!-- WARNING!!! This file is auto generated for documentation purposes ONLY! -->
   <!-- WARNING!!! Any changes you make to this file will be ignored by Hive.   -->
   <!-- WARNING!!! You must make your changes in hive-site.xml instead.         -->
@@ -2224,7 +2221,7 @@
   </property>
   <property>
     <key>hive.fetch.task.conversion</key>
-    <value>minimal</value>
+    <value>more</value>
     <description>
       Some select queries can be converted to single FETCH task minimizing latency.
       Currently the query should be single sourced not having any subquery and should not have
@@ -2235,7 +2232,7 @@
   </property>
   <property>
     <key>hive.fetch.task.conversion.threshold</key>
-    <value>-1</value>
+    <value>1073741824</value>
     <description>
       Input threshold for applying hive.fetch.task.conversion. If target table is native, input length
       is calculated by summation of file lengths. If it's not native, storage handler for the table
diff --git a/data/conf/hive-site.xml b/data/conf/hive-site.xml
index 37ac8c09ca..fe8080addc 100644
--- a/data/conf/hive-site.xml
+++ b/data/conf/hive-site.xml
@@ -235,5 +235,9 @@
 </property>
 
 
+<property>
+  <name>hive.fetch.task.conversion</name>
+  <value>minimal</value>
+</property>
 
 </configuration>
diff --git a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
index 526f3dc0b0..02bf643f7d 100644
--- a/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
+++ b/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java
@@ -275,22 +275,22 @@ public void testParentReferences() throws Exception {
     Statement s = this.con.createStatement();
     ResultSet rs = s.executeQuery("SELECT * FROM " + dataTypeTableName);
 
-    rs.close();
-    s.close();
-
     assertTrue(s.getConnection() == this.con);
     assertTrue(rs.getStatement() == s);
 
+    rs.close();
+    s.close();
+
     /* Test parent references from PreparedStatement */
     PreparedStatement ps = this.con.prepareStatement("SELECT * FROM " + dataTypeTableName);
     rs = ps.executeQuery();
 
-    rs.close();
-    ps.close();
-
     assertTrue(ps.getConnection() == this.con);
     assertTrue(rs.getStatement() == ps);
 
+    rs.close();
+    ps.close();
+
     /* Test DatabaseMetaData queries which do not have a parent Statement */
     DatabaseMetaData md = this.con.getMetaData();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
index 7413d2ba62..c85662345c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java
@@ -18,6 +18,7 @@
 
 package org.apache.hadoop.hive.ql.optimizer;
 
+import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashSet;
@@ -27,6 +28,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.fs.ContentSummary;
+import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.FetchTask;
@@ -106,9 +108,9 @@ private FetchTask optimize(ParseContext pctx, String alias, TableScanOperator so
         pctx.getConf(), HiveConf.ConfVars.HIVEFETCHTASKCONVERSION);
 
     boolean aggressive = "more".equals(mode);
+    final int limit = pctx.getQB().getParseInfo().getOuterQueryLimit();
     FetchData fetch = checkTree(aggressive, pctx, alias, source);
-    if (fetch != null && checkThreshold(fetch, pctx)) {
-      int limit = pctx.getQB().getParseInfo().getOuterQueryLimit();
+    if (fetch != null && checkThreshold(fetch, limit, pctx)) {
       FetchWork fetchWork = fetch.convertToWork();
       FetchTask fetchTask = (FetchTask) TaskFactory.get(fetchWork, pctx.getConf());
       fetchWork.setSink(fetch.completed(pctx, fetchWork));
@@ -119,7 +121,10 @@ private FetchTask optimize(ParseContext pctx, String alias, TableScanOperator so
     return null;
   }
 
-  private boolean checkThreshold(FetchData data, ParseContext pctx) throws Exception {
+  private boolean checkThreshold(FetchData data, int limit, ParseContext pctx) throws Exception {
+    if (limit > 0 && data.hasOnlyPruningFilter()) {
+      return true;
+    }
     long threshold = HiveConf.getLongVar(pctx.getConf(),
         HiveConf.ConfVars.HIVEFETCHTASKCONVERSIONTHRESHOLD);
     if (threshold < 0) {
@@ -169,7 +174,7 @@ private FetchData checkTree(boolean aggressive, ParseContext pctx, String alias,
       PrunedPartitionList pruned = pctx.getPrunedPartitions(alias, ts);
       if (aggressive || !pruned.hasUnknownPartitions()) {
         bypassFilter &= !pruned.hasUnknownPartitions();
-        return checkOperators(new FetchData(parent, table, pruned, splitSample), ts,
+        return checkOperators(new FetchData(parent, table, pruned, splitSample, bypassFilter), ts,
             aggressive, bypassFilter);
       }
     }
@@ -211,6 +216,7 @@ private class FetchData {
     private final SplitSample splitSample;
     private final PrunedPartitionList partsList;
     private final HashSet<ReadEntity> inputs = new HashSet<ReadEntity>();
+    private final boolean onlyPruningFilter;
 
     // source table scan
     private TableScanOperator scanOp;
@@ -223,14 +229,23 @@ private FetchData(ReadEntity parent, Table table, SplitSample splitSample) {
       this.table = table;
       this.partsList = null;
       this.splitSample = splitSample;
+      this.onlyPruningFilter = false;
     }
 
     private FetchData(ReadEntity parent, Table table, PrunedPartitionList partsList,
-        SplitSample splitSample) {
+        SplitSample splitSample, boolean bypassFilter) {
       this.parent = parent;
       this.table = table;
       this.partsList = partsList;
       this.splitSample = splitSample;
+      this.onlyPruningFilter = bypassFilter;
+    }
+    
+    /*
+     * all filters were executed during partition pruning
+     */
+    public boolean hasOnlyPruningFilter() {
+      return this.onlyPruningFilter;
     }
 
     private FetchWork convertToWork() throws HiveException {
@@ -317,7 +332,12 @@ private long getFileLength(JobConf conf, Path path, Class<? extends InputFormat>
         InputFormat input = HiveInputFormat.getInputFormatFromCache(clazz, conf);
         summary = ((ContentSummaryInputFormat)input).getContentSummary(path, conf);
       } else {
-        summary = path.getFileSystem(conf).getContentSummary(path);
+        FileSystem fs = path.getFileSystem(conf);
+        try {
+          summary = fs.getContentSummary(path);
+        } catch (FileNotFoundException e) {
+          return 0;
+        }
       }
       return summary.getLength();
     }
diff --git a/ql/src/test/queries/clientpositive/nonmr_fetch_threshold.q b/ql/src/test/queries/clientpositive/nonmr_fetch_threshold.q
index e6343e2f53..b1a7cb5638 100644
--- a/ql/src/test/queries/clientpositive/nonmr_fetch_threshold.q
+++ b/ql/src/test/queries/clientpositive/nonmr_fetch_threshold.q
@@ -5,5 +5,6 @@ explain select cast(key as int) * 10, upper(value) from src limit 10;
 
 set hive.fetch.task.conversion.threshold=100;
 
+-- from HIVE-7397, limit + partition pruning filter
 explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;
 explain select cast(key as int) * 10, upper(value) from src limit 10;
diff --git a/ql/src/test/results/clientnegative/authorization_fail_8.q.out b/ql/src/test/results/clientnegative/authorization_fail_8.q.out
index 9918801a77..fecb15c4c5 100644
--- a/ql/src/test/results/clientnegative/authorization_fail_8.q.out
+++ b/ql/src/test/results/clientnegative/authorization_fail_8.q.out
@@ -8,4 +8,41 @@ POSTHOOK: Output: default@authorization_fail
 PREHOOK: query: GRANT SELECT ON authorization_fail TO USER user2 WITH GRANT OPTION
 PREHOOK: type: GRANT_PRIVILEGE
 PREHOOK: Output: default@authorization_fail
-FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. null
+POSTHOOK: query: GRANT SELECT ON authorization_fail TO USER user2 WITH GRANT OPTION
+POSTHOOK: type: GRANT_PRIVILEGE
+POSTHOOK: Output: default@authorization_fail
+PREHOOK: query: SHOW GRANT USER user2 ON TABLE authorization_fail
+PREHOOK: type: SHOW_GRANT
+POSTHOOK: query: SHOW GRANT USER user2 ON TABLE authorization_fail
+POSTHOOK: type: SHOW_GRANT
+default	authorization_fail			user2	USER	SELECT	true	-1	user1
+PREHOOK: query: -- user2 current has grant option, this should work
+GRANT SELECT ON authorization_fail TO USER user3
+PREHOOK: type: GRANT_PRIVILEGE
+PREHOOK: Output: default@authorization_fail
+POSTHOOK: query: -- user2 current has grant option, this should work
+GRANT SELECT ON authorization_fail TO USER user3
+POSTHOOK: type: GRANT_PRIVILEGE
+POSTHOOK: Output: default@authorization_fail
+PREHOOK: query: REVOKE SELECT ON authorization_fail FROM USER user3
+PREHOOK: type: REVOKE_PRIVILEGE
+PREHOOK: Output: default@authorization_fail
+POSTHOOK: query: REVOKE SELECT ON authorization_fail FROM USER user3
+POSTHOOK: type: REVOKE_PRIVILEGE
+POSTHOOK: Output: default@authorization_fail
+PREHOOK: query: REVOKE GRANT OPTION FOR SELECT ON authorization_fail FROM USER user2
+PREHOOK: type: REVOKE_PRIVILEGE
+PREHOOK: Output: default@authorization_fail
+POSTHOOK: query: REVOKE GRANT OPTION FOR SELECT ON authorization_fail FROM USER user2
+POSTHOOK: type: REVOKE_PRIVILEGE
+POSTHOOK: Output: default@authorization_fail
+PREHOOK: query: SHOW GRANT USER user2 ON TABLE authorization_fail
+PREHOOK: type: SHOW_GRANT
+POSTHOOK: query: SHOW GRANT USER user2 ON TABLE authorization_fail
+POSTHOOK: type: SHOW_GRANT
+default	authorization_fail			user2	USER	SELECT	false	-1	user1
+PREHOOK: query: -- Now that grant option has been revoked, granting to other users should fail
+GRANT SELECT ON authorization_fail TO USER user3
+PREHOOK: type: GRANT_PRIVILEGE
+PREHOOK: Output: default@authorization_fail
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Permission denied. Principal [name=user2, type=USER] does not have following privileges on Object [type=TABLE_OR_VIEW, name=default.authorization_fail] for operation GRANT_PRIVILEGE : [SELECT with grant]
diff --git a/ql/src/test/results/clientpositive/nonmr_fetch_threshold.q.out b/ql/src/test/results/clientpositive/nonmr_fetch_threshold.q.out
index 39cdfa6a77..6d5ea3474b 100644
--- a/ql/src/test/results/clientpositive/nonmr_fetch_threshold.q.out
+++ b/ql/src/test/results/clientpositive/nonmr_fetch_threshold.q.out
@@ -46,41 +46,31 @@ STAGE PLANS:
               Statistics: Num rows: 10 Data size: 2000 Basic stats: COMPLETE Column stats: NONE
               ListSink
 
-PREHOOK: query: explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10
+PREHOOK: query: -- from HIVE-7397, limit + partition pruning filter
+explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10
 PREHOOK: type: QUERY
-POSTHOOK: query: explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10
+POSTHOOK: query: -- from HIVE-7397, limit + partition pruning filter
+explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            alias: srcpart
-            Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
-            Select Operator
-              expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
-              outputColumnNames: _col0, _col1, _col2, _col3
-              Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
-              Limit
-                Number of rows: 10
-                Statistics: Num rows: 10 Data size: 2000 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 10 Data size: 2000 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
       limit: 10
       Processor Tree:
-        ListSink
+        TableScan
+          alias: srcpart
+          Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
+            outputColumnNames: _col0, _col1, _col2, _col3
+            Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Limit
+              Number of rows: 10
+              Statistics: Num rows: 10 Data size: 2000 Basic stats: COMPLETE Column stats: NONE
+              ListSink
 
 PREHOOK: query: explain select cast(key as int) * 10, upper(value) from src limit 10
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/tez/bucket2.q.out b/ql/src/test/results/clientpositive/tez/bucket2.q.out
index c953757f04..66eba00a35 100644
--- a/ql/src/test/results/clientpositive/tez/bucket2.q.out
+++ b/ql/src/test/results/clientpositive/tez/bucket2.q.out
@@ -199,39 +199,24 @@ POSTHOOK: query: explain
 select * from bucket2_1 tablesample (bucket 1 out of 2) s
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Tez
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: s
-                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                  Filter Operator
-                    predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)
-                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: key (type: int), value (type: string)
-                      outputColumnNames: _col0, _col1
-                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
       limit: -1
       Processor Tree:
-        ListSink
+        TableScan
+          alias: s
+          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+          Filter Operator
+            predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)
+            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value (type: string)
+              outputColumnNames: _col0, _col1
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+              ListSink
 
 PREHOOK: query: select * from bucket2_1 tablesample (bucket 1 out of 2) s
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/tez/bucket3.q.out b/ql/src/test/results/clientpositive/tez/bucket3.q.out
index 8507c7dd5b..41c680e961 100644
--- a/ql/src/test/results/clientpositive/tez/bucket3.q.out
+++ b/ql/src/test/results/clientpositive/tez/bucket3.q.out
@@ -222,39 +222,24 @@ POSTHOOK: query: explain
 select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1'
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Tez
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: s
-                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                  Filter Operator
-                    predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)
-                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: key (type: int), value (type: string), ds (type: string)
-                      outputColumnNames: _col0, _col1, _col2
-                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
       limit: -1
       Processor Tree:
-        ListSink
+        TableScan
+          alias: s
+          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+          Filter Operator
+            predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)
+            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value (type: string), ds (type: string)
+              outputColumnNames: _col0, _col1, _col2
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+              ListSink
 
 PREHOOK: query: select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1'
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/tez/bucket4.q.out b/ql/src/test/results/clientpositive/tez/bucket4.q.out
index 68788eb3e7..28544f2f23 100644
--- a/ql/src/test/results/clientpositive/tez/bucket4.q.out
+++ b/ql/src/test/results/clientpositive/tez/bucket4.q.out
@@ -198,39 +198,24 @@ POSTHOOK: query: explain
 select * from bucket4_1 tablesample (bucket 1 out of 2) s
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Tez
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: s
-                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                  Filter Operator
-                    predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)
-                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                    Select Operator
-                      expressions: key (type: int), value (type: string)
-                      outputColumnNames: _col0, _col1
-                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-
   Stage: Stage-0
     Fetch Operator
       limit: -1
       Processor Tree:
-        ListSink
+        TableScan
+          alias: s
+          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+          Filter Operator
+            predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)
+            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value (type: string)
+              outputColumnNames: _col0, _col1
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+              ListSink
 
 PREHOOK: query: select * from bucket4_1 tablesample (bucket 1 out of 2) s
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out b/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out
index 1bc314b9dd..163cb02b9b 100644
--- a/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out
+++ b/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out
@@ -24,104 +24,26 @@ TOK_QUERY
 
 
 STAGE DEPENDENCIES:
-  Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-0 is a root stage
 
 STAGE PLANS:
-  Stage: Stage-1
-    Tez
-#### A masked pattern was here ####
-      Vertices:
-        Map 1 
-            Map Operator Tree:
-                TableScan
-                  alias: src
-                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
-                  GatherStats: false
-                  Filter Operator
-                    isSamplingPred: false
-                    predicate: false (type: boolean)
-                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
-                    Select Operator
-                      expressions: key (type: string)
-                      outputColumnNames: _col0
-                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
-                      File Output Operator
-                        compressed: false
-                        GlobalTableId: 0
-#### A masked pattern was here ####
-                        NumFilesPerFileSink: 1
-                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
-#### A masked pattern was here ####
-                        table:
-                            input format: org.apache.hadoop.mapred.TextInputFormat
-                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                            properties:
-                              columns _col0
-                              columns.types string
-                              escape.delim \
-                              hive.serialization.extend.nesting.levels true
-                              serialization.format 1
-                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                        TotalFiles: 1
-                        GatherStats: false
-                        MultiFileSpray: false
-            Path -> Alias:
-              -mr-10002default.src{} [src]
-            Path -> Partition:
-              -mr-10002default.src{} 
-                Partition
-                  base file name: src
-                  input format: org.apache.hadoop.hive.ql.io.OneNullRowInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                  properties:
-                    COLUMN_STATS_ACCURATE true
-                    bucket_count -1
-                    columns key,value
-                    columns.comments default default
-                    columns.types string:string
-#### A masked pattern was here ####
-                    name default.src
-                    numFiles 1
-                    numRows 0
-                    rawDataSize 0
-                    serialization.ddl struct src { string key, string value}
-                    serialization.format 1
-                    serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
-                    totalSize 5812
-#### A masked pattern was here ####
-                  serde: org.apache.hadoop.hive.serde2.NullStructSerDe
-                
-                    input format: org.apache.hadoop.mapred.TextInputFormat
-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                    properties:
-                      COLUMN_STATS_ACCURATE true
-                      bucket_count -1
-                      columns key,value
-                      columns.comments default default
-                      columns.types string:string
-#### A masked pattern was here ####
-                      name default.src
-                      numFiles 1
-                      numRows 0
-                      rawDataSize 0
-                      serialization.ddl struct src { string key, string value}
-                      serialization.format 1
-                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                      totalSize 5812
-#### A masked pattern was here ####
-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                    name: default.src
-                  name: default.src
-            Truncated Path -> Alias:
-              -mr-10002default.src{} [src]
-
   Stage: Stage-0
     Fetch Operator
       limit: -1
       Processor Tree:
-        ListSink
+        TableScan
+          alias: src
+          Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+          GatherStats: false
+          Filter Operator
+            isSamplingPred: false
+            predicate: false (type: boolean)
+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
+            Select Operator
+              expressions: key (type: string)
+              outputColumnNames: _col0
+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE
+              ListSink
 
 PREHOOK: query: select key from src where false
 PREHOOK: type: QUERY
