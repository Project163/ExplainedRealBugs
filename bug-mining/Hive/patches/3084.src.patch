diff --git a/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java b/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
index c73749439c..b17f465fc8 100644
--- a/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
+++ b/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
@@ -24,6 +24,7 @@
 import java.net.MalformedURLException;
 import java.net.URI;
 import java.net.URL;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Comparator;
 import java.util.HashMap;
@@ -47,10 +48,13 @@
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobInProgress;
 import org.apache.hadoop.mapred.JobTracker;
 import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.RecordReader;
+import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.TaskLogServlet;
 import org.apache.hadoop.mapred.WebHCatJTShim20S;
 import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
@@ -74,6 +78,39 @@
  */
 public class Hadoop20SShims extends HadoopShimsSecure {
 
+  @Override
+  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
+    return new CombineFileInputFormatShim() {
+      @Override
+      public RecordReader getRecordReader(InputSplit split,
+          JobConf job, Reporter reporter) throws IOException {
+        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
+      }
+
+      @Override
+      protected FileStatus[] listStatus(JobConf job) throws IOException {
+        FileStatus[] result = super.listStatus(job);
+        boolean foundDir = false;
+        for (FileStatus stat: result) {
+          if (stat.isDir()) {
+            foundDir = true;
+            break;
+          }
+        }
+        if (!foundDir) {
+          return result;
+        }
+        ArrayList<FileStatus> files = new ArrayList<FileStatus>();
+        for (FileStatus stat: result) {
+          if (!stat.isDir()) {
+            files.add(stat);
+          }
+        }
+        return files.toArray(new FileStatus[files.size()]);
+      }
+    };
+  }
+
   @Override
   public String getTaskAttemptLogUrl(JobConf conf,
     String taskTrackerHttpAddress, String taskAttemptId)
diff --git a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index 69026023da..a61c3acc3d 100644
--- a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -29,7 +29,7 @@
 import java.util.ArrayList;
 import java.util.Comparator;
 import java.util.HashMap;
-import java.util.HashSet;
+import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
@@ -68,8 +68,10 @@
 import org.apache.hadoop.hdfs.protocol.EncryptionZone;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.ClusterStatus;
+import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.MiniMRCluster;
+import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.mapred.WebHCatJTShim23;
 import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
@@ -128,6 +130,30 @@ public Hadoop23Shims() {
     this.zeroCopy = zcr;
   }
 
+  @Override
+  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
+    return new CombineFileInputFormatShim() {
+      @Override
+      public RecordReader getRecordReader(InputSplit split,
+          JobConf job, Reporter reporter) throws IOException {
+        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
+      }
+
+      @Override
+      protected List<FileStatus> listStatus(JobContext job) throws IOException {
+        List<FileStatus> result = super.listStatus(job);
+        Iterator<FileStatus> it = result.iterator();
+        while (it.hasNext()) {
+          FileStatus stat = it.next();
+          if (!stat.isFile()) {
+            it.remove();
+          }
+        }
+        return result;
+      }
+    };
+  }
+
   @Override
   public String getTaskAttemptLogUrl(JobConf conf,
     String taskTrackerHttpAddress, String taskAttemptId)
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java b/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
index 15aab4c780..279a02cf21 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShimsSecure.java
@@ -58,17 +58,6 @@ public abstract class HadoopShimsSecure implements HadoopShims {
 
   static final Log LOG = LogFactory.getLog(HadoopShimsSecure.class);
 
-  @Override
-  public HadoopShims.CombineFileInputFormatShim getCombineFileInputFormat() {
-    return new CombineFileInputFormatShim() {
-      @Override
-      public RecordReader getRecordReader(InputSplit split,
-          JobConf job, Reporter reporter) throws IOException {
-        throw new IOException("CombineFileInputFormat.getRecordReader not needed.");
-      }
-    };
-  }
-
   public static class InputSplitShim extends CombineFileSplit {
     long shrinkedLength;
     boolean _isShrinked;
@@ -342,28 +331,6 @@ public RecordReader getRecordReader(JobConf job, CombineFileSplit split,
       CombineFileSplit cfSplit = split;
       return new CombineFileRecordReader(job, cfSplit, reporter, rrClass);
     }
-
-    @Override
-    protected FileStatus[] listStatus(JobConf job) throws IOException {
-      FileStatus[] result = super.listStatus(job);
-      boolean foundDir = false;
-      for (FileStatus stat: result) {
-        if (stat.isDir()) {
-          foundDir = true;
-          break;
-        }
-      }
-      if (!foundDir) {
-        return result;
-      }
-      ArrayList<FileStatus> files = new ArrayList<FileStatus>();
-      for (FileStatus stat: result) {
-        if (!stat.isDir()) {
-          files.add(stat);
-        }
-      }
-      return files.toArray(new FileStatus[files.size()]);
-    }
   }
 
   @Override
