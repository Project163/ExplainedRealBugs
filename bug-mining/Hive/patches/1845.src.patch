diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index a78b72ff3d..84ee78f56c 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -642,6 +642,7 @@ public static enum ConfVars {
     // higher compute cost.
     HIVE_STATS_NDV_ERROR("hive.stats.ndv.error", (float)20.0),
     HIVE_STATS_KEY_PREFIX_MAX_LENGTH("hive.stats.key.prefix.max.length", 150),
+    HIVE_STATS_KEY_PREFIX_RESERVE_LENGTH("hive.stats.key.prefix.reserve.length", 24),
     HIVE_STATS_KEY_PREFIX("hive.stats.key.prefix", ""), // internal usage only
     // if length of variable length data type cannot be determined this length will be used.
     HIVE_STATS_MAX_VARIABLE_LENGTH("hive.stats.max.variable.length", 100),
diff --git a/conf/hive-default.xml.template b/conf/hive-default.xml.template
index 7cd8a1fa0d..66d22f90b3 100644
--- a/conf/hive-default.xml.template
+++ b/conf/hive-default.xml.template
@@ -1307,6 +1307,17 @@
     exceeds a certain length, a hash of the key is used instead.  If the value &lt; 0 then hashing
     is never used, if the value >= 0 then hashing is used only when the key prefixes length
     exceeds that value.  The key prefix is defined as everything preceding the task ID in the key.
+    For counter type stats, it's maxed by mapreduce.job.counters.group.name.max, which is by default 128.
+  </description>
+</property>
+
+<property>
+  <name>hive.stats.key.prefix.reserve.length</name>
+  <value>24</value>
+  <description>
+    Reserved length for postfix of stats key. Currently only meaningful for counter type which should
+    keep length of full stats key smaller than max length configured by hive.stats.key.prefix.max.length.
+    For counter type, it should be bigger than the length of LB spec if exists.
   </description>
 </property>
 
@@ -2127,7 +2138,7 @@
 
 <property>
   <name>hive.metastore.integral.jdo.pushdown</name>
-  <value>false</false>
+  <value>false</value>
   <description>
    Allow JDO query pushdown for integral partition columns in metastore. Off by default. This
    improves metastore perf for integral columns, especially if there's a large number of partitions.
diff --git a/data/conf/hive-site.xml b/data/conf/hive-site.xml
index eac1a3f75b..88f3bda717 100644
--- a/data/conf/hive-site.xml
+++ b/data/conf/hive-site.xml
@@ -183,6 +183,12 @@
 <property>
   <name>hive.stats.dbclass</name>
   <value>jdbc:derby</value>
-  <description>The default storatge that stores temporary hive statistics. Currently, jdbc, hbase and counter type is supported</description>
+  <description>The storage for temporary stats generated by tasks. Currently, jdbc, hbase and counter types are supported</description>
 </property>
+
+<property>
+  <name>hive.stats.key.prefix.reserve.length</name>
+  <value>0</value>
+</property>
+
 </configuration>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index bd9516183d..5af1ec62ab 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -1289,6 +1289,8 @@ public int execute() throws CommandNeedRetryException {
       Map<TaskResult, TaskRunner> running = new HashMap<TaskResult, TaskRunner>();
 
       DriverContext driverCxt = new DriverContext(runnable, ctx);
+      driverCxt.prepare(plan);
+
       ctx.setHDFSCleanup(true);
 
       SessionState.get().setLastMapRedStatsList(new ArrayList<MapRedStats>());
@@ -1368,6 +1370,8 @@ public int execute() throws CommandNeedRetryException {
           }
         }
 
+        driverCxt.finished(tskRun);
+
         if (SessionState.get() != null) {
           SessionState.get().getHiveHistory().setTaskProperty(queryId, tsk.getId(),
               Keys.TASK_RET_CODE, String.valueOf(exitVal));
@@ -1529,6 +1533,8 @@ public void launchTask(Task<? extends Serializable> tsk, String queryId, boolean
     TaskResult tskRes = new TaskResult();
     TaskRunner tskRun = new TaskRunner(tsk, tskRes);
 
+    cxt.prepare(tskRun);
+
     // Launch Task
     if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.EXECPARALLEL) && tsk.isMapRedTask()) {
       // Launch it in the parallel mode, as a separate thread only for MR tasks
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java b/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
index 1c84523b1d..c51a9c8bdd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/DriverContext.java
@@ -18,13 +18,25 @@
 
 package org.apache.hadoop.hive.ql;
 
+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
+import org.apache.hadoop.hive.ql.exec.NodeUtils;
+import org.apache.hadoop.hive.ql.exec.NodeUtils.Function;
+import org.apache.hadoop.hive.ql.exec.Operator;
+import org.apache.hadoop.hive.ql.exec.StatsTask;
+import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.TaskRunner;
+import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;
+import org.apache.hadoop.hive.ql.plan.MapWork;
+import org.apache.hadoop.hive.ql.plan.ReduceWork;
+
 import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.LinkedList;
+import java.util.List;
+import java.util.Map;
 import java.util.Queue;
 
-import org.apache.hadoop.hive.ql.exec.Task;
-import org.apache.hadoop.mapred.JobConf;
-
 /**
  * DriverContext.
  *
@@ -38,6 +50,8 @@ public class DriverContext {
 
   Context ctx;
 
+  final Map<String, StatsTask> statsTasks = new HashMap<String, StatsTask>(1);
+
   public DriverContext() {
     this.runnable = null;
     this.ctx = null;
@@ -82,5 +96,42 @@ public Context getCtx() {
   public void incCurJobNo(int amount) {
     this.curJobNo = this.curJobNo + amount;
   }
-  
+
+  public void prepare(QueryPlan plan) {
+    // extract stats keys from StatsTask
+    List<Task<?>> rootTasks = plan.getRootTasks();
+    NodeUtils.iterateTask(rootTasks, StatsTask.class, new Function<StatsTask>() {
+      public void apply(StatsTask statsTask) {
+        statsTasks.put(statsTask.getWork().getAggKey(), statsTask);
+      }
+    });
+  }
+
+  public void prepare(TaskRunner runner) {
+  }
+
+  public void finished(TaskRunner runner) {
+    if (statsTasks.isEmpty() || !(runner.getTask() instanceof MapRedTask)) {
+      return;
+    }
+    MapRedTask mapredTask = (MapRedTask) runner.getTask();
+
+    MapWork mapWork = mapredTask.getWork().getMapWork();
+    ReduceWork reduceWork = mapredTask.getWork().getReduceWork();
+    List<Operator> operators = new ArrayList<Operator>(mapWork.getAliasToWork().values());
+    if (reduceWork != null) {
+      operators.add(reduceWork.getReducer());
+    }
+    final List<String> statKeys = new ArrayList<String>(1);
+    NodeUtils.iterate(operators, FileSinkOperator.class, new Function<FileSinkOperator>() {
+      public void apply(FileSinkOperator fsOp) {
+        if (fsOp.getConf().isGatherStats()) {
+          statKeys.add(fsOp.getConf().getStatsAggPrefix());
+        }
+      }
+    });
+    for (String statKey : statKeys) {
+      statsTasks.get(statKey).getWork().setSourceTask(mapredTask);
+    }
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index b6c09eb61e..fb0b772f7f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -924,7 +924,7 @@ private void publishStats() throws HiveException {
         postfix = Utilities.join(lbSpec, taskID);
       }
       prefix = Utilities.join(prefix, spSpec, dpSpec);
-      prefix = Utilities.getHashedStatsPrefix(prefix, maxKeyLength, postfix.length());
+      prefix = Utilities.getHashedStatsPrefix(prefix, maxKeyLength);
 
       String key = Utilities.join(prefix, postfix);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
index a22a4c2f43..597358a00b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
@@ -171,9 +171,7 @@ private int aggregateStats() {
 
         if (statsAggregator != null) {
           String prefix = getAggregationPrefix(counterStat, table, null);
-          String aggKey = Utilities.getHashedStatsPrefix(prefix, maxPrefixLength, 0);
-          updateStats(statsAggregator, parameters, aggKey, atomic);
-          statsAggregator.cleanUp(aggKey);
+          updateStats(statsAggregator, parameters, prefix, maxPrefixLength, atomic);
         }
 
         updateQuickStats(wh, parameters, tTable.getSd());
@@ -207,9 +205,7 @@ private int aggregateStats() {
 
           if (statsAggregator != null) {
             String prefix = getAggregationPrefix(counterStat, table, partn);
-            String aggKey = Utilities.getHashedStatsPrefix(prefix, maxPrefixLength, 0);
-            updateStats(statsAggregator, parameters, aggKey, atomic);
-            statsAggregator.cleanUp(aggKey);
+            updateStats(statsAggregator, parameters, prefix, maxPrefixLength, atomic);
           }
 
           updateQuickStats(wh, parameters, tPart.getSd());
@@ -296,7 +292,10 @@ private boolean existStats(Map<String, String> parameters) {
   }
 
   private void updateStats(StatsAggregator statsAggregator,
-      Map<String, String> parameters, String aggKey, boolean atomic) throws HiveException {
+      Map<String, String> parameters, String prefix, int maxPrefixLength, boolean atomic)
+      throws HiveException {
+
+    String aggKey = Utilities.getHashedStatsPrefix(prefix, maxPrefixLength);
 
     for (String statType : StatsSetupConst.statsRequireCompute) {
       String value = statsAggregator.aggregateStats(aggKey, statType);
@@ -317,6 +316,7 @@ private void updateStats(StatsAggregator statsAggregator,
         }
       }
     }
+    statsAggregator.cleanUp(aggKey);
   }
 
   private void updateQuickStats(Warehouse wh, Map<String, String> parameters,
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index 0e3cfe76cb..fda263be35 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -291,14 +291,11 @@ private void publishStats() throws HiveException {
       statsToPublish.clear();
       String prefix = Utilities.join(conf.getStatsAggPrefix(), pspecs);
 
-      String key;
       int maxKeyLength = conf.getMaxStatsKeyPrefixLength();
-      if (statsPublisher instanceof CounterStatsPublisher) {
-        key = Utilities.getHashedStatsPrefix(prefix, maxKeyLength, 0);
-      } else {
+      String key = Utilities.getHashedStatsPrefix(prefix, maxKeyLength);
+      if (!(statsPublisher instanceof CounterStatsPublisher)) {
         // stats publisher except counter type needs postfix 'taskID'
-        prefix = Utilities.getHashedStatsPrefix(prefix, maxKeyLength, taskID.length());
-        key = prefix + taskID;
+        key = Utilities.join(prefix, taskID);
       }
       for(String statType : stats.get(pspecs).getStoredStats()) {
         statsToPublish.put(statType, Long.toString(stats.get(pspecs).getStat(statType)));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index b9b5b4a01d..706f205469 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -817,8 +817,9 @@ private static <T> T deserializePlan(InputStream in, Class<T> planClass, Configu
   /**
    * Deserializes the plan.
    * @param in The stream to read from.
+   * @param planClass class of plan
+   * @param conf configuration
    * @return The plan, such as QueryPlan, MapredWork, etc.
-   * @param To know what serialization format plan is in
    */
   public static <T> T deserializePlan(InputStream in, Class<T> planClass, Configuration conf) {
     return deserializePlan(in, planClass, conf, false);
@@ -2397,16 +2398,15 @@ public static StatsPublisher getStatsPublisher(JobConf jc) {
    * then it returns an MD5 hash of statsPrefix followed by path separator, otherwise
    * it returns statsPrefix
    *
-   * @param statsPrefix
-   * @param maxPrefixLength
-   * @return
+   * @param statsPrefix prefix of stats key
+   * @param maxPrefixLength max length of stats key
+   * @return if the length of prefix is longer than max, return MD5 hashed value of the prefix
    */
-  public static String getHashedStatsPrefix(String statsPrefix,
-      int maxPrefixLength, int postfixLength) {
+  public static String getHashedStatsPrefix(String statsPrefix, int maxPrefixLength) {
     // todo: this might return possibly longer prefix than
     // maxPrefixLength (if set) when maxPrefixLength - postfixLength < 17,
     // which would make stat values invalid (especially for 'counter' type)
-    if (maxPrefixLength >= 0 && statsPrefix.length() > maxPrefixLength - postfixLength) {
+    if (maxPrefixLength >= 0 && statsPrefix.length() > maxPrefixLength) {
       try {
         MessageDigest digester = MessageDigest.getInstance("MD5");
         digester.update(statsPrefix.getBytes());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
index e319fe487b..12953aff12 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
@@ -32,6 +32,7 @@
 import org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileKeyBufferWrapper;
 import org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.stats.CounterStatsPublisher;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.shims.CombineHiveKey;
@@ -139,11 +140,13 @@ private void publishStats() throws HiveException {
       throw new HiveException(ErrorMsg.STATSPUBLISHER_CONNECTION_ERROR.getErrorCodedMsg());
     }
 
+    int maxPrefixLength = StatsFactory.getMaxPrefixLength(jc);
     // construct key used to store stats in intermediate db
-    String taskID = Utilities.getTaskIdFromFilename(Utilities.getTaskId(jc));
-    String keyPrefix = Utilities.getHashedStatsPrefix(
-        statsAggKeyPrefix, StatsFactory.getMaxPrefixLength(jc), taskID.length());
-    String key = keyPrefix + taskID;
+    String key = Utilities.getHashedStatsPrefix(statsAggKeyPrefix, maxPrefixLength);
+    if (!(statsPublisher instanceof CounterStatsPublisher)) {
+      String taskID = Utilities.getTaskIdFromFilename(Utilities.getTaskId(jc));
+      key = Utilities.join(key, taskID);
+    }
 
     // construct statistics to be stored
     Map<String, String> statsToPublish = new HashMap<String, String>();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java
index 0f0e825a4e..66d4d4ac02 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java
@@ -50,6 +50,9 @@ public class StatsWork implements Serializable {
 
   private boolean isPartialScanAnalyzeCommand = false;
 
+  // sourceTask for TS is not changed (currently) but that of FS might be changed
+  // by various optimizers (auto.convert.join, for example)
+  // so this is set by DriverContext in runtime
   private transient Task sourceTask;
 
   public StatsWork() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java
index 2fb880d9eb..3216cf33b8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsFactory.java
@@ -30,6 +30,7 @@
 
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVESTATSDBCLASS;
 import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVE_STATS_KEY_PREFIX_MAX_LENGTH;
+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVE_STATS_KEY_PREFIX_RESERVE_LENGTH;
 
 /**
  * A factory of stats publisher and aggregator implementations of the
@@ -51,6 +52,10 @@ public static int getMaxPrefixLength(Configuration conf) {
       maxPrefixLength = maxPrefixLength < 0 ? groupNameMax :
           Math.min(maxPrefixLength, groupNameMax);
     }
+    if (maxPrefixLength > 0) {
+      int reserve = HiveConf.getIntVar(conf, HIVE_STATS_KEY_PREFIX_RESERVE_LENGTH);
+      return reserve < 0 ? maxPrefixLength : maxPrefixLength - reserve;
+    }
     return maxPrefixLength;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
index 41e237fcde..384b49e484 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java
@@ -1,3 +1,21 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 package org.apache.hadoop.hive.ql.stats;
 
 import java.util.List;
