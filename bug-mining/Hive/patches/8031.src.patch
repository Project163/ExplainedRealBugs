diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
index ace8d4fa83..d8ac0ce188 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationOnHDFSEncryptedZones.java
@@ -25,6 +25,7 @@
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.junit.AfterClass;
+import org.junit.Assert;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Ignore;
@@ -36,6 +37,7 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.nio.file.Files;
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.List;
@@ -45,6 +47,7 @@
 
 public class TestReplicationOnHDFSEncryptedZones {
   private static String jksFile = System.getProperty("java.io.tmpdir") + "/test.jks";
+  private static String jksFile2 = System.getProperty("java.io.tmpdir") + "/test2.jks";
   @Rule
   public final TestName testName = new TestName();
 
@@ -56,6 +59,9 @@ public class TestReplicationOnHDFSEncryptedZones {
 
   @BeforeClass
   public static void beforeClassSetup() throws Exception {
+    System.setProperty("jceks.key.serialFilter", "java.lang.Enum;java.security.KeyRep;" +
+            "java.security.KeyRep$Type;javax.crypto.spec.SecretKeySpec;" +
+            "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata;!*");
     conf = new Configuration();
     conf.set("dfs.client.use.datanode.hostname", "true");
     conf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
@@ -81,6 +87,7 @@ public static void beforeClassSetup() throws Exception {
   public static void classLevelTearDown() throws IOException {
     primary.close();
     FileUtils.deleteQuietly(new File(jksFile));
+    FileUtils.deleteQuietly(new File(jksFile2));
   }
 
   @Before
@@ -93,16 +100,28 @@ public void setup() throws Throwable {
 
   @Test
   public void targetAndSourceHaveDifferentEncryptionZoneKeys() throws Throwable {
-    DFSTestUtil.createKey("test_key123", miniDFSCluster, conf);
-
-    WarehouseInstance replica = new WarehouseInstance(LOG, miniDFSCluster,
-        new HashMap<String, String>() {{
-          put(HiveConf.ConfVars.HIVE_IN_TEST.varname, "false");
-          put(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname, "false");
-          put(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname,
-                  UserGroupInformation.getCurrentUser().getUserName());
-          put(HiveConf.ConfVars.REPLDIR.varname, primary.repldDir);
-        }}, "test_key123");
+    String replicaBaseDir = Files.createTempDirectory("replica").toFile().getAbsolutePath();
+    Configuration replicaConf = new Configuration();
+    replicaConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, replicaBaseDir);
+    replicaConf.set("dfs.client.use.datanode.hostname", "true");
+    replicaConf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
+    replicaConf.set("hadoop.security.key.provider.path", "jceks://file" + jksFile2);
+    replicaConf.setBoolean("dfs.namenode.delegation.token.always-use", true);
+
+    MiniDFSCluster miniReplicaDFSCluster =
+            new MiniDFSCluster.Builder(replicaConf).numDataNodes(2).format(true).build();
+    replicaConf.setBoolean(METASTORE_AGGREGATE_STATS_CACHE_ENABLED.varname, false);
+
+    DFSTestUtil.createKey("test_key123", miniReplicaDFSCluster, replicaConf);
+
+    WarehouseInstance replica = new WarehouseInstance(LOG, miniReplicaDFSCluster,
+            new HashMap<String, String>() {{
+              put(HiveConf.ConfVars.HIVE_IN_TEST.varname, "false");
+              put(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname, "false");
+              put(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname,
+                      UserGroupInformation.getCurrentUser().getUserName());
+              put(HiveConf.ConfVars.REPLDIR.varname, primary.repldDir);
+            }}, "test_key123");
 
     List<String> dumpWithClause = Arrays.asList(
             "'hive.repl.add.raw.reserved.namespace'='true'",
@@ -113,46 +132,94 @@ public void targetAndSourceHaveDifferentEncryptionZoneKeys() throws Throwable {
             "'" + HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname + "'='"
                     + UserGroupInformation.getCurrentUser().getUserName() +"'");
     WarehouseInstance.Tuple tuple =
-        primary.run("use " + primaryDbName)
-            .run("create table encrypted_table (id int, value string)")
-            .run("insert into table encrypted_table values (1,'value1')")
-            .run("insert into table encrypted_table values (2,'value2')")
+            primary.run("use " + primaryDbName)
+                    .run("create table encrypted_table (id int, value string)")
+                    .run("insert into table encrypted_table values (1,'value1')")
+                    .run("insert into table encrypted_table values (2,'value2')")
+                    .dump(primaryDbName, dumpWithClause);
+
+    replica
+            .run("repl load " + primaryDbName + " into " + replicatedDbName
+                    + " with('hive.repl.add.raw.reserved.namespace'='true', "
+                    + "'hive.repl.replica.external.table.base.dir'='" + replica.externalTableWarehouseRoot + "', "
+                    + "'hive.exec.copyfile.maxsize'='0', 'distcp.options.skipcrccheck'='')")
+            .run("use " + replicatedDbName)
+            .run("repl status " + replicatedDbName)
+            .verifyResult(tuple.lastReplicationId);
+
+    try {
+      replica
+              .run("select value from encrypted_table")
+              .verifyResults(new String[] { "value1", "value2" });
+      Assert.fail("Src EZKey shouldn't be present on target");
+    } catch (IOException e) {
+      Assert.assertTrue(e.getCause().getMessage().contains("KeyVersion name 'test_key@0' does not exist"));
+    }
+
+    //read should pass without raw-byte distcp
+    dumpWithClause = Arrays.asList( "'" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname + "'='"
+            + replica.externalTableWarehouseRoot + "'");
+    tuple = primary.run("use " + primaryDbName)
+            .run("create external table encrypted_table2 (id int, value string)")
+            .run("insert into table encrypted_table2 values (1,'value1')")
+            .run("insert into table encrypted_table2 values (2,'value2')")
             .dump(primaryDbName, dumpWithClause);
 
     replica
-        .run("repl load " + primaryDbName + " into " + replicatedDbName
-                + " with('hive.repl.add.raw.reserved.namespace'='true', "
-                + "'hive.repl.replica.external.table.base.dir'='" + replica.externalTableWarehouseRoot + "', "
-                + "'distcp.options.pugpbx'='', 'distcp.options.skipcrccheck'='')")
-        .run("use " + replicatedDbName)
-        .run("repl status " + replicatedDbName)
-        .verifyResult(tuple.lastReplicationId)
-        .run("select value from encrypted_table")
-        .verifyFailure(new String[] { "value1", "value2" });
+            .run("repl load " + primaryDbName + " into " + replicatedDbName
+                    + " with('hive.repl.replica.external.table.base.dir'='" + replica.externalTableWarehouseRoot + "', "
+                    + "'hive.exec.copyfile.maxsize'='0', 'distcp.options.skipcrccheck'='')")
+            .run("use " + replicatedDbName)
+            .run("repl status " + replicatedDbName)
+            .verifyResult(tuple.lastReplicationId)
+            .run("select value from encrypted_table2")
+            .verifyResults(new String[] { "value1", "value2" });
   }
 
-  @Ignore("this is ignored as minidfs cluster as of writing this test looked like did not copy the "
-              + "files correctly")
   @Test
   public void targetAndSourceHaveSameEncryptionZoneKeys() throws Throwable {
-    WarehouseInstance replica = new WarehouseInstance(LOG, miniDFSCluster,
+    String replicaBaseDir = Files.createTempDirectory("replica2").toFile().getAbsolutePath();
+    Configuration replicaConf = new Configuration();
+    replicaConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, replicaBaseDir);
+    replicaConf.set("dfs.client.use.datanode.hostname", "true");
+    replicaConf.set("hadoop.proxyuser." + Utils.getUGI().getShortUserName() + ".hosts", "*");
+    replicaConf.set("hadoop.security.key.provider.path", "jceks://file" + jksFile);
+    replicaConf.setBoolean("dfs.namenode.delegation.token.always-use", true);
+
+    MiniDFSCluster miniReplicaDFSCluster =
+            new MiniDFSCluster.Builder(replicaConf).numDataNodes(2).format(true).build();
+    replicaConf.setBoolean(METASTORE_AGGREGATE_STATS_CACHE_ENABLED.varname, false);
+
+    WarehouseInstance replica = new WarehouseInstance(LOG, miniReplicaDFSCluster,
         new HashMap<String, String>() {{
           put(HiveConf.ConfVars.HIVE_IN_TEST.varname, "false");
           put(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname, "false");
           put(HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname,
               UserGroupInformation.getCurrentUser().getUserName());
+          put(HiveConf.ConfVars.REPLDIR.varname, primary.repldDir);
         }}, "test_key");
 
+    List<String> dumpWithClause = Arrays.asList(
+            "'hive.repl.add.raw.reserved.namespace'='true'",
+            "'" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname + "'='"
+                    + replica.externalTableWarehouseRoot + "'",
+            "'distcp.options.skipcrccheck'=''",
+            "'" + HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.varname + "'='false'",
+            "'" + HiveConf.ConfVars.HIVE_DISTCP_DOAS_USER.varname + "'='"
+                    + UserGroupInformation.getCurrentUser().getUserName() +"'");
+
     WarehouseInstance.Tuple tuple =
         primary.run("use " + primaryDbName)
             .run("create table encrypted_table (id int, value string)")
             .run("insert into table encrypted_table values (1,'value1')")
             .run("insert into table encrypted_table values (2,'value2')")
-            .dump(primaryDbName);
+            .dump(primaryDbName, dumpWithClause);
 
     replica
         .run("repl load " + primaryDbName + " into " + replicatedDbName
-            + " with('hive.repl.add.raw.reserved.namespace'='true')")
+            + " with('hive.repl.add.raw.reserved.namespace'='true',"
+            + "'" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname + "'='"
+            +     replica.externalTableWarehouseRoot + "')")
         .run("use " + replicatedDbName)
         .run("repl status " + replicatedDbName)
         .verifyResult(tuple.lastReplicationId)
