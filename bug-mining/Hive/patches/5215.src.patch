diff --git a/common/pom.xml b/common/pom.xml
index e54ec98a01..fd948f816b 100644
--- a/common/pom.xml
+++ b/common/pom.xml
@@ -43,11 +43,6 @@
       <groupId>org.apache.hive</groupId>
       <artifactId>hive-storage-api</artifactId>
     </dependency>
-    <dependency>
-      <groupId>org.apache.hive</groupId>
-      <artifactId>hive-orc</artifactId>
-      <version>${project.version}</version>
-    </dependency>
     <!-- inter-project -->
     <dependency>
       <groupId>commons-cli</groupId>
@@ -64,6 +59,10 @@
        <artifactId>commons-lang3</artifactId>
        <version>${commons-lang3.version}</version>
     </dependency>
+    <dependency>
+      <groupId>org.apache.orc</groupId>
+      <artifactId>orc-core</artifactId>
+    </dependency>
     <dependency>
       <groupId>org.eclipse.jetty.aggregate</groupId>
       <artifactId>jetty-all</artifactId>
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 53b9b0c696..19490956b8 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -1318,53 +1318,9 @@ public static enum ConfVars {
     HIVE_INT_TIMESTAMP_CONVERSION_IN_SECONDS("hive.int.timestamp.conversion.in.seconds", false,
         "Boolean/tinyint/smallint/int/bigint value is interpreted as milliseconds during the timestamp conversion.\n" +
         "Set this flag to true to interpret the value as seconds to be consistent with float/double." ),
-    HIVE_ORC_FILE_MEMORY_POOL("hive.exec.orc.memory.pool", 0.5f,
-        "Maximum fraction of heap that can be used by ORC file writers"),
-    HIVE_ORC_WRITE_FORMAT("hive.exec.orc.write.format", null,
-        "Define the version of the file to write. Possible values are 0.11 and 0.12.\n" +
-        "If this parameter is not defined, ORC will use the run length encoding (RLE)\n" +
-        "introduced in Hive 0.12. Any value other than 0.11 results in the 0.12 encoding."),
-    HIVE_ORC_DEFAULT_STRIPE_SIZE("hive.exec.orc.default.stripe.size",
-        64L * 1024 * 1024,
-        "Define the default ORC stripe size, in bytes."),
-    HIVE_ORC_DEFAULT_BLOCK_SIZE("hive.exec.orc.default.block.size", 256L * 1024 * 1024,
-        "Define the default file system block size for ORC files."),
-
-    HIVE_ORC_DICTIONARY_KEY_SIZE_THRESHOLD("hive.exec.orc.dictionary.key.size.threshold", 0.8f,
-        "If the number of keys in a dictionary is greater than this fraction of the total number of\n" +
-        "non-null rows, turn off dictionary encoding.  Use 1 to always use dictionary encoding."),
-    HIVE_ORC_DEFAULT_ROW_INDEX_STRIDE("hive.exec.orc.default.row.index.stride", 10000,
-        "Define the default ORC index stride in number of rows. (Stride is the number of rows\n" +
-        "an index entry represents.)"),
-    HIVE_ORC_ROW_INDEX_STRIDE_DICTIONARY_CHECK("hive.orc.row.index.stride.dictionary.check", true,
-        "If enabled dictionary check will happen after first row index stride (default 10000 rows)\n" +
-        "else dictionary check will happen before writing first stripe. In both cases, the decision\n" +
-        "to use dictionary or not will be retained thereafter."),
-    HIVE_ORC_DEFAULT_BUFFER_SIZE("hive.exec.orc.default.buffer.size", 256 * 1024,
-        "Define the default ORC buffer size, in bytes."),
+
     HIVE_ORC_BASE_DELTA_RATIO("hive.exec.orc.base.delta.ratio", 8, "The ratio of base writer and\n" +
         "delta writer in terms of STRIPE_SIZE and BUFFER_SIZE."),
-    HIVE_ORC_DEFAULT_BLOCK_PADDING("hive.exec.orc.default.block.padding", true,
-        "Define the default block padding, which pads stripes to the HDFS block boundaries."),
-    HIVE_ORC_BLOCK_PADDING_TOLERANCE("hive.exec.orc.block.padding.tolerance", 0.05f,
-        "Define the tolerance for block padding as a decimal fraction of stripe size (for\n" +
-        "example, the default value 0.05 is 5% of the stripe size). For the defaults of 64Mb\n" +
-        "ORC stripe and 256Mb HDFS blocks, the default block padding tolerance of 5% will\n" +
-        "reserve a maximum of 3.2Mb for padding within the 256Mb block. In that case, if the\n" +
-        "available size within the block is more than 3.2Mb, a new smaller stripe will be\n" +
-        "inserted to fit within that space. This will make sure that no stripe written will\n" +
-        "cross block boundaries and cause remote reads within a node local task."),
-    HIVE_ORC_DEFAULT_COMPRESS("hive.exec.orc.default.compress", "ZLIB", "Define the default compression codec for ORC file"),
-
-    HIVE_ORC_ENCODING_STRATEGY("hive.exec.orc.encoding.strategy", "SPEED", new StringSet("SPEED", "COMPRESSION"),
-        "Define the encoding strategy to use while writing data. Changing this will\n" +
-        "only affect the light weight encoding for integers. This flag will not\n" +
-        "change the compression level of higher level compression codec (like ZLIB)."),
-
-    HIVE_ORC_COMPRESSION_STRATEGY("hive.exec.orc.compression.strategy", "SPEED", new StringSet("SPEED", "COMPRESSION"),
-         "Define the compression strategy to use while writing data. \n" +
-         "This changes the compression level of higher level compression codec (like ZLIB)."),
-
     HIVE_ORC_SPLIT_STRATEGY("hive.exec.orc.split.strategy", "HYBRID", new StringSet("HYBRID", "BI", "ETL"),
         "This is not a user level config. BI strategy is used when the requirement is to spend less time in split generation" +
         " as opposed to query execution (split generation does not read or cache file footers)." +
@@ -1398,12 +1354,6 @@ public static enum ConfVars {
         "references for the cached object. Setting this to true can help avoid out of memory\n" +
         "issues under memory pressure (in some cases) at the cost of slight unpredictability in\n" +
         "overall query performance."),
-    HIVE_ORC_SKIP_CORRUPT_DATA("hive.exec.orc.skip.corrupt.data", false,
-        "If ORC reader encounters corrupt data, this value will be used to determine\n" +
-        "whether to skip the corrupt data or throw exception. The default behavior is to throw exception."),
-
-    HIVE_ORC_ZEROCOPY("hive.exec.orc.zerocopy", false,
-        "Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)"),
 
     HIVE_LAZYSIMPLE_EXTENDED_BOOLEAN_LITERAL("hive.lazysimple.extended_boolean_literal", false,
         "LazySimpleSerde uses this property to determine if it treats 'T', 't', 'F', 'f',\n" +
diff --git a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
index 0587e80349..66ed8ca970 100644
--- a/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
+++ b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java
@@ -1165,7 +1165,7 @@ public void testTableProperties() throws Exception {
         "'transactional'='true'," +
         "'compactor.mapreduce.map.memory.mb'='2048'," + // 2048 MB memory for compaction map job
         "'compactorthreshold.hive.compactor.delta.num.threshold'='4'," +  // minor compaction if more than 4 delta dirs
-        "'compactorthreshold.hive.compactor.delta.pct.threshold'='0.5'" + // major compaction if more than 50%
+        "'compactorthreshold.hive.compactor.delta.pct.threshold'='0.49'" + // major compaction if more than 49%
         ")", driver);
 
     // Insert 5 rows to both tables
diff --git a/jdbc/pom.xml b/jdbc/pom.xml
index 522eb8dd4c..dba1ae1d78 100644
--- a/jdbc/pom.xml
+++ b/jdbc/pom.xml
@@ -196,10 +196,11 @@
                   <exclude>com.sun.jersey.contribs:*</exclude>
                   <exclude>org.eclipse.jetty.aggregate:*</exclude>
                   <exclude>org.tukaani:*</exclude>
-                  <exclude>org.iq80.snappy:*</exclude>
+                  <exclude>io.airlift:*</exclude>
                   <exclude>org.apache.velocity:*</exclude>
                   <exclude>net.sf.jpam:*</exclude>
                   <exclude>org.apache.avro:*</exclude>
+                  <exclude>org.apache.orc:*</exclude>
                   <exclude>net.sf.opencsv:*</exclude>
                   <exclude>org.antlr:*</exclude>
                   <exclude>org.slf4j:slf4j-log4j12</exclude>
diff --git a/llap-server/pom.xml b/llap-server/pom.xml
index 379ffd647b..fc392fbdde 100644
--- a/llap-server/pom.xml
+++ b/llap-server/pom.xml
@@ -136,6 +136,10 @@
         </exclusion>
       </exclusions>
     </dependency>
+    <dependency>
+      <groupId>org.apache.orc</groupId>
+      <artifactId>orc-core</artifactId>
+    </dependency>
     <dependency>
       <groupId>org.apache.tez</groupId>
       <artifactId>tez-runtime-internals</artifactId>
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
index 9ef9ca4867..9b1a905703 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java
@@ -91,7 +91,7 @@ class LlapRecordReader
   private final ExecutorService executor;
   private final int columnCount;
 
-  private TypeDescription fileSchema;
+  private SchemaEvolution evolution;
 
   public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols,
       String hostName, ColumnVectorProducer cvp, ExecutorService executor,
@@ -147,7 +147,7 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols
     feedback = rp = cvp.createReadPipeline(this, split, columnIds, sarg, columnNames,
         counters, schema, sourceInputFormat, sourceSerDe, reporter, job,
         mapWork.getPathToPartitionInfo());
-    fileSchema = rp.getFileSchema();
+    evolution = rp.getSchemaEvolution();
     includedColumns = rp.getIncludedColumns();
   }
 
@@ -168,10 +168,9 @@ public boolean init() {
   }
 
   private boolean checkOrcSchemaEvolution() {
-    SchemaEvolution schemaEvolution = new SchemaEvolution(
-        fileSchema, rp.getReaderSchema(), includedColumns);
     for (int i = 0; i < columnCount; ++i) {
-      if (!schemaEvolution.isPPDSafeConversion(columnIds.get(i))) {
+      int colId = columnIds == null ? i : columnIds.get(i);
+      if (!evolution.isPPDSafeConversion(colId)) {
         LlapIoImpl.LOG.warn("Unsupported schema evolution! Disabling Llap IO for {}", split);
         return false;
       }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
index 312f008f37..3cafad16c0 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/EncodedDataConsumer.java
@@ -133,10 +133,4 @@ public void unpause() {
     // We are just a relay; send unpause to encoded data producer.
     upstreamFeedback.unpause();
   }
-
-  @Override
-  public TypeDescription getFileSchema() {
-    // TODO: the ORC-specific method should be removed from the interface instead.
-    throw new UnsupportedOperationException();
-  }
 }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
index 61d385e4c1..0d9779f4c8 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/GenericColumnVectorProducer.java
@@ -174,10 +174,93 @@ public SerDeFileMetadata(Deserializer sourceSerDe) throws SerDeException {
       addTypesFromSchema(schema);
     }
 
+    // Copied here until a utility version of this released in ORC.
+    public static List<TypeDescription> setTypeBuilderFromSchema(
+        OrcProto.Type.Builder type, TypeDescription schema) {
+      List<TypeDescription> children = schema.getChildren();
+      switch (schema.getCategory()) {
+        case BOOLEAN:
+          type.setKind(OrcProto.Type.Kind.BOOLEAN);
+          break;
+        case BYTE:
+          type.setKind(OrcProto.Type.Kind.BYTE);
+          break;
+        case SHORT:
+          type.setKind(OrcProto.Type.Kind.SHORT);
+          break;
+        case INT:
+          type.setKind(OrcProto.Type.Kind.INT);
+          break;
+        case LONG:
+          type.setKind(OrcProto.Type.Kind.LONG);
+          break;
+        case FLOAT:
+          type.setKind(OrcProto.Type.Kind.FLOAT);
+          break;
+        case DOUBLE:
+          type.setKind(OrcProto.Type.Kind.DOUBLE);
+          break;
+        case STRING:
+          type.setKind(OrcProto.Type.Kind.STRING);
+          break;
+        case CHAR:
+          type.setKind(OrcProto.Type.Kind.CHAR);
+          type.setMaximumLength(schema.getMaxLength());
+          break;
+        case VARCHAR:
+          type.setKind(OrcProto.Type.Kind.VARCHAR);
+          type.setMaximumLength(schema.getMaxLength());
+          break;
+        case BINARY:
+          type.setKind(OrcProto.Type.Kind.BINARY);
+          break;
+        case TIMESTAMP:
+          type.setKind(OrcProto.Type.Kind.TIMESTAMP);
+          break;
+        case DATE:
+          type.setKind(OrcProto.Type.Kind.DATE);
+          break;
+        case DECIMAL:
+          type.setKind(OrcProto.Type.Kind.DECIMAL);
+          type.setPrecision(schema.getPrecision());
+          type.setScale(schema.getScale());
+          break;
+        case LIST:
+          type.setKind(OrcProto.Type.Kind.LIST);
+          type.addSubtypes(children.get(0).getId());
+          break;
+        case MAP:
+          type.setKind(OrcProto.Type.Kind.MAP);
+          for(TypeDescription t: children) {
+            type.addSubtypes(t.getId());
+          }
+          break;
+        case STRUCT:
+          type.setKind(OrcProto.Type.Kind.STRUCT);
+          for(TypeDescription t: children) {
+            type.addSubtypes(t.getId());
+          }
+          for(String field: schema.getFieldNames()) {
+            type.addFieldNames(field);
+          }
+          break;
+        case UNION:
+          type.setKind(OrcProto.Type.Kind.UNION);
+          for(TypeDescription t: children) {
+            type.addSubtypes(t.getId());
+          }
+          break;
+        default:
+          throw new IllegalArgumentException("Unknown category: " +
+              schema.getCategory());
+      }
+      return children;
+    }
+
     private void addTypesFromSchema(TypeDescription schema) {
       // The same thing that WriterImpl does when writing the footer, but w/o the footer.
       OrcProto.Type.Builder type = OrcProto.Type.newBuilder();
-      List<TypeDescription> children = OrcUtils.setTypeBuilderFromSchema(type, schema);
+      List<TypeDescription> children = setTypeBuilderFromSchema(type, schema);
       orcTypes.add(type.build());
       if (children == null) return;
       for(TypeDescription child : children) {
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
index 7c89e821ff..ac031aa68a 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java
@@ -44,6 +44,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.orc.TypeDescription;
+import org.apache.orc.OrcConf;
 
 public class OrcColumnVectorProducer implements ColumnVectorProducer {
 
@@ -64,7 +65,7 @@ public OrcColumnVectorProducer(OrcMetadataCache metadataCache,
     this.lowLevelCache = lowLevelCache;
     this.bufferManager = bufferManager;
     this.conf = conf;
-    this._skipCorrupt = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_ORC_SKIP_CORRUPT_DATA);
+    this._skipCorrupt = OrcConf.SKIP_CORRUPT_DATA.getBoolean(conf);
     this.cacheMetrics = cacheMetrics;
     this.ioMetrics = ioMetrics;
   }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
index d0e70d1043..4295c1c5c2 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;
 import org.apache.hadoop.hive.llap.io.metadata.ConsumerFileMetadata;
 import org.apache.hadoop.hive.llap.io.metadata.ConsumerStripeMetadata;
+import org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata;
 import org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
@@ -47,13 +48,13 @@
 import org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl;
-import org.apache.orc.OrcUtils;
 import org.apache.orc.TypeDescription;
-import org.apache.orc.impl.PhysicalFsWriter;
+import org.apache.orc.impl.SchemaEvolution;
 import org.apache.orc.impl.TreeReaderFactory;
 import org.apache.orc.impl.TreeReaderFactory.StructTreeReader;
 import org.apache.orc.impl.TreeReaderFactory.TreeReader;
 import org.apache.orc.OrcProto;
+import org.apache.orc.impl.WriterImpl;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -71,7 +72,7 @@ public class OrcEncodedDataConsumer
   private final boolean skipCorrupt; // TODO: get rid of this
   private final QueryFragmentCounters counters;
   private boolean[] includedColumns;
-  private TypeDescription readerSchema;
+  private SchemaEvolution evolution;
 
   public OrcEncodedDataConsumer(
       Consumer<ColumnVectorBatch> consumer, int colCount, boolean skipCorrupt,
@@ -86,7 +87,7 @@ public void setFileMetadata(ConsumerFileMetadata f) {
     assert fileMetadata == null;
     fileMetadata = f;
     stripes = new ArrayList<>(f.getStripeCount());
-    codec = PhysicalFsWriter.createCodec(f.getCompressionKind());
+    codec = WriterImpl.createCodec(fileMetadata.getCompressionKind());
   }
 
   public void setStripeMetadata(ConsumerStripeMetadata m) {
@@ -124,9 +125,13 @@ protected void decodeBatch(OrcEncodedColumnBatch batch,
 
       if (columnReaders == null || !sameStripe) {
         int[] columnMapping = new int[schema.getChildren().size()];
+        TreeReaderFactory.Context context =
+            new TreeReaderFactory.ReaderContext()
+                .setSchemaEvolution(evolution)
+                .writerTimeZone(stripeMetadata.getWriterTimezone())
+                .skipCorrupt(skipCorrupt);
         StructTreeReader treeReader = EncodedTreeReaderFactory.createRootTreeReader(
-            schema, stripeMetadata.getEncodings(), batch, codec, skipCorrupt,
-            stripeMetadata.getWriterTimezone(), columnMapping);
+            schema, stripeMetadata.getEncodings(), batch, codec, context, columnMapping);
         this.columnReaders = treeReader.getChildReaders();
         this.columnMapping = Arrays.copyOf(columnMapping, columnReaders.length);
         positionInStreams(columnReaders, batch, stripeMetadata);
@@ -294,11 +299,6 @@ private long getRowCount(OrcProto.RowIndexEntry rowIndexEntry) {
     return rowIndexEntry.getStatistics().getNumberOfValues();
   }
 
-  @Override
-  public TypeDescription getFileSchema() {
-    return OrcUtils.convertTypeFromProtobuf(fileMetadata.getTypes(), 0);
-  }
-
   @Override
   public boolean[] getIncludedColumns() {
     return includedColumns;
@@ -308,11 +308,11 @@ public void setIncludedColumns(final boolean[] includedColumns) {
     this.includedColumns = includedColumns;
   }
 
-  public void setReaderSchema(TypeDescription readerSchema) {
-    this.readerSchema = readerSchema;
+  public void setSchemaEvolution(SchemaEvolution evolution) {
+    this.evolution = evolution;
   }
 
-  public TypeDescription getReaderSchema() {
-    return readerSchema;
+  public SchemaEvolution getSchemaEvolution() {
+    return evolution;
   }
 }
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ReadPipeline.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ReadPipeline.java
index 36f6c9c7bb..09a350b663 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ReadPipeline.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ReadPipeline.java
@@ -22,10 +22,10 @@
 import org.apache.hadoop.hive.llap.ConsumerFeedback;
 import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;
 import org.apache.orc.TypeDescription;
+import org.apache.orc.impl.SchemaEvolution;
 
 public interface ReadPipeline extends ConsumerFeedback<ColumnVectorBatch> {
   public Callable<Void> getReadCallable();
-  TypeDescription getFileSchema(); // TODO: this is ORC-specific and should be removed
-  TypeDescription getReaderSchema();
+  SchemaEvolution getSchemaEvolution();
   boolean[] getIncludedColumns();
-}
\ No newline at end of file
+}
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
index 79443452ab..fb004190df 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java
@@ -202,10 +202,11 @@ public OrcEncodedDataReader(LowLevelCache lowLevelCache, BufferUsageManager buff
       readerSchema = fileMetadata.getSchema();
     }
     globalIncludes = OrcInputFormat.genIncludedColumns(readerSchema, includedColumnIds);
-    evolution = new SchemaEvolution(fileMetadata.getSchema(), readerSchema, globalIncludes);
+    Reader.Options options = new Reader.Options(conf).include(globalIncludes);
+    evolution = new SchemaEvolution(fileMetadata.getSchema(), readerSchema, options);
     consumer.setFileMetadata(fileMetadata);
     consumer.setIncludedColumns(globalIncludes);
-    consumer.setReaderSchema(readerSchema);
+    consumer.setSchemaEvolution(evolution);
   }
 
   @Override
@@ -274,9 +275,6 @@ protected Void performDataRead() throws IOException {
     try {
       if (sarg != null && stride != 0) {
         // TODO: move this to a common method
-        TypeDescription schema = OrcUtils.convertTypeFromProtobuf(fileMetadata.getTypes(), 0);
-        SchemaEvolution evolution = new SchemaEvolution(schema,
-            null, globalIncludes);
         int[] filterColumns = RecordReaderImpl.mapSargColumnsToOrcInternalColIdx(
           sarg.getLeaves(), evolution);
         // included will not be null, row options will fill the array with trues if null
@@ -369,7 +367,8 @@ protected Void performDataRead() throws IOException {
             ensureMetadataReader();
             long startTimeHdfs = counters.startTimeCounter();
             stripeMetadata = new OrcStripeMetadata(new OrcBatchKey(fileKey, stripeIx, 0),
-                metadataReader, stripe, globalIncludes, sargColumns);
+                metadataReader, stripe, globalIncludes, sargColumns,
+                orcReader.getSchema(), orcReader.getWriterVersion());
             counters.incrTimeCounter(LlapIOCounters.HDFS_TIME_NS, startTimeHdfs);
             if (hasFileId && metadataCache != null) {
               stripeMetadata = metadataCache.putStripeMetadata(stripeMetadata);
@@ -625,7 +624,8 @@ private ArrayList<OrcStripeMetadata> readStripesMetadata(
         if (value == null) {
           long startTime = counters.startTimeCounter();
           value = new OrcStripeMetadata(new OrcBatchKey(fileKey, stripeIx, 0),
-              metadataReader, si, globalInc, sargColumns);
+              metadataReader, si, globalInc, sargColumns, orcReader.getSchema(),
+              orcReader.getWriterVersion());
           counters.incrTimeCounter(LlapIOCounters.HDFS_TIME_NS, startTime);
           if (hasFileId && metadataCache != null) {
             value = metadataCache.putStripeMetadata(value);
@@ -700,10 +700,9 @@ private boolean determineRgsToRead(boolean[] globalIncludes, int rowIndexStride,
       List<OrcProto.Type> types = fileMetadata.getTypes();
       String[] colNamesForSarg = OrcInputFormat.getSargColumnNames(
           columnNames, types, globalIncludes, fileMetadata.isOriginalFormat());
-      TypeDescription schema = OrcUtils.convertTypeFromProtobuf(types, 0);
-      SchemaEvolution schemaEvolution = new SchemaEvolution(schema, globalIncludes);
       sargApp = new RecordReaderImpl.SargApplier(sarg, colNamesForSarg,
-          rowIndexStride, schemaEvolution);
+          rowIndexStride, evolution,
+          OrcFile.WriterVersion.from(fileMetadata.getWriterVersionNum()));
     }
     boolean hasAnyData = false;
     // readState should have been initialized by this time with an empty array.
@@ -715,6 +714,7 @@ private boolean determineRgsToRead(boolean[] globalIncludes, int rowIndexStride,
       if (sargApp != null) {
         OrcStripeMetadata stripeMetadata = metadata.get(stripeIxMod);
         rgsToRead = sargApp.pickRowGroups(stripe, stripeMetadata.getRowIndexes(),
+            stripeMetadata.getBloomFilterKinds(),
             stripeMetadata.getBloomFilterIndexes(), true);
       }
       boolean isNone = rgsToRead == RecordReaderImpl.SargApplier.READ_NO_RGS,
@@ -914,14 +914,19 @@ public void open() throws IOException {
 
     @Override
     public OrcIndex readRowIndex(StripeInformation stripe,
+                                 TypeDescription fileSchema,
                                  OrcProto.StripeFooter footer,
+                                 boolean ignoreNonUtf8BloomFilter,
                                  boolean[] included,
                                  OrcProto.RowIndex[] indexes,
                                  boolean[] sargColumns,
+                                 org.apache.orc.OrcFile.WriterVersion version,
+                                 OrcProto.Stream.Kind[] bloomFilterKinds,
                                  OrcProto.BloomFilterIndex[] bloomFilterIndices
                                  ) throws IOException {
-      return orcDataReader.readRowIndex(stripe, footer, included, indexes,
-          sargColumns, bloomFilterIndices);
+      return orcDataReader.readRowIndex(stripe, fileSchema, footer,
+          ignoreNonUtf8BloomFilter, included, indexes,
+          sargColumns, version, bloomFilterKinds, bloomFilterIndices);
     }
 
     @Override
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
index 9ab26e6943..8d86d17ff6 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java
@@ -56,7 +56,8 @@
 import org.apache.hadoop.hive.ql.io.orc.OrcFile;
 import org.apache.hadoop.hive.ql.io.orc.OrcFile.WriterOptions;
 import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
-import org.apache.hadoop.hive.ql.io.orc.WriterImpl;
+import org.apache.hadoop.hive.ql.io.orc.Reader;
+import org.apache.hadoop.hive.ql.io.orc.Writer;
 import org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk;
 import org.apache.hadoop.hive.ql.io.orc.encoded.Reader.OrcEncodedColumnBatch;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
@@ -77,6 +78,7 @@
 import org.apache.hive.common.util.Ref;
 import org.apache.orc.CompressionCodec;
 import org.apache.orc.CompressionKind;
+import org.apache.orc.OrcConf;
 import org.apache.orc.OrcUtils;
 import org.apache.orc.OrcFile.EncodingStrategy;
 import org.apache.orc.OrcFile.Version;
@@ -84,8 +86,9 @@
 import org.apache.orc.OrcProto.ColumnEncoding;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.impl.OutStream;
-import org.apache.orc.impl.OutStream.OutputReceiver;
-import org.apache.orc.impl.PhysicalWriter;
+import org.apache.orc.PhysicalWriter;
+import org.apache.orc.PhysicalWriter.OutputReceiver;
+import org.apache.orc.impl.SchemaEvolution;
 import org.apache.orc.impl.StreamName;
 import org.apache.tez.common.CallableWithNdc;
 import org.apache.tez.common.counters.TezCounters;
@@ -180,7 +183,7 @@ public SerDeEncodedDataReader(SerDeLowLevelCacheImpl cache,
     this.parts = parts;
     this.daemonConf = new Configuration(daemonConf);
     // Disable dictionary encoding for the writer.
-    this.daemonConf.setDouble(ConfVars.HIVE_ORC_DICTIONARY_KEY_SIZE_THRESHOLD.varname, 0);
+    this.daemonConf.setDouble(OrcConf.DICTIONARY_KEY_SIZE_THRESHOLD.name(), 0);
     this.split = split;
     this.columnIds = columnIds;
     this.allocSize = determineAllocSize(bufferManager, daemonConf);
@@ -208,6 +211,9 @@ public SerDeEncodedDataReader(SerDeLowLevelCacheImpl cache,
     this.reporter = reporter;
     this.jobConf = jobConf;
     this.writerIncludes = OrcInputFormat.genIncludedColumns(schema, columnIds);
+    SchemaEvolution evolution = new SchemaEvolution(schema,
+        new Reader.Options(jobConf).include(writerIncludes));
+    consumer.setSchemaEvolution(evolution);
   }
 
   private static int determineAllocSize(BufferUsageManager bufferManager, Configuration conf) {
@@ -331,8 +337,8 @@ public String toCoordinateString() {
     private final List<Integer> columnIds;
     private final boolean[] writerIncludes;
     // These are global since ORC reuses objects between stripes.
-    private final Map<StreamName, OutStream> streams = new HashMap<>();
-    private final Map<Integer, List<CacheOutStream>> colStreams = new HashMap<>();
+    private final Map<StreamName, OutputReceiver> streams = new HashMap<>();
+    private final Map<Integer, List<CacheOutputReceiver>> colStreams = new HashMap<>();
     private final boolean doesSourceHaveIncludes;
 
     public CacheWriter(BufferUsageManager bufferManager, int bufferSize,
@@ -353,10 +359,6 @@ private void startStripe() {
       currentStripe = new CacheStripeData();
     }
 
-    @Override
-    public void initialize() throws IOException {
-    }
-
     @Override
     public void writeFileMetadata(OrcProto.Metadata.Builder builder) throws IOException {
     }
@@ -399,7 +401,8 @@ private String throwIncludesMismatchError(boolean[] translated) throws IOExcepti
     }
 
     @Override
-    public void writePostScript(OrcProto.PostScript.Builder builder) throws IOException {
+    public long writePostScript(OrcProto.PostScript.Builder builder) {
+      return 0;
     }
 
     @Override
@@ -426,41 +429,45 @@ public void discardData() {
     }
 
     @Override
-    public long getPhysicalStripeSize() {
-      return 0; // Always 0, no memory checks.
-    }
-
-    @Override
-    public boolean isCompressed() {
-      return false;
-    }
-
-    @Override
-    public OutStream getOrCreatePhysicalStream(StreamName name) throws IOException {
-      OutStream os = streams.get(name);
-      if (os != null) return os;
+    public OutputReceiver createDataStream(StreamName name) throws IOException {
+      OutputReceiver or = streams.get(name);
+      if (or != null) return or;
       if (isNeeded(name)) {
         if (LlapIoImpl.LOG.isTraceEnabled()) {
           LlapIoImpl.LOG.trace("Creating cache receiver for " + name);
         }
-        CacheOutputReceiver or = new CacheOutputReceiver(bufferManager, name);
-        CacheOutStream cos = new CacheOutStream(name.toString(), bufferSize, null, or);
-        os = cos;
-        List<CacheOutStream> list = colStreams.get(name.getColumn());
+        CacheOutputReceiver cor = new CacheOutputReceiver(bufferManager, name);
+        or = cor;
+        List<CacheOutputReceiver> list = colStreams.get(name.getColumn());
         if (list == null) {
           list = new ArrayList<>();
           colStreams.put(name.getColumn(), list);
         }
-        list.add(cos);
+        list.add(cor);
       } else {
         if (LlapIoImpl.LOG.isTraceEnabled()) {
           LlapIoImpl.LOG.trace("Creating null receiver for " + name);
         }
-        OutputReceiver or = new NullOutputReceiver(name);
-        os = new OutStream(name.toString(), bufferSize, null, or);
+        or = new NullOutputReceiver(name);
       }
-      streams.put(name, os);
-      return os;
+      streams.put(name, or);
+      return or;
+    }
+
+    @Override
+    public void writeHeader() throws IOException {
+
+    }
+
+    @Override
+    public void writeIndex(StreamName name, OrcProto.RowIndex.Builder index, CompressionCodec codec) throws IOException {
+      // TODO: right now we treat each slice as a stripe with a single RG and never bother
+      //       with indexes. In phase 4, we need to add indexing and filtering.
+    }
+
+    @Override
+    public void writeBloomFilter(StreamName name, OrcProto.BloomFilterIndex.Builder bloom, CompressionCodec codec) throws IOException {
+
     }
 
     @Override
@@ -495,21 +502,20 @@ public void finalizeStripe(
       }
       currentStripe.rowCount = si.getNumberOfRows();
       // ORC writer reuses streams, so we need to clean them here and extract data.
-      for (Map.Entry<Integer, List<CacheOutStream>> e : colStreams.entrySet()) {
-        List<CacheOutStream> streams = e.getValue();
+      for (Map.Entry<Integer, List<CacheOutputReceiver>> e : colStreams.entrySet()) {
+        int colIx = e.getKey();
+        List<CacheOutputReceiver> streams = e.getValue();
         List<CacheStreamData> data = new ArrayList<>(streams.size());
-        for (CacheOutStream stream : streams) {
-          stream.flush();
-          List<MemoryBuffer> buffers = stream.receiver.buffers;
+        for (CacheOutputReceiver receiver : streams) {
+          List<MemoryBuffer> buffers = receiver.buffers;
           if (buffers == null) {
             // This can happen e.g. for a data stream when all the values are null.
-            LlapIoImpl.LOG.debug("Buffers are null for " + stream.receiver.name);
+            LlapIoImpl.LOG.debug("Buffers are null for " + receiver.name);
           }
-          data.add(new CacheStreamData(stream.isSuppressed(), stream.receiver.name,
+          data.add(new CacheStreamData(receiver.suppressed, receiver.name,
               buffers == null ? new ArrayList<MemoryBuffer>() : new ArrayList<>(buffers)));
-          stream.clear();
+          receiver.clear();
         }
-        int colIx = e.getKey();
         if (doesSourceHaveIncludes) {
           int newColIx = getSparseOrcIndexFromDenseDest(colIx);
           if (LlapIoImpl.LOG.isTraceEnabled()) {
@@ -530,40 +536,18 @@ private int getSparseOrcIndexFromDenseDest(int denseColIx) {
       return columnIds.get(denseColIx - 1) + 1;
     }
 
-    @Override
-    public long estimateMemory() {
-      return 0; // We never ever use any memory.
-    }
-
-    @Override
-    public void writeIndexStream(StreamName name,
-        OrcProto.RowIndex.Builder rowIndex) throws IOException {
-      // TODO: right now we treat each slice as a stripe with a single RG and never bother
-      //       with indexes. In phase 4, we need to add indexing and filtering.
-    }
-
     private boolean isNeeded(StreamName name) {
       return doesSourceHaveIncludes || writerIncludes[name.getColumn()];
     }
 
-    @Override
-    public void writeBloomFilterStream(StreamName streamName,
-        OrcProto.BloomFilterIndex.Builder bloomFilterIndex) throws IOException {
-    }
-
-
     @Override
     public void flush() throws IOException {
     }
 
     @Override
-    public long getRawWriterPosition() throws IOException {
-      return -1; // Meaningless for this writer.
-    }
-
-    @Override
-    public void appendRawStripe(byte[] stripe, int offset, int length,
-        OrcProto.StripeInformation.Builder dirEntry) throws IOException {
+    public void appendRawStripe(ByteBuffer stripe,
+                                OrcProto.StripeInformation.Builder dirEntry
+                                ) throws IOException {
       throw new UnsupportedOperationException(); // Only used in ACID writer.
     }
 
@@ -586,6 +570,7 @@ private static final class CacheOutputReceiver implements CacheOutput, OutputRec
     private final StreamName name;
     private List<MemoryBuffer> buffers = null;
     private int lastBufferPos = -1;
+    private boolean suppressed = false;
 
     public CacheOutputReceiver(BufferUsageManager bufferManager, StreamName name) {
       this.bufferManager = bufferManager;
@@ -595,6 +580,14 @@ public CacheOutputReceiver(BufferUsageManager bufferManager, StreamName name) {
     public void clear() {
       buffers = null;
       lastBufferPos = -1;
+      suppressed = false;
+    }
+
+    @Override
+    public void suppress() {
+      suppressed = true;
+      buffers = null;
+      lastBufferPos = -1;
     }
 
     @Override
@@ -655,6 +648,10 @@ public NullOutputReceiver(StreamName name) {
     @Override
     public void output(ByteBuffer buffer) throws IOException {
     }
+
+    @Override
+    public void suppress() {
+    }
   }
 
   protected Void performDataRead() throws IOException {
@@ -1024,7 +1021,7 @@ public void readSplitFromFile(FileSplit split, boolean[] splitIncludes, StripeDa
           daemonConf, jobConf, split.getPath(), originalOi, columnIds);
       cacheWriter = new CacheWriter(
           bufferManager, allocSize, columnIds, splitIncludes, writer.hasIncludes());
-      writer.init(new WriterImpl(cacheWriter, null,
+      writer.init(OrcFile.createWriter(split.getPath(),
           createOrcWriterOptions(writer.getDestinationOi())));
 
       int rowsPerSlice = 0;
@@ -1106,7 +1103,7 @@ public void readSplitFromFile(FileSplit split, boolean[] splitIncludes, StripeDa
   interface EncodingWriter {
     void writeOneRow(Writable row) throws IOException;
     StructObjectInspector getDestinationOi();
-    void init(WriterImpl orcWriter);
+    void init(Writer orcWriter);
     boolean hasIncludes();
     void writeIntermediateFooter() throws IOException;
     void flushIntermediateData() throws IOException;
@@ -1114,7 +1111,7 @@ interface EncodingWriter {
   }
 
   static class DeserialerOrcWriter implements EncodingWriter {
-    private WriterImpl orcWriter;
+    private Writer orcWriter;
     private final Deserializer sourceSerDe;
     private final StructObjectInspector sourceOi;
 
@@ -1124,7 +1121,7 @@ public DeserialerOrcWriter(Deserializer sourceSerDe, StructObjectInspector sourc
     }
 
     @Override
-    public void init(WriterImpl orcWriter) {
+    public void init(Writer orcWriter) {
       this.orcWriter = orcWriter;
     }
 
@@ -1169,7 +1166,8 @@ private WriterOptions createOrcWriterOptions(ObjectInspector sourceOi) throws IO
     return OrcFile.writerOptions(daemonConf).stripeSize(Long.MAX_VALUE).blockSize(Long.MAX_VALUE)
         .rowIndexStride(Integer.MAX_VALUE) // For now, do not limit this - one RG per split
         .blockPadding(false).compress(CompressionKind.NONE).version(Version.CURRENT)
-        .encodingStrategy(EncodingStrategy.SPEED).bloomFilterColumns(null).inspector(sourceOi);
+        .encodingStrategy(EncodingStrategy.SPEED).bloomFilterColumns(null).inspector(sourceOi)
+        .physicalWriter(cacheWriter);
   }
 
   private ObjectInspector getOiFromSerDe() throws IOException {
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java
index 98fc9dfb4f..63a3be26e1 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/VertorDeserializeOrcWriter.java
@@ -37,7 +37,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
-import org.apache.hadoop.hive.ql.io.orc.WriterImpl;
+import org.apache.hadoop.hive.ql.io.orc.Writer;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.PartitionDesc;
 import org.apache.hadoop.hive.serde.serdeConstants;
@@ -58,7 +58,7 @@
 
 /** The class that writes rows from a text reader to an ORC writer using VectorDeserializeRow. */
 class VertorDeserializeOrcWriter implements EncodingWriter {
-  private WriterImpl orcWriter;
+  private Writer orcWriter;
   private final LazySimpleDeserializeRead deserializeRead;
   private final VectorDeserializeRow<?> vectorDeserializeRow;
   private final VectorizedRowBatch sourceBatch, destinationBatch;
@@ -255,7 +255,7 @@ public void close() throws IOException {
   }
 
   @Override
-  public void init(WriterImpl orcWriter) {
+  public void init(Writer orcWriter) {
     this.orcWriter = orcWriter;
   }
 }
\ No newline at end of file
diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java
index 5ef16789b4..1f3f7ea05e 100644
--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java
+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcStripeMetadata.java
@@ -22,27 +22,30 @@
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
-import java.util.TimeZone;
 
 import org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator;
 import org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator.ObjectEstimator;
 import org.apache.hadoop.hive.llap.cache.EvictionDispatcher;
 import org.apache.hadoop.hive.llap.cache.LlapCacheableBuffer;
 import org.apache.hadoop.hive.ql.io.SyntheticFileId;
+import org.apache.hadoop.hive.ql.io.orc.OrcFile;
 import org.apache.hadoop.hive.ql.io.orc.encoded.OrcBatchKey;
 import org.apache.orc.DataReader;
 import org.apache.orc.OrcProto;
 import org.apache.orc.OrcProto.RowIndexEntry;
 import org.apache.orc.StripeInformation;
+import org.apache.orc.TypeDescription;
 import org.apache.orc.impl.OrcIndex;
 
 public class OrcStripeMetadata extends LlapCacheableBuffer implements ConsumerStripeMetadata {
+  private final TypeDescription schema;
   private final OrcBatchKey stripeKey;
   private final List<OrcProto.ColumnEncoding> encodings;
   private final List<OrcProto.Stream> streams;
   private final String writerTimezone;
   private final long rowCount;
   private OrcIndex rowIndex;
+  private OrcFile.WriterVersion writerVersion;
 
   private final int estimatedMemUsage;
 
@@ -59,16 +62,20 @@ public class OrcStripeMetadata extends LlapCacheableBuffer implements ConsumerSt
   }
 
   public OrcStripeMetadata(OrcBatchKey stripeKey, DataReader mr, StripeInformation stripe,
-      boolean[] includes, boolean[] sargColumns) throws IOException {
+                           boolean[] includes, boolean[] sargColumns, TypeDescription schema,
+                           OrcFile.WriterVersion writerVersion) throws IOException {
+    this.schema = schema;
     this.stripeKey = stripeKey;
     OrcProto.StripeFooter footer = mr.readStripeFooter(stripe);
     streams = footer.getStreamsList();
     encodings = footer.getColumnsList();
     writerTimezone = footer.getWriterTimezone();
     rowCount = stripe.getNumberOfRows();
-    rowIndex = mr.readRowIndex(stripe, footer, includes, null, sargColumns, null);
+    rowIndex = mr.readRowIndex(stripe, schema, footer, true, includes, null,
+        sargColumns, writerVersion, null, null);
 
     estimatedMemUsage = SIZE_ESTIMATOR.estimate(this, SIZE_ESTIMATORS);
+    this.writerVersion = writerVersion;
   }
 
   private OrcStripeMetadata(Object id) {
@@ -76,6 +83,7 @@ private OrcStripeMetadata(Object id) {
     encodings = new ArrayList<>();
     streams = new ArrayList<>();
     writerTimezone = "";
+    schema = TypeDescription.fromString("struct<x:int>");
     rowCount = estimatedMemUsage = 0;
   }
 
@@ -90,7 +98,9 @@ public static OrcStripeMetadata createDummy(Object id) {
     OrcProto.BloomFilterIndex bfi = OrcProto.BloomFilterIndex.newBuilder().addBloomFilter(
         OrcProto.BloomFilter.newBuilder().addBitset(0)).build();
     dummy.rowIndex = new OrcIndex(
-        new OrcProto.RowIndex[] { ri }, new OrcProto.BloomFilterIndex[] { bfi });
+        new OrcProto.RowIndex[] { ri },
+        new OrcProto.Stream.Kind[] { OrcProto.Stream.Kind.BLOOM_FILTER_UTF8 },
+        new OrcProto.BloomFilterIndex[] { bfi });
     return dummy;
   }
 
@@ -113,8 +123,10 @@ public void loadMissingIndexes(DataReader mr, StripeInformation stripe, boolean[
       superset[i] = superset[i] || (existing[i] != null);
     }
     // TODO: should we save footer to avoid a read here?
-    rowIndex = mr.readRowIndex(stripe, null, superset, rowIndex.getRowGroupIndex(),
-        sargColumns, rowIndex.getBloomFilterIndex());
+    rowIndex = mr.readRowIndex(stripe, schema, null, true, includes,
+        rowIndex.getRowGroupIndex(),
+        sargColumns, writerVersion, rowIndex.getBloomFilterKinds(),
+        rowIndex.getBloomFilterIndex());
     // TODO: theoretically, we should re-estimate memory usage here and update memory manager
   }
 
@@ -126,6 +138,10 @@ public OrcProto.RowIndex[] getRowIndexes() {
     return rowIndex.getRowGroupIndex();
   }
 
+  public OrcProto.Stream.Kind[] getBloomFilterKinds() {
+    return rowIndex.getBloomFilterKinds();
+  }
+
   public OrcProto.BloomFilterIndex[] getBloomFilterIndexes() {
     return rowIndex.getBloomFilterIndex();
   }
diff --git a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestIncrementalObjectSizeEstimator.java b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestIncrementalObjectSizeEstimator.java
index 183fb1b450..13c7767a3b 100644
--- a/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestIncrementalObjectSizeEstimator.java
+++ b/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestIncrementalObjectSizeEstimator.java
@@ -29,6 +29,8 @@
 
 import org.apache.hadoop.hive.common.io.DiskRangeList;
 import org.apache.orc.DataReader;
+import org.apache.orc.OrcFile;
+import org.apache.orc.TypeDescription;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.llap.IncrementalObjectSizeEstimator;
@@ -59,11 +61,19 @@ public void open() throws IOException {
 
     @Override
     public OrcIndex readRowIndex(StripeInformation stripe,
-                              OrcProto.StripeFooter footer,
-        boolean[] included, OrcProto.RowIndex[] indexes, boolean[] sargColumns,
-        OrcProto.BloomFilterIndex[] bloomFilterIndices) throws IOException {
+                                 TypeDescription fileSchema,
+                                 OrcProto.StripeFooter footer,
+                                 boolean ignoreNonUtf8BloomFilter,
+                                 boolean[] included,
+                                 OrcProto.RowIndex[] indexes,
+                                 boolean[] sargColumns,
+                                 OrcFile.WriterVersion version,
+                                 OrcProto.Stream.Kind[] bloomFilterKinds,
+                                 OrcProto.BloomFilterIndex[] bloomFilterIndices
+                                 ) throws IOException {
       if (isEmpty) {
         return new OrcIndex(new OrcProto.RowIndex[] { },
+            bloomFilterKinds,
             new OrcProto.BloomFilterIndex[] { });
       }
       OrcProto.ColumnStatistics cs = OrcProto.ColumnStatistics.newBuilder()
@@ -102,7 +112,9 @@ public OrcIndex readRowIndex(StripeInformation stripe,
         bfi = OrcProto.BloomFilterIndex.newBuilder().mergeFrom(baos.toByteArray()).build();
       }
       return new OrcIndex(
-          new OrcProto.RowIndex[] { ri, ri2 }, new OrcProto.BloomFilterIndex[] { bfi });
+          new OrcProto.RowIndex[] { ri, ri2 },
+          bloomFilterKinds,
+          new OrcProto.BloomFilterIndex[] { bfi });
     }
 
     @Override
@@ -166,20 +178,20 @@ public void testMetadata() throws IOException {
     mr.isEmpty = true;
     StripeInformation si = Mockito.mock(StripeInformation.class);
     Mockito.when(si.getNumberOfRows()).thenReturn(0L);
-    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null);
+    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);
     LOG.info("Estimated " + root.estimate(osm, map) + " for an empty OSM");
     mr.doStreamStep = true;
-    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null);
+    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);
     LOG.info("Estimated " + root.estimate(osm, map) + " for an empty OSM after serde");
 
     mr.isEmpty = false;
     stripeKey = new OrcBatchKey(0, 0, 0);
-    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null);
+    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);
     LOG.info("Estimated " + root.estimate(osm, map) + " for a test OSM");
     osm.resetRowIndex();
     LOG.info("Estimated " + root.estimate(osm, map) + " for a test OSM w/o row index");
     mr.doStreamStep = true;
-    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null);
+    osm = new OrcStripeMetadata(stripeKey, mr, si, null, null, null, null);
     LOG.info("Estimated " + root.estimate(osm, map) + " for a test OSM after serde");
     osm.resetRowIndex();
     LOG.info("Estimated " + root.estimate(osm, map) + " for a test OSM w/o row index after serde");
diff --git a/metastore/pom.xml b/metastore/pom.xml
index c1b707a7c3..35752ff86d 100644
--- a/metastore/pom.xml
+++ b/metastore/pom.xml
@@ -146,17 +146,33 @@
       <artifactId>hadoop-common</artifactId>
       <version>${hadoop.version}</version>
       <optional>true</optional>
-           <exclusions>
-            <exclusion>
-            <groupId>org.slf4j</groupId>
-            <artifactId>slf4j-log4j12</artifactId>
-          </exclusion>
-          <exclusion>
-            <groupId>commmons-logging</groupId>
-            <artifactId>commons-logging</artifactId>
-          </exclusion>
-        </exclusions>
-   </dependency>
+      <exclusions>
+        <exclusion>
+          <groupId>org.slf4j</groupId>
+          <artifactId>slf4j-log4j12</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>commmons-logging</groupId>
+          <artifactId>commons-logging</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
+    <dependency>
+      <groupId>org.apache.hadoop</groupId>
+      <artifactId>hadoop-hdfs</artifactId>
+      <version>${hadoop.version}</version>
+      <optional>true</optional>
+      <exclusions>
+        <exclusion>
+          <groupId>org.slf4j</groupId>
+          <artifactId>slf4j-log4j12</artifactId>
+        </exclusion>
+        <exclusion>
+          <groupId>commmons-logging</groupId>
+          <artifactId>commons-logging</artifactId>
+        </exclusion>
+      </exclusions>
+    </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-mapreduce-client-core</artifactId>
diff --git a/orc/pom.xml b/orc/pom.xml
deleted file mode 100644
index f75b91ce63..0000000000
--- a/orc/pom.xml
+++ /dev/null
@@ -1,184 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-      http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.hive</groupId>
-    <artifactId>hive</artifactId>
-    <version>2.2.0-SNAPSHOT</version>
-    <relativePath>../pom.xml</relativePath>
-  </parent>
-
-  <artifactId>hive-orc</artifactId>
-  <packaging>jar</packaging>
-  <name>Hive ORC</name>
-
-  <properties>
-    <hive.path.to.root>..</hive.path.to.root>
-  </properties>
-
-  <dependencies>
-    <dependency>
-      <groupId>org.apache.hive</groupId>
-      <artifactId>hive-storage-api</artifactId>
-    </dependency>
-
-    <!-- inter-project -->
-    <dependency>
-      <groupId>com.google.protobuf</groupId>
-      <artifactId>protobuf-java</artifactId>
-      <version>${protobuf.version}</version>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <version>${hadoop.version}</version>
-      <exclusions>
-        <exclusion>
-          <groupId>javax.servlet</groupId>
-          <artifactId>servlet-api</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>javax.servlet.jsp</groupId>
-          <artifactId>jsp-api</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.mortbay.jetty</groupId>
-          <artifactId>jetty</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.mortbay.jetty</groupId>
-          <artifactId>jetty-util</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.avro</groupId>
-          <artifactId>avro</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <version>${hadoop.version}</version>
-      <exclusions>
-        <exclusion>
-          <groupId>javax.servlet</groupId>
-          <artifactId>servlet-api</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>javax.servlet.jsp</groupId>
-          <artifactId>jsp-api</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.mortbay.jetty</groupId>
-          <artifactId>jetty</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.mortbay.jetty</groupId>
-          <artifactId>jetty-util</artifactId>
-        </exclusion>
-        <exclusion>
-          <groupId>org.apache.avro</groupId>
-          <artifactId>avro</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-    <dependency>
-      <groupId>org.iq80.snappy</groupId>
-      <artifactId>snappy</artifactId>
-      <version>${snappy.version}</version>
-    </dependency>
-
-    <!-- test inter-project -->
-    <dependency>
-      <groupId>junit</groupId>
-      <artifactId>junit</artifactId>
-      <version>${junit.version}</version>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.mockito</groupId>
-      <artifactId>mockito-all</artifactId>
-      <version>${mockito-all.version}</version>
-      <scope>test</scope>
-    </dependency>
-  </dependencies>
-
-  <profiles>
-    <profile>
-      <id>protobuf</id>
-      <build>
-        <plugins>
-	  <plugin>
-            <groupId>com.github.os72</groupId>
-            <artifactId>protoc-jar-maven-plugin</artifactId>
-            <version>3.0.0-a3</version>
-            <executions>
-              <execution>
-                <phase>generate-sources</phase>
-                <goals>
-                  <goal>run</goal>
-                </goals>
-                <configuration>
-                  <protocVersion>2.5.0</protocVersion>
-                  <addSources>none</addSources>
-                  <outputDirectory>src/gen/protobuf-java</outputDirectory>
-                  <includeDirectories>
-                    <include>src/protobuf</include>
-                  </includeDirectories>
-                  <inputDirectories>
-                    <include>src/protobuf</include>
-                  </inputDirectories>
-                </configuration>
-              </execution>
-            </executions>
-	  </plugin>
-        </plugins>
-      </build>
-    </profile>
-  </profiles>
-
-  <build>
-    <sourceDirectory>${basedir}/src/java</sourceDirectory>
-    <testSourceDirectory>${basedir}/src/test</testSourceDirectory>
-    <testResources>
-      <testResource>
-        <directory>${basedir}/src/test/resources</directory>
-      </testResource>
-    </testResources>
-    <plugins>
-      <plugin>
-        <groupId>org.codehaus.mojo</groupId>
-        <artifactId>build-helper-maven-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>add-source</id>
-            <phase>generate-sources</phase>
-            <goals>
-              <goal>add-source</goal>
-            </goals>
-            <configuration>
-              <sources>
-                <source>src/gen/protobuf-java</source>
-              </sources>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-  </build>
-</project>
diff --git a/orc/src/gen/protobuf-java/org/apache/orc/OrcProto.java b/orc/src/gen/protobuf-java/org/apache/orc/OrcProto.java
deleted file mode 100644
index 32193ec5f2..0000000000
--- a/orc/src/gen/protobuf-java/org/apache/orc/OrcProto.java
+++ /dev/null
@@ -1,20179 +0,0 @@
-// Generated by the protocol buffer compiler.  DO NOT EDIT!
-// source: orc_proto.proto
-
-package org.apache.orc;
-
-public final class OrcProto {
-  private OrcProto() {}
-  public static void registerAllExtensions(
-      com.google.protobuf.ExtensionRegistry registry) {
-  }
-  /**
-   * Protobuf enum {@code orc.proto.CompressionKind}
-   */
-  public enum CompressionKind
-      implements com.google.protobuf.ProtocolMessageEnum {
-    /**
-     * <code>NONE = 0;</code>
-     */
-    NONE(0, 0),
-    /**
-     * <code>ZLIB = 1;</code>
-     */
-    ZLIB(1, 1),
-    /**
-     * <code>SNAPPY = 2;</code>
-     */
-    SNAPPY(2, 2),
-    /**
-     * <code>LZO = 3;</code>
-     */
-    LZO(3, 3),
-    ;
-
-    /**
-     * <code>NONE = 0;</code>
-     */
-    public static final int NONE_VALUE = 0;
-    /**
-     * <code>ZLIB = 1;</code>
-     */
-    public static final int ZLIB_VALUE = 1;
-    /**
-     * <code>SNAPPY = 2;</code>
-     */
-    public static final int SNAPPY_VALUE = 2;
-    /**
-     * <code>LZO = 3;</code>
-     */
-    public static final int LZO_VALUE = 3;
-
-
-    public final int getNumber() { return value; }
-
-    public static CompressionKind valueOf(int value) {
-      switch (value) {
-        case 0: return NONE;
-        case 1: return ZLIB;
-        case 2: return SNAPPY;
-        case 3: return LZO;
-        default: return null;
-      }
-    }
-
-    public static com.google.protobuf.Internal.EnumLiteMap<CompressionKind>
-        internalGetValueMap() {
-      return internalValueMap;
-    }
-    private static com.google.protobuf.Internal.EnumLiteMap<CompressionKind>
-        internalValueMap =
-          new com.google.protobuf.Internal.EnumLiteMap<CompressionKind>() {
-            public CompressionKind findValueByNumber(int number) {
-              return CompressionKind.valueOf(number);
-            }
-          };
-
-    public final com.google.protobuf.Descriptors.EnumValueDescriptor
-        getValueDescriptor() {
-      return getDescriptor().getValues().get(index);
-    }
-    public final com.google.protobuf.Descriptors.EnumDescriptor
-        getDescriptorForType() {
-      return getDescriptor();
-    }
-    public static final com.google.protobuf.Descriptors.EnumDescriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.getDescriptor().getEnumTypes().get(0);
-    }
-
-    private static final CompressionKind[] VALUES = values();
-
-    public static CompressionKind valueOf(
-        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
-      if (desc.getType() != getDescriptor()) {
-        throw new java.lang.IllegalArgumentException(
-          "EnumValueDescriptor is not for this type.");
-      }
-      return VALUES[desc.getIndex()];
-    }
-
-    private final int index;
-    private final int value;
-
-    private CompressionKind(int index, int value) {
-      this.index = index;
-      this.value = value;
-    }
-
-    // @@protoc_insertion_point(enum_scope:orc.proto.CompressionKind)
-  }
-
-  public interface IntegerStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional sint64 minimum = 1;
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     */
-    boolean hasMinimum();
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     */
-    long getMinimum();
-
-    // optional sint64 maximum = 2;
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    boolean hasMaximum();
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    long getMaximum();
-
-    // optional sint64 sum = 3;
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     */
-    boolean hasSum();
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     */
-    long getSum();
-  }
-  /**
-   * Protobuf type {@code orc.proto.IntegerStatistics}
-   */
-  public static final class IntegerStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements IntegerStatisticsOrBuilder {
-    // Use IntegerStatistics.newBuilder() to construct.
-    private IntegerStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private IntegerStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final IntegerStatistics defaultInstance;
-    public static IntegerStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public IntegerStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private IntegerStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readSInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readSInt64();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readSInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_IntegerStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_IntegerStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.IntegerStatistics.class, org.apache.orc.OrcProto.IntegerStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<IntegerStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<IntegerStatistics>() {
-      public IntegerStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new IntegerStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<IntegerStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional sint64 minimum = 1;
-    public static final int MINIMUM_FIELD_NUMBER = 1;
-    private long minimum_;
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     */
-    public boolean hasMinimum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     */
-    public long getMinimum() {
-      return minimum_;
-    }
-
-    // optional sint64 maximum = 2;
-    public static final int MAXIMUM_FIELD_NUMBER = 2;
-    private long maximum_;
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    public boolean hasMaximum() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    public long getMaximum() {
-      return maximum_;
-    }
-
-    // optional sint64 sum = 3;
-    public static final int SUM_FIELD_NUMBER = 3;
-    private long sum_;
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     */
-    public boolean hasSum() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     */
-    public long getSum() {
-      return sum_;
-    }
-
-    private void initFields() {
-      minimum_ = 0L;
-      maximum_ = 0L;
-      sum_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeSInt64(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeSInt64(2, maximum_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeSInt64(3, sum_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(2, maximum_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(3, sum_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.IntegerStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.IntegerStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.IntegerStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.IntegerStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_IntegerStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_IntegerStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.IntegerStatistics.class, org.apache.orc.OrcProto.IntegerStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.IntegerStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        minimum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        maximum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        sum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_IntegerStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.IntegerStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.IntegerStatistics build() {
-        org.apache.orc.OrcProto.IntegerStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.IntegerStatistics buildPartial() {
-        org.apache.orc.OrcProto.IntegerStatistics result = new org.apache.orc.OrcProto.IntegerStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.minimum_ = minimum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximum_ = maximum_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.sum_ = sum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.IntegerStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.IntegerStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.IntegerStatistics other) {
-        if (other == org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance()) return this;
-        if (other.hasMinimum()) {
-          setMinimum(other.getMinimum());
-        }
-        if (other.hasMaximum()) {
-          setMaximum(other.getMaximum());
-        }
-        if (other.hasSum()) {
-          setSum(other.getSum());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.IntegerStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.IntegerStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional sint64 minimum = 1;
-      private long minimum_ ;
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       */
-      public boolean hasMinimum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       */
-      public long getMinimum() {
-        return minimum_;
-      }
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       */
-      public Builder setMinimum(long value) {
-        bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       */
-      public Builder clearMinimum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        minimum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional sint64 maximum = 2;
-      private long maximum_ ;
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public boolean hasMaximum() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public long getMaximum() {
-        return maximum_;
-      }
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public Builder setMaximum(long value) {
-        bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public Builder clearMaximum() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        maximum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional sint64 sum = 3;
-      private long sum_ ;
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       */
-      public boolean hasSum() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       */
-      public long getSum() {
-        return sum_;
-      }
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       */
-      public Builder setSum(long value) {
-        bitField0_ |= 0x00000004;
-        sum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       */
-      public Builder clearSum() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        sum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.IntegerStatistics)
-    }
-
-    static {
-      defaultInstance = new IntegerStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.IntegerStatistics)
-  }
-
-  public interface DoubleStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional double minimum = 1;
-    /**
-     * <code>optional double minimum = 1;</code>
-     */
-    boolean hasMinimum();
-    /**
-     * <code>optional double minimum = 1;</code>
-     */
-    double getMinimum();
-
-    // optional double maximum = 2;
-    /**
-     * <code>optional double maximum = 2;</code>
-     */
-    boolean hasMaximum();
-    /**
-     * <code>optional double maximum = 2;</code>
-     */
-    double getMaximum();
-
-    // optional double sum = 3;
-    /**
-     * <code>optional double sum = 3;</code>
-     */
-    boolean hasSum();
-    /**
-     * <code>optional double sum = 3;</code>
-     */
-    double getSum();
-  }
-  /**
-   * Protobuf type {@code orc.proto.DoubleStatistics}
-   */
-  public static final class DoubleStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements DoubleStatisticsOrBuilder {
-    // Use DoubleStatistics.newBuilder() to construct.
-    private DoubleStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private DoubleStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final DoubleStatistics defaultInstance;
-    public static DoubleStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public DoubleStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private DoubleStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 9: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readDouble();
-              break;
-            }
-            case 17: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readDouble();
-              break;
-            }
-            case 25: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readDouble();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_DoubleStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_DoubleStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.DoubleStatistics.class, org.apache.orc.OrcProto.DoubleStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<DoubleStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<DoubleStatistics>() {
-      public DoubleStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new DoubleStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<DoubleStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional double minimum = 1;
-    public static final int MINIMUM_FIELD_NUMBER = 1;
-    private double minimum_;
-    /**
-     * <code>optional double minimum = 1;</code>
-     */
-    public boolean hasMinimum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional double minimum = 1;</code>
-     */
-    public double getMinimum() {
-      return minimum_;
-    }
-
-    // optional double maximum = 2;
-    public static final int MAXIMUM_FIELD_NUMBER = 2;
-    private double maximum_;
-    /**
-     * <code>optional double maximum = 2;</code>
-     */
-    public boolean hasMaximum() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional double maximum = 2;</code>
-     */
-    public double getMaximum() {
-      return maximum_;
-    }
-
-    // optional double sum = 3;
-    public static final int SUM_FIELD_NUMBER = 3;
-    private double sum_;
-    /**
-     * <code>optional double sum = 3;</code>
-     */
-    public boolean hasSum() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional double sum = 3;</code>
-     */
-    public double getSum() {
-      return sum_;
-    }
-
-    private void initFields() {
-      minimum_ = 0D;
-      maximum_ = 0D;
-      sum_ = 0D;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeDouble(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeDouble(2, maximum_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeDouble(3, sum_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeDoubleSize(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeDoubleSize(2, maximum_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeDoubleSize(3, sum_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DoubleStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.DoubleStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.DoubleStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.DoubleStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DoubleStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DoubleStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.DoubleStatistics.class, org.apache.orc.OrcProto.DoubleStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.DoubleStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        minimum_ = 0D;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        maximum_ = 0D;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        sum_ = 0D;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DoubleStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.DoubleStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.DoubleStatistics build() {
-        org.apache.orc.OrcProto.DoubleStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.DoubleStatistics buildPartial() {
-        org.apache.orc.OrcProto.DoubleStatistics result = new org.apache.orc.OrcProto.DoubleStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.minimum_ = minimum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximum_ = maximum_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.sum_ = sum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.DoubleStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.DoubleStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.DoubleStatistics other) {
-        if (other == org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance()) return this;
-        if (other.hasMinimum()) {
-          setMinimum(other.getMinimum());
-        }
-        if (other.hasMaximum()) {
-          setMaximum(other.getMaximum());
-        }
-        if (other.hasSum()) {
-          setSum(other.getSum());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.DoubleStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.DoubleStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional double minimum = 1;
-      private double minimum_ ;
-      /**
-       * <code>optional double minimum = 1;</code>
-       */
-      public boolean hasMinimum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional double minimum = 1;</code>
-       */
-      public double getMinimum() {
-        return minimum_;
-      }
-      /**
-       * <code>optional double minimum = 1;</code>
-       */
-      public Builder setMinimum(double value) {
-        bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional double minimum = 1;</code>
-       */
-      public Builder clearMinimum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        minimum_ = 0D;
-        onChanged();
-        return this;
-      }
-
-      // optional double maximum = 2;
-      private double maximum_ ;
-      /**
-       * <code>optional double maximum = 2;</code>
-       */
-      public boolean hasMaximum() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional double maximum = 2;</code>
-       */
-      public double getMaximum() {
-        return maximum_;
-      }
-      /**
-       * <code>optional double maximum = 2;</code>
-       */
-      public Builder setMaximum(double value) {
-        bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional double maximum = 2;</code>
-       */
-      public Builder clearMaximum() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        maximum_ = 0D;
-        onChanged();
-        return this;
-      }
-
-      // optional double sum = 3;
-      private double sum_ ;
-      /**
-       * <code>optional double sum = 3;</code>
-       */
-      public boolean hasSum() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional double sum = 3;</code>
-       */
-      public double getSum() {
-        return sum_;
-      }
-      /**
-       * <code>optional double sum = 3;</code>
-       */
-      public Builder setSum(double value) {
-        bitField0_ |= 0x00000004;
-        sum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional double sum = 3;</code>
-       */
-      public Builder clearSum() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        sum_ = 0D;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.DoubleStatistics)
-    }
-
-    static {
-      defaultInstance = new DoubleStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.DoubleStatistics)
-  }
-
-  public interface StringStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional string minimum = 1;
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    boolean hasMinimum();
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    java.lang.String getMinimum();
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    com.google.protobuf.ByteString
-        getMinimumBytes();
-
-    // optional string maximum = 2;
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    boolean hasMaximum();
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    java.lang.String getMaximum();
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    com.google.protobuf.ByteString
-        getMaximumBytes();
-
-    // optional sint64 sum = 3;
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     *
-     * <pre>
-     * sum will store the total length of all strings in a stripe
-     * </pre>
-     */
-    boolean hasSum();
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     *
-     * <pre>
-     * sum will store the total length of all strings in a stripe
-     * </pre>
-     */
-    long getSum();
-  }
-  /**
-   * Protobuf type {@code orc.proto.StringStatistics}
-   */
-  public static final class StringStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements StringStatisticsOrBuilder {
-    // Use StringStatistics.newBuilder() to construct.
-    private StringStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private StringStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final StringStatistics defaultInstance;
-    public static StringStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public StringStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private StringStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readBytes();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readSInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StringStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StringStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.StringStatistics.class, org.apache.orc.OrcProto.StringStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<StringStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<StringStatistics>() {
-      public StringStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new StringStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<StringStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional string minimum = 1;
-    public static final int MINIMUM_FIELD_NUMBER = 1;
-    private java.lang.Object minimum_;
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    public boolean hasMinimum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    public java.lang.String getMinimum() {
-      java.lang.Object ref = minimum_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          minimum_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    public com.google.protobuf.ByteString
-        getMinimumBytes() {
-      java.lang.Object ref = minimum_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        minimum_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    // optional string maximum = 2;
-    public static final int MAXIMUM_FIELD_NUMBER = 2;
-    private java.lang.Object maximum_;
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    public boolean hasMaximum() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    public java.lang.String getMaximum() {
-      java.lang.Object ref = maximum_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          maximum_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    public com.google.protobuf.ByteString
-        getMaximumBytes() {
-      java.lang.Object ref = maximum_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        maximum_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    // optional sint64 sum = 3;
-    public static final int SUM_FIELD_NUMBER = 3;
-    private long sum_;
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     *
-     * <pre>
-     * sum will store the total length of all strings in a stripe
-     * </pre>
-     */
-    public boolean hasSum() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional sint64 sum = 3;</code>
-     *
-     * <pre>
-     * sum will store the total length of all strings in a stripe
-     * </pre>
-     */
-    public long getSum() {
-      return sum_;
-    }
-
-    private void initFields() {
-      minimum_ = "";
-      maximum_ = "";
-      sum_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getMinimumBytes());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getMaximumBytes());
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeSInt64(3, sum_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getMinimumBytes());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getMaximumBytes());
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(3, sum_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StringStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.StringStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.StringStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.StringStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StringStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StringStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.StringStatistics.class, org.apache.orc.OrcProto.StringStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.StringStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        minimum_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        maximum_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
-        sum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StringStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.StringStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.StringStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.StringStatistics build() {
-        org.apache.orc.OrcProto.StringStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.StringStatistics buildPartial() {
-        org.apache.orc.OrcProto.StringStatistics result = new org.apache.orc.OrcProto.StringStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.minimum_ = minimum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximum_ = maximum_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.sum_ = sum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.StringStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.StringStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.StringStatistics other) {
-        if (other == org.apache.orc.OrcProto.StringStatistics.getDefaultInstance()) return this;
-        if (other.hasMinimum()) {
-          bitField0_ |= 0x00000001;
-          minimum_ = other.minimum_;
-          onChanged();
-        }
-        if (other.hasMaximum()) {
-          bitField0_ |= 0x00000002;
-          maximum_ = other.maximum_;
-          onChanged();
-        }
-        if (other.hasSum()) {
-          setSum(other.getSum());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.StringStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.StringStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional string minimum = 1;
-      private java.lang.Object minimum_ = "";
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public boolean hasMinimum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public java.lang.String getMinimum() {
-        java.lang.Object ref = minimum_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          minimum_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public com.google.protobuf.ByteString
-          getMinimumBytes() {
-        java.lang.Object ref = minimum_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          minimum_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public Builder setMinimum(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public Builder clearMinimum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        minimum_ = getDefaultInstance().getMinimum();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public Builder setMinimumBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-
-      // optional string maximum = 2;
-      private java.lang.Object maximum_ = "";
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public boolean hasMaximum() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public java.lang.String getMaximum() {
-        java.lang.Object ref = maximum_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          maximum_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public com.google.protobuf.ByteString
-          getMaximumBytes() {
-        java.lang.Object ref = maximum_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          maximum_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public Builder setMaximum(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public Builder clearMaximum() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        maximum_ = getDefaultInstance().getMaximum();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public Builder setMaximumBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-
-      // optional sint64 sum = 3;
-      private long sum_ ;
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       *
-       * <pre>
-       * sum will store the total length of all strings in a stripe
-       * </pre>
-       */
-      public boolean hasSum() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       *
-       * <pre>
-       * sum will store the total length of all strings in a stripe
-       * </pre>
-       */
-      public long getSum() {
-        return sum_;
-      }
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       *
-       * <pre>
-       * sum will store the total length of all strings in a stripe
-       * </pre>
-       */
-      public Builder setSum(long value) {
-        bitField0_ |= 0x00000004;
-        sum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 sum = 3;</code>
-       *
-       * <pre>
-       * sum will store the total length of all strings in a stripe
-       * </pre>
-       */
-      public Builder clearSum() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        sum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.StringStatistics)
-    }
-
-    static {
-      defaultInstance = new StringStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.StringStatistics)
-  }
-
-  public interface BucketStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated uint64 count = 1 [packed = true];
-    /**
-     * <code>repeated uint64 count = 1 [packed = true];</code>
-     */
-    java.util.List<java.lang.Long> getCountList();
-    /**
-     * <code>repeated uint64 count = 1 [packed = true];</code>
-     */
-    int getCountCount();
-    /**
-     * <code>repeated uint64 count = 1 [packed = true];</code>
-     */
-    long getCount(int index);
-  }
-  /**
-   * Protobuf type {@code orc.proto.BucketStatistics}
-   */
-  public static final class BucketStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements BucketStatisticsOrBuilder {
-    // Use BucketStatistics.newBuilder() to construct.
-    private BucketStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private BucketStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final BucketStatistics defaultInstance;
-    public static BucketStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public BucketStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private BucketStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                count_ = new java.util.ArrayList<java.lang.Long>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              count_.add(input.readUInt64());
-              break;
-            }
-            case 10: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
-                count_ = new java.util.ArrayList<java.lang.Long>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              while (input.getBytesUntilLimit() > 0) {
-                count_.add(input.readUInt64());
-              }
-              input.popLimit(limit);
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          count_ = java.util.Collections.unmodifiableList(count_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BucketStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BucketStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.BucketStatistics.class, org.apache.orc.OrcProto.BucketStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<BucketStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<BucketStatistics>() {
-      public BucketStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new BucketStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<BucketStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated uint64 count = 1 [packed = true];
-    public static final int COUNT_FIELD_NUMBER = 1;
-    private java.util.List<java.lang.Long> count_;
-    /**
-     * <code>repeated uint64 count = 1 [packed = true];</code>
-     */
-    public java.util.List<java.lang.Long>
-        getCountList() {
-      return count_;
-    }
-    /**
-     * <code>repeated uint64 count = 1 [packed = true];</code>
-     */
-    public int getCountCount() {
-      return count_.size();
-    }
-    /**
-     * <code>repeated uint64 count = 1 [packed = true];</code>
-     */
-    public long getCount(int index) {
-      return count_.get(index);
-    }
-    private int countMemoizedSerializedSize = -1;
-
-    private void initFields() {
-      count_ = java.util.Collections.emptyList();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (getCountList().size() > 0) {
-        output.writeRawVarint32(10);
-        output.writeRawVarint32(countMemoizedSerializedSize);
-      }
-      for (int i = 0; i < count_.size(); i++) {
-        output.writeUInt64NoTag(count_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      {
-        int dataSize = 0;
-        for (int i = 0; i < count_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeUInt64SizeNoTag(count_.get(i));
-        }
-        size += dataSize;
-        if (!getCountList().isEmpty()) {
-          size += 1;
-          size += com.google.protobuf.CodedOutputStream
-              .computeInt32SizeNoTag(dataSize);
-        }
-        countMemoizedSerializedSize = dataSize;
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BucketStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.BucketStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.BucketStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.BucketStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BucketStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BucketStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.BucketStatistics.class, org.apache.orc.OrcProto.BucketStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.BucketStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        count_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BucketStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.BucketStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.BucketStatistics build() {
-        org.apache.orc.OrcProto.BucketStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.BucketStatistics buildPartial() {
-        org.apache.orc.OrcProto.BucketStatistics result = new org.apache.orc.OrcProto.BucketStatistics(this);
-        int from_bitField0_ = bitField0_;
-        if (((bitField0_ & 0x00000001) == 0x00000001)) {
-          count_ = java.util.Collections.unmodifiableList(count_);
-          bitField0_ = (bitField0_ & ~0x00000001);
-        }
-        result.count_ = count_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.BucketStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.BucketStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.BucketStatistics other) {
-        if (other == org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance()) return this;
-        if (!other.count_.isEmpty()) {
-          if (count_.isEmpty()) {
-            count_ = other.count_;
-            bitField0_ = (bitField0_ & ~0x00000001);
-          } else {
-            ensureCountIsMutable();
-            count_.addAll(other.count_);
-          }
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.BucketStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.BucketStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated uint64 count = 1 [packed = true];
-      private java.util.List<java.lang.Long> count_ = java.util.Collections.emptyList();
-      private void ensureCountIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          count_ = new java.util.ArrayList<java.lang.Long>(count_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public java.util.List<java.lang.Long>
-          getCountList() {
-        return java.util.Collections.unmodifiableList(count_);
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public int getCountCount() {
-        return count_.size();
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public long getCount(int index) {
-        return count_.get(index);
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public Builder setCount(
-          int index, long value) {
-        ensureCountIsMutable();
-        count_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public Builder addCount(long value) {
-        ensureCountIsMutable();
-        count_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public Builder addAllCount(
-          java.lang.Iterable<? extends java.lang.Long> values) {
-        ensureCountIsMutable();
-        super.addAll(values, count_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint64 count = 1 [packed = true];</code>
-       */
-      public Builder clearCount() {
-        count_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000001);
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.BucketStatistics)
-    }
-
-    static {
-      defaultInstance = new BucketStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.BucketStatistics)
-  }
-
-  public interface DecimalStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional string minimum = 1;
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    boolean hasMinimum();
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    java.lang.String getMinimum();
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    com.google.protobuf.ByteString
-        getMinimumBytes();
-
-    // optional string maximum = 2;
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    boolean hasMaximum();
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    java.lang.String getMaximum();
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    com.google.protobuf.ByteString
-        getMaximumBytes();
-
-    // optional string sum = 3;
-    /**
-     * <code>optional string sum = 3;</code>
-     */
-    boolean hasSum();
-    /**
-     * <code>optional string sum = 3;</code>
-     */
-    java.lang.String getSum();
-    /**
-     * <code>optional string sum = 3;</code>
-     */
-    com.google.protobuf.ByteString
-        getSumBytes();
-  }
-  /**
-   * Protobuf type {@code orc.proto.DecimalStatistics}
-   */
-  public static final class DecimalStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements DecimalStatisticsOrBuilder {
-    // Use DecimalStatistics.newBuilder() to construct.
-    private DecimalStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private DecimalStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final DecimalStatistics defaultInstance;
-    public static DecimalStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public DecimalStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private DecimalStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readBytes();
-              break;
-            }
-            case 26: {
-              bitField0_ |= 0x00000004;
-              sum_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_DecimalStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_DecimalStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.DecimalStatistics.class, org.apache.orc.OrcProto.DecimalStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<DecimalStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<DecimalStatistics>() {
-      public DecimalStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new DecimalStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<DecimalStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional string minimum = 1;
-    public static final int MINIMUM_FIELD_NUMBER = 1;
-    private java.lang.Object minimum_;
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    public boolean hasMinimum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    public java.lang.String getMinimum() {
-      java.lang.Object ref = minimum_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          minimum_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string minimum = 1;</code>
-     */
-    public com.google.protobuf.ByteString
-        getMinimumBytes() {
-      java.lang.Object ref = minimum_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        minimum_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    // optional string maximum = 2;
-    public static final int MAXIMUM_FIELD_NUMBER = 2;
-    private java.lang.Object maximum_;
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    public boolean hasMaximum() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    public java.lang.String getMaximum() {
-      java.lang.Object ref = maximum_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          maximum_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string maximum = 2;</code>
-     */
-    public com.google.protobuf.ByteString
-        getMaximumBytes() {
-      java.lang.Object ref = maximum_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        maximum_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    // optional string sum = 3;
-    public static final int SUM_FIELD_NUMBER = 3;
-    private java.lang.Object sum_;
-    /**
-     * <code>optional string sum = 3;</code>
-     */
-    public boolean hasSum() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional string sum = 3;</code>
-     */
-    public java.lang.String getSum() {
-      java.lang.Object ref = sum_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          sum_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string sum = 3;</code>
-     */
-    public com.google.protobuf.ByteString
-        getSumBytes() {
-      java.lang.Object ref = sum_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        sum_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    private void initFields() {
-      minimum_ = "";
-      maximum_ = "";
-      sum_ = "";
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getMinimumBytes());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, getMaximumBytes());
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeBytes(3, getSumBytes());
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getMinimumBytes());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, getMaximumBytes());
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(3, getSumBytes());
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DecimalStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.DecimalStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.DecimalStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.DecimalStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DecimalStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DecimalStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.DecimalStatistics.class, org.apache.orc.OrcProto.DecimalStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.DecimalStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        minimum_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        maximum_ = "";
-        bitField0_ = (bitField0_ & ~0x00000002);
-        sum_ = "";
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DecimalStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.DecimalStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.DecimalStatistics build() {
-        org.apache.orc.OrcProto.DecimalStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.DecimalStatistics buildPartial() {
-        org.apache.orc.OrcProto.DecimalStatistics result = new org.apache.orc.OrcProto.DecimalStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.minimum_ = minimum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximum_ = maximum_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.sum_ = sum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.DecimalStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.DecimalStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.DecimalStatistics other) {
-        if (other == org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance()) return this;
-        if (other.hasMinimum()) {
-          bitField0_ |= 0x00000001;
-          minimum_ = other.minimum_;
-          onChanged();
-        }
-        if (other.hasMaximum()) {
-          bitField0_ |= 0x00000002;
-          maximum_ = other.maximum_;
-          onChanged();
-        }
-        if (other.hasSum()) {
-          bitField0_ |= 0x00000004;
-          sum_ = other.sum_;
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.DecimalStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.DecimalStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional string minimum = 1;
-      private java.lang.Object minimum_ = "";
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public boolean hasMinimum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public java.lang.String getMinimum() {
-        java.lang.Object ref = minimum_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          minimum_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public com.google.protobuf.ByteString
-          getMinimumBytes() {
-        java.lang.Object ref = minimum_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          minimum_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public Builder setMinimum(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public Builder clearMinimum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        minimum_ = getDefaultInstance().getMinimum();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string minimum = 1;</code>
-       */
-      public Builder setMinimumBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-
-      // optional string maximum = 2;
-      private java.lang.Object maximum_ = "";
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public boolean hasMaximum() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public java.lang.String getMaximum() {
-        java.lang.Object ref = maximum_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          maximum_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public com.google.protobuf.ByteString
-          getMaximumBytes() {
-        java.lang.Object ref = maximum_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          maximum_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public Builder setMaximum(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public Builder clearMaximum() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        maximum_ = getDefaultInstance().getMaximum();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string maximum = 2;</code>
-       */
-      public Builder setMaximumBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-
-      // optional string sum = 3;
-      private java.lang.Object sum_ = "";
-      /**
-       * <code>optional string sum = 3;</code>
-       */
-      public boolean hasSum() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional string sum = 3;</code>
-       */
-      public java.lang.String getSum() {
-        java.lang.Object ref = sum_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          sum_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string sum = 3;</code>
-       */
-      public com.google.protobuf.ByteString
-          getSumBytes() {
-        java.lang.Object ref = sum_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          sum_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string sum = 3;</code>
-       */
-      public Builder setSum(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
-        sum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string sum = 3;</code>
-       */
-      public Builder clearSum() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        sum_ = getDefaultInstance().getSum();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string sum = 3;</code>
-       */
-      public Builder setSumBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
-        sum_ = value;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.DecimalStatistics)
-    }
-
-    static {
-      defaultInstance = new DecimalStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.DecimalStatistics)
-  }
-
-  public interface DateStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional sint32 minimum = 1;
-    /**
-     * <code>optional sint32 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as days since epoch
-     * </pre>
-     */
-    boolean hasMinimum();
-    /**
-     * <code>optional sint32 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as days since epoch
-     * </pre>
-     */
-    int getMinimum();
-
-    // optional sint32 maximum = 2;
-    /**
-     * <code>optional sint32 maximum = 2;</code>
-     */
-    boolean hasMaximum();
-    /**
-     * <code>optional sint32 maximum = 2;</code>
-     */
-    int getMaximum();
-  }
-  /**
-   * Protobuf type {@code orc.proto.DateStatistics}
-   */
-  public static final class DateStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements DateStatisticsOrBuilder {
-    // Use DateStatistics.newBuilder() to construct.
-    private DateStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private DateStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final DateStatistics defaultInstance;
-    public static DateStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public DateStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private DateStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readSInt32();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readSInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_DateStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_DateStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.DateStatistics.class, org.apache.orc.OrcProto.DateStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<DateStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<DateStatistics>() {
-      public DateStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new DateStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<DateStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional sint32 minimum = 1;
-    public static final int MINIMUM_FIELD_NUMBER = 1;
-    private int minimum_;
-    /**
-     * <code>optional sint32 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as days since epoch
-     * </pre>
-     */
-    public boolean hasMinimum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional sint32 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as days since epoch
-     * </pre>
-     */
-    public int getMinimum() {
-      return minimum_;
-    }
-
-    // optional sint32 maximum = 2;
-    public static final int MAXIMUM_FIELD_NUMBER = 2;
-    private int maximum_;
-    /**
-     * <code>optional sint32 maximum = 2;</code>
-     */
-    public boolean hasMaximum() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional sint32 maximum = 2;</code>
-     */
-    public int getMaximum() {
-      return maximum_;
-    }
-
-    private void initFields() {
-      minimum_ = 0;
-      maximum_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeSInt32(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeSInt32(2, maximum_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt32Size(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt32Size(2, maximum_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.DateStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.DateStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.DateStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.DateStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DateStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DateStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.DateStatistics.class, org.apache.orc.OrcProto.DateStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.DateStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        minimum_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        maximum_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_DateStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.DateStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.DateStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.DateStatistics build() {
-        org.apache.orc.OrcProto.DateStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.DateStatistics buildPartial() {
-        org.apache.orc.OrcProto.DateStatistics result = new org.apache.orc.OrcProto.DateStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.minimum_ = minimum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximum_ = maximum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.DateStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.DateStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.DateStatistics other) {
-        if (other == org.apache.orc.OrcProto.DateStatistics.getDefaultInstance()) return this;
-        if (other.hasMinimum()) {
-          setMinimum(other.getMinimum());
-        }
-        if (other.hasMaximum()) {
-          setMaximum(other.getMaximum());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.DateStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.DateStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional sint32 minimum = 1;
-      private int minimum_ ;
-      /**
-       * <code>optional sint32 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as days since epoch
-       * </pre>
-       */
-      public boolean hasMinimum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional sint32 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as days since epoch
-       * </pre>
-       */
-      public int getMinimum() {
-        return minimum_;
-      }
-      /**
-       * <code>optional sint32 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as days since epoch
-       * </pre>
-       */
-      public Builder setMinimum(int value) {
-        bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint32 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as days since epoch
-       * </pre>
-       */
-      public Builder clearMinimum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        minimum_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // optional sint32 maximum = 2;
-      private int maximum_ ;
-      /**
-       * <code>optional sint32 maximum = 2;</code>
-       */
-      public boolean hasMaximum() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional sint32 maximum = 2;</code>
-       */
-      public int getMaximum() {
-        return maximum_;
-      }
-      /**
-       * <code>optional sint32 maximum = 2;</code>
-       */
-      public Builder setMaximum(int value) {
-        bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint32 maximum = 2;</code>
-       */
-      public Builder clearMaximum() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        maximum_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.DateStatistics)
-    }
-
-    static {
-      defaultInstance = new DateStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.DateStatistics)
-  }
-
-  public interface TimestampStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional sint64 minimum = 1;
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as milliseconds since epoch
-     * </pre>
-     */
-    boolean hasMinimum();
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as milliseconds since epoch
-     * </pre>
-     */
-    long getMinimum();
-
-    // optional sint64 maximum = 2;
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    boolean hasMaximum();
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    long getMaximum();
-  }
-  /**
-   * Protobuf type {@code orc.proto.TimestampStatistics}
-   */
-  public static final class TimestampStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements TimestampStatisticsOrBuilder {
-    // Use TimestampStatistics.newBuilder() to construct.
-    private TimestampStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private TimestampStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final TimestampStatistics defaultInstance;
-    public static TimestampStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public TimestampStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private TimestampStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              minimum_ = input.readSInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              maximum_ = input.readSInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_TimestampStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_TimestampStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.TimestampStatistics.class, org.apache.orc.OrcProto.TimestampStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<TimestampStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<TimestampStatistics>() {
-      public TimestampStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new TimestampStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<TimestampStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional sint64 minimum = 1;
-    public static final int MINIMUM_FIELD_NUMBER = 1;
-    private long minimum_;
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as milliseconds since epoch
-     * </pre>
-     */
-    public boolean hasMinimum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional sint64 minimum = 1;</code>
-     *
-     * <pre>
-     * min,max values saved as milliseconds since epoch
-     * </pre>
-     */
-    public long getMinimum() {
-      return minimum_;
-    }
-
-    // optional sint64 maximum = 2;
-    public static final int MAXIMUM_FIELD_NUMBER = 2;
-    private long maximum_;
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    public boolean hasMaximum() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional sint64 maximum = 2;</code>
-     */
-    public long getMaximum() {
-      return maximum_;
-    }
-
-    private void initFields() {
-      minimum_ = 0L;
-      maximum_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeSInt64(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeSInt64(2, maximum_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(1, minimum_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(2, maximum_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.TimestampStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.TimestampStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.TimestampStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.TimestampStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_TimestampStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_TimestampStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.TimestampStatistics.class, org.apache.orc.OrcProto.TimestampStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.TimestampStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        minimum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        maximum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_TimestampStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.TimestampStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.TimestampStatistics build() {
-        org.apache.orc.OrcProto.TimestampStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.TimestampStatistics buildPartial() {
-        org.apache.orc.OrcProto.TimestampStatistics result = new org.apache.orc.OrcProto.TimestampStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.minimum_ = minimum_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximum_ = maximum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.TimestampStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.TimestampStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.TimestampStatistics other) {
-        if (other == org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance()) return this;
-        if (other.hasMinimum()) {
-          setMinimum(other.getMinimum());
-        }
-        if (other.hasMaximum()) {
-          setMaximum(other.getMaximum());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.TimestampStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.TimestampStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional sint64 minimum = 1;
-      private long minimum_ ;
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as milliseconds since epoch
-       * </pre>
-       */
-      public boolean hasMinimum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as milliseconds since epoch
-       * </pre>
-       */
-      public long getMinimum() {
-        return minimum_;
-      }
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as milliseconds since epoch
-       * </pre>
-       */
-      public Builder setMinimum(long value) {
-        bitField0_ |= 0x00000001;
-        minimum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 minimum = 1;</code>
-       *
-       * <pre>
-       * min,max values saved as milliseconds since epoch
-       * </pre>
-       */
-      public Builder clearMinimum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        minimum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional sint64 maximum = 2;
-      private long maximum_ ;
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public boolean hasMaximum() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public long getMaximum() {
-        return maximum_;
-      }
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public Builder setMaximum(long value) {
-        bitField0_ |= 0x00000002;
-        maximum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 maximum = 2;</code>
-       */
-      public Builder clearMaximum() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        maximum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.TimestampStatistics)
-    }
-
-    static {
-      defaultInstance = new TimestampStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.TimestampStatistics)
-  }
-
-  public interface BinaryStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional sint64 sum = 1;
-    /**
-     * <code>optional sint64 sum = 1;</code>
-     *
-     * <pre>
-     * sum will store the total binary blob length in a stripe
-     * </pre>
-     */
-    boolean hasSum();
-    /**
-     * <code>optional sint64 sum = 1;</code>
-     *
-     * <pre>
-     * sum will store the total binary blob length in a stripe
-     * </pre>
-     */
-    long getSum();
-  }
-  /**
-   * Protobuf type {@code orc.proto.BinaryStatistics}
-   */
-  public static final class BinaryStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements BinaryStatisticsOrBuilder {
-    // Use BinaryStatistics.newBuilder() to construct.
-    private BinaryStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private BinaryStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final BinaryStatistics defaultInstance;
-    public static BinaryStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public BinaryStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private BinaryStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              sum_ = input.readSInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BinaryStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BinaryStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.BinaryStatistics.class, org.apache.orc.OrcProto.BinaryStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<BinaryStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<BinaryStatistics>() {
-      public BinaryStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new BinaryStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<BinaryStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional sint64 sum = 1;
-    public static final int SUM_FIELD_NUMBER = 1;
-    private long sum_;
-    /**
-     * <code>optional sint64 sum = 1;</code>
-     *
-     * <pre>
-     * sum will store the total binary blob length in a stripe
-     * </pre>
-     */
-    public boolean hasSum() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional sint64 sum = 1;</code>
-     *
-     * <pre>
-     * sum will store the total binary blob length in a stripe
-     * </pre>
-     */
-    public long getSum() {
-      return sum_;
-    }
-
-    private void initFields() {
-      sum_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeSInt64(1, sum_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeSInt64Size(1, sum_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BinaryStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.BinaryStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.BinaryStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.BinaryStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BinaryStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BinaryStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.BinaryStatistics.class, org.apache.orc.OrcProto.BinaryStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.BinaryStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        sum_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BinaryStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.BinaryStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.BinaryStatistics build() {
-        org.apache.orc.OrcProto.BinaryStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.BinaryStatistics buildPartial() {
-        org.apache.orc.OrcProto.BinaryStatistics result = new org.apache.orc.OrcProto.BinaryStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.sum_ = sum_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.BinaryStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.BinaryStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.BinaryStatistics other) {
-        if (other == org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance()) return this;
-        if (other.hasSum()) {
-          setSum(other.getSum());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.BinaryStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.BinaryStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional sint64 sum = 1;
-      private long sum_ ;
-      /**
-       * <code>optional sint64 sum = 1;</code>
-       *
-       * <pre>
-       * sum will store the total binary blob length in a stripe
-       * </pre>
-       */
-      public boolean hasSum() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional sint64 sum = 1;</code>
-       *
-       * <pre>
-       * sum will store the total binary blob length in a stripe
-       * </pre>
-       */
-      public long getSum() {
-        return sum_;
-      }
-      /**
-       * <code>optional sint64 sum = 1;</code>
-       *
-       * <pre>
-       * sum will store the total binary blob length in a stripe
-       * </pre>
-       */
-      public Builder setSum(long value) {
-        bitField0_ |= 0x00000001;
-        sum_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional sint64 sum = 1;</code>
-       *
-       * <pre>
-       * sum will store the total binary blob length in a stripe
-       * </pre>
-       */
-      public Builder clearSum() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        sum_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.BinaryStatistics)
-    }
-
-    static {
-      defaultInstance = new BinaryStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.BinaryStatistics)
-  }
-
-  public interface ColumnStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional uint64 numberOfValues = 1;
-    /**
-     * <code>optional uint64 numberOfValues = 1;</code>
-     */
-    boolean hasNumberOfValues();
-    /**
-     * <code>optional uint64 numberOfValues = 1;</code>
-     */
-    long getNumberOfValues();
-
-    // optional .orc.proto.IntegerStatistics intStatistics = 2;
-    /**
-     * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-     */
-    boolean hasIntStatistics();
-    /**
-     * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-     */
-    org.apache.orc.OrcProto.IntegerStatistics getIntStatistics();
-    /**
-     * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-     */
-    org.apache.orc.OrcProto.IntegerStatisticsOrBuilder getIntStatisticsOrBuilder();
-
-    // optional .orc.proto.DoubleStatistics doubleStatistics = 3;
-    /**
-     * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-     */
-    boolean hasDoubleStatistics();
-    /**
-     * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-     */
-    org.apache.orc.OrcProto.DoubleStatistics getDoubleStatistics();
-    /**
-     * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-     */
-    org.apache.orc.OrcProto.DoubleStatisticsOrBuilder getDoubleStatisticsOrBuilder();
-
-    // optional .orc.proto.StringStatistics stringStatistics = 4;
-    /**
-     * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-     */
-    boolean hasStringStatistics();
-    /**
-     * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-     */
-    org.apache.orc.OrcProto.StringStatistics getStringStatistics();
-    /**
-     * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-     */
-    org.apache.orc.OrcProto.StringStatisticsOrBuilder getStringStatisticsOrBuilder();
-
-    // optional .orc.proto.BucketStatistics bucketStatistics = 5;
-    /**
-     * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-     */
-    boolean hasBucketStatistics();
-    /**
-     * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-     */
-    org.apache.orc.OrcProto.BucketStatistics getBucketStatistics();
-    /**
-     * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-     */
-    org.apache.orc.OrcProto.BucketStatisticsOrBuilder getBucketStatisticsOrBuilder();
-
-    // optional .orc.proto.DecimalStatistics decimalStatistics = 6;
-    /**
-     * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-     */
-    boolean hasDecimalStatistics();
-    /**
-     * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-     */
-    org.apache.orc.OrcProto.DecimalStatistics getDecimalStatistics();
-    /**
-     * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-     */
-    org.apache.orc.OrcProto.DecimalStatisticsOrBuilder getDecimalStatisticsOrBuilder();
-
-    // optional .orc.proto.DateStatistics dateStatistics = 7;
-    /**
-     * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-     */
-    boolean hasDateStatistics();
-    /**
-     * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-     */
-    org.apache.orc.OrcProto.DateStatistics getDateStatistics();
-    /**
-     * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-     */
-    org.apache.orc.OrcProto.DateStatisticsOrBuilder getDateStatisticsOrBuilder();
-
-    // optional .orc.proto.BinaryStatistics binaryStatistics = 8;
-    /**
-     * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-     */
-    boolean hasBinaryStatistics();
-    /**
-     * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-     */
-    org.apache.orc.OrcProto.BinaryStatistics getBinaryStatistics();
-    /**
-     * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-     */
-    org.apache.orc.OrcProto.BinaryStatisticsOrBuilder getBinaryStatisticsOrBuilder();
-
-    // optional .orc.proto.TimestampStatistics timestampStatistics = 9;
-    /**
-     * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-     */
-    boolean hasTimestampStatistics();
-    /**
-     * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-     */
-    org.apache.orc.OrcProto.TimestampStatistics getTimestampStatistics();
-    /**
-     * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-     */
-    org.apache.orc.OrcProto.TimestampStatisticsOrBuilder getTimestampStatisticsOrBuilder();
-
-    // optional bool hasNull = 10;
-    /**
-     * <code>optional bool hasNull = 10;</code>
-     */
-    boolean hasHasNull();
-    /**
-     * <code>optional bool hasNull = 10;</code>
-     */
-    boolean getHasNull();
-  }
-  /**
-   * Protobuf type {@code orc.proto.ColumnStatistics}
-   */
-  public static final class ColumnStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements ColumnStatisticsOrBuilder {
-    // Use ColumnStatistics.newBuilder() to construct.
-    private ColumnStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private ColumnStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final ColumnStatistics defaultInstance;
-    public static ColumnStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public ColumnStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private ColumnStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              numberOfValues_ = input.readUInt64();
-              break;
-            }
-            case 18: {
-              org.apache.orc.OrcProto.IntegerStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000002) == 0x00000002)) {
-                subBuilder = intStatistics_.toBuilder();
-              }
-              intStatistics_ = input.readMessage(org.apache.orc.OrcProto.IntegerStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(intStatistics_);
-                intStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000002;
-              break;
-            }
-            case 26: {
-              org.apache.orc.OrcProto.DoubleStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000004) == 0x00000004)) {
-                subBuilder = doubleStatistics_.toBuilder();
-              }
-              doubleStatistics_ = input.readMessage(org.apache.orc.OrcProto.DoubleStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(doubleStatistics_);
-                doubleStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000004;
-              break;
-            }
-            case 34: {
-              org.apache.orc.OrcProto.StringStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000008) == 0x00000008)) {
-                subBuilder = stringStatistics_.toBuilder();
-              }
-              stringStatistics_ = input.readMessage(org.apache.orc.OrcProto.StringStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(stringStatistics_);
-                stringStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000008;
-              break;
-            }
-            case 42: {
-              org.apache.orc.OrcProto.BucketStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000010) == 0x00000010)) {
-                subBuilder = bucketStatistics_.toBuilder();
-              }
-              bucketStatistics_ = input.readMessage(org.apache.orc.OrcProto.BucketStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(bucketStatistics_);
-                bucketStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000010;
-              break;
-            }
-            case 50: {
-              org.apache.orc.OrcProto.DecimalStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000020) == 0x00000020)) {
-                subBuilder = decimalStatistics_.toBuilder();
-              }
-              decimalStatistics_ = input.readMessage(org.apache.orc.OrcProto.DecimalStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(decimalStatistics_);
-                decimalStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000020;
-              break;
-            }
-            case 58: {
-              org.apache.orc.OrcProto.DateStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000040) == 0x00000040)) {
-                subBuilder = dateStatistics_.toBuilder();
-              }
-              dateStatistics_ = input.readMessage(org.apache.orc.OrcProto.DateStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(dateStatistics_);
-                dateStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000040;
-              break;
-            }
-            case 66: {
-              org.apache.orc.OrcProto.BinaryStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000080) == 0x00000080)) {
-                subBuilder = binaryStatistics_.toBuilder();
-              }
-              binaryStatistics_ = input.readMessage(org.apache.orc.OrcProto.BinaryStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(binaryStatistics_);
-                binaryStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000080;
-              break;
-            }
-            case 74: {
-              org.apache.orc.OrcProto.TimestampStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000100) == 0x00000100)) {
-                subBuilder = timestampStatistics_.toBuilder();
-              }
-              timestampStatistics_ = input.readMessage(org.apache.orc.OrcProto.TimestampStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(timestampStatistics_);
-                timestampStatistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000100;
-              break;
-            }
-            case 80: {
-              bitField0_ |= 0x00000200;
-              hasNull_ = input.readBool();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.ColumnStatistics.class, org.apache.orc.OrcProto.ColumnStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<ColumnStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<ColumnStatistics>() {
-      public ColumnStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new ColumnStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<ColumnStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional uint64 numberOfValues = 1;
-    public static final int NUMBEROFVALUES_FIELD_NUMBER = 1;
-    private long numberOfValues_;
-    /**
-     * <code>optional uint64 numberOfValues = 1;</code>
-     */
-    public boolean hasNumberOfValues() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional uint64 numberOfValues = 1;</code>
-     */
-    public long getNumberOfValues() {
-      return numberOfValues_;
-    }
-
-    // optional .orc.proto.IntegerStatistics intStatistics = 2;
-    public static final int INTSTATISTICS_FIELD_NUMBER = 2;
-    private org.apache.orc.OrcProto.IntegerStatistics intStatistics_;
-    /**
-     * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-     */
-    public boolean hasIntStatistics() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-     */
-    public org.apache.orc.OrcProto.IntegerStatistics getIntStatistics() {
-      return intStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-     */
-    public org.apache.orc.OrcProto.IntegerStatisticsOrBuilder getIntStatisticsOrBuilder() {
-      return intStatistics_;
-    }
-
-    // optional .orc.proto.DoubleStatistics doubleStatistics = 3;
-    public static final int DOUBLESTATISTICS_FIELD_NUMBER = 3;
-    private org.apache.orc.OrcProto.DoubleStatistics doubleStatistics_;
-    /**
-     * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-     */
-    public boolean hasDoubleStatistics() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-     */
-    public org.apache.orc.OrcProto.DoubleStatistics getDoubleStatistics() {
-      return doubleStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-     */
-    public org.apache.orc.OrcProto.DoubleStatisticsOrBuilder getDoubleStatisticsOrBuilder() {
-      return doubleStatistics_;
-    }
-
-    // optional .orc.proto.StringStatistics stringStatistics = 4;
-    public static final int STRINGSTATISTICS_FIELD_NUMBER = 4;
-    private org.apache.orc.OrcProto.StringStatistics stringStatistics_;
-    /**
-     * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-     */
-    public boolean hasStringStatistics() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    /**
-     * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-     */
-    public org.apache.orc.OrcProto.StringStatistics getStringStatistics() {
-      return stringStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-     */
-    public org.apache.orc.OrcProto.StringStatisticsOrBuilder getStringStatisticsOrBuilder() {
-      return stringStatistics_;
-    }
-
-    // optional .orc.proto.BucketStatistics bucketStatistics = 5;
-    public static final int BUCKETSTATISTICS_FIELD_NUMBER = 5;
-    private org.apache.orc.OrcProto.BucketStatistics bucketStatistics_;
-    /**
-     * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-     */
-    public boolean hasBucketStatistics() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
-    }
-    /**
-     * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-     */
-    public org.apache.orc.OrcProto.BucketStatistics getBucketStatistics() {
-      return bucketStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-     */
-    public org.apache.orc.OrcProto.BucketStatisticsOrBuilder getBucketStatisticsOrBuilder() {
-      return bucketStatistics_;
-    }
-
-    // optional .orc.proto.DecimalStatistics decimalStatistics = 6;
-    public static final int DECIMALSTATISTICS_FIELD_NUMBER = 6;
-    private org.apache.orc.OrcProto.DecimalStatistics decimalStatistics_;
-    /**
-     * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-     */
-    public boolean hasDecimalStatistics() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
-    }
-    /**
-     * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-     */
-    public org.apache.orc.OrcProto.DecimalStatistics getDecimalStatistics() {
-      return decimalStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-     */
-    public org.apache.orc.OrcProto.DecimalStatisticsOrBuilder getDecimalStatisticsOrBuilder() {
-      return decimalStatistics_;
-    }
-
-    // optional .orc.proto.DateStatistics dateStatistics = 7;
-    public static final int DATESTATISTICS_FIELD_NUMBER = 7;
-    private org.apache.orc.OrcProto.DateStatistics dateStatistics_;
-    /**
-     * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-     */
-    public boolean hasDateStatistics() {
-      return ((bitField0_ & 0x00000040) == 0x00000040);
-    }
-    /**
-     * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-     */
-    public org.apache.orc.OrcProto.DateStatistics getDateStatistics() {
-      return dateStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-     */
-    public org.apache.orc.OrcProto.DateStatisticsOrBuilder getDateStatisticsOrBuilder() {
-      return dateStatistics_;
-    }
-
-    // optional .orc.proto.BinaryStatistics binaryStatistics = 8;
-    public static final int BINARYSTATISTICS_FIELD_NUMBER = 8;
-    private org.apache.orc.OrcProto.BinaryStatistics binaryStatistics_;
-    /**
-     * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-     */
-    public boolean hasBinaryStatistics() {
-      return ((bitField0_ & 0x00000080) == 0x00000080);
-    }
-    /**
-     * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-     */
-    public org.apache.orc.OrcProto.BinaryStatistics getBinaryStatistics() {
-      return binaryStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-     */
-    public org.apache.orc.OrcProto.BinaryStatisticsOrBuilder getBinaryStatisticsOrBuilder() {
-      return binaryStatistics_;
-    }
-
-    // optional .orc.proto.TimestampStatistics timestampStatistics = 9;
-    public static final int TIMESTAMPSTATISTICS_FIELD_NUMBER = 9;
-    private org.apache.orc.OrcProto.TimestampStatistics timestampStatistics_;
-    /**
-     * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-     */
-    public boolean hasTimestampStatistics() {
-      return ((bitField0_ & 0x00000100) == 0x00000100);
-    }
-    /**
-     * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-     */
-    public org.apache.orc.OrcProto.TimestampStatistics getTimestampStatistics() {
-      return timestampStatistics_;
-    }
-    /**
-     * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-     */
-    public org.apache.orc.OrcProto.TimestampStatisticsOrBuilder getTimestampStatisticsOrBuilder() {
-      return timestampStatistics_;
-    }
-
-    // optional bool hasNull = 10;
-    public static final int HASNULL_FIELD_NUMBER = 10;
-    private boolean hasNull_;
-    /**
-     * <code>optional bool hasNull = 10;</code>
-     */
-    public boolean hasHasNull() {
-      return ((bitField0_ & 0x00000200) == 0x00000200);
-    }
-    /**
-     * <code>optional bool hasNull = 10;</code>
-     */
-    public boolean getHasNull() {
-      return hasNull_;
-    }
-
-    private void initFields() {
-      numberOfValues_ = 0L;
-      intStatistics_ = org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance();
-      doubleStatistics_ = org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance();
-      stringStatistics_ = org.apache.orc.OrcProto.StringStatistics.getDefaultInstance();
-      bucketStatistics_ = org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance();
-      decimalStatistics_ = org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance();
-      dateStatistics_ = org.apache.orc.OrcProto.DateStatistics.getDefaultInstance();
-      binaryStatistics_ = org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance();
-      timestampStatistics_ = org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance();
-      hasNull_ = false;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, numberOfValues_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(2, intStatistics_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeMessage(3, doubleStatistics_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeMessage(4, stringStatistics_);
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeMessage(5, bucketStatistics_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeMessage(6, decimalStatistics_);
-      }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        output.writeMessage(7, dateStatistics_);
-      }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        output.writeMessage(8, binaryStatistics_);
-      }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
-        output.writeMessage(9, timestampStatistics_);
-      }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
-        output.writeBool(10, hasNull_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, numberOfValues_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, intStatistics_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(3, doubleStatistics_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(4, stringStatistics_);
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(5, bucketStatistics_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(6, decimalStatistics_);
-      }
-      if (((bitField0_ & 0x00000040) == 0x00000040)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(7, dateStatistics_);
-      }
-      if (((bitField0_ & 0x00000080) == 0x00000080)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(8, binaryStatistics_);
-      }
-      if (((bitField0_ & 0x00000100) == 0x00000100)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(9, timestampStatistics_);
-      }
-      if (((bitField0_ & 0x00000200) == 0x00000200)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBoolSize(10, hasNull_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.ColumnStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.ColumnStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.ColumnStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.ColumnStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.ColumnStatistics.class, org.apache.orc.OrcProto.ColumnStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.ColumnStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getIntStatisticsFieldBuilder();
-          getDoubleStatisticsFieldBuilder();
-          getStringStatisticsFieldBuilder();
-          getBucketStatisticsFieldBuilder();
-          getDecimalStatisticsFieldBuilder();
-          getDateStatisticsFieldBuilder();
-          getBinaryStatisticsFieldBuilder();
-          getTimestampStatisticsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        numberOfValues_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        if (intStatisticsBuilder_ == null) {
-          intStatistics_ = org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance();
-        } else {
-          intStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        if (doubleStatisticsBuilder_ == null) {
-          doubleStatistics_ = org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance();
-        } else {
-          doubleStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000004);
-        if (stringStatisticsBuilder_ == null) {
-          stringStatistics_ = org.apache.orc.OrcProto.StringStatistics.getDefaultInstance();
-        } else {
-          stringStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000008);
-        if (bucketStatisticsBuilder_ == null) {
-          bucketStatistics_ = org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance();
-        } else {
-          bucketStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000010);
-        if (decimalStatisticsBuilder_ == null) {
-          decimalStatistics_ = org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance();
-        } else {
-          decimalStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000020);
-        if (dateStatisticsBuilder_ == null) {
-          dateStatistics_ = org.apache.orc.OrcProto.DateStatistics.getDefaultInstance();
-        } else {
-          dateStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000040);
-        if (binaryStatisticsBuilder_ == null) {
-          binaryStatistics_ = org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance();
-        } else {
-          binaryStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000080);
-        if (timestampStatisticsBuilder_ == null) {
-          timestampStatistics_ = org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance();
-        } else {
-          timestampStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000100);
-        hasNull_ = false;
-        bitField0_ = (bitField0_ & ~0x00000200);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.ColumnStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.ColumnStatistics build() {
-        org.apache.orc.OrcProto.ColumnStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.ColumnStatistics buildPartial() {
-        org.apache.orc.OrcProto.ColumnStatistics result = new org.apache.orc.OrcProto.ColumnStatistics(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.numberOfValues_ = numberOfValues_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        if (intStatisticsBuilder_ == null) {
-          result.intStatistics_ = intStatistics_;
-        } else {
-          result.intStatistics_ = intStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        if (doubleStatisticsBuilder_ == null) {
-          result.doubleStatistics_ = doubleStatistics_;
-        } else {
-          result.doubleStatistics_ = doubleStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        if (stringStatisticsBuilder_ == null) {
-          result.stringStatistics_ = stringStatistics_;
-        } else {
-          result.stringStatistics_ = stringStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000010;
-        }
-        if (bucketStatisticsBuilder_ == null) {
-          result.bucketStatistics_ = bucketStatistics_;
-        } else {
-          result.bucketStatistics_ = bucketStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000020;
-        }
-        if (decimalStatisticsBuilder_ == null) {
-          result.decimalStatistics_ = decimalStatistics_;
-        } else {
-          result.decimalStatistics_ = decimalStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
-          to_bitField0_ |= 0x00000040;
-        }
-        if (dateStatisticsBuilder_ == null) {
-          result.dateStatistics_ = dateStatistics_;
-        } else {
-          result.dateStatistics_ = dateStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
-          to_bitField0_ |= 0x00000080;
-        }
-        if (binaryStatisticsBuilder_ == null) {
-          result.binaryStatistics_ = binaryStatistics_;
-        } else {
-          result.binaryStatistics_ = binaryStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
-          to_bitField0_ |= 0x00000100;
-        }
-        if (timestampStatisticsBuilder_ == null) {
-          result.timestampStatistics_ = timestampStatistics_;
-        } else {
-          result.timestampStatistics_ = timestampStatisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
-          to_bitField0_ |= 0x00000200;
-        }
-        result.hasNull_ = hasNull_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.ColumnStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.ColumnStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.ColumnStatistics other) {
-        if (other == org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance()) return this;
-        if (other.hasNumberOfValues()) {
-          setNumberOfValues(other.getNumberOfValues());
-        }
-        if (other.hasIntStatistics()) {
-          mergeIntStatistics(other.getIntStatistics());
-        }
-        if (other.hasDoubleStatistics()) {
-          mergeDoubleStatistics(other.getDoubleStatistics());
-        }
-        if (other.hasStringStatistics()) {
-          mergeStringStatistics(other.getStringStatistics());
-        }
-        if (other.hasBucketStatistics()) {
-          mergeBucketStatistics(other.getBucketStatistics());
-        }
-        if (other.hasDecimalStatistics()) {
-          mergeDecimalStatistics(other.getDecimalStatistics());
-        }
-        if (other.hasDateStatistics()) {
-          mergeDateStatistics(other.getDateStatistics());
-        }
-        if (other.hasBinaryStatistics()) {
-          mergeBinaryStatistics(other.getBinaryStatistics());
-        }
-        if (other.hasTimestampStatistics()) {
-          mergeTimestampStatistics(other.getTimestampStatistics());
-        }
-        if (other.hasHasNull()) {
-          setHasNull(other.getHasNull());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.ColumnStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.ColumnStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional uint64 numberOfValues = 1;
-      private long numberOfValues_ ;
-      /**
-       * <code>optional uint64 numberOfValues = 1;</code>
-       */
-      public boolean hasNumberOfValues() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional uint64 numberOfValues = 1;</code>
-       */
-      public long getNumberOfValues() {
-        return numberOfValues_;
-      }
-      /**
-       * <code>optional uint64 numberOfValues = 1;</code>
-       */
-      public Builder setNumberOfValues(long value) {
-        bitField0_ |= 0x00000001;
-        numberOfValues_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 numberOfValues = 1;</code>
-       */
-      public Builder clearNumberOfValues() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        numberOfValues_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional .orc.proto.IntegerStatistics intStatistics = 2;
-      private org.apache.orc.OrcProto.IntegerStatistics intStatistics_ = org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.IntegerStatistics, org.apache.orc.OrcProto.IntegerStatistics.Builder, org.apache.orc.OrcProto.IntegerStatisticsOrBuilder> intStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public boolean hasIntStatistics() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public org.apache.orc.OrcProto.IntegerStatistics getIntStatistics() {
-        if (intStatisticsBuilder_ == null) {
-          return intStatistics_;
-        } else {
-          return intStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public Builder setIntStatistics(org.apache.orc.OrcProto.IntegerStatistics value) {
-        if (intStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          intStatistics_ = value;
-          onChanged();
-        } else {
-          intStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public Builder setIntStatistics(
-          org.apache.orc.OrcProto.IntegerStatistics.Builder builderForValue) {
-        if (intStatisticsBuilder_ == null) {
-          intStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          intStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public Builder mergeIntStatistics(org.apache.orc.OrcProto.IntegerStatistics value) {
-        if (intStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              intStatistics_ != org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance()) {
-            intStatistics_ =
-              org.apache.orc.OrcProto.IntegerStatistics.newBuilder(intStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            intStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          intStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public Builder clearIntStatistics() {
-        if (intStatisticsBuilder_ == null) {
-          intStatistics_ = org.apache.orc.OrcProto.IntegerStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          intStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public org.apache.orc.OrcProto.IntegerStatistics.Builder getIntStatisticsBuilder() {
-        bitField0_ |= 0x00000002;
-        onChanged();
-        return getIntStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      public org.apache.orc.OrcProto.IntegerStatisticsOrBuilder getIntStatisticsOrBuilder() {
-        if (intStatisticsBuilder_ != null) {
-          return intStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return intStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.IntegerStatistics intStatistics = 2;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.IntegerStatistics, org.apache.orc.OrcProto.IntegerStatistics.Builder, org.apache.orc.OrcProto.IntegerStatisticsOrBuilder> 
-          getIntStatisticsFieldBuilder() {
-        if (intStatisticsBuilder_ == null) {
-          intStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.IntegerStatistics, org.apache.orc.OrcProto.IntegerStatistics.Builder, org.apache.orc.OrcProto.IntegerStatisticsOrBuilder>(
-                  intStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          intStatistics_ = null;
-        }
-        return intStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.DoubleStatistics doubleStatistics = 3;
-      private org.apache.orc.OrcProto.DoubleStatistics doubleStatistics_ = org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.DoubleStatistics, org.apache.orc.OrcProto.DoubleStatistics.Builder, org.apache.orc.OrcProto.DoubleStatisticsOrBuilder> doubleStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public boolean hasDoubleStatistics() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public org.apache.orc.OrcProto.DoubleStatistics getDoubleStatistics() {
-        if (doubleStatisticsBuilder_ == null) {
-          return doubleStatistics_;
-        } else {
-          return doubleStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public Builder setDoubleStatistics(org.apache.orc.OrcProto.DoubleStatistics value) {
-        if (doubleStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          doubleStatistics_ = value;
-          onChanged();
-        } else {
-          doubleStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000004;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public Builder setDoubleStatistics(
-          org.apache.orc.OrcProto.DoubleStatistics.Builder builderForValue) {
-        if (doubleStatisticsBuilder_ == null) {
-          doubleStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          doubleStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000004;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public Builder mergeDoubleStatistics(org.apache.orc.OrcProto.DoubleStatistics value) {
-        if (doubleStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000004) == 0x00000004) &&
-              doubleStatistics_ != org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance()) {
-            doubleStatistics_ =
-              org.apache.orc.OrcProto.DoubleStatistics.newBuilder(doubleStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            doubleStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          doubleStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000004;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public Builder clearDoubleStatistics() {
-        if (doubleStatisticsBuilder_ == null) {
-          doubleStatistics_ = org.apache.orc.OrcProto.DoubleStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          doubleStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public org.apache.orc.OrcProto.DoubleStatistics.Builder getDoubleStatisticsBuilder() {
-        bitField0_ |= 0x00000004;
-        onChanged();
-        return getDoubleStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      public org.apache.orc.OrcProto.DoubleStatisticsOrBuilder getDoubleStatisticsOrBuilder() {
-        if (doubleStatisticsBuilder_ != null) {
-          return doubleStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return doubleStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.DoubleStatistics doubleStatistics = 3;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.DoubleStatistics, org.apache.orc.OrcProto.DoubleStatistics.Builder, org.apache.orc.OrcProto.DoubleStatisticsOrBuilder> 
-          getDoubleStatisticsFieldBuilder() {
-        if (doubleStatisticsBuilder_ == null) {
-          doubleStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.DoubleStatistics, org.apache.orc.OrcProto.DoubleStatistics.Builder, org.apache.orc.OrcProto.DoubleStatisticsOrBuilder>(
-                  doubleStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          doubleStatistics_ = null;
-        }
-        return doubleStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.StringStatistics stringStatistics = 4;
-      private org.apache.orc.OrcProto.StringStatistics stringStatistics_ = org.apache.orc.OrcProto.StringStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.StringStatistics, org.apache.orc.OrcProto.StringStatistics.Builder, org.apache.orc.OrcProto.StringStatisticsOrBuilder> stringStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public boolean hasStringStatistics() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public org.apache.orc.OrcProto.StringStatistics getStringStatistics() {
-        if (stringStatisticsBuilder_ == null) {
-          return stringStatistics_;
-        } else {
-          return stringStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public Builder setStringStatistics(org.apache.orc.OrcProto.StringStatistics value) {
-        if (stringStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          stringStatistics_ = value;
-          onChanged();
-        } else {
-          stringStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000008;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public Builder setStringStatistics(
-          org.apache.orc.OrcProto.StringStatistics.Builder builderForValue) {
-        if (stringStatisticsBuilder_ == null) {
-          stringStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          stringStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000008;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public Builder mergeStringStatistics(org.apache.orc.OrcProto.StringStatistics value) {
-        if (stringStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000008) == 0x00000008) &&
-              stringStatistics_ != org.apache.orc.OrcProto.StringStatistics.getDefaultInstance()) {
-            stringStatistics_ =
-              org.apache.orc.OrcProto.StringStatistics.newBuilder(stringStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            stringStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          stringStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000008;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public Builder clearStringStatistics() {
-        if (stringStatisticsBuilder_ == null) {
-          stringStatistics_ = org.apache.orc.OrcProto.StringStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          stringStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000008);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public org.apache.orc.OrcProto.StringStatistics.Builder getStringStatisticsBuilder() {
-        bitField0_ |= 0x00000008;
-        onChanged();
-        return getStringStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      public org.apache.orc.OrcProto.StringStatisticsOrBuilder getStringStatisticsOrBuilder() {
-        if (stringStatisticsBuilder_ != null) {
-          return stringStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return stringStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.StringStatistics stringStatistics = 4;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.StringStatistics, org.apache.orc.OrcProto.StringStatistics.Builder, org.apache.orc.OrcProto.StringStatisticsOrBuilder> 
-          getStringStatisticsFieldBuilder() {
-        if (stringStatisticsBuilder_ == null) {
-          stringStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.StringStatistics, org.apache.orc.OrcProto.StringStatistics.Builder, org.apache.orc.OrcProto.StringStatisticsOrBuilder>(
-                  stringStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          stringStatistics_ = null;
-        }
-        return stringStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.BucketStatistics bucketStatistics = 5;
-      private org.apache.orc.OrcProto.BucketStatistics bucketStatistics_ = org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.BucketStatistics, org.apache.orc.OrcProto.BucketStatistics.Builder, org.apache.orc.OrcProto.BucketStatisticsOrBuilder> bucketStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public boolean hasBucketStatistics() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public org.apache.orc.OrcProto.BucketStatistics getBucketStatistics() {
-        if (bucketStatisticsBuilder_ == null) {
-          return bucketStatistics_;
-        } else {
-          return bucketStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public Builder setBucketStatistics(org.apache.orc.OrcProto.BucketStatistics value) {
-        if (bucketStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          bucketStatistics_ = value;
-          onChanged();
-        } else {
-          bucketStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000010;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public Builder setBucketStatistics(
-          org.apache.orc.OrcProto.BucketStatistics.Builder builderForValue) {
-        if (bucketStatisticsBuilder_ == null) {
-          bucketStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          bucketStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000010;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public Builder mergeBucketStatistics(org.apache.orc.OrcProto.BucketStatistics value) {
-        if (bucketStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000010) == 0x00000010) &&
-              bucketStatistics_ != org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance()) {
-            bucketStatistics_ =
-              org.apache.orc.OrcProto.BucketStatistics.newBuilder(bucketStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            bucketStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          bucketStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000010;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public Builder clearBucketStatistics() {
-        if (bucketStatisticsBuilder_ == null) {
-          bucketStatistics_ = org.apache.orc.OrcProto.BucketStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          bucketStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000010);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public org.apache.orc.OrcProto.BucketStatistics.Builder getBucketStatisticsBuilder() {
-        bitField0_ |= 0x00000010;
-        onChanged();
-        return getBucketStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      public org.apache.orc.OrcProto.BucketStatisticsOrBuilder getBucketStatisticsOrBuilder() {
-        if (bucketStatisticsBuilder_ != null) {
-          return bucketStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return bucketStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.BucketStatistics bucketStatistics = 5;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.BucketStatistics, org.apache.orc.OrcProto.BucketStatistics.Builder, org.apache.orc.OrcProto.BucketStatisticsOrBuilder> 
-          getBucketStatisticsFieldBuilder() {
-        if (bucketStatisticsBuilder_ == null) {
-          bucketStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.BucketStatistics, org.apache.orc.OrcProto.BucketStatistics.Builder, org.apache.orc.OrcProto.BucketStatisticsOrBuilder>(
-                  bucketStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          bucketStatistics_ = null;
-        }
-        return bucketStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.DecimalStatistics decimalStatistics = 6;
-      private org.apache.orc.OrcProto.DecimalStatistics decimalStatistics_ = org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.DecimalStatistics, org.apache.orc.OrcProto.DecimalStatistics.Builder, org.apache.orc.OrcProto.DecimalStatisticsOrBuilder> decimalStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public boolean hasDecimalStatistics() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public org.apache.orc.OrcProto.DecimalStatistics getDecimalStatistics() {
-        if (decimalStatisticsBuilder_ == null) {
-          return decimalStatistics_;
-        } else {
-          return decimalStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public Builder setDecimalStatistics(org.apache.orc.OrcProto.DecimalStatistics value) {
-        if (decimalStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          decimalStatistics_ = value;
-          onChanged();
-        } else {
-          decimalStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000020;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public Builder setDecimalStatistics(
-          org.apache.orc.OrcProto.DecimalStatistics.Builder builderForValue) {
-        if (decimalStatisticsBuilder_ == null) {
-          decimalStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          decimalStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000020;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public Builder mergeDecimalStatistics(org.apache.orc.OrcProto.DecimalStatistics value) {
-        if (decimalStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000020) == 0x00000020) &&
-              decimalStatistics_ != org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance()) {
-            decimalStatistics_ =
-              org.apache.orc.OrcProto.DecimalStatistics.newBuilder(decimalStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            decimalStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          decimalStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000020;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public Builder clearDecimalStatistics() {
-        if (decimalStatisticsBuilder_ == null) {
-          decimalStatistics_ = org.apache.orc.OrcProto.DecimalStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          decimalStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000020);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public org.apache.orc.OrcProto.DecimalStatistics.Builder getDecimalStatisticsBuilder() {
-        bitField0_ |= 0x00000020;
-        onChanged();
-        return getDecimalStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      public org.apache.orc.OrcProto.DecimalStatisticsOrBuilder getDecimalStatisticsOrBuilder() {
-        if (decimalStatisticsBuilder_ != null) {
-          return decimalStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return decimalStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.DecimalStatistics decimalStatistics = 6;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.DecimalStatistics, org.apache.orc.OrcProto.DecimalStatistics.Builder, org.apache.orc.OrcProto.DecimalStatisticsOrBuilder> 
-          getDecimalStatisticsFieldBuilder() {
-        if (decimalStatisticsBuilder_ == null) {
-          decimalStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.DecimalStatistics, org.apache.orc.OrcProto.DecimalStatistics.Builder, org.apache.orc.OrcProto.DecimalStatisticsOrBuilder>(
-                  decimalStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          decimalStatistics_ = null;
-        }
-        return decimalStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.DateStatistics dateStatistics = 7;
-      private org.apache.orc.OrcProto.DateStatistics dateStatistics_ = org.apache.orc.OrcProto.DateStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.DateStatistics, org.apache.orc.OrcProto.DateStatistics.Builder, org.apache.orc.OrcProto.DateStatisticsOrBuilder> dateStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public boolean hasDateStatistics() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.DateStatistics getDateStatistics() {
-        if (dateStatisticsBuilder_ == null) {
-          return dateStatistics_;
-        } else {
-          return dateStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public Builder setDateStatistics(org.apache.orc.OrcProto.DateStatistics value) {
-        if (dateStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          dateStatistics_ = value;
-          onChanged();
-        } else {
-          dateStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000040;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public Builder setDateStatistics(
-          org.apache.orc.OrcProto.DateStatistics.Builder builderForValue) {
-        if (dateStatisticsBuilder_ == null) {
-          dateStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          dateStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000040;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public Builder mergeDateStatistics(org.apache.orc.OrcProto.DateStatistics value) {
-        if (dateStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000040) == 0x00000040) &&
-              dateStatistics_ != org.apache.orc.OrcProto.DateStatistics.getDefaultInstance()) {
-            dateStatistics_ =
-              org.apache.orc.OrcProto.DateStatistics.newBuilder(dateStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            dateStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          dateStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000040;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public Builder clearDateStatistics() {
-        if (dateStatisticsBuilder_ == null) {
-          dateStatistics_ = org.apache.orc.OrcProto.DateStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          dateStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000040);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.DateStatistics.Builder getDateStatisticsBuilder() {
-        bitField0_ |= 0x00000040;
-        onChanged();
-        return getDateStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.DateStatisticsOrBuilder getDateStatisticsOrBuilder() {
-        if (dateStatisticsBuilder_ != null) {
-          return dateStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return dateStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.DateStatistics dateStatistics = 7;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.DateStatistics, org.apache.orc.OrcProto.DateStatistics.Builder, org.apache.orc.OrcProto.DateStatisticsOrBuilder> 
-          getDateStatisticsFieldBuilder() {
-        if (dateStatisticsBuilder_ == null) {
-          dateStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.DateStatistics, org.apache.orc.OrcProto.DateStatistics.Builder, org.apache.orc.OrcProto.DateStatisticsOrBuilder>(
-                  dateStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          dateStatistics_ = null;
-        }
-        return dateStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.BinaryStatistics binaryStatistics = 8;
-      private org.apache.orc.OrcProto.BinaryStatistics binaryStatistics_ = org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.BinaryStatistics, org.apache.orc.OrcProto.BinaryStatistics.Builder, org.apache.orc.OrcProto.BinaryStatisticsOrBuilder> binaryStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public boolean hasBinaryStatistics() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public org.apache.orc.OrcProto.BinaryStatistics getBinaryStatistics() {
-        if (binaryStatisticsBuilder_ == null) {
-          return binaryStatistics_;
-        } else {
-          return binaryStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public Builder setBinaryStatistics(org.apache.orc.OrcProto.BinaryStatistics value) {
-        if (binaryStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          binaryStatistics_ = value;
-          onChanged();
-        } else {
-          binaryStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000080;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public Builder setBinaryStatistics(
-          org.apache.orc.OrcProto.BinaryStatistics.Builder builderForValue) {
-        if (binaryStatisticsBuilder_ == null) {
-          binaryStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          binaryStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000080;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public Builder mergeBinaryStatistics(org.apache.orc.OrcProto.BinaryStatistics value) {
-        if (binaryStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000080) == 0x00000080) &&
-              binaryStatistics_ != org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance()) {
-            binaryStatistics_ =
-              org.apache.orc.OrcProto.BinaryStatistics.newBuilder(binaryStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            binaryStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          binaryStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000080;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public Builder clearBinaryStatistics() {
-        if (binaryStatisticsBuilder_ == null) {
-          binaryStatistics_ = org.apache.orc.OrcProto.BinaryStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          binaryStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000080);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public org.apache.orc.OrcProto.BinaryStatistics.Builder getBinaryStatisticsBuilder() {
-        bitField0_ |= 0x00000080;
-        onChanged();
-        return getBinaryStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      public org.apache.orc.OrcProto.BinaryStatisticsOrBuilder getBinaryStatisticsOrBuilder() {
-        if (binaryStatisticsBuilder_ != null) {
-          return binaryStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return binaryStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.BinaryStatistics binaryStatistics = 8;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.BinaryStatistics, org.apache.orc.OrcProto.BinaryStatistics.Builder, org.apache.orc.OrcProto.BinaryStatisticsOrBuilder> 
-          getBinaryStatisticsFieldBuilder() {
-        if (binaryStatisticsBuilder_ == null) {
-          binaryStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.BinaryStatistics, org.apache.orc.OrcProto.BinaryStatistics.Builder, org.apache.orc.OrcProto.BinaryStatisticsOrBuilder>(
-                  binaryStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          binaryStatistics_ = null;
-        }
-        return binaryStatisticsBuilder_;
-      }
-
-      // optional .orc.proto.TimestampStatistics timestampStatistics = 9;
-      private org.apache.orc.OrcProto.TimestampStatistics timestampStatistics_ = org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.TimestampStatistics, org.apache.orc.OrcProto.TimestampStatistics.Builder, org.apache.orc.OrcProto.TimestampStatisticsOrBuilder> timestampStatisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public boolean hasTimestampStatistics() {
-        return ((bitField0_ & 0x00000100) == 0x00000100);
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public org.apache.orc.OrcProto.TimestampStatistics getTimestampStatistics() {
-        if (timestampStatisticsBuilder_ == null) {
-          return timestampStatistics_;
-        } else {
-          return timestampStatisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public Builder setTimestampStatistics(org.apache.orc.OrcProto.TimestampStatistics value) {
-        if (timestampStatisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          timestampStatistics_ = value;
-          onChanged();
-        } else {
-          timestampStatisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000100;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public Builder setTimestampStatistics(
-          org.apache.orc.OrcProto.TimestampStatistics.Builder builderForValue) {
-        if (timestampStatisticsBuilder_ == null) {
-          timestampStatistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          timestampStatisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000100;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public Builder mergeTimestampStatistics(org.apache.orc.OrcProto.TimestampStatistics value) {
-        if (timestampStatisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000100) == 0x00000100) &&
-              timestampStatistics_ != org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance()) {
-            timestampStatistics_ =
-              org.apache.orc.OrcProto.TimestampStatistics.newBuilder(timestampStatistics_).mergeFrom(value).buildPartial();
-          } else {
-            timestampStatistics_ = value;
-          }
-          onChanged();
-        } else {
-          timestampStatisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000100;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public Builder clearTimestampStatistics() {
-        if (timestampStatisticsBuilder_ == null) {
-          timestampStatistics_ = org.apache.orc.OrcProto.TimestampStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          timestampStatisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000100);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public org.apache.orc.OrcProto.TimestampStatistics.Builder getTimestampStatisticsBuilder() {
-        bitField0_ |= 0x00000100;
-        onChanged();
-        return getTimestampStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      public org.apache.orc.OrcProto.TimestampStatisticsOrBuilder getTimestampStatisticsOrBuilder() {
-        if (timestampStatisticsBuilder_ != null) {
-          return timestampStatisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return timestampStatistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.TimestampStatistics timestampStatistics = 9;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.TimestampStatistics, org.apache.orc.OrcProto.TimestampStatistics.Builder, org.apache.orc.OrcProto.TimestampStatisticsOrBuilder> 
-          getTimestampStatisticsFieldBuilder() {
-        if (timestampStatisticsBuilder_ == null) {
-          timestampStatisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.TimestampStatistics, org.apache.orc.OrcProto.TimestampStatistics.Builder, org.apache.orc.OrcProto.TimestampStatisticsOrBuilder>(
-                  timestampStatistics_,
-                  getParentForChildren(),
-                  isClean());
-          timestampStatistics_ = null;
-        }
-        return timestampStatisticsBuilder_;
-      }
-
-      // optional bool hasNull = 10;
-      private boolean hasNull_ ;
-      /**
-       * <code>optional bool hasNull = 10;</code>
-       */
-      public boolean hasHasNull() {
-        return ((bitField0_ & 0x00000200) == 0x00000200);
-      }
-      /**
-       * <code>optional bool hasNull = 10;</code>
-       */
-      public boolean getHasNull() {
-        return hasNull_;
-      }
-      /**
-       * <code>optional bool hasNull = 10;</code>
-       */
-      public Builder setHasNull(boolean value) {
-        bitField0_ |= 0x00000200;
-        hasNull_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional bool hasNull = 10;</code>
-       */
-      public Builder clearHasNull() {
-        bitField0_ = (bitField0_ & ~0x00000200);
-        hasNull_ = false;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.ColumnStatistics)
-    }
-
-    static {
-      defaultInstance = new ColumnStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.ColumnStatistics)
-  }
-
-  public interface RowIndexEntryOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated uint64 positions = 1 [packed = true];
-    /**
-     * <code>repeated uint64 positions = 1 [packed = true];</code>
-     */
-    java.util.List<java.lang.Long> getPositionsList();
-    /**
-     * <code>repeated uint64 positions = 1 [packed = true];</code>
-     */
-    int getPositionsCount();
-    /**
-     * <code>repeated uint64 positions = 1 [packed = true];</code>
-     */
-    long getPositions(int index);
-
-    // optional .orc.proto.ColumnStatistics statistics = 2;
-    /**
-     * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-     */
-    boolean hasStatistics();
-    /**
-     * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-     */
-    org.apache.orc.OrcProto.ColumnStatistics getStatistics();
-    /**
-     * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-     */
-    org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder();
-  }
-  /**
-   * Protobuf type {@code orc.proto.RowIndexEntry}
-   */
-  public static final class RowIndexEntry extends
-      com.google.protobuf.GeneratedMessage
-      implements RowIndexEntryOrBuilder {
-    // Use RowIndexEntry.newBuilder() to construct.
-    private RowIndexEntry(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private RowIndexEntry(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final RowIndexEntry defaultInstance;
-    public static RowIndexEntry getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public RowIndexEntry getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private RowIndexEntry(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                positions_ = new java.util.ArrayList<java.lang.Long>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              positions_.add(input.readUInt64());
-              break;
-            }
-            case 10: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
-                positions_ = new java.util.ArrayList<java.lang.Long>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              while (input.getBytesUntilLimit() > 0) {
-                positions_.add(input.readUInt64());
-              }
-              input.popLimit(limit);
-              break;
-            }
-            case 18: {
-              org.apache.orc.OrcProto.ColumnStatistics.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = statistics_.toBuilder();
-              }
-              statistics_ = input.readMessage(org.apache.orc.OrcProto.ColumnStatistics.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(statistics_);
-                statistics_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          positions_ = java.util.Collections.unmodifiableList(positions_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndexEntry_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndexEntry_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.RowIndexEntry.class, org.apache.orc.OrcProto.RowIndexEntry.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<RowIndexEntry> PARSER =
-        new com.google.protobuf.AbstractParser<RowIndexEntry>() {
-      public RowIndexEntry parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new RowIndexEntry(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<RowIndexEntry> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // repeated uint64 positions = 1 [packed = true];
-    public static final int POSITIONS_FIELD_NUMBER = 1;
-    private java.util.List<java.lang.Long> positions_;
-    /**
-     * <code>repeated uint64 positions = 1 [packed = true];</code>
-     */
-    public java.util.List<java.lang.Long>
-        getPositionsList() {
-      return positions_;
-    }
-    /**
-     * <code>repeated uint64 positions = 1 [packed = true];</code>
-     */
-    public int getPositionsCount() {
-      return positions_.size();
-    }
-    /**
-     * <code>repeated uint64 positions = 1 [packed = true];</code>
-     */
-    public long getPositions(int index) {
-      return positions_.get(index);
-    }
-    private int positionsMemoizedSerializedSize = -1;
-
-    // optional .orc.proto.ColumnStatistics statistics = 2;
-    public static final int STATISTICS_FIELD_NUMBER = 2;
-    private org.apache.orc.OrcProto.ColumnStatistics statistics_;
-    /**
-     * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-     */
-    public boolean hasStatistics() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnStatistics getStatistics() {
-      return statistics_;
-    }
-    /**
-     * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder() {
-      return statistics_;
-    }
-
-    private void initFields() {
-      positions_ = java.util.Collections.emptyList();
-      statistics_ = org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (getPositionsList().size() > 0) {
-        output.writeRawVarint32(10);
-        output.writeRawVarint32(positionsMemoizedSerializedSize);
-      }
-      for (int i = 0; i < positions_.size(); i++) {
-        output.writeUInt64NoTag(positions_.get(i));
-      }
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(2, statistics_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      {
-        int dataSize = 0;
-        for (int i = 0; i < positions_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeUInt64SizeNoTag(positions_.get(i));
-        }
-        size += dataSize;
-        if (!getPositionsList().isEmpty()) {
-          size += 1;
-          size += com.google.protobuf.CodedOutputStream
-              .computeInt32SizeNoTag(dataSize);
-        }
-        positionsMemoizedSerializedSize = dataSize;
-      }
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, statistics_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.RowIndexEntry parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.RowIndexEntry prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.RowIndexEntry}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.RowIndexEntryOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndexEntry_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndexEntry_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.RowIndexEntry.class, org.apache.orc.OrcProto.RowIndexEntry.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.RowIndexEntry.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getStatisticsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        positions_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000001);
-        if (statisticsBuilder_ == null) {
-          statistics_ = org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance();
-        } else {
-          statisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndexEntry_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.RowIndexEntry getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.RowIndexEntry.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.RowIndexEntry build() {
-        org.apache.orc.OrcProto.RowIndexEntry result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.RowIndexEntry buildPartial() {
-        org.apache.orc.OrcProto.RowIndexEntry result = new org.apache.orc.OrcProto.RowIndexEntry(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((bitField0_ & 0x00000001) == 0x00000001)) {
-          positions_ = java.util.Collections.unmodifiableList(positions_);
-          bitField0_ = (bitField0_ & ~0x00000001);
-        }
-        result.positions_ = positions_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        if (statisticsBuilder_ == null) {
-          result.statistics_ = statistics_;
-        } else {
-          result.statistics_ = statisticsBuilder_.build();
-        }
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.RowIndexEntry) {
-          return mergeFrom((org.apache.orc.OrcProto.RowIndexEntry)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.RowIndexEntry other) {
-        if (other == org.apache.orc.OrcProto.RowIndexEntry.getDefaultInstance()) return this;
-        if (!other.positions_.isEmpty()) {
-          if (positions_.isEmpty()) {
-            positions_ = other.positions_;
-            bitField0_ = (bitField0_ & ~0x00000001);
-          } else {
-            ensurePositionsIsMutable();
-            positions_.addAll(other.positions_);
-          }
-          onChanged();
-        }
-        if (other.hasStatistics()) {
-          mergeStatistics(other.getStatistics());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.RowIndexEntry parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.RowIndexEntry) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated uint64 positions = 1 [packed = true];
-      private java.util.List<java.lang.Long> positions_ = java.util.Collections.emptyList();
-      private void ensurePositionsIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          positions_ = new java.util.ArrayList<java.lang.Long>(positions_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public java.util.List<java.lang.Long>
-          getPositionsList() {
-        return java.util.Collections.unmodifiableList(positions_);
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public int getPositionsCount() {
-        return positions_.size();
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public long getPositions(int index) {
-        return positions_.get(index);
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public Builder setPositions(
-          int index, long value) {
-        ensurePositionsIsMutable();
-        positions_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public Builder addPositions(long value) {
-        ensurePositionsIsMutable();
-        positions_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public Builder addAllPositions(
-          java.lang.Iterable<? extends java.lang.Long> values) {
-        ensurePositionsIsMutable();
-        super.addAll(values, positions_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint64 positions = 1 [packed = true];</code>
-       */
-      public Builder clearPositions() {
-        positions_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000001);
-        onChanged();
-        return this;
-      }
-
-      // optional .orc.proto.ColumnStatistics statistics = 2;
-      private org.apache.orc.OrcProto.ColumnStatistics statistics_ = org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> statisticsBuilder_;
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public boolean hasStatistics() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics getStatistics() {
-        if (statisticsBuilder_ == null) {
-          return statistics_;
-        } else {
-          return statisticsBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public Builder setStatistics(org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (statisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          statistics_ = value;
-          onChanged();
-        } else {
-          statisticsBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public Builder setStatistics(
-          org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (statisticsBuilder_ == null) {
-          statistics_ = builderForValue.build();
-          onChanged();
-        } else {
-          statisticsBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public Builder mergeStatistics(org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (statisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              statistics_ != org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance()) {
-            statistics_ =
-              org.apache.orc.OrcProto.ColumnStatistics.newBuilder(statistics_).mergeFrom(value).buildPartial();
-          } else {
-            statistics_ = value;
-          }
-          onChanged();
-        } else {
-          statisticsBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public Builder clearStatistics() {
-        if (statisticsBuilder_ == null) {
-          statistics_ = org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance();
-          onChanged();
-        } else {
-          statisticsBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder getStatisticsBuilder() {
-        bitField0_ |= 0x00000002;
-        onChanged();
-        return getStatisticsFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder() {
-        if (statisticsBuilder_ != null) {
-          return statisticsBuilder_.getMessageOrBuilder();
-        } else {
-          return statistics_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.ColumnStatistics statistics = 2;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-          getStatisticsFieldBuilder() {
-        if (statisticsBuilder_ == null) {
-          statisticsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder>(
-                  statistics_,
-                  getParentForChildren(),
-                  isClean());
-          statistics_ = null;
-        }
-        return statisticsBuilder_;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.RowIndexEntry)
-    }
-
-    static {
-      defaultInstance = new RowIndexEntry(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.RowIndexEntry)
-  }
-
-  public interface RowIndexOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated .orc.proto.RowIndexEntry entry = 1;
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.RowIndexEntry> 
-        getEntryList();
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    org.apache.orc.OrcProto.RowIndexEntry getEntry(int index);
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    int getEntryCount();
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.RowIndexEntryOrBuilder> 
-        getEntryOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    org.apache.orc.OrcProto.RowIndexEntryOrBuilder getEntryOrBuilder(
-        int index);
-  }
-  /**
-   * Protobuf type {@code orc.proto.RowIndex}
-   */
-  public static final class RowIndex extends
-      com.google.protobuf.GeneratedMessage
-      implements RowIndexOrBuilder {
-    // Use RowIndex.newBuilder() to construct.
-    private RowIndex(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private RowIndex(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final RowIndex defaultInstance;
-    public static RowIndex getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public RowIndex getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private RowIndex(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                entry_ = new java.util.ArrayList<org.apache.orc.OrcProto.RowIndexEntry>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              entry_.add(input.readMessage(org.apache.orc.OrcProto.RowIndexEntry.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          entry_ = java.util.Collections.unmodifiableList(entry_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndex_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndex_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.RowIndex.class, org.apache.orc.OrcProto.RowIndex.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<RowIndex> PARSER =
-        new com.google.protobuf.AbstractParser<RowIndex>() {
-      public RowIndex parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new RowIndex(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<RowIndex> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated .orc.proto.RowIndexEntry entry = 1;
-    public static final int ENTRY_FIELD_NUMBER = 1;
-    private java.util.List<org.apache.orc.OrcProto.RowIndexEntry> entry_;
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.RowIndexEntry> getEntryList() {
-      return entry_;
-    }
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.RowIndexEntryOrBuilder> 
-        getEntryOrBuilderList() {
-      return entry_;
-    }
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    public int getEntryCount() {
-      return entry_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    public org.apache.orc.OrcProto.RowIndexEntry getEntry(int index) {
-      return entry_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-     */
-    public org.apache.orc.OrcProto.RowIndexEntryOrBuilder getEntryOrBuilder(
-        int index) {
-      return entry_.get(index);
-    }
-
-    private void initFields() {
-      entry_ = java.util.Collections.emptyList();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      for (int i = 0; i < entry_.size(); i++) {
-        output.writeMessage(1, entry_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      for (int i = 0; i < entry_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, entry_.get(i));
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.RowIndex parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.RowIndex prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.RowIndex}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.RowIndexOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndex_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndex_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.RowIndex.class, org.apache.orc.OrcProto.RowIndex.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.RowIndex.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getEntryFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        if (entryBuilder_ == null) {
-          entry_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-        } else {
-          entryBuilder_.clear();
-        }
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_RowIndex_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.RowIndex getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.RowIndex.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.RowIndex build() {
-        org.apache.orc.OrcProto.RowIndex result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.RowIndex buildPartial() {
-        org.apache.orc.OrcProto.RowIndex result = new org.apache.orc.OrcProto.RowIndex(this);
-        int from_bitField0_ = bitField0_;
-        if (entryBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            entry_ = java.util.Collections.unmodifiableList(entry_);
-            bitField0_ = (bitField0_ & ~0x00000001);
-          }
-          result.entry_ = entry_;
-        } else {
-          result.entry_ = entryBuilder_.build();
-        }
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.RowIndex) {
-          return mergeFrom((org.apache.orc.OrcProto.RowIndex)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.RowIndex other) {
-        if (other == org.apache.orc.OrcProto.RowIndex.getDefaultInstance()) return this;
-        if (entryBuilder_ == null) {
-          if (!other.entry_.isEmpty()) {
-            if (entry_.isEmpty()) {
-              entry_ = other.entry_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-            } else {
-              ensureEntryIsMutable();
-              entry_.addAll(other.entry_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.entry_.isEmpty()) {
-            if (entryBuilder_.isEmpty()) {
-              entryBuilder_.dispose();
-              entryBuilder_ = null;
-              entry_ = other.entry_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-              entryBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getEntryFieldBuilder() : null;
-            } else {
-              entryBuilder_.addAllMessages(other.entry_);
-            }
-          }
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.RowIndex parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.RowIndex) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated .orc.proto.RowIndexEntry entry = 1;
-      private java.util.List<org.apache.orc.OrcProto.RowIndexEntry> entry_ =
-        java.util.Collections.emptyList();
-      private void ensureEntryIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          entry_ = new java.util.ArrayList<org.apache.orc.OrcProto.RowIndexEntry>(entry_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.RowIndexEntry, org.apache.orc.OrcProto.RowIndexEntry.Builder, org.apache.orc.OrcProto.RowIndexEntryOrBuilder> entryBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.RowIndexEntry> getEntryList() {
-        if (entryBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(entry_);
-        } else {
-          return entryBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public int getEntryCount() {
-        if (entryBuilder_ == null) {
-          return entry_.size();
-        } else {
-          return entryBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public org.apache.orc.OrcProto.RowIndexEntry getEntry(int index) {
-        if (entryBuilder_ == null) {
-          return entry_.get(index);
-        } else {
-          return entryBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder setEntry(
-          int index, org.apache.orc.OrcProto.RowIndexEntry value) {
-        if (entryBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureEntryIsMutable();
-          entry_.set(index, value);
-          onChanged();
-        } else {
-          entryBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder setEntry(
-          int index, org.apache.orc.OrcProto.RowIndexEntry.Builder builderForValue) {
-        if (entryBuilder_ == null) {
-          ensureEntryIsMutable();
-          entry_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          entryBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder addEntry(org.apache.orc.OrcProto.RowIndexEntry value) {
-        if (entryBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureEntryIsMutable();
-          entry_.add(value);
-          onChanged();
-        } else {
-          entryBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder addEntry(
-          int index, org.apache.orc.OrcProto.RowIndexEntry value) {
-        if (entryBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureEntryIsMutable();
-          entry_.add(index, value);
-          onChanged();
-        } else {
-          entryBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder addEntry(
-          org.apache.orc.OrcProto.RowIndexEntry.Builder builderForValue) {
-        if (entryBuilder_ == null) {
-          ensureEntryIsMutable();
-          entry_.add(builderForValue.build());
-          onChanged();
-        } else {
-          entryBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder addEntry(
-          int index, org.apache.orc.OrcProto.RowIndexEntry.Builder builderForValue) {
-        if (entryBuilder_ == null) {
-          ensureEntryIsMutable();
-          entry_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          entryBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder addAllEntry(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.RowIndexEntry> values) {
-        if (entryBuilder_ == null) {
-          ensureEntryIsMutable();
-          super.addAll(values, entry_);
-          onChanged();
-        } else {
-          entryBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder clearEntry() {
-        if (entryBuilder_ == null) {
-          entry_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-          onChanged();
-        } else {
-          entryBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public Builder removeEntry(int index) {
-        if (entryBuilder_ == null) {
-          ensureEntryIsMutable();
-          entry_.remove(index);
-          onChanged();
-        } else {
-          entryBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public org.apache.orc.OrcProto.RowIndexEntry.Builder getEntryBuilder(
-          int index) {
-        return getEntryFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public org.apache.orc.OrcProto.RowIndexEntryOrBuilder getEntryOrBuilder(
-          int index) {
-        if (entryBuilder_ == null) {
-          return entry_.get(index);  } else {
-          return entryBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.RowIndexEntryOrBuilder> 
-           getEntryOrBuilderList() {
-        if (entryBuilder_ != null) {
-          return entryBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(entry_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public org.apache.orc.OrcProto.RowIndexEntry.Builder addEntryBuilder() {
-        return getEntryFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.RowIndexEntry.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public org.apache.orc.OrcProto.RowIndexEntry.Builder addEntryBuilder(
-          int index) {
-        return getEntryFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.RowIndexEntry.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.RowIndexEntry entry = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.RowIndexEntry.Builder> 
-           getEntryBuilderList() {
-        return getEntryFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.RowIndexEntry, org.apache.orc.OrcProto.RowIndexEntry.Builder, org.apache.orc.OrcProto.RowIndexEntryOrBuilder> 
-          getEntryFieldBuilder() {
-        if (entryBuilder_ == null) {
-          entryBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.RowIndexEntry, org.apache.orc.OrcProto.RowIndexEntry.Builder, org.apache.orc.OrcProto.RowIndexEntryOrBuilder>(
-                  entry_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
-                  getParentForChildren(),
-                  isClean());
-          entry_ = null;
-        }
-        return entryBuilder_;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.RowIndex)
-    }
-
-    static {
-      defaultInstance = new RowIndex(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.RowIndex)
-  }
-
-  public interface BloomFilterOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional uint32 numHashFunctions = 1;
-    /**
-     * <code>optional uint32 numHashFunctions = 1;</code>
-     */
-    boolean hasNumHashFunctions();
-    /**
-     * <code>optional uint32 numHashFunctions = 1;</code>
-     */
-    int getNumHashFunctions();
-
-    // repeated fixed64 bitset = 2;
-    /**
-     * <code>repeated fixed64 bitset = 2;</code>
-     */
-    java.util.List<java.lang.Long> getBitsetList();
-    /**
-     * <code>repeated fixed64 bitset = 2;</code>
-     */
-    int getBitsetCount();
-    /**
-     * <code>repeated fixed64 bitset = 2;</code>
-     */
-    long getBitset(int index);
-  }
-  /**
-   * Protobuf type {@code orc.proto.BloomFilter}
-   */
-  public static final class BloomFilter extends
-      com.google.protobuf.GeneratedMessage
-      implements BloomFilterOrBuilder {
-    // Use BloomFilter.newBuilder() to construct.
-    private BloomFilter(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private BloomFilter(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final BloomFilter defaultInstance;
-    public static BloomFilter getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public BloomFilter getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private BloomFilter(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              numHashFunctions_ = input.readUInt32();
-              break;
-            }
-            case 17: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                bitset_ = new java.util.ArrayList<java.lang.Long>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              bitset_.add(input.readFixed64());
-              break;
-            }
-            case 18: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
-                bitset_ = new java.util.ArrayList<java.lang.Long>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              while (input.getBytesUntilLimit() > 0) {
-                bitset_.add(input.readFixed64());
-              }
-              input.popLimit(limit);
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          bitset_ = java.util.Collections.unmodifiableList(bitset_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilter_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilter_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.BloomFilter.class, org.apache.orc.OrcProto.BloomFilter.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<BloomFilter> PARSER =
-        new com.google.protobuf.AbstractParser<BloomFilter>() {
-      public BloomFilter parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new BloomFilter(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<BloomFilter> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional uint32 numHashFunctions = 1;
-    public static final int NUMHASHFUNCTIONS_FIELD_NUMBER = 1;
-    private int numHashFunctions_;
-    /**
-     * <code>optional uint32 numHashFunctions = 1;</code>
-     */
-    public boolean hasNumHashFunctions() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional uint32 numHashFunctions = 1;</code>
-     */
-    public int getNumHashFunctions() {
-      return numHashFunctions_;
-    }
-
-    // repeated fixed64 bitset = 2;
-    public static final int BITSET_FIELD_NUMBER = 2;
-    private java.util.List<java.lang.Long> bitset_;
-    /**
-     * <code>repeated fixed64 bitset = 2;</code>
-     */
-    public java.util.List<java.lang.Long>
-        getBitsetList() {
-      return bitset_;
-    }
-    /**
-     * <code>repeated fixed64 bitset = 2;</code>
-     */
-    public int getBitsetCount() {
-      return bitset_.size();
-    }
-    /**
-     * <code>repeated fixed64 bitset = 2;</code>
-     */
-    public long getBitset(int index) {
-      return bitset_.get(index);
-    }
-
-    private void initFields() {
-      numHashFunctions_ = 0;
-      bitset_ = java.util.Collections.emptyList();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt32(1, numHashFunctions_);
-      }
-      for (int i = 0; i < bitset_.size(); i++) {
-        output.writeFixed64(2, bitset_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(1, numHashFunctions_);
-      }
-      {
-        int dataSize = 0;
-        dataSize = 8 * getBitsetList().size();
-        size += dataSize;
-        size += 1 * getBitsetList().size();
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BloomFilter parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.BloomFilter prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.BloomFilter}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.BloomFilterOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilter_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilter_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.BloomFilter.class, org.apache.orc.OrcProto.BloomFilter.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.BloomFilter.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        numHashFunctions_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        bitset_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilter_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.BloomFilter getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.BloomFilter.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.BloomFilter build() {
-        org.apache.orc.OrcProto.BloomFilter result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.BloomFilter buildPartial() {
-        org.apache.orc.OrcProto.BloomFilter result = new org.apache.orc.OrcProto.BloomFilter(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.numHashFunctions_ = numHashFunctions_;
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          bitset_ = java.util.Collections.unmodifiableList(bitset_);
-          bitField0_ = (bitField0_ & ~0x00000002);
-        }
-        result.bitset_ = bitset_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.BloomFilter) {
-          return mergeFrom((org.apache.orc.OrcProto.BloomFilter)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.BloomFilter other) {
-        if (other == org.apache.orc.OrcProto.BloomFilter.getDefaultInstance()) return this;
-        if (other.hasNumHashFunctions()) {
-          setNumHashFunctions(other.getNumHashFunctions());
-        }
-        if (!other.bitset_.isEmpty()) {
-          if (bitset_.isEmpty()) {
-            bitset_ = other.bitset_;
-            bitField0_ = (bitField0_ & ~0x00000002);
-          } else {
-            ensureBitsetIsMutable();
-            bitset_.addAll(other.bitset_);
-          }
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.BloomFilter parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.BloomFilter) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional uint32 numHashFunctions = 1;
-      private int numHashFunctions_ ;
-      /**
-       * <code>optional uint32 numHashFunctions = 1;</code>
-       */
-      public boolean hasNumHashFunctions() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional uint32 numHashFunctions = 1;</code>
-       */
-      public int getNumHashFunctions() {
-        return numHashFunctions_;
-      }
-      /**
-       * <code>optional uint32 numHashFunctions = 1;</code>
-       */
-      public Builder setNumHashFunctions(int value) {
-        bitField0_ |= 0x00000001;
-        numHashFunctions_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 numHashFunctions = 1;</code>
-       */
-      public Builder clearNumHashFunctions() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        numHashFunctions_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // repeated fixed64 bitset = 2;
-      private java.util.List<java.lang.Long> bitset_ = java.util.Collections.emptyList();
-      private void ensureBitsetIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-          bitset_ = new java.util.ArrayList<java.lang.Long>(bitset_);
-          bitField0_ |= 0x00000002;
-         }
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public java.util.List<java.lang.Long>
-          getBitsetList() {
-        return java.util.Collections.unmodifiableList(bitset_);
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public int getBitsetCount() {
-        return bitset_.size();
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public long getBitset(int index) {
-        return bitset_.get(index);
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public Builder setBitset(
-          int index, long value) {
-        ensureBitsetIsMutable();
-        bitset_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public Builder addBitset(long value) {
-        ensureBitsetIsMutable();
-        bitset_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public Builder addAllBitset(
-          java.lang.Iterable<? extends java.lang.Long> values) {
-        ensureBitsetIsMutable();
-        super.addAll(values, bitset_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated fixed64 bitset = 2;</code>
-       */
-      public Builder clearBitset() {
-        bitset_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000002);
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.BloomFilter)
-    }
-
-    static {
-      defaultInstance = new BloomFilter(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.BloomFilter)
-  }
-
-  public interface BloomFilterIndexOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated .orc.proto.BloomFilter bloomFilter = 1;
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.BloomFilter> 
-        getBloomFilterList();
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    org.apache.orc.OrcProto.BloomFilter getBloomFilter(int index);
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    int getBloomFilterCount();
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.BloomFilterOrBuilder> 
-        getBloomFilterOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    org.apache.orc.OrcProto.BloomFilterOrBuilder getBloomFilterOrBuilder(
-        int index);
-  }
-  /**
-   * Protobuf type {@code orc.proto.BloomFilterIndex}
-   */
-  public static final class BloomFilterIndex extends
-      com.google.protobuf.GeneratedMessage
-      implements BloomFilterIndexOrBuilder {
-    // Use BloomFilterIndex.newBuilder() to construct.
-    private BloomFilterIndex(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private BloomFilterIndex(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final BloomFilterIndex defaultInstance;
-    public static BloomFilterIndex getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public BloomFilterIndex getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private BloomFilterIndex(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                bloomFilter_ = new java.util.ArrayList<org.apache.orc.OrcProto.BloomFilter>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              bloomFilter_.add(input.readMessage(org.apache.orc.OrcProto.BloomFilter.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          bloomFilter_ = java.util.Collections.unmodifiableList(bloomFilter_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilterIndex_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilterIndex_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.BloomFilterIndex.class, org.apache.orc.OrcProto.BloomFilterIndex.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<BloomFilterIndex> PARSER =
-        new com.google.protobuf.AbstractParser<BloomFilterIndex>() {
-      public BloomFilterIndex parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new BloomFilterIndex(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<BloomFilterIndex> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated .orc.proto.BloomFilter bloomFilter = 1;
-    public static final int BLOOMFILTER_FIELD_NUMBER = 1;
-    private java.util.List<org.apache.orc.OrcProto.BloomFilter> bloomFilter_;
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.BloomFilter> getBloomFilterList() {
-      return bloomFilter_;
-    }
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.BloomFilterOrBuilder> 
-        getBloomFilterOrBuilderList() {
-      return bloomFilter_;
-    }
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    public int getBloomFilterCount() {
-      return bloomFilter_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    public org.apache.orc.OrcProto.BloomFilter getBloomFilter(int index) {
-      return bloomFilter_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-     */
-    public org.apache.orc.OrcProto.BloomFilterOrBuilder getBloomFilterOrBuilder(
-        int index) {
-      return bloomFilter_.get(index);
-    }
-
-    private void initFields() {
-      bloomFilter_ = java.util.Collections.emptyList();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      for (int i = 0; i < bloomFilter_.size(); i++) {
-        output.writeMessage(1, bloomFilter_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      for (int i = 0; i < bloomFilter_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, bloomFilter_.get(i));
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.BloomFilterIndex parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.BloomFilterIndex prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.BloomFilterIndex}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.BloomFilterIndexOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilterIndex_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilterIndex_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.BloomFilterIndex.class, org.apache.orc.OrcProto.BloomFilterIndex.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.BloomFilterIndex.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getBloomFilterFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        if (bloomFilterBuilder_ == null) {
-          bloomFilter_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-        } else {
-          bloomFilterBuilder_.clear();
-        }
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_BloomFilterIndex_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.BloomFilterIndex getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.BloomFilterIndex.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.BloomFilterIndex build() {
-        org.apache.orc.OrcProto.BloomFilterIndex result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.BloomFilterIndex buildPartial() {
-        org.apache.orc.OrcProto.BloomFilterIndex result = new org.apache.orc.OrcProto.BloomFilterIndex(this);
-        int from_bitField0_ = bitField0_;
-        if (bloomFilterBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            bloomFilter_ = java.util.Collections.unmodifiableList(bloomFilter_);
-            bitField0_ = (bitField0_ & ~0x00000001);
-          }
-          result.bloomFilter_ = bloomFilter_;
-        } else {
-          result.bloomFilter_ = bloomFilterBuilder_.build();
-        }
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.BloomFilterIndex) {
-          return mergeFrom((org.apache.orc.OrcProto.BloomFilterIndex)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.BloomFilterIndex other) {
-        if (other == org.apache.orc.OrcProto.BloomFilterIndex.getDefaultInstance()) return this;
-        if (bloomFilterBuilder_ == null) {
-          if (!other.bloomFilter_.isEmpty()) {
-            if (bloomFilter_.isEmpty()) {
-              bloomFilter_ = other.bloomFilter_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-            } else {
-              ensureBloomFilterIsMutable();
-              bloomFilter_.addAll(other.bloomFilter_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.bloomFilter_.isEmpty()) {
-            if (bloomFilterBuilder_.isEmpty()) {
-              bloomFilterBuilder_.dispose();
-              bloomFilterBuilder_ = null;
-              bloomFilter_ = other.bloomFilter_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-              bloomFilterBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getBloomFilterFieldBuilder() : null;
-            } else {
-              bloomFilterBuilder_.addAllMessages(other.bloomFilter_);
-            }
-          }
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.BloomFilterIndex parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.BloomFilterIndex) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated .orc.proto.BloomFilter bloomFilter = 1;
-      private java.util.List<org.apache.orc.OrcProto.BloomFilter> bloomFilter_ =
-        java.util.Collections.emptyList();
-      private void ensureBloomFilterIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          bloomFilter_ = new java.util.ArrayList<org.apache.orc.OrcProto.BloomFilter>(bloomFilter_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.BloomFilter, org.apache.orc.OrcProto.BloomFilter.Builder, org.apache.orc.OrcProto.BloomFilterOrBuilder> bloomFilterBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.BloomFilter> getBloomFilterList() {
-        if (bloomFilterBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(bloomFilter_);
-        } else {
-          return bloomFilterBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public int getBloomFilterCount() {
-        if (bloomFilterBuilder_ == null) {
-          return bloomFilter_.size();
-        } else {
-          return bloomFilterBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public org.apache.orc.OrcProto.BloomFilter getBloomFilter(int index) {
-        if (bloomFilterBuilder_ == null) {
-          return bloomFilter_.get(index);
-        } else {
-          return bloomFilterBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder setBloomFilter(
-          int index, org.apache.orc.OrcProto.BloomFilter value) {
-        if (bloomFilterBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureBloomFilterIsMutable();
-          bloomFilter_.set(index, value);
-          onChanged();
-        } else {
-          bloomFilterBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder setBloomFilter(
-          int index, org.apache.orc.OrcProto.BloomFilter.Builder builderForValue) {
-        if (bloomFilterBuilder_ == null) {
-          ensureBloomFilterIsMutable();
-          bloomFilter_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          bloomFilterBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder addBloomFilter(org.apache.orc.OrcProto.BloomFilter value) {
-        if (bloomFilterBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureBloomFilterIsMutable();
-          bloomFilter_.add(value);
-          onChanged();
-        } else {
-          bloomFilterBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder addBloomFilter(
-          int index, org.apache.orc.OrcProto.BloomFilter value) {
-        if (bloomFilterBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureBloomFilterIsMutable();
-          bloomFilter_.add(index, value);
-          onChanged();
-        } else {
-          bloomFilterBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder addBloomFilter(
-          org.apache.orc.OrcProto.BloomFilter.Builder builderForValue) {
-        if (bloomFilterBuilder_ == null) {
-          ensureBloomFilterIsMutable();
-          bloomFilter_.add(builderForValue.build());
-          onChanged();
-        } else {
-          bloomFilterBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder addBloomFilter(
-          int index, org.apache.orc.OrcProto.BloomFilter.Builder builderForValue) {
-        if (bloomFilterBuilder_ == null) {
-          ensureBloomFilterIsMutable();
-          bloomFilter_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          bloomFilterBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder addAllBloomFilter(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.BloomFilter> values) {
-        if (bloomFilterBuilder_ == null) {
-          ensureBloomFilterIsMutable();
-          super.addAll(values, bloomFilter_);
-          onChanged();
-        } else {
-          bloomFilterBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder clearBloomFilter() {
-        if (bloomFilterBuilder_ == null) {
-          bloomFilter_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-          onChanged();
-        } else {
-          bloomFilterBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public Builder removeBloomFilter(int index) {
-        if (bloomFilterBuilder_ == null) {
-          ensureBloomFilterIsMutable();
-          bloomFilter_.remove(index);
-          onChanged();
-        } else {
-          bloomFilterBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public org.apache.orc.OrcProto.BloomFilter.Builder getBloomFilterBuilder(
-          int index) {
-        return getBloomFilterFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public org.apache.orc.OrcProto.BloomFilterOrBuilder getBloomFilterOrBuilder(
-          int index) {
-        if (bloomFilterBuilder_ == null) {
-          return bloomFilter_.get(index);  } else {
-          return bloomFilterBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.BloomFilterOrBuilder> 
-           getBloomFilterOrBuilderList() {
-        if (bloomFilterBuilder_ != null) {
-          return bloomFilterBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(bloomFilter_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public org.apache.orc.OrcProto.BloomFilter.Builder addBloomFilterBuilder() {
-        return getBloomFilterFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.BloomFilter.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public org.apache.orc.OrcProto.BloomFilter.Builder addBloomFilterBuilder(
-          int index) {
-        return getBloomFilterFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.BloomFilter.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.BloomFilter bloomFilter = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.BloomFilter.Builder> 
-           getBloomFilterBuilderList() {
-        return getBloomFilterFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.BloomFilter, org.apache.orc.OrcProto.BloomFilter.Builder, org.apache.orc.OrcProto.BloomFilterOrBuilder> 
-          getBloomFilterFieldBuilder() {
-        if (bloomFilterBuilder_ == null) {
-          bloomFilterBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.BloomFilter, org.apache.orc.OrcProto.BloomFilter.Builder, org.apache.orc.OrcProto.BloomFilterOrBuilder>(
-                  bloomFilter_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
-                  getParentForChildren(),
-                  isClean());
-          bloomFilter_ = null;
-        }
-        return bloomFilterBuilder_;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.BloomFilterIndex)
-    }
-
-    static {
-      defaultInstance = new BloomFilterIndex(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.BloomFilterIndex)
-  }
-
-  public interface StreamOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional .orc.proto.Stream.Kind kind = 1;
-    /**
-     * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-     */
-    boolean hasKind();
-    /**
-     * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-     */
-    org.apache.orc.OrcProto.Stream.Kind getKind();
-
-    // optional uint32 column = 2;
-    /**
-     * <code>optional uint32 column = 2;</code>
-     */
-    boolean hasColumn();
-    /**
-     * <code>optional uint32 column = 2;</code>
-     */
-    int getColumn();
-
-    // optional uint64 length = 3;
-    /**
-     * <code>optional uint64 length = 3;</code>
-     */
-    boolean hasLength();
-    /**
-     * <code>optional uint64 length = 3;</code>
-     */
-    long getLength();
-  }
-  /**
-   * Protobuf type {@code orc.proto.Stream}
-   */
-  public static final class Stream extends
-      com.google.protobuf.GeneratedMessage
-      implements StreamOrBuilder {
-    // Use Stream.newBuilder() to construct.
-    private Stream(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private Stream(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final Stream defaultInstance;
-    public static Stream getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public Stream getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private Stream(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.orc.OrcProto.Stream.Kind value = org.apache.orc.OrcProto.Stream.Kind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                kind_ = value;
-              }
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              column_ = input.readUInt32();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              length_ = input.readUInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Stream_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Stream_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.Stream.class, org.apache.orc.OrcProto.Stream.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<Stream> PARSER =
-        new com.google.protobuf.AbstractParser<Stream>() {
-      public Stream parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new Stream(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<Stream> getParserForType() {
-      return PARSER;
-    }
-
-    /**
-     * Protobuf enum {@code orc.proto.Stream.Kind}
-     *
-     * <pre>
-     * if you add new index stream kinds, you need to make sure to update
-     * StreamName to ensure it is added to the stripe in the right area
-     * </pre>
-     */
-    public enum Kind
-        implements com.google.protobuf.ProtocolMessageEnum {
-      /**
-       * <code>PRESENT = 0;</code>
-       */
-      PRESENT(0, 0),
-      /**
-       * <code>DATA = 1;</code>
-       */
-      DATA(1, 1),
-      /**
-       * <code>LENGTH = 2;</code>
-       */
-      LENGTH(2, 2),
-      /**
-       * <code>DICTIONARY_DATA = 3;</code>
-       */
-      DICTIONARY_DATA(3, 3),
-      /**
-       * <code>DICTIONARY_COUNT = 4;</code>
-       */
-      DICTIONARY_COUNT(4, 4),
-      /**
-       * <code>SECONDARY = 5;</code>
-       */
-      SECONDARY(5, 5),
-      /**
-       * <code>ROW_INDEX = 6;</code>
-       */
-      ROW_INDEX(6, 6),
-      /**
-       * <code>BLOOM_FILTER = 7;</code>
-       */
-      BLOOM_FILTER(7, 7),
-      ;
-
-      /**
-       * <code>PRESENT = 0;</code>
-       */
-      public static final int PRESENT_VALUE = 0;
-      /**
-       * <code>DATA = 1;</code>
-       */
-      public static final int DATA_VALUE = 1;
-      /**
-       * <code>LENGTH = 2;</code>
-       */
-      public static final int LENGTH_VALUE = 2;
-      /**
-       * <code>DICTIONARY_DATA = 3;</code>
-       */
-      public static final int DICTIONARY_DATA_VALUE = 3;
-      /**
-       * <code>DICTIONARY_COUNT = 4;</code>
-       */
-      public static final int DICTIONARY_COUNT_VALUE = 4;
-      /**
-       * <code>SECONDARY = 5;</code>
-       */
-      public static final int SECONDARY_VALUE = 5;
-      /**
-       * <code>ROW_INDEX = 6;</code>
-       */
-      public static final int ROW_INDEX_VALUE = 6;
-      /**
-       * <code>BLOOM_FILTER = 7;</code>
-       */
-      public static final int BLOOM_FILTER_VALUE = 7;
-
-
-      public final int getNumber() { return value; }
-
-      public static Kind valueOf(int value) {
-        switch (value) {
-          case 0: return PRESENT;
-          case 1: return DATA;
-          case 2: return LENGTH;
-          case 3: return DICTIONARY_DATA;
-          case 4: return DICTIONARY_COUNT;
-          case 5: return SECONDARY;
-          case 6: return ROW_INDEX;
-          case 7: return BLOOM_FILTER;
-          default: return null;
-        }
-      }
-
-      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
-          internalGetValueMap() {
-        return internalValueMap;
-      }
-      private static com.google.protobuf.Internal.EnumLiteMap<Kind>
-          internalValueMap =
-            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
-              public Kind findValueByNumber(int number) {
-                return Kind.valueOf(number);
-              }
-            };
-
-      public final com.google.protobuf.Descriptors.EnumValueDescriptor
-          getValueDescriptor() {
-        return getDescriptor().getValues().get(index);
-      }
-      public final com.google.protobuf.Descriptors.EnumDescriptor
-          getDescriptorForType() {
-        return getDescriptor();
-      }
-      public static final com.google.protobuf.Descriptors.EnumDescriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.Stream.getDescriptor().getEnumTypes().get(0);
-      }
-
-      private static final Kind[] VALUES = values();
-
-      public static Kind valueOf(
-          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
-        if (desc.getType() != getDescriptor()) {
-          throw new java.lang.IllegalArgumentException(
-            "EnumValueDescriptor is not for this type.");
-        }
-        return VALUES[desc.getIndex()];
-      }
-
-      private final int index;
-      private final int value;
-
-      private Kind(int index, int value) {
-        this.index = index;
-        this.value = value;
-      }
-
-      // @@protoc_insertion_point(enum_scope:orc.proto.Stream.Kind)
-    }
-
-    private int bitField0_;
-    // optional .orc.proto.Stream.Kind kind = 1;
-    public static final int KIND_FIELD_NUMBER = 1;
-    private org.apache.orc.OrcProto.Stream.Kind kind_;
-    /**
-     * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-     */
-    public boolean hasKind() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-     */
-    public org.apache.orc.OrcProto.Stream.Kind getKind() {
-      return kind_;
-    }
-
-    // optional uint32 column = 2;
-    public static final int COLUMN_FIELD_NUMBER = 2;
-    private int column_;
-    /**
-     * <code>optional uint32 column = 2;</code>
-     */
-    public boolean hasColumn() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional uint32 column = 2;</code>
-     */
-    public int getColumn() {
-      return column_;
-    }
-
-    // optional uint64 length = 3;
-    public static final int LENGTH_FIELD_NUMBER = 3;
-    private long length_;
-    /**
-     * <code>optional uint64 length = 3;</code>
-     */
-    public boolean hasLength() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional uint64 length = 3;</code>
-     */
-    public long getLength() {
-      return length_;
-    }
-
-    private void initFields() {
-      kind_ = org.apache.orc.OrcProto.Stream.Kind.PRESENT;
-      column_ = 0;
-      length_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeEnum(1, kind_.getNumber());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt32(2, column_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt64(3, length_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(1, kind_.getNumber());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(2, column_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(3, length_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.Stream parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Stream parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Stream parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Stream parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.Stream prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.Stream}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.StreamOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Stream_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Stream_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.Stream.class, org.apache.orc.OrcProto.Stream.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.Stream.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        kind_ = org.apache.orc.OrcProto.Stream.Kind.PRESENT;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        column_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        length_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Stream_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.Stream getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.Stream.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.Stream build() {
-        org.apache.orc.OrcProto.Stream result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.Stream buildPartial() {
-        org.apache.orc.OrcProto.Stream result = new org.apache.orc.OrcProto.Stream(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.kind_ = kind_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.column_ = column_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.length_ = length_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.Stream) {
-          return mergeFrom((org.apache.orc.OrcProto.Stream)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.Stream other) {
-        if (other == org.apache.orc.OrcProto.Stream.getDefaultInstance()) return this;
-        if (other.hasKind()) {
-          setKind(other.getKind());
-        }
-        if (other.hasColumn()) {
-          setColumn(other.getColumn());
-        }
-        if (other.hasLength()) {
-          setLength(other.getLength());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.Stream parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.Stream) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional .orc.proto.Stream.Kind kind = 1;
-      private org.apache.orc.OrcProto.Stream.Kind kind_ = org.apache.orc.OrcProto.Stream.Kind.PRESENT;
-      /**
-       * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-       */
-      public boolean hasKind() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-       */
-      public org.apache.orc.OrcProto.Stream.Kind getKind() {
-        return kind_;
-      }
-      /**
-       * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-       */
-      public Builder setKind(org.apache.orc.OrcProto.Stream.Kind value) {
-        if (value == null) {
-          throw new NullPointerException();
-        }
-        bitField0_ |= 0x00000001;
-        kind_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.Stream.Kind kind = 1;</code>
-       */
-      public Builder clearKind() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        kind_ = org.apache.orc.OrcProto.Stream.Kind.PRESENT;
-        onChanged();
-        return this;
-      }
-
-      // optional uint32 column = 2;
-      private int column_ ;
-      /**
-       * <code>optional uint32 column = 2;</code>
-       */
-      public boolean hasColumn() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional uint32 column = 2;</code>
-       */
-      public int getColumn() {
-        return column_;
-      }
-      /**
-       * <code>optional uint32 column = 2;</code>
-       */
-      public Builder setColumn(int value) {
-        bitField0_ |= 0x00000002;
-        column_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 column = 2;</code>
-       */
-      public Builder clearColumn() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        column_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 length = 3;
-      private long length_ ;
-      /**
-       * <code>optional uint64 length = 3;</code>
-       */
-      public boolean hasLength() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional uint64 length = 3;</code>
-       */
-      public long getLength() {
-        return length_;
-      }
-      /**
-       * <code>optional uint64 length = 3;</code>
-       */
-      public Builder setLength(long value) {
-        bitField0_ |= 0x00000004;
-        length_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 length = 3;</code>
-       */
-      public Builder clearLength() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        length_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.Stream)
-    }
-
-    static {
-      defaultInstance = new Stream(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.Stream)
-  }
-
-  public interface ColumnEncodingOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional .orc.proto.ColumnEncoding.Kind kind = 1;
-    /**
-     * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-     */
-    boolean hasKind();
-    /**
-     * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-     */
-    org.apache.orc.OrcProto.ColumnEncoding.Kind getKind();
-
-    // optional uint32 dictionarySize = 2;
-    /**
-     * <code>optional uint32 dictionarySize = 2;</code>
-     */
-    boolean hasDictionarySize();
-    /**
-     * <code>optional uint32 dictionarySize = 2;</code>
-     */
-    int getDictionarySize();
-  }
-  /**
-   * Protobuf type {@code orc.proto.ColumnEncoding}
-   */
-  public static final class ColumnEncoding extends
-      com.google.protobuf.GeneratedMessage
-      implements ColumnEncodingOrBuilder {
-    // Use ColumnEncoding.newBuilder() to construct.
-    private ColumnEncoding(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private ColumnEncoding(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final ColumnEncoding defaultInstance;
-    public static ColumnEncoding getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public ColumnEncoding getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private ColumnEncoding(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.orc.OrcProto.ColumnEncoding.Kind value = org.apache.orc.OrcProto.ColumnEncoding.Kind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                kind_ = value;
-              }
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              dictionarySize_ = input.readUInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnEncoding_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnEncoding_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.ColumnEncoding.class, org.apache.orc.OrcProto.ColumnEncoding.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<ColumnEncoding> PARSER =
-        new com.google.protobuf.AbstractParser<ColumnEncoding>() {
-      public ColumnEncoding parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new ColumnEncoding(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<ColumnEncoding> getParserForType() {
-      return PARSER;
-    }
-
-    /**
-     * Protobuf enum {@code orc.proto.ColumnEncoding.Kind}
-     */
-    public enum Kind
-        implements com.google.protobuf.ProtocolMessageEnum {
-      /**
-       * <code>DIRECT = 0;</code>
-       */
-      DIRECT(0, 0),
-      /**
-       * <code>DICTIONARY = 1;</code>
-       */
-      DICTIONARY(1, 1),
-      /**
-       * <code>DIRECT_V2 = 2;</code>
-       */
-      DIRECT_V2(2, 2),
-      /**
-       * <code>DICTIONARY_V2 = 3;</code>
-       */
-      DICTIONARY_V2(3, 3),
-      ;
-
-      /**
-       * <code>DIRECT = 0;</code>
-       */
-      public static final int DIRECT_VALUE = 0;
-      /**
-       * <code>DICTIONARY = 1;</code>
-       */
-      public static final int DICTIONARY_VALUE = 1;
-      /**
-       * <code>DIRECT_V2 = 2;</code>
-       */
-      public static final int DIRECT_V2_VALUE = 2;
-      /**
-       * <code>DICTIONARY_V2 = 3;</code>
-       */
-      public static final int DICTIONARY_V2_VALUE = 3;
-
-
-      public final int getNumber() { return value; }
-
-      public static Kind valueOf(int value) {
-        switch (value) {
-          case 0: return DIRECT;
-          case 1: return DICTIONARY;
-          case 2: return DIRECT_V2;
-          case 3: return DICTIONARY_V2;
-          default: return null;
-        }
-      }
-
-      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
-          internalGetValueMap() {
-        return internalValueMap;
-      }
-      private static com.google.protobuf.Internal.EnumLiteMap<Kind>
-          internalValueMap =
-            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
-              public Kind findValueByNumber(int number) {
-                return Kind.valueOf(number);
-              }
-            };
-
-      public final com.google.protobuf.Descriptors.EnumValueDescriptor
-          getValueDescriptor() {
-        return getDescriptor().getValues().get(index);
-      }
-      public final com.google.protobuf.Descriptors.EnumDescriptor
-          getDescriptorForType() {
-        return getDescriptor();
-      }
-      public static final com.google.protobuf.Descriptors.EnumDescriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.ColumnEncoding.getDescriptor().getEnumTypes().get(0);
-      }
-
-      private static final Kind[] VALUES = values();
-
-      public static Kind valueOf(
-          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
-        if (desc.getType() != getDescriptor()) {
-          throw new java.lang.IllegalArgumentException(
-            "EnumValueDescriptor is not for this type.");
-        }
-        return VALUES[desc.getIndex()];
-      }
-
-      private final int index;
-      private final int value;
-
-      private Kind(int index, int value) {
-        this.index = index;
-        this.value = value;
-      }
-
-      // @@protoc_insertion_point(enum_scope:orc.proto.ColumnEncoding.Kind)
-    }
-
-    private int bitField0_;
-    // optional .orc.proto.ColumnEncoding.Kind kind = 1;
-    public static final int KIND_FIELD_NUMBER = 1;
-    private org.apache.orc.OrcProto.ColumnEncoding.Kind kind_;
-    /**
-     * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-     */
-    public boolean hasKind() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnEncoding.Kind getKind() {
-      return kind_;
-    }
-
-    // optional uint32 dictionarySize = 2;
-    public static final int DICTIONARYSIZE_FIELD_NUMBER = 2;
-    private int dictionarySize_;
-    /**
-     * <code>optional uint32 dictionarySize = 2;</code>
-     */
-    public boolean hasDictionarySize() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional uint32 dictionarySize = 2;</code>
-     */
-    public int getDictionarySize() {
-      return dictionarySize_;
-    }
-
-    private void initFields() {
-      kind_ = org.apache.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
-      dictionarySize_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeEnum(1, kind_.getNumber());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt32(2, dictionarySize_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(1, kind_.getNumber());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(2, dictionarySize_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.ColumnEncoding parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.ColumnEncoding prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.ColumnEncoding}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.ColumnEncodingOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnEncoding_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnEncoding_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.ColumnEncoding.class, org.apache.orc.OrcProto.ColumnEncoding.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.ColumnEncoding.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        kind_ = org.apache.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        dictionarySize_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_ColumnEncoding_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.ColumnEncoding getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.ColumnEncoding.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.ColumnEncoding build() {
-        org.apache.orc.OrcProto.ColumnEncoding result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.ColumnEncoding buildPartial() {
-        org.apache.orc.OrcProto.ColumnEncoding result = new org.apache.orc.OrcProto.ColumnEncoding(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.kind_ = kind_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.dictionarySize_ = dictionarySize_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.ColumnEncoding) {
-          return mergeFrom((org.apache.orc.OrcProto.ColumnEncoding)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.ColumnEncoding other) {
-        if (other == org.apache.orc.OrcProto.ColumnEncoding.getDefaultInstance()) return this;
-        if (other.hasKind()) {
-          setKind(other.getKind());
-        }
-        if (other.hasDictionarySize()) {
-          setDictionarySize(other.getDictionarySize());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.ColumnEncoding parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.ColumnEncoding) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional .orc.proto.ColumnEncoding.Kind kind = 1;
-      private org.apache.orc.OrcProto.ColumnEncoding.Kind kind_ = org.apache.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
-      /**
-       * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-       */
-      public boolean hasKind() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnEncoding.Kind getKind() {
-        return kind_;
-      }
-      /**
-       * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-       */
-      public Builder setKind(org.apache.orc.OrcProto.ColumnEncoding.Kind value) {
-        if (value == null) {
-          throw new NullPointerException();
-        }
-        bitField0_ |= 0x00000001;
-        kind_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.ColumnEncoding.Kind kind = 1;</code>
-       */
-      public Builder clearKind() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        kind_ = org.apache.orc.OrcProto.ColumnEncoding.Kind.DIRECT;
-        onChanged();
-        return this;
-      }
-
-      // optional uint32 dictionarySize = 2;
-      private int dictionarySize_ ;
-      /**
-       * <code>optional uint32 dictionarySize = 2;</code>
-       */
-      public boolean hasDictionarySize() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional uint32 dictionarySize = 2;</code>
-       */
-      public int getDictionarySize() {
-        return dictionarySize_;
-      }
-      /**
-       * <code>optional uint32 dictionarySize = 2;</code>
-       */
-      public Builder setDictionarySize(int value) {
-        bitField0_ |= 0x00000002;
-        dictionarySize_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 dictionarySize = 2;</code>
-       */
-      public Builder clearDictionarySize() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        dictionarySize_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.ColumnEncoding)
-    }
-
-    static {
-      defaultInstance = new ColumnEncoding(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.ColumnEncoding)
-  }
-
-  public interface StripeFooterOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated .orc.proto.Stream streams = 1;
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.Stream> 
-        getStreamsList();
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    org.apache.orc.OrcProto.Stream getStreams(int index);
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    int getStreamsCount();
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.StreamOrBuilder> 
-        getStreamsOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    org.apache.orc.OrcProto.StreamOrBuilder getStreamsOrBuilder(
-        int index);
-
-    // repeated .orc.proto.ColumnEncoding columns = 2;
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.ColumnEncoding> 
-        getColumnsList();
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    org.apache.orc.OrcProto.ColumnEncoding getColumns(int index);
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    int getColumnsCount();
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.ColumnEncodingOrBuilder> 
-        getColumnsOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    org.apache.orc.OrcProto.ColumnEncodingOrBuilder getColumnsOrBuilder(
-        int index);
-
-    // optional string writerTimezone = 3;
-    /**
-     * <code>optional string writerTimezone = 3;</code>
-     */
-    boolean hasWriterTimezone();
-    /**
-     * <code>optional string writerTimezone = 3;</code>
-     */
-    java.lang.String getWriterTimezone();
-    /**
-     * <code>optional string writerTimezone = 3;</code>
-     */
-    com.google.protobuf.ByteString
-        getWriterTimezoneBytes();
-  }
-  /**
-   * Protobuf type {@code orc.proto.StripeFooter}
-   */
-  public static final class StripeFooter extends
-      com.google.protobuf.GeneratedMessage
-      implements StripeFooterOrBuilder {
-    // Use StripeFooter.newBuilder() to construct.
-    private StripeFooter(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private StripeFooter(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final StripeFooter defaultInstance;
-    public static StripeFooter getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public StripeFooter getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private StripeFooter(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                streams_ = new java.util.ArrayList<org.apache.orc.OrcProto.Stream>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              streams_.add(input.readMessage(org.apache.orc.OrcProto.Stream.PARSER, extensionRegistry));
-              break;
-            }
-            case 18: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                columns_ = new java.util.ArrayList<org.apache.orc.OrcProto.ColumnEncoding>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              columns_.add(input.readMessage(org.apache.orc.OrcProto.ColumnEncoding.PARSER, extensionRegistry));
-              break;
-            }
-            case 26: {
-              bitField0_ |= 0x00000001;
-              writerTimezone_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          streams_ = java.util.Collections.unmodifiableList(streams_);
-        }
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          columns_ = java.util.Collections.unmodifiableList(columns_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StripeFooter_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StripeFooter_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.StripeFooter.class, org.apache.orc.OrcProto.StripeFooter.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<StripeFooter> PARSER =
-        new com.google.protobuf.AbstractParser<StripeFooter>() {
-      public StripeFooter parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new StripeFooter(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<StripeFooter> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // repeated .orc.proto.Stream streams = 1;
-    public static final int STREAMS_FIELD_NUMBER = 1;
-    private java.util.List<org.apache.orc.OrcProto.Stream> streams_;
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.Stream> getStreamsList() {
-      return streams_;
-    }
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.StreamOrBuilder> 
-        getStreamsOrBuilderList() {
-      return streams_;
-    }
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    public int getStreamsCount() {
-      return streams_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    public org.apache.orc.OrcProto.Stream getStreams(int index) {
-      return streams_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.Stream streams = 1;</code>
-     */
-    public org.apache.orc.OrcProto.StreamOrBuilder getStreamsOrBuilder(
-        int index) {
-      return streams_.get(index);
-    }
-
-    // repeated .orc.proto.ColumnEncoding columns = 2;
-    public static final int COLUMNS_FIELD_NUMBER = 2;
-    private java.util.List<org.apache.orc.OrcProto.ColumnEncoding> columns_;
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.ColumnEncoding> getColumnsList() {
-      return columns_;
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.ColumnEncodingOrBuilder> 
-        getColumnsOrBuilderList() {
-      return columns_;
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    public int getColumnsCount() {
-      return columns_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnEncoding getColumns(int index) {
-      return columns_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnEncodingOrBuilder getColumnsOrBuilder(
-        int index) {
-      return columns_.get(index);
-    }
-
-    // optional string writerTimezone = 3;
-    public static final int WRITERTIMEZONE_FIELD_NUMBER = 3;
-    private java.lang.Object writerTimezone_;
-    /**
-     * <code>optional string writerTimezone = 3;</code>
-     */
-    public boolean hasWriterTimezone() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional string writerTimezone = 3;</code>
-     */
-    public java.lang.String getWriterTimezone() {
-      java.lang.Object ref = writerTimezone_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          writerTimezone_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string writerTimezone = 3;</code>
-     */
-    public com.google.protobuf.ByteString
-        getWriterTimezoneBytes() {
-      java.lang.Object ref = writerTimezone_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        writerTimezone_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    private void initFields() {
-      streams_ = java.util.Collections.emptyList();
-      columns_ = java.util.Collections.emptyList();
-      writerTimezone_ = "";
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      for (int i = 0; i < streams_.size(); i++) {
-        output.writeMessage(1, streams_.get(i));
-      }
-      for (int i = 0; i < columns_.size(); i++) {
-        output.writeMessage(2, columns_.get(i));
-      }
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(3, getWriterTimezoneBytes());
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      for (int i = 0; i < streams_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, streams_.get(i));
-      }
-      for (int i = 0; i < columns_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, columns_.get(i));
-      }
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(3, getWriterTimezoneBytes());
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeFooter parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.StripeFooter prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.StripeFooter}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.StripeFooterOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeFooter_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeFooter_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.StripeFooter.class, org.apache.orc.OrcProto.StripeFooter.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.StripeFooter.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getStreamsFieldBuilder();
-          getColumnsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        if (streamsBuilder_ == null) {
-          streams_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-        } else {
-          streamsBuilder_.clear();
-        }
-        if (columnsBuilder_ == null) {
-          columns_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000002);
-        } else {
-          columnsBuilder_.clear();
-        }
-        writerTimezone_ = "";
-        bitField0_ = (bitField0_ & ~0x00000004);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeFooter_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.StripeFooter getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.StripeFooter.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.StripeFooter build() {
-        org.apache.orc.OrcProto.StripeFooter result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.StripeFooter buildPartial() {
-        org.apache.orc.OrcProto.StripeFooter result = new org.apache.orc.OrcProto.StripeFooter(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (streamsBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            streams_ = java.util.Collections.unmodifiableList(streams_);
-            bitField0_ = (bitField0_ & ~0x00000001);
-          }
-          result.streams_ = streams_;
-        } else {
-          result.streams_ = streamsBuilder_.build();
-        }
-        if (columnsBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002)) {
-            columns_ = java.util.Collections.unmodifiableList(columns_);
-            bitField0_ = (bitField0_ & ~0x00000002);
-          }
-          result.columns_ = columns_;
-        } else {
-          result.columns_ = columnsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.writerTimezone_ = writerTimezone_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.StripeFooter) {
-          return mergeFrom((org.apache.orc.OrcProto.StripeFooter)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.StripeFooter other) {
-        if (other == org.apache.orc.OrcProto.StripeFooter.getDefaultInstance()) return this;
-        if (streamsBuilder_ == null) {
-          if (!other.streams_.isEmpty()) {
-            if (streams_.isEmpty()) {
-              streams_ = other.streams_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-            } else {
-              ensureStreamsIsMutable();
-              streams_.addAll(other.streams_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.streams_.isEmpty()) {
-            if (streamsBuilder_.isEmpty()) {
-              streamsBuilder_.dispose();
-              streamsBuilder_ = null;
-              streams_ = other.streams_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-              streamsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getStreamsFieldBuilder() : null;
-            } else {
-              streamsBuilder_.addAllMessages(other.streams_);
-            }
-          }
-        }
-        if (columnsBuilder_ == null) {
-          if (!other.columns_.isEmpty()) {
-            if (columns_.isEmpty()) {
-              columns_ = other.columns_;
-              bitField0_ = (bitField0_ & ~0x00000002);
-            } else {
-              ensureColumnsIsMutable();
-              columns_.addAll(other.columns_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.columns_.isEmpty()) {
-            if (columnsBuilder_.isEmpty()) {
-              columnsBuilder_.dispose();
-              columnsBuilder_ = null;
-              columns_ = other.columns_;
-              bitField0_ = (bitField0_ & ~0x00000002);
-              columnsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getColumnsFieldBuilder() : null;
-            } else {
-              columnsBuilder_.addAllMessages(other.columns_);
-            }
-          }
-        }
-        if (other.hasWriterTimezone()) {
-          bitField0_ |= 0x00000004;
-          writerTimezone_ = other.writerTimezone_;
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.StripeFooter parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.StripeFooter) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated .orc.proto.Stream streams = 1;
-      private java.util.List<org.apache.orc.OrcProto.Stream> streams_ =
-        java.util.Collections.emptyList();
-      private void ensureStreamsIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          streams_ = new java.util.ArrayList<org.apache.orc.OrcProto.Stream>(streams_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.Stream, org.apache.orc.OrcProto.Stream.Builder, org.apache.orc.OrcProto.StreamOrBuilder> streamsBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.Stream> getStreamsList() {
-        if (streamsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(streams_);
-        } else {
-          return streamsBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public int getStreamsCount() {
-        if (streamsBuilder_ == null) {
-          return streams_.size();
-        } else {
-          return streamsBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public org.apache.orc.OrcProto.Stream getStreams(int index) {
-        if (streamsBuilder_ == null) {
-          return streams_.get(index);
-        } else {
-          return streamsBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder setStreams(
-          int index, org.apache.orc.OrcProto.Stream value) {
-        if (streamsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStreamsIsMutable();
-          streams_.set(index, value);
-          onChanged();
-        } else {
-          streamsBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder setStreams(
-          int index, org.apache.orc.OrcProto.Stream.Builder builderForValue) {
-        if (streamsBuilder_ == null) {
-          ensureStreamsIsMutable();
-          streams_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          streamsBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder addStreams(org.apache.orc.OrcProto.Stream value) {
-        if (streamsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStreamsIsMutable();
-          streams_.add(value);
-          onChanged();
-        } else {
-          streamsBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder addStreams(
-          int index, org.apache.orc.OrcProto.Stream value) {
-        if (streamsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStreamsIsMutable();
-          streams_.add(index, value);
-          onChanged();
-        } else {
-          streamsBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder addStreams(
-          org.apache.orc.OrcProto.Stream.Builder builderForValue) {
-        if (streamsBuilder_ == null) {
-          ensureStreamsIsMutable();
-          streams_.add(builderForValue.build());
-          onChanged();
-        } else {
-          streamsBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder addStreams(
-          int index, org.apache.orc.OrcProto.Stream.Builder builderForValue) {
-        if (streamsBuilder_ == null) {
-          ensureStreamsIsMutable();
-          streams_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          streamsBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder addAllStreams(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.Stream> values) {
-        if (streamsBuilder_ == null) {
-          ensureStreamsIsMutable();
-          super.addAll(values, streams_);
-          onChanged();
-        } else {
-          streamsBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder clearStreams() {
-        if (streamsBuilder_ == null) {
-          streams_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-          onChanged();
-        } else {
-          streamsBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public Builder removeStreams(int index) {
-        if (streamsBuilder_ == null) {
-          ensureStreamsIsMutable();
-          streams_.remove(index);
-          onChanged();
-        } else {
-          streamsBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public org.apache.orc.OrcProto.Stream.Builder getStreamsBuilder(
-          int index) {
-        return getStreamsFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public org.apache.orc.OrcProto.StreamOrBuilder getStreamsOrBuilder(
-          int index) {
-        if (streamsBuilder_ == null) {
-          return streams_.get(index);  } else {
-          return streamsBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.StreamOrBuilder> 
-           getStreamsOrBuilderList() {
-        if (streamsBuilder_ != null) {
-          return streamsBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(streams_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public org.apache.orc.OrcProto.Stream.Builder addStreamsBuilder() {
-        return getStreamsFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.Stream.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public org.apache.orc.OrcProto.Stream.Builder addStreamsBuilder(
-          int index) {
-        return getStreamsFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.Stream.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.Stream streams = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.Stream.Builder> 
-           getStreamsBuilderList() {
-        return getStreamsFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.Stream, org.apache.orc.OrcProto.Stream.Builder, org.apache.orc.OrcProto.StreamOrBuilder> 
-          getStreamsFieldBuilder() {
-        if (streamsBuilder_ == null) {
-          streamsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.Stream, org.apache.orc.OrcProto.Stream.Builder, org.apache.orc.OrcProto.StreamOrBuilder>(
-                  streams_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
-                  getParentForChildren(),
-                  isClean());
-          streams_ = null;
-        }
-        return streamsBuilder_;
-      }
-
-      // repeated .orc.proto.ColumnEncoding columns = 2;
-      private java.util.List<org.apache.orc.OrcProto.ColumnEncoding> columns_ =
-        java.util.Collections.emptyList();
-      private void ensureColumnsIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-          columns_ = new java.util.ArrayList<org.apache.orc.OrcProto.ColumnEncoding>(columns_);
-          bitField0_ |= 0x00000002;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.ColumnEncoding, org.apache.orc.OrcProto.ColumnEncoding.Builder, org.apache.orc.OrcProto.ColumnEncodingOrBuilder> columnsBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.ColumnEncoding> getColumnsList() {
-        if (columnsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(columns_);
-        } else {
-          return columnsBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public int getColumnsCount() {
-        if (columnsBuilder_ == null) {
-          return columns_.size();
-        } else {
-          return columnsBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnEncoding getColumns(int index) {
-        if (columnsBuilder_ == null) {
-          return columns_.get(index);
-        } else {
-          return columnsBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder setColumns(
-          int index, org.apache.orc.OrcProto.ColumnEncoding value) {
-        if (columnsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureColumnsIsMutable();
-          columns_.set(index, value);
-          onChanged();
-        } else {
-          columnsBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder setColumns(
-          int index, org.apache.orc.OrcProto.ColumnEncoding.Builder builderForValue) {
-        if (columnsBuilder_ == null) {
-          ensureColumnsIsMutable();
-          columns_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          columnsBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder addColumns(org.apache.orc.OrcProto.ColumnEncoding value) {
-        if (columnsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureColumnsIsMutable();
-          columns_.add(value);
-          onChanged();
-        } else {
-          columnsBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder addColumns(
-          int index, org.apache.orc.OrcProto.ColumnEncoding value) {
-        if (columnsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureColumnsIsMutable();
-          columns_.add(index, value);
-          onChanged();
-        } else {
-          columnsBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder addColumns(
-          org.apache.orc.OrcProto.ColumnEncoding.Builder builderForValue) {
-        if (columnsBuilder_ == null) {
-          ensureColumnsIsMutable();
-          columns_.add(builderForValue.build());
-          onChanged();
-        } else {
-          columnsBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder addColumns(
-          int index, org.apache.orc.OrcProto.ColumnEncoding.Builder builderForValue) {
-        if (columnsBuilder_ == null) {
-          ensureColumnsIsMutable();
-          columns_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          columnsBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder addAllColumns(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.ColumnEncoding> values) {
-        if (columnsBuilder_ == null) {
-          ensureColumnsIsMutable();
-          super.addAll(values, columns_);
-          onChanged();
-        } else {
-          columnsBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder clearColumns() {
-        if (columnsBuilder_ == null) {
-          columns_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000002);
-          onChanged();
-        } else {
-          columnsBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public Builder removeColumns(int index) {
-        if (columnsBuilder_ == null) {
-          ensureColumnsIsMutable();
-          columns_.remove(index);
-          onChanged();
-        } else {
-          columnsBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnEncoding.Builder getColumnsBuilder(
-          int index) {
-        return getColumnsFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnEncodingOrBuilder getColumnsOrBuilder(
-          int index) {
-        if (columnsBuilder_ == null) {
-          return columns_.get(index);  } else {
-          return columnsBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.ColumnEncodingOrBuilder> 
-           getColumnsOrBuilderList() {
-        if (columnsBuilder_ != null) {
-          return columnsBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(columns_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnEncoding.Builder addColumnsBuilder() {
-        return getColumnsFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.ColumnEncoding.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnEncoding.Builder addColumnsBuilder(
-          int index) {
-        return getColumnsFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.ColumnEncoding.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnEncoding columns = 2;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.ColumnEncoding.Builder> 
-           getColumnsBuilderList() {
-        return getColumnsFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.ColumnEncoding, org.apache.orc.OrcProto.ColumnEncoding.Builder, org.apache.orc.OrcProto.ColumnEncodingOrBuilder> 
-          getColumnsFieldBuilder() {
-        if (columnsBuilder_ == null) {
-          columnsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.ColumnEncoding, org.apache.orc.OrcProto.ColumnEncoding.Builder, org.apache.orc.OrcProto.ColumnEncodingOrBuilder>(
-                  columns_,
-                  ((bitField0_ & 0x00000002) == 0x00000002),
-                  getParentForChildren(),
-                  isClean());
-          columns_ = null;
-        }
-        return columnsBuilder_;
-      }
-
-      // optional string writerTimezone = 3;
-      private java.lang.Object writerTimezone_ = "";
-      /**
-       * <code>optional string writerTimezone = 3;</code>
-       */
-      public boolean hasWriterTimezone() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional string writerTimezone = 3;</code>
-       */
-      public java.lang.String getWriterTimezone() {
-        java.lang.Object ref = writerTimezone_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          writerTimezone_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string writerTimezone = 3;</code>
-       */
-      public com.google.protobuf.ByteString
-          getWriterTimezoneBytes() {
-        java.lang.Object ref = writerTimezone_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          writerTimezone_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string writerTimezone = 3;</code>
-       */
-      public Builder setWriterTimezone(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
-        writerTimezone_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string writerTimezone = 3;</code>
-       */
-      public Builder clearWriterTimezone() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        writerTimezone_ = getDefaultInstance().getWriterTimezone();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string writerTimezone = 3;</code>
-       */
-      public Builder setWriterTimezoneBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000004;
-        writerTimezone_ = value;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.StripeFooter)
-    }
-
-    static {
-      defaultInstance = new StripeFooter(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.StripeFooter)
-  }
-
-  public interface TypeOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional .orc.proto.Type.Kind kind = 1;
-    /**
-     * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-     */
-    boolean hasKind();
-    /**
-     * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-     */
-    org.apache.orc.OrcProto.Type.Kind getKind();
-
-    // repeated uint32 subtypes = 2 [packed = true];
-    /**
-     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-     */
-    java.util.List<java.lang.Integer> getSubtypesList();
-    /**
-     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-     */
-    int getSubtypesCount();
-    /**
-     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-     */
-    int getSubtypes(int index);
-
-    // repeated string fieldNames = 3;
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    java.util.List<java.lang.String>
-    getFieldNamesList();
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    int getFieldNamesCount();
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    java.lang.String getFieldNames(int index);
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    com.google.protobuf.ByteString
-        getFieldNamesBytes(int index);
-
-    // optional uint32 maximumLength = 4;
-    /**
-     * <code>optional uint32 maximumLength = 4;</code>
-     */
-    boolean hasMaximumLength();
-    /**
-     * <code>optional uint32 maximumLength = 4;</code>
-     */
-    int getMaximumLength();
-
-    // optional uint32 precision = 5;
-    /**
-     * <code>optional uint32 precision = 5;</code>
-     */
-    boolean hasPrecision();
-    /**
-     * <code>optional uint32 precision = 5;</code>
-     */
-    int getPrecision();
-
-    // optional uint32 scale = 6;
-    /**
-     * <code>optional uint32 scale = 6;</code>
-     */
-    boolean hasScale();
-    /**
-     * <code>optional uint32 scale = 6;</code>
-     */
-    int getScale();
-  }
-  /**
-   * Protobuf type {@code orc.proto.Type}
-   */
-  public static final class Type extends
-      com.google.protobuf.GeneratedMessage
-      implements TypeOrBuilder {
-    // Use Type.newBuilder() to construct.
-    private Type(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private Type(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final Type defaultInstance;
-    public static Type getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public Type getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private Type(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              int rawValue = input.readEnum();
-              org.apache.orc.OrcProto.Type.Kind value = org.apache.orc.OrcProto.Type.Kind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(1, rawValue);
-              } else {
-                bitField0_ |= 0x00000001;
-                kind_ = value;
-              }
-              break;
-            }
-            case 16: {
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-                subtypes_ = new java.util.ArrayList<java.lang.Integer>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              subtypes_.add(input.readUInt32());
-              break;
-            }
-            case 18: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
-                subtypes_ = new java.util.ArrayList<java.lang.Integer>();
-                mutable_bitField0_ |= 0x00000002;
-              }
-              while (input.getBytesUntilLimit() > 0) {
-                subtypes_.add(input.readUInt32());
-              }
-              input.popLimit(limit);
-              break;
-            }
-            case 26: {
-              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-                fieldNames_ = new com.google.protobuf.LazyStringArrayList();
-                mutable_bitField0_ |= 0x00000004;
-              }
-              fieldNames_.add(input.readBytes());
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000002;
-              maximumLength_ = input.readUInt32();
-              break;
-            }
-            case 40: {
-              bitField0_ |= 0x00000004;
-              precision_ = input.readUInt32();
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000008;
-              scale_ = input.readUInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
-          subtypes_ = java.util.Collections.unmodifiableList(subtypes_);
-        }
-        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-          fieldNames_ = new com.google.protobuf.UnmodifiableLazyStringList(fieldNames_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Type_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Type_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.Type.class, org.apache.orc.OrcProto.Type.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<Type> PARSER =
-        new com.google.protobuf.AbstractParser<Type>() {
-      public Type parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new Type(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<Type> getParserForType() {
-      return PARSER;
-    }
-
-    /**
-     * Protobuf enum {@code orc.proto.Type.Kind}
-     */
-    public enum Kind
-        implements com.google.protobuf.ProtocolMessageEnum {
-      /**
-       * <code>BOOLEAN = 0;</code>
-       */
-      BOOLEAN(0, 0),
-      /**
-       * <code>BYTE = 1;</code>
-       */
-      BYTE(1, 1),
-      /**
-       * <code>SHORT = 2;</code>
-       */
-      SHORT(2, 2),
-      /**
-       * <code>INT = 3;</code>
-       */
-      INT(3, 3),
-      /**
-       * <code>LONG = 4;</code>
-       */
-      LONG(4, 4),
-      /**
-       * <code>FLOAT = 5;</code>
-       */
-      FLOAT(5, 5),
-      /**
-       * <code>DOUBLE = 6;</code>
-       */
-      DOUBLE(6, 6),
-      /**
-       * <code>STRING = 7;</code>
-       */
-      STRING(7, 7),
-      /**
-       * <code>BINARY = 8;</code>
-       */
-      BINARY(8, 8),
-      /**
-       * <code>TIMESTAMP = 9;</code>
-       */
-      TIMESTAMP(9, 9),
-      /**
-       * <code>LIST = 10;</code>
-       */
-      LIST(10, 10),
-      /**
-       * <code>MAP = 11;</code>
-       */
-      MAP(11, 11),
-      /**
-       * <code>STRUCT = 12;</code>
-       */
-      STRUCT(12, 12),
-      /**
-       * <code>UNION = 13;</code>
-       */
-      UNION(13, 13),
-      /**
-       * <code>DECIMAL = 14;</code>
-       */
-      DECIMAL(14, 14),
-      /**
-       * <code>DATE = 15;</code>
-       */
-      DATE(15, 15),
-      /**
-       * <code>VARCHAR = 16;</code>
-       */
-      VARCHAR(16, 16),
-      /**
-       * <code>CHAR = 17;</code>
-       */
-      CHAR(17, 17),
-      ;
-
-      /**
-       * <code>BOOLEAN = 0;</code>
-       */
-      public static final int BOOLEAN_VALUE = 0;
-      /**
-       * <code>BYTE = 1;</code>
-       */
-      public static final int BYTE_VALUE = 1;
-      /**
-       * <code>SHORT = 2;</code>
-       */
-      public static final int SHORT_VALUE = 2;
-      /**
-       * <code>INT = 3;</code>
-       */
-      public static final int INT_VALUE = 3;
-      /**
-       * <code>LONG = 4;</code>
-       */
-      public static final int LONG_VALUE = 4;
-      /**
-       * <code>FLOAT = 5;</code>
-       */
-      public static final int FLOAT_VALUE = 5;
-      /**
-       * <code>DOUBLE = 6;</code>
-       */
-      public static final int DOUBLE_VALUE = 6;
-      /**
-       * <code>STRING = 7;</code>
-       */
-      public static final int STRING_VALUE = 7;
-      /**
-       * <code>BINARY = 8;</code>
-       */
-      public static final int BINARY_VALUE = 8;
-      /**
-       * <code>TIMESTAMP = 9;</code>
-       */
-      public static final int TIMESTAMP_VALUE = 9;
-      /**
-       * <code>LIST = 10;</code>
-       */
-      public static final int LIST_VALUE = 10;
-      /**
-       * <code>MAP = 11;</code>
-       */
-      public static final int MAP_VALUE = 11;
-      /**
-       * <code>STRUCT = 12;</code>
-       */
-      public static final int STRUCT_VALUE = 12;
-      /**
-       * <code>UNION = 13;</code>
-       */
-      public static final int UNION_VALUE = 13;
-      /**
-       * <code>DECIMAL = 14;</code>
-       */
-      public static final int DECIMAL_VALUE = 14;
-      /**
-       * <code>DATE = 15;</code>
-       */
-      public static final int DATE_VALUE = 15;
-      /**
-       * <code>VARCHAR = 16;</code>
-       */
-      public static final int VARCHAR_VALUE = 16;
-      /**
-       * <code>CHAR = 17;</code>
-       */
-      public static final int CHAR_VALUE = 17;
-
-
-      public final int getNumber() { return value; }
-
-      public static Kind valueOf(int value) {
-        switch (value) {
-          case 0: return BOOLEAN;
-          case 1: return BYTE;
-          case 2: return SHORT;
-          case 3: return INT;
-          case 4: return LONG;
-          case 5: return FLOAT;
-          case 6: return DOUBLE;
-          case 7: return STRING;
-          case 8: return BINARY;
-          case 9: return TIMESTAMP;
-          case 10: return LIST;
-          case 11: return MAP;
-          case 12: return STRUCT;
-          case 13: return UNION;
-          case 14: return DECIMAL;
-          case 15: return DATE;
-          case 16: return VARCHAR;
-          case 17: return CHAR;
-          default: return null;
-        }
-      }
-
-      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
-          internalGetValueMap() {
-        return internalValueMap;
-      }
-      private static com.google.protobuf.Internal.EnumLiteMap<Kind>
-          internalValueMap =
-            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
-              public Kind findValueByNumber(int number) {
-                return Kind.valueOf(number);
-              }
-            };
-
-      public final com.google.protobuf.Descriptors.EnumValueDescriptor
-          getValueDescriptor() {
-        return getDescriptor().getValues().get(index);
-      }
-      public final com.google.protobuf.Descriptors.EnumDescriptor
-          getDescriptorForType() {
-        return getDescriptor();
-      }
-      public static final com.google.protobuf.Descriptors.EnumDescriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.Type.getDescriptor().getEnumTypes().get(0);
-      }
-
-      private static final Kind[] VALUES = values();
-
-      public static Kind valueOf(
-          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
-        if (desc.getType() != getDescriptor()) {
-          throw new java.lang.IllegalArgumentException(
-            "EnumValueDescriptor is not for this type.");
-        }
-        return VALUES[desc.getIndex()];
-      }
-
-      private final int index;
-      private final int value;
-
-      private Kind(int index, int value) {
-        this.index = index;
-        this.value = value;
-      }
-
-      // @@protoc_insertion_point(enum_scope:orc.proto.Type.Kind)
-    }
-
-    private int bitField0_;
-    // optional .orc.proto.Type.Kind kind = 1;
-    public static final int KIND_FIELD_NUMBER = 1;
-    private org.apache.orc.OrcProto.Type.Kind kind_;
-    /**
-     * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-     */
-    public boolean hasKind() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-     */
-    public org.apache.orc.OrcProto.Type.Kind getKind() {
-      return kind_;
-    }
-
-    // repeated uint32 subtypes = 2 [packed = true];
-    public static final int SUBTYPES_FIELD_NUMBER = 2;
-    private java.util.List<java.lang.Integer> subtypes_;
-    /**
-     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-     */
-    public java.util.List<java.lang.Integer>
-        getSubtypesList() {
-      return subtypes_;
-    }
-    /**
-     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-     */
-    public int getSubtypesCount() {
-      return subtypes_.size();
-    }
-    /**
-     * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-     */
-    public int getSubtypes(int index) {
-      return subtypes_.get(index);
-    }
-    private int subtypesMemoizedSerializedSize = -1;
-
-    // repeated string fieldNames = 3;
-    public static final int FIELDNAMES_FIELD_NUMBER = 3;
-    private com.google.protobuf.LazyStringList fieldNames_;
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    public java.util.List<java.lang.String>
-        getFieldNamesList() {
-      return fieldNames_;
-    }
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    public int getFieldNamesCount() {
-      return fieldNames_.size();
-    }
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    public java.lang.String getFieldNames(int index) {
-      return fieldNames_.get(index);
-    }
-    /**
-     * <code>repeated string fieldNames = 3;</code>
-     */
-    public com.google.protobuf.ByteString
-        getFieldNamesBytes(int index) {
-      return fieldNames_.getByteString(index);
-    }
-
-    // optional uint32 maximumLength = 4;
-    public static final int MAXIMUMLENGTH_FIELD_NUMBER = 4;
-    private int maximumLength_;
-    /**
-     * <code>optional uint32 maximumLength = 4;</code>
-     */
-    public boolean hasMaximumLength() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional uint32 maximumLength = 4;</code>
-     */
-    public int getMaximumLength() {
-      return maximumLength_;
-    }
-
-    // optional uint32 precision = 5;
-    public static final int PRECISION_FIELD_NUMBER = 5;
-    private int precision_;
-    /**
-     * <code>optional uint32 precision = 5;</code>
-     */
-    public boolean hasPrecision() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional uint32 precision = 5;</code>
-     */
-    public int getPrecision() {
-      return precision_;
-    }
-
-    // optional uint32 scale = 6;
-    public static final int SCALE_FIELD_NUMBER = 6;
-    private int scale_;
-    /**
-     * <code>optional uint32 scale = 6;</code>
-     */
-    public boolean hasScale() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    /**
-     * <code>optional uint32 scale = 6;</code>
-     */
-    public int getScale() {
-      return scale_;
-    }
-
-    private void initFields() {
-      kind_ = org.apache.orc.OrcProto.Type.Kind.BOOLEAN;
-      subtypes_ = java.util.Collections.emptyList();
-      fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      maximumLength_ = 0;
-      precision_ = 0;
-      scale_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeEnum(1, kind_.getNumber());
-      }
-      if (getSubtypesList().size() > 0) {
-        output.writeRawVarint32(18);
-        output.writeRawVarint32(subtypesMemoizedSerializedSize);
-      }
-      for (int i = 0; i < subtypes_.size(); i++) {
-        output.writeUInt32NoTag(subtypes_.get(i));
-      }
-      for (int i = 0; i < fieldNames_.size(); i++) {
-        output.writeBytes(3, fieldNames_.getByteString(i));
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt32(4, maximumLength_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt32(5, precision_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt32(6, scale_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(1, kind_.getNumber());
-      }
-      {
-        int dataSize = 0;
-        for (int i = 0; i < subtypes_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeUInt32SizeNoTag(subtypes_.get(i));
-        }
-        size += dataSize;
-        if (!getSubtypesList().isEmpty()) {
-          size += 1;
-          size += com.google.protobuf.CodedOutputStream
-              .computeInt32SizeNoTag(dataSize);
-        }
-        subtypesMemoizedSerializedSize = dataSize;
-      }
-      {
-        int dataSize = 0;
-        for (int i = 0; i < fieldNames_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeBytesSizeNoTag(fieldNames_.getByteString(i));
-        }
-        size += dataSize;
-        size += 1 * getFieldNamesList().size();
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(4, maximumLength_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(5, precision_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(6, scale_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.Type parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Type parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Type parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Type parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.Type prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.Type}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.TypeOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Type_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Type_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.Type.class, org.apache.orc.OrcProto.Type.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.Type.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        kind_ = org.apache.orc.OrcProto.Type.Kind.BOOLEAN;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        subtypes_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000002);
-        fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        maximumLength_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000008);
-        precision_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000010);
-        scale_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000020);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Type_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.Type getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.Type.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.Type build() {
-        org.apache.orc.OrcProto.Type result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.Type buildPartial() {
-        org.apache.orc.OrcProto.Type result = new org.apache.orc.OrcProto.Type(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.kind_ = kind_;
-        if (((bitField0_ & 0x00000002) == 0x00000002)) {
-          subtypes_ = java.util.Collections.unmodifiableList(subtypes_);
-          bitField0_ = (bitField0_ & ~0x00000002);
-        }
-        result.subtypes_ = subtypes_;
-        if (((bitField0_ & 0x00000004) == 0x00000004)) {
-          fieldNames_ = new com.google.protobuf.UnmodifiableLazyStringList(
-              fieldNames_);
-          bitField0_ = (bitField0_ & ~0x00000004);
-        }
-        result.fieldNames_ = fieldNames_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.maximumLength_ = maximumLength_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.precision_ = precision_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        result.scale_ = scale_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.Type) {
-          return mergeFrom((org.apache.orc.OrcProto.Type)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.Type other) {
-        if (other == org.apache.orc.OrcProto.Type.getDefaultInstance()) return this;
-        if (other.hasKind()) {
-          setKind(other.getKind());
-        }
-        if (!other.subtypes_.isEmpty()) {
-          if (subtypes_.isEmpty()) {
-            subtypes_ = other.subtypes_;
-            bitField0_ = (bitField0_ & ~0x00000002);
-          } else {
-            ensureSubtypesIsMutable();
-            subtypes_.addAll(other.subtypes_);
-          }
-          onChanged();
-        }
-        if (!other.fieldNames_.isEmpty()) {
-          if (fieldNames_.isEmpty()) {
-            fieldNames_ = other.fieldNames_;
-            bitField0_ = (bitField0_ & ~0x00000004);
-          } else {
-            ensureFieldNamesIsMutable();
-            fieldNames_.addAll(other.fieldNames_);
-          }
-          onChanged();
-        }
-        if (other.hasMaximumLength()) {
-          setMaximumLength(other.getMaximumLength());
-        }
-        if (other.hasPrecision()) {
-          setPrecision(other.getPrecision());
-        }
-        if (other.hasScale()) {
-          setScale(other.getScale());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.Type parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.Type) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional .orc.proto.Type.Kind kind = 1;
-      private org.apache.orc.OrcProto.Type.Kind kind_ = org.apache.orc.OrcProto.Type.Kind.BOOLEAN;
-      /**
-       * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-       */
-      public boolean hasKind() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-       */
-      public org.apache.orc.OrcProto.Type.Kind getKind() {
-        return kind_;
-      }
-      /**
-       * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-       */
-      public Builder setKind(org.apache.orc.OrcProto.Type.Kind value) {
-        if (value == null) {
-          throw new NullPointerException();
-        }
-        bitField0_ |= 0x00000001;
-        kind_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.Type.Kind kind = 1;</code>
-       */
-      public Builder clearKind() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        kind_ = org.apache.orc.OrcProto.Type.Kind.BOOLEAN;
-        onChanged();
-        return this;
-      }
-
-      // repeated uint32 subtypes = 2 [packed = true];
-      private java.util.List<java.lang.Integer> subtypes_ = java.util.Collections.emptyList();
-      private void ensureSubtypesIsMutable() {
-        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
-          subtypes_ = new java.util.ArrayList<java.lang.Integer>(subtypes_);
-          bitField0_ |= 0x00000002;
-         }
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public java.util.List<java.lang.Integer>
-          getSubtypesList() {
-        return java.util.Collections.unmodifiableList(subtypes_);
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public int getSubtypesCount() {
-        return subtypes_.size();
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public int getSubtypes(int index) {
-        return subtypes_.get(index);
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public Builder setSubtypes(
-          int index, int value) {
-        ensureSubtypesIsMutable();
-        subtypes_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public Builder addSubtypes(int value) {
-        ensureSubtypesIsMutable();
-        subtypes_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public Builder addAllSubtypes(
-          java.lang.Iterable<? extends java.lang.Integer> values) {
-        ensureSubtypesIsMutable();
-        super.addAll(values, subtypes_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint32 subtypes = 2 [packed = true];</code>
-       */
-      public Builder clearSubtypes() {
-        subtypes_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000002);
-        onChanged();
-        return this;
-      }
-
-      // repeated string fieldNames = 3;
-      private com.google.protobuf.LazyStringList fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-      private void ensureFieldNamesIsMutable() {
-        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
-          fieldNames_ = new com.google.protobuf.LazyStringArrayList(fieldNames_);
-          bitField0_ |= 0x00000004;
-         }
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public java.util.List<java.lang.String>
-          getFieldNamesList() {
-        return java.util.Collections.unmodifiableList(fieldNames_);
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public int getFieldNamesCount() {
-        return fieldNames_.size();
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public java.lang.String getFieldNames(int index) {
-        return fieldNames_.get(index);
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public com.google.protobuf.ByteString
-          getFieldNamesBytes(int index) {
-        return fieldNames_.getByteString(index);
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public Builder setFieldNames(
-          int index, java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureFieldNamesIsMutable();
-        fieldNames_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public Builder addFieldNames(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureFieldNamesIsMutable();
-        fieldNames_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public Builder addAllFieldNames(
-          java.lang.Iterable<java.lang.String> values) {
-        ensureFieldNamesIsMutable();
-        super.addAll(values, fieldNames_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public Builder clearFieldNames() {
-        fieldNames_ = com.google.protobuf.LazyStringArrayList.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated string fieldNames = 3;</code>
-       */
-      public Builder addFieldNamesBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  ensureFieldNamesIsMutable();
-        fieldNames_.add(value);
-        onChanged();
-        return this;
-      }
-
-      // optional uint32 maximumLength = 4;
-      private int maximumLength_ ;
-      /**
-       * <code>optional uint32 maximumLength = 4;</code>
-       */
-      public boolean hasMaximumLength() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      /**
-       * <code>optional uint32 maximumLength = 4;</code>
-       */
-      public int getMaximumLength() {
-        return maximumLength_;
-      }
-      /**
-       * <code>optional uint32 maximumLength = 4;</code>
-       */
-      public Builder setMaximumLength(int value) {
-        bitField0_ |= 0x00000008;
-        maximumLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 maximumLength = 4;</code>
-       */
-      public Builder clearMaximumLength() {
-        bitField0_ = (bitField0_ & ~0x00000008);
-        maximumLength_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // optional uint32 precision = 5;
-      private int precision_ ;
-      /**
-       * <code>optional uint32 precision = 5;</code>
-       */
-      public boolean hasPrecision() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
-      }
-      /**
-       * <code>optional uint32 precision = 5;</code>
-       */
-      public int getPrecision() {
-        return precision_;
-      }
-      /**
-       * <code>optional uint32 precision = 5;</code>
-       */
-      public Builder setPrecision(int value) {
-        bitField0_ |= 0x00000010;
-        precision_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 precision = 5;</code>
-       */
-      public Builder clearPrecision() {
-        bitField0_ = (bitField0_ & ~0x00000010);
-        precision_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // optional uint32 scale = 6;
-      private int scale_ ;
-      /**
-       * <code>optional uint32 scale = 6;</code>
-       */
-      public boolean hasScale() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
-      }
-      /**
-       * <code>optional uint32 scale = 6;</code>
-       */
-      public int getScale() {
-        return scale_;
-      }
-      /**
-       * <code>optional uint32 scale = 6;</code>
-       */
-      public Builder setScale(int value) {
-        bitField0_ |= 0x00000020;
-        scale_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 scale = 6;</code>
-       */
-      public Builder clearScale() {
-        bitField0_ = (bitField0_ & ~0x00000020);
-        scale_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.Type)
-    }
-
-    static {
-      defaultInstance = new Type(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.Type)
-  }
-
-  public interface StripeInformationOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional uint64 offset = 1;
-    /**
-     * <code>optional uint64 offset = 1;</code>
-     */
-    boolean hasOffset();
-    /**
-     * <code>optional uint64 offset = 1;</code>
-     */
-    long getOffset();
-
-    // optional uint64 indexLength = 2;
-    /**
-     * <code>optional uint64 indexLength = 2;</code>
-     */
-    boolean hasIndexLength();
-    /**
-     * <code>optional uint64 indexLength = 2;</code>
-     */
-    long getIndexLength();
-
-    // optional uint64 dataLength = 3;
-    /**
-     * <code>optional uint64 dataLength = 3;</code>
-     */
-    boolean hasDataLength();
-    /**
-     * <code>optional uint64 dataLength = 3;</code>
-     */
-    long getDataLength();
-
-    // optional uint64 footerLength = 4;
-    /**
-     * <code>optional uint64 footerLength = 4;</code>
-     */
-    boolean hasFooterLength();
-    /**
-     * <code>optional uint64 footerLength = 4;</code>
-     */
-    long getFooterLength();
-
-    // optional uint64 numberOfRows = 5;
-    /**
-     * <code>optional uint64 numberOfRows = 5;</code>
-     */
-    boolean hasNumberOfRows();
-    /**
-     * <code>optional uint64 numberOfRows = 5;</code>
-     */
-    long getNumberOfRows();
-  }
-  /**
-   * Protobuf type {@code orc.proto.StripeInformation}
-   */
-  public static final class StripeInformation extends
-      com.google.protobuf.GeneratedMessage
-      implements StripeInformationOrBuilder {
-    // Use StripeInformation.newBuilder() to construct.
-    private StripeInformation(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private StripeInformation(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final StripeInformation defaultInstance;
-    public static StripeInformation getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public StripeInformation getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private StripeInformation(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              offset_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              indexLength_ = input.readUInt64();
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              dataLength_ = input.readUInt64();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              footerLength_ = input.readUInt64();
-              break;
-            }
-            case 40: {
-              bitField0_ |= 0x00000010;
-              numberOfRows_ = input.readUInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StripeInformation_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StripeInformation_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.StripeInformation.class, org.apache.orc.OrcProto.StripeInformation.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<StripeInformation> PARSER =
-        new com.google.protobuf.AbstractParser<StripeInformation>() {
-      public StripeInformation parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new StripeInformation(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<StripeInformation> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional uint64 offset = 1;
-    public static final int OFFSET_FIELD_NUMBER = 1;
-    private long offset_;
-    /**
-     * <code>optional uint64 offset = 1;</code>
-     */
-    public boolean hasOffset() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional uint64 offset = 1;</code>
-     */
-    public long getOffset() {
-      return offset_;
-    }
-
-    // optional uint64 indexLength = 2;
-    public static final int INDEXLENGTH_FIELD_NUMBER = 2;
-    private long indexLength_;
-    /**
-     * <code>optional uint64 indexLength = 2;</code>
-     */
-    public boolean hasIndexLength() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional uint64 indexLength = 2;</code>
-     */
-    public long getIndexLength() {
-      return indexLength_;
-    }
-
-    // optional uint64 dataLength = 3;
-    public static final int DATALENGTH_FIELD_NUMBER = 3;
-    private long dataLength_;
-    /**
-     * <code>optional uint64 dataLength = 3;</code>
-     */
-    public boolean hasDataLength() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional uint64 dataLength = 3;</code>
-     */
-    public long getDataLength() {
-      return dataLength_;
-    }
-
-    // optional uint64 footerLength = 4;
-    public static final int FOOTERLENGTH_FIELD_NUMBER = 4;
-    private long footerLength_;
-    /**
-     * <code>optional uint64 footerLength = 4;</code>
-     */
-    public boolean hasFooterLength() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    /**
-     * <code>optional uint64 footerLength = 4;</code>
-     */
-    public long getFooterLength() {
-      return footerLength_;
-    }
-
-    // optional uint64 numberOfRows = 5;
-    public static final int NUMBEROFROWS_FIELD_NUMBER = 5;
-    private long numberOfRows_;
-    /**
-     * <code>optional uint64 numberOfRows = 5;</code>
-     */
-    public boolean hasNumberOfRows() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
-    }
-    /**
-     * <code>optional uint64 numberOfRows = 5;</code>
-     */
-    public long getNumberOfRows() {
-      return numberOfRows_;
-    }
-
-    private void initFields() {
-      offset_ = 0L;
-      indexLength_ = 0L;
-      dataLength_ = 0L;
-      footerLength_ = 0L;
-      numberOfRows_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, offset_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(2, indexLength_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt64(3, dataLength_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt64(4, footerLength_);
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeUInt64(5, numberOfRows_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, offset_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(2, indexLength_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(3, dataLength_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(4, footerLength_);
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(5, numberOfRows_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeInformation parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.StripeInformation prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.StripeInformation}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.StripeInformationOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeInformation_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeInformation_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.StripeInformation.class, org.apache.orc.OrcProto.StripeInformation.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.StripeInformation.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        offset_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        indexLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        dataLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        footerLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000008);
-        numberOfRows_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000010);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeInformation_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.StripeInformation getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.StripeInformation.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.StripeInformation build() {
-        org.apache.orc.OrcProto.StripeInformation result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.StripeInformation buildPartial() {
-        org.apache.orc.OrcProto.StripeInformation result = new org.apache.orc.OrcProto.StripeInformation(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.offset_ = offset_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.indexLength_ = indexLength_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.dataLength_ = dataLength_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        result.footerLength_ = footerLength_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000010;
-        }
-        result.numberOfRows_ = numberOfRows_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.StripeInformation) {
-          return mergeFrom((org.apache.orc.OrcProto.StripeInformation)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.StripeInformation other) {
-        if (other == org.apache.orc.OrcProto.StripeInformation.getDefaultInstance()) return this;
-        if (other.hasOffset()) {
-          setOffset(other.getOffset());
-        }
-        if (other.hasIndexLength()) {
-          setIndexLength(other.getIndexLength());
-        }
-        if (other.hasDataLength()) {
-          setDataLength(other.getDataLength());
-        }
-        if (other.hasFooterLength()) {
-          setFooterLength(other.getFooterLength());
-        }
-        if (other.hasNumberOfRows()) {
-          setNumberOfRows(other.getNumberOfRows());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.StripeInformation parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.StripeInformation) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional uint64 offset = 1;
-      private long offset_ ;
-      /**
-       * <code>optional uint64 offset = 1;</code>
-       */
-      public boolean hasOffset() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional uint64 offset = 1;</code>
-       */
-      public long getOffset() {
-        return offset_;
-      }
-      /**
-       * <code>optional uint64 offset = 1;</code>
-       */
-      public Builder setOffset(long value) {
-        bitField0_ |= 0x00000001;
-        offset_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 offset = 1;</code>
-       */
-      public Builder clearOffset() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        offset_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 indexLength = 2;
-      private long indexLength_ ;
-      /**
-       * <code>optional uint64 indexLength = 2;</code>
-       */
-      public boolean hasIndexLength() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional uint64 indexLength = 2;</code>
-       */
-      public long getIndexLength() {
-        return indexLength_;
-      }
-      /**
-       * <code>optional uint64 indexLength = 2;</code>
-       */
-      public Builder setIndexLength(long value) {
-        bitField0_ |= 0x00000002;
-        indexLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 indexLength = 2;</code>
-       */
-      public Builder clearIndexLength() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        indexLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 dataLength = 3;
-      private long dataLength_ ;
-      /**
-       * <code>optional uint64 dataLength = 3;</code>
-       */
-      public boolean hasDataLength() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional uint64 dataLength = 3;</code>
-       */
-      public long getDataLength() {
-        return dataLength_;
-      }
-      /**
-       * <code>optional uint64 dataLength = 3;</code>
-       */
-      public Builder setDataLength(long value) {
-        bitField0_ |= 0x00000004;
-        dataLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 dataLength = 3;</code>
-       */
-      public Builder clearDataLength() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        dataLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 footerLength = 4;
-      private long footerLength_ ;
-      /**
-       * <code>optional uint64 footerLength = 4;</code>
-       */
-      public boolean hasFooterLength() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      /**
-       * <code>optional uint64 footerLength = 4;</code>
-       */
-      public long getFooterLength() {
-        return footerLength_;
-      }
-      /**
-       * <code>optional uint64 footerLength = 4;</code>
-       */
-      public Builder setFooterLength(long value) {
-        bitField0_ |= 0x00000008;
-        footerLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 footerLength = 4;</code>
-       */
-      public Builder clearFooterLength() {
-        bitField0_ = (bitField0_ & ~0x00000008);
-        footerLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 numberOfRows = 5;
-      private long numberOfRows_ ;
-      /**
-       * <code>optional uint64 numberOfRows = 5;</code>
-       */
-      public boolean hasNumberOfRows() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
-      }
-      /**
-       * <code>optional uint64 numberOfRows = 5;</code>
-       */
-      public long getNumberOfRows() {
-        return numberOfRows_;
-      }
-      /**
-       * <code>optional uint64 numberOfRows = 5;</code>
-       */
-      public Builder setNumberOfRows(long value) {
-        bitField0_ |= 0x00000010;
-        numberOfRows_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 numberOfRows = 5;</code>
-       */
-      public Builder clearNumberOfRows() {
-        bitField0_ = (bitField0_ & ~0x00000010);
-        numberOfRows_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.StripeInformation)
-    }
-
-    static {
-      defaultInstance = new StripeInformation(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.StripeInformation)
-  }
-
-  public interface UserMetadataItemOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional string name = 1;
-    /**
-     * <code>optional string name = 1;</code>
-     */
-    boolean hasName();
-    /**
-     * <code>optional string name = 1;</code>
-     */
-    java.lang.String getName();
-    /**
-     * <code>optional string name = 1;</code>
-     */
-    com.google.protobuf.ByteString
-        getNameBytes();
-
-    // optional bytes value = 2;
-    /**
-     * <code>optional bytes value = 2;</code>
-     */
-    boolean hasValue();
-    /**
-     * <code>optional bytes value = 2;</code>
-     */
-    com.google.protobuf.ByteString getValue();
-  }
-  /**
-   * Protobuf type {@code orc.proto.UserMetadataItem}
-   */
-  public static final class UserMetadataItem extends
-      com.google.protobuf.GeneratedMessage
-      implements UserMetadataItemOrBuilder {
-    // Use UserMetadataItem.newBuilder() to construct.
-    private UserMetadataItem(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private UserMetadataItem(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final UserMetadataItem defaultInstance;
-    public static UserMetadataItem getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public UserMetadataItem getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private UserMetadataItem(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              bitField0_ |= 0x00000001;
-              name_ = input.readBytes();
-              break;
-            }
-            case 18: {
-              bitField0_ |= 0x00000002;
-              value_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_UserMetadataItem_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_UserMetadataItem_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.UserMetadataItem.class, org.apache.orc.OrcProto.UserMetadataItem.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<UserMetadataItem> PARSER =
-        new com.google.protobuf.AbstractParser<UserMetadataItem>() {
-      public UserMetadataItem parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new UserMetadataItem(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<UserMetadataItem> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional string name = 1;
-    public static final int NAME_FIELD_NUMBER = 1;
-    private java.lang.Object name_;
-    /**
-     * <code>optional string name = 1;</code>
-     */
-    public boolean hasName() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional string name = 1;</code>
-     */
-    public java.lang.String getName() {
-      java.lang.Object ref = name_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          name_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string name = 1;</code>
-     */
-    public com.google.protobuf.ByteString
-        getNameBytes() {
-      java.lang.Object ref = name_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        name_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    // optional bytes value = 2;
-    public static final int VALUE_FIELD_NUMBER = 2;
-    private com.google.protobuf.ByteString value_;
-    /**
-     * <code>optional bytes value = 2;</code>
-     */
-    public boolean hasValue() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional bytes value = 2;</code>
-     */
-    public com.google.protobuf.ByteString getValue() {
-      return value_;
-    }
-
-    private void initFields() {
-      name_ = "";
-      value_ = com.google.protobuf.ByteString.EMPTY;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeBytes(1, getNameBytes());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeBytes(2, value_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(1, getNameBytes());
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(2, value_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.UserMetadataItem parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.UserMetadataItem prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.UserMetadataItem}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.UserMetadataItemOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_UserMetadataItem_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_UserMetadataItem_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.UserMetadataItem.class, org.apache.orc.OrcProto.UserMetadataItem.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.UserMetadataItem.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        name_ = "";
-        bitField0_ = (bitField0_ & ~0x00000001);
-        value_ = com.google.protobuf.ByteString.EMPTY;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_UserMetadataItem_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.UserMetadataItem getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.UserMetadataItem.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.UserMetadataItem build() {
-        org.apache.orc.OrcProto.UserMetadataItem result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.UserMetadataItem buildPartial() {
-        org.apache.orc.OrcProto.UserMetadataItem result = new org.apache.orc.OrcProto.UserMetadataItem(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.name_ = name_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.value_ = value_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.UserMetadataItem) {
-          return mergeFrom((org.apache.orc.OrcProto.UserMetadataItem)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.UserMetadataItem other) {
-        if (other == org.apache.orc.OrcProto.UserMetadataItem.getDefaultInstance()) return this;
-        if (other.hasName()) {
-          bitField0_ |= 0x00000001;
-          name_ = other.name_;
-          onChanged();
-        }
-        if (other.hasValue()) {
-          setValue(other.getValue());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.UserMetadataItem parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.UserMetadataItem) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional string name = 1;
-      private java.lang.Object name_ = "";
-      /**
-       * <code>optional string name = 1;</code>
-       */
-      public boolean hasName() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional string name = 1;</code>
-       */
-      public java.lang.String getName() {
-        java.lang.Object ref = name_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          name_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string name = 1;</code>
-       */
-      public com.google.protobuf.ByteString
-          getNameBytes() {
-        java.lang.Object ref = name_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          name_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string name = 1;</code>
-       */
-      public Builder setName(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-        name_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string name = 1;</code>
-       */
-      public Builder clearName() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        name_ = getDefaultInstance().getName();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string name = 1;</code>
-       */
-      public Builder setNameBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000001;
-        name_ = value;
-        onChanged();
-        return this;
-      }
-
-      // optional bytes value = 2;
-      private com.google.protobuf.ByteString value_ = com.google.protobuf.ByteString.EMPTY;
-      /**
-       * <code>optional bytes value = 2;</code>
-       */
-      public boolean hasValue() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional bytes value = 2;</code>
-       */
-      public com.google.protobuf.ByteString getValue() {
-        return value_;
-      }
-      /**
-       * <code>optional bytes value = 2;</code>
-       */
-      public Builder setValue(com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000002;
-        value_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional bytes value = 2;</code>
-       */
-      public Builder clearValue() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        value_ = getDefaultInstance().getValue();
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.UserMetadataItem)
-    }
-
-    static {
-      defaultInstance = new UserMetadataItem(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.UserMetadataItem)
-  }
-
-  public interface StripeStatisticsOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated .orc.proto.ColumnStatistics colStats = 1;
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.ColumnStatistics> 
-        getColStatsList();
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    org.apache.orc.OrcProto.ColumnStatistics getColStats(int index);
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    int getColStatsCount();
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-        getColStatsOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getColStatsOrBuilder(
-        int index);
-  }
-  /**
-   * Protobuf type {@code orc.proto.StripeStatistics}
-   */
-  public static final class StripeStatistics extends
-      com.google.protobuf.GeneratedMessage
-      implements StripeStatisticsOrBuilder {
-    // Use StripeStatistics.newBuilder() to construct.
-    private StripeStatistics(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private StripeStatistics(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final StripeStatistics defaultInstance;
-    public static StripeStatistics getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public StripeStatistics getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private StripeStatistics(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                colStats_ = new java.util.ArrayList<org.apache.orc.OrcProto.ColumnStatistics>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              colStats_.add(input.readMessage(org.apache.orc.OrcProto.ColumnStatistics.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          colStats_ = java.util.Collections.unmodifiableList(colStats_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StripeStatistics_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_StripeStatistics_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.StripeStatistics.class, org.apache.orc.OrcProto.StripeStatistics.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<StripeStatistics> PARSER =
-        new com.google.protobuf.AbstractParser<StripeStatistics>() {
-      public StripeStatistics parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new StripeStatistics(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<StripeStatistics> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated .orc.proto.ColumnStatistics colStats = 1;
-    public static final int COLSTATS_FIELD_NUMBER = 1;
-    private java.util.List<org.apache.orc.OrcProto.ColumnStatistics> colStats_;
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.ColumnStatistics> getColStatsList() {
-      return colStats_;
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-        getColStatsOrBuilderList() {
-      return colStats_;
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    public int getColStatsCount() {
-      return colStats_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnStatistics getColStats(int index) {
-      return colStats_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getColStatsOrBuilder(
-        int index) {
-      return colStats_.get(index);
-    }
-
-    private void initFields() {
-      colStats_ = java.util.Collections.emptyList();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      for (int i = 0; i < colStats_.size(); i++) {
-        output.writeMessage(1, colStats_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      for (int i = 0; i < colStats_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, colStats_.get(i));
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.StripeStatistics parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.StripeStatistics prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.StripeStatistics}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.StripeStatisticsOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeStatistics_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeStatistics_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.StripeStatistics.class, org.apache.orc.OrcProto.StripeStatistics.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.StripeStatistics.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getColStatsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        if (colStatsBuilder_ == null) {
-          colStats_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-        } else {
-          colStatsBuilder_.clear();
-        }
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_StripeStatistics_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.StripeStatistics getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.StripeStatistics.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.StripeStatistics build() {
-        org.apache.orc.OrcProto.StripeStatistics result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.StripeStatistics buildPartial() {
-        org.apache.orc.OrcProto.StripeStatistics result = new org.apache.orc.OrcProto.StripeStatistics(this);
-        int from_bitField0_ = bitField0_;
-        if (colStatsBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            colStats_ = java.util.Collections.unmodifiableList(colStats_);
-            bitField0_ = (bitField0_ & ~0x00000001);
-          }
-          result.colStats_ = colStats_;
-        } else {
-          result.colStats_ = colStatsBuilder_.build();
-        }
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.StripeStatistics) {
-          return mergeFrom((org.apache.orc.OrcProto.StripeStatistics)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.StripeStatistics other) {
-        if (other == org.apache.orc.OrcProto.StripeStatistics.getDefaultInstance()) return this;
-        if (colStatsBuilder_ == null) {
-          if (!other.colStats_.isEmpty()) {
-            if (colStats_.isEmpty()) {
-              colStats_ = other.colStats_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-            } else {
-              ensureColStatsIsMutable();
-              colStats_.addAll(other.colStats_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.colStats_.isEmpty()) {
-            if (colStatsBuilder_.isEmpty()) {
-              colStatsBuilder_.dispose();
-              colStatsBuilder_ = null;
-              colStats_ = other.colStats_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-              colStatsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getColStatsFieldBuilder() : null;
-            } else {
-              colStatsBuilder_.addAllMessages(other.colStats_);
-            }
-          }
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.StripeStatistics parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.StripeStatistics) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated .orc.proto.ColumnStatistics colStats = 1;
-      private java.util.List<org.apache.orc.OrcProto.ColumnStatistics> colStats_ =
-        java.util.Collections.emptyList();
-      private void ensureColStatsIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          colStats_ = new java.util.ArrayList<org.apache.orc.OrcProto.ColumnStatistics>(colStats_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> colStatsBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.ColumnStatistics> getColStatsList() {
-        if (colStatsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(colStats_);
-        } else {
-          return colStatsBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public int getColStatsCount() {
-        if (colStatsBuilder_ == null) {
-          return colStats_.size();
-        } else {
-          return colStatsBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics getColStats(int index) {
-        if (colStatsBuilder_ == null) {
-          return colStats_.get(index);
-        } else {
-          return colStatsBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder setColStats(
-          int index, org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (colStatsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureColStatsIsMutable();
-          colStats_.set(index, value);
-          onChanged();
-        } else {
-          colStatsBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder setColStats(
-          int index, org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (colStatsBuilder_ == null) {
-          ensureColStatsIsMutable();
-          colStats_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          colStatsBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder addColStats(org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (colStatsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureColStatsIsMutable();
-          colStats_.add(value);
-          onChanged();
-        } else {
-          colStatsBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder addColStats(
-          int index, org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (colStatsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureColStatsIsMutable();
-          colStats_.add(index, value);
-          onChanged();
-        } else {
-          colStatsBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder addColStats(
-          org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (colStatsBuilder_ == null) {
-          ensureColStatsIsMutable();
-          colStats_.add(builderForValue.build());
-          onChanged();
-        } else {
-          colStatsBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder addColStats(
-          int index, org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (colStatsBuilder_ == null) {
-          ensureColStatsIsMutable();
-          colStats_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          colStatsBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder addAllColStats(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.ColumnStatistics> values) {
-        if (colStatsBuilder_ == null) {
-          ensureColStatsIsMutable();
-          super.addAll(values, colStats_);
-          onChanged();
-        } else {
-          colStatsBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder clearColStats() {
-        if (colStatsBuilder_ == null) {
-          colStats_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-          onChanged();
-        } else {
-          colStatsBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public Builder removeColStats(int index) {
-        if (colStatsBuilder_ == null) {
-          ensureColStatsIsMutable();
-          colStats_.remove(index);
-          onChanged();
-        } else {
-          colStatsBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder getColStatsBuilder(
-          int index) {
-        return getColStatsFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getColStatsOrBuilder(
-          int index) {
-        if (colStatsBuilder_ == null) {
-          return colStats_.get(index);  } else {
-          return colStatsBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-           getColStatsOrBuilderList() {
-        if (colStatsBuilder_ != null) {
-          return colStatsBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(colStats_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder addColStatsBuilder() {
-        return getColStatsFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder addColStatsBuilder(
-          int index) {
-        return getColStatsFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics colStats = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.ColumnStatistics.Builder> 
-           getColStatsBuilderList() {
-        return getColStatsFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-          getColStatsFieldBuilder() {
-        if (colStatsBuilder_ == null) {
-          colStatsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder>(
-                  colStats_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
-                  getParentForChildren(),
-                  isClean());
-          colStats_ = null;
-        }
-        return colStatsBuilder_;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.StripeStatistics)
-    }
-
-    static {
-      defaultInstance = new StripeStatistics(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.StripeStatistics)
-  }
-
-  public interface MetadataOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // repeated .orc.proto.StripeStatistics stripeStats = 1;
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.StripeStatistics> 
-        getStripeStatsList();
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    org.apache.orc.OrcProto.StripeStatistics getStripeStats(int index);
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    int getStripeStatsCount();
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.StripeStatisticsOrBuilder> 
-        getStripeStatsOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    org.apache.orc.OrcProto.StripeStatisticsOrBuilder getStripeStatsOrBuilder(
-        int index);
-  }
-  /**
-   * Protobuf type {@code orc.proto.Metadata}
-   */
-  public static final class Metadata extends
-      com.google.protobuf.GeneratedMessage
-      implements MetadataOrBuilder {
-    // Use Metadata.newBuilder() to construct.
-    private Metadata(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private Metadata(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final Metadata defaultInstance;
-    public static Metadata getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public Metadata getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private Metadata(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-                stripeStats_ = new java.util.ArrayList<org.apache.orc.OrcProto.StripeStatistics>();
-                mutable_bitField0_ |= 0x00000001;
-              }
-              stripeStats_.add(input.readMessage(org.apache.orc.OrcProto.StripeStatistics.PARSER, extensionRegistry));
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
-          stripeStats_ = java.util.Collections.unmodifiableList(stripeStats_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Metadata_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Metadata_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.Metadata.class, org.apache.orc.OrcProto.Metadata.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<Metadata> PARSER =
-        new com.google.protobuf.AbstractParser<Metadata>() {
-      public Metadata parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new Metadata(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<Metadata> getParserForType() {
-      return PARSER;
-    }
-
-    // repeated .orc.proto.StripeStatistics stripeStats = 1;
-    public static final int STRIPESTATS_FIELD_NUMBER = 1;
-    private java.util.List<org.apache.orc.OrcProto.StripeStatistics> stripeStats_;
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.StripeStatistics> getStripeStatsList() {
-      return stripeStats_;
-    }
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.StripeStatisticsOrBuilder> 
-        getStripeStatsOrBuilderList() {
-      return stripeStats_;
-    }
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    public int getStripeStatsCount() {
-      return stripeStats_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    public org.apache.orc.OrcProto.StripeStatistics getStripeStats(int index) {
-      return stripeStats_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-     */
-    public org.apache.orc.OrcProto.StripeStatisticsOrBuilder getStripeStatsOrBuilder(
-        int index) {
-      return stripeStats_.get(index);
-    }
-
-    private void initFields() {
-      stripeStats_ = java.util.Collections.emptyList();
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      for (int i = 0; i < stripeStats_.size(); i++) {
-        output.writeMessage(1, stripeStats_.get(i));
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      for (int i = 0; i < stripeStats_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, stripeStats_.get(i));
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.Metadata parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Metadata parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.Metadata prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.Metadata}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.MetadataOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Metadata_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Metadata_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.Metadata.class, org.apache.orc.OrcProto.Metadata.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.Metadata.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getStripeStatsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        if (stripeStatsBuilder_ == null) {
-          stripeStats_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-        } else {
-          stripeStatsBuilder_.clear();
-        }
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Metadata_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.Metadata getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.Metadata.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.Metadata build() {
-        org.apache.orc.OrcProto.Metadata result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.Metadata buildPartial() {
-        org.apache.orc.OrcProto.Metadata result = new org.apache.orc.OrcProto.Metadata(this);
-        int from_bitField0_ = bitField0_;
-        if (stripeStatsBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001)) {
-            stripeStats_ = java.util.Collections.unmodifiableList(stripeStats_);
-            bitField0_ = (bitField0_ & ~0x00000001);
-          }
-          result.stripeStats_ = stripeStats_;
-        } else {
-          result.stripeStats_ = stripeStatsBuilder_.build();
-        }
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.Metadata) {
-          return mergeFrom((org.apache.orc.OrcProto.Metadata)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.Metadata other) {
-        if (other == org.apache.orc.OrcProto.Metadata.getDefaultInstance()) return this;
-        if (stripeStatsBuilder_ == null) {
-          if (!other.stripeStats_.isEmpty()) {
-            if (stripeStats_.isEmpty()) {
-              stripeStats_ = other.stripeStats_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-            } else {
-              ensureStripeStatsIsMutable();
-              stripeStats_.addAll(other.stripeStats_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.stripeStats_.isEmpty()) {
-            if (stripeStatsBuilder_.isEmpty()) {
-              stripeStatsBuilder_.dispose();
-              stripeStatsBuilder_ = null;
-              stripeStats_ = other.stripeStats_;
-              bitField0_ = (bitField0_ & ~0x00000001);
-              stripeStatsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getStripeStatsFieldBuilder() : null;
-            } else {
-              stripeStatsBuilder_.addAllMessages(other.stripeStats_);
-            }
-          }
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.Metadata parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.Metadata) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // repeated .orc.proto.StripeStatistics stripeStats = 1;
-      private java.util.List<org.apache.orc.OrcProto.StripeStatistics> stripeStats_ =
-        java.util.Collections.emptyList();
-      private void ensureStripeStatsIsMutable() {
-        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
-          stripeStats_ = new java.util.ArrayList<org.apache.orc.OrcProto.StripeStatistics>(stripeStats_);
-          bitField0_ |= 0x00000001;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.StripeStatistics, org.apache.orc.OrcProto.StripeStatistics.Builder, org.apache.orc.OrcProto.StripeStatisticsOrBuilder> stripeStatsBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.StripeStatistics> getStripeStatsList() {
-        if (stripeStatsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(stripeStats_);
-        } else {
-          return stripeStatsBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public int getStripeStatsCount() {
-        if (stripeStatsBuilder_ == null) {
-          return stripeStats_.size();
-        } else {
-          return stripeStatsBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.StripeStatistics getStripeStats(int index) {
-        if (stripeStatsBuilder_ == null) {
-          return stripeStats_.get(index);
-        } else {
-          return stripeStatsBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder setStripeStats(
-          int index, org.apache.orc.OrcProto.StripeStatistics value) {
-        if (stripeStatsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStripeStatsIsMutable();
-          stripeStats_.set(index, value);
-          onChanged();
-        } else {
-          stripeStatsBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder setStripeStats(
-          int index, org.apache.orc.OrcProto.StripeStatistics.Builder builderForValue) {
-        if (stripeStatsBuilder_ == null) {
-          ensureStripeStatsIsMutable();
-          stripeStats_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          stripeStatsBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder addStripeStats(org.apache.orc.OrcProto.StripeStatistics value) {
-        if (stripeStatsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStripeStatsIsMutable();
-          stripeStats_.add(value);
-          onChanged();
-        } else {
-          stripeStatsBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder addStripeStats(
-          int index, org.apache.orc.OrcProto.StripeStatistics value) {
-        if (stripeStatsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStripeStatsIsMutable();
-          stripeStats_.add(index, value);
-          onChanged();
-        } else {
-          stripeStatsBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder addStripeStats(
-          org.apache.orc.OrcProto.StripeStatistics.Builder builderForValue) {
-        if (stripeStatsBuilder_ == null) {
-          ensureStripeStatsIsMutable();
-          stripeStats_.add(builderForValue.build());
-          onChanged();
-        } else {
-          stripeStatsBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder addStripeStats(
-          int index, org.apache.orc.OrcProto.StripeStatistics.Builder builderForValue) {
-        if (stripeStatsBuilder_ == null) {
-          ensureStripeStatsIsMutable();
-          stripeStats_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          stripeStatsBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder addAllStripeStats(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.StripeStatistics> values) {
-        if (stripeStatsBuilder_ == null) {
-          ensureStripeStatsIsMutable();
-          super.addAll(values, stripeStats_);
-          onChanged();
-        } else {
-          stripeStatsBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder clearStripeStats() {
-        if (stripeStatsBuilder_ == null) {
-          stripeStats_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000001);
-          onChanged();
-        } else {
-          stripeStatsBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public Builder removeStripeStats(int index) {
-        if (stripeStatsBuilder_ == null) {
-          ensureStripeStatsIsMutable();
-          stripeStats_.remove(index);
-          onChanged();
-        } else {
-          stripeStatsBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.StripeStatistics.Builder getStripeStatsBuilder(
-          int index) {
-        return getStripeStatsFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.StripeStatisticsOrBuilder getStripeStatsOrBuilder(
-          int index) {
-        if (stripeStatsBuilder_ == null) {
-          return stripeStats_.get(index);  } else {
-          return stripeStatsBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.StripeStatisticsOrBuilder> 
-           getStripeStatsOrBuilderList() {
-        if (stripeStatsBuilder_ != null) {
-          return stripeStatsBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(stripeStats_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.StripeStatistics.Builder addStripeStatsBuilder() {
-        return getStripeStatsFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.StripeStatistics.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public org.apache.orc.OrcProto.StripeStatistics.Builder addStripeStatsBuilder(
-          int index) {
-        return getStripeStatsFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.StripeStatistics.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.StripeStatistics stripeStats = 1;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.StripeStatistics.Builder> 
-           getStripeStatsBuilderList() {
-        return getStripeStatsFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.StripeStatistics, org.apache.orc.OrcProto.StripeStatistics.Builder, org.apache.orc.OrcProto.StripeStatisticsOrBuilder> 
-          getStripeStatsFieldBuilder() {
-        if (stripeStatsBuilder_ == null) {
-          stripeStatsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.StripeStatistics, org.apache.orc.OrcProto.StripeStatistics.Builder, org.apache.orc.OrcProto.StripeStatisticsOrBuilder>(
-                  stripeStats_,
-                  ((bitField0_ & 0x00000001) == 0x00000001),
-                  getParentForChildren(),
-                  isClean());
-          stripeStats_ = null;
-        }
-        return stripeStatsBuilder_;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.Metadata)
-    }
-
-    static {
-      defaultInstance = new Metadata(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.Metadata)
-  }
-
-  public interface FooterOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional uint64 headerLength = 1;
-    /**
-     * <code>optional uint64 headerLength = 1;</code>
-     */
-    boolean hasHeaderLength();
-    /**
-     * <code>optional uint64 headerLength = 1;</code>
-     */
-    long getHeaderLength();
-
-    // optional uint64 contentLength = 2;
-    /**
-     * <code>optional uint64 contentLength = 2;</code>
-     */
-    boolean hasContentLength();
-    /**
-     * <code>optional uint64 contentLength = 2;</code>
-     */
-    long getContentLength();
-
-    // repeated .orc.proto.StripeInformation stripes = 3;
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.StripeInformation> 
-        getStripesList();
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    org.apache.orc.OrcProto.StripeInformation getStripes(int index);
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    int getStripesCount();
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.StripeInformationOrBuilder> 
-        getStripesOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    org.apache.orc.OrcProto.StripeInformationOrBuilder getStripesOrBuilder(
-        int index);
-
-    // repeated .orc.proto.Type types = 4;
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.Type> 
-        getTypesList();
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    org.apache.orc.OrcProto.Type getTypes(int index);
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    int getTypesCount();
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.TypeOrBuilder> 
-        getTypesOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    org.apache.orc.OrcProto.TypeOrBuilder getTypesOrBuilder(
-        int index);
-
-    // repeated .orc.proto.UserMetadataItem metadata = 5;
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.UserMetadataItem> 
-        getMetadataList();
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    org.apache.orc.OrcProto.UserMetadataItem getMetadata(int index);
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    int getMetadataCount();
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.UserMetadataItemOrBuilder> 
-        getMetadataOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    org.apache.orc.OrcProto.UserMetadataItemOrBuilder getMetadataOrBuilder(
-        int index);
-
-    // optional uint64 numberOfRows = 6;
-    /**
-     * <code>optional uint64 numberOfRows = 6;</code>
-     */
-    boolean hasNumberOfRows();
-    /**
-     * <code>optional uint64 numberOfRows = 6;</code>
-     */
-    long getNumberOfRows();
-
-    // repeated .orc.proto.ColumnStatistics statistics = 7;
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    java.util.List<org.apache.orc.OrcProto.ColumnStatistics> 
-        getStatisticsList();
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    org.apache.orc.OrcProto.ColumnStatistics getStatistics(int index);
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    int getStatisticsCount();
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    java.util.List<? extends org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-        getStatisticsOrBuilderList();
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder(
-        int index);
-
-    // optional uint32 rowIndexStride = 8;
-    /**
-     * <code>optional uint32 rowIndexStride = 8;</code>
-     */
-    boolean hasRowIndexStride();
-    /**
-     * <code>optional uint32 rowIndexStride = 8;</code>
-     */
-    int getRowIndexStride();
-  }
-  /**
-   * Protobuf type {@code orc.proto.Footer}
-   */
-  public static final class Footer extends
-      com.google.protobuf.GeneratedMessage
-      implements FooterOrBuilder {
-    // Use Footer.newBuilder() to construct.
-    private Footer(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private Footer(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final Footer defaultInstance;
-    public static Footer getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public Footer getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private Footer(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              headerLength_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              bitField0_ |= 0x00000002;
-              contentLength_ = input.readUInt64();
-              break;
-            }
-            case 26: {
-              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-                stripes_ = new java.util.ArrayList<org.apache.orc.OrcProto.StripeInformation>();
-                mutable_bitField0_ |= 0x00000004;
-              }
-              stripes_.add(input.readMessage(org.apache.orc.OrcProto.StripeInformation.PARSER, extensionRegistry));
-              break;
-            }
-            case 34: {
-              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
-                types_ = new java.util.ArrayList<org.apache.orc.OrcProto.Type>();
-                mutable_bitField0_ |= 0x00000008;
-              }
-              types_.add(input.readMessage(org.apache.orc.OrcProto.Type.PARSER, extensionRegistry));
-              break;
-            }
-            case 42: {
-              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
-                metadata_ = new java.util.ArrayList<org.apache.orc.OrcProto.UserMetadataItem>();
-                mutable_bitField0_ |= 0x00000010;
-              }
-              metadata_.add(input.readMessage(org.apache.orc.OrcProto.UserMetadataItem.PARSER, extensionRegistry));
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000004;
-              numberOfRows_ = input.readUInt64();
-              break;
-            }
-            case 58: {
-              if (!((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
-                statistics_ = new java.util.ArrayList<org.apache.orc.OrcProto.ColumnStatistics>();
-                mutable_bitField0_ |= 0x00000040;
-              }
-              statistics_.add(input.readMessage(org.apache.orc.OrcProto.ColumnStatistics.PARSER, extensionRegistry));
-              break;
-            }
-            case 64: {
-              bitField0_ |= 0x00000008;
-              rowIndexStride_ = input.readUInt32();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
-          stripes_ = java.util.Collections.unmodifiableList(stripes_);
-        }
-        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
-          types_ = java.util.Collections.unmodifiableList(types_);
-        }
-        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
-          metadata_ = java.util.Collections.unmodifiableList(metadata_);
-        }
-        if (((mutable_bitField0_ & 0x00000040) == 0x00000040)) {
-          statistics_ = java.util.Collections.unmodifiableList(statistics_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Footer_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_Footer_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.Footer.class, org.apache.orc.OrcProto.Footer.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<Footer> PARSER =
-        new com.google.protobuf.AbstractParser<Footer>() {
-      public Footer parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new Footer(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<Footer> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional uint64 headerLength = 1;
-    public static final int HEADERLENGTH_FIELD_NUMBER = 1;
-    private long headerLength_;
-    /**
-     * <code>optional uint64 headerLength = 1;</code>
-     */
-    public boolean hasHeaderLength() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional uint64 headerLength = 1;</code>
-     */
-    public long getHeaderLength() {
-      return headerLength_;
-    }
-
-    // optional uint64 contentLength = 2;
-    public static final int CONTENTLENGTH_FIELD_NUMBER = 2;
-    private long contentLength_;
-    /**
-     * <code>optional uint64 contentLength = 2;</code>
-     */
-    public boolean hasContentLength() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional uint64 contentLength = 2;</code>
-     */
-    public long getContentLength() {
-      return contentLength_;
-    }
-
-    // repeated .orc.proto.StripeInformation stripes = 3;
-    public static final int STRIPES_FIELD_NUMBER = 3;
-    private java.util.List<org.apache.orc.OrcProto.StripeInformation> stripes_;
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.StripeInformation> getStripesList() {
-      return stripes_;
-    }
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.StripeInformationOrBuilder> 
-        getStripesOrBuilderList() {
-      return stripes_;
-    }
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    public int getStripesCount() {
-      return stripes_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    public org.apache.orc.OrcProto.StripeInformation getStripes(int index) {
-      return stripes_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-     */
-    public org.apache.orc.OrcProto.StripeInformationOrBuilder getStripesOrBuilder(
-        int index) {
-      return stripes_.get(index);
-    }
-
-    // repeated .orc.proto.Type types = 4;
-    public static final int TYPES_FIELD_NUMBER = 4;
-    private java.util.List<org.apache.orc.OrcProto.Type> types_;
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.Type> getTypesList() {
-      return types_;
-    }
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.TypeOrBuilder> 
-        getTypesOrBuilderList() {
-      return types_;
-    }
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    public int getTypesCount() {
-      return types_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    public org.apache.orc.OrcProto.Type getTypes(int index) {
-      return types_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.Type types = 4;</code>
-     */
-    public org.apache.orc.OrcProto.TypeOrBuilder getTypesOrBuilder(
-        int index) {
-      return types_.get(index);
-    }
-
-    // repeated .orc.proto.UserMetadataItem metadata = 5;
-    public static final int METADATA_FIELD_NUMBER = 5;
-    private java.util.List<org.apache.orc.OrcProto.UserMetadataItem> metadata_;
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.UserMetadataItem> getMetadataList() {
-      return metadata_;
-    }
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.UserMetadataItemOrBuilder> 
-        getMetadataOrBuilderList() {
-      return metadata_;
-    }
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    public int getMetadataCount() {
-      return metadata_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    public org.apache.orc.OrcProto.UserMetadataItem getMetadata(int index) {
-      return metadata_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-     */
-    public org.apache.orc.OrcProto.UserMetadataItemOrBuilder getMetadataOrBuilder(
-        int index) {
-      return metadata_.get(index);
-    }
-
-    // optional uint64 numberOfRows = 6;
-    public static final int NUMBEROFROWS_FIELD_NUMBER = 6;
-    private long numberOfRows_;
-    /**
-     * <code>optional uint64 numberOfRows = 6;</code>
-     */
-    public boolean hasNumberOfRows() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional uint64 numberOfRows = 6;</code>
-     */
-    public long getNumberOfRows() {
-      return numberOfRows_;
-    }
-
-    // repeated .orc.proto.ColumnStatistics statistics = 7;
-    public static final int STATISTICS_FIELD_NUMBER = 7;
-    private java.util.List<org.apache.orc.OrcProto.ColumnStatistics> statistics_;
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    public java.util.List<org.apache.orc.OrcProto.ColumnStatistics> getStatisticsList() {
-      return statistics_;
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    public java.util.List<? extends org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-        getStatisticsOrBuilderList() {
-      return statistics_;
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    public int getStatisticsCount() {
-      return statistics_.size();
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnStatistics getStatistics(int index) {
-      return statistics_.get(index);
-    }
-    /**
-     * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-     */
-    public org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder(
-        int index) {
-      return statistics_.get(index);
-    }
-
-    // optional uint32 rowIndexStride = 8;
-    public static final int ROWINDEXSTRIDE_FIELD_NUMBER = 8;
-    private int rowIndexStride_;
-    /**
-     * <code>optional uint32 rowIndexStride = 8;</code>
-     */
-    public boolean hasRowIndexStride() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    /**
-     * <code>optional uint32 rowIndexStride = 8;</code>
-     */
-    public int getRowIndexStride() {
-      return rowIndexStride_;
-    }
-
-    private void initFields() {
-      headerLength_ = 0L;
-      contentLength_ = 0L;
-      stripes_ = java.util.Collections.emptyList();
-      types_ = java.util.Collections.emptyList();
-      metadata_ = java.util.Collections.emptyList();
-      numberOfRows_ = 0L;
-      statistics_ = java.util.Collections.emptyList();
-      rowIndexStride_ = 0;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, headerLength_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeUInt64(2, contentLength_);
-      }
-      for (int i = 0; i < stripes_.size(); i++) {
-        output.writeMessage(3, stripes_.get(i));
-      }
-      for (int i = 0; i < types_.size(); i++) {
-        output.writeMessage(4, types_.get(i));
-      }
-      for (int i = 0; i < metadata_.size(); i++) {
-        output.writeMessage(5, metadata_.get(i));
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt64(6, numberOfRows_);
-      }
-      for (int i = 0; i < statistics_.size(); i++) {
-        output.writeMessage(7, statistics_.get(i));
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt32(8, rowIndexStride_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, headerLength_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(2, contentLength_);
-      }
-      for (int i = 0; i < stripes_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(3, stripes_.get(i));
-      }
-      for (int i = 0; i < types_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(4, types_.get(i));
-      }
-      for (int i = 0; i < metadata_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(5, metadata_.get(i));
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(6, numberOfRows_);
-      }
-      for (int i = 0; i < statistics_.size(); i++) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(7, statistics_.get(i));
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(8, rowIndexStride_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.Footer parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Footer parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Footer parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.Footer parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.Footer prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.Footer}
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.FooterOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Footer_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Footer_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.Footer.class, org.apache.orc.OrcProto.Footer.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.Footer.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getStripesFieldBuilder();
-          getTypesFieldBuilder();
-          getMetadataFieldBuilder();
-          getStatisticsFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        headerLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        contentLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        if (stripesBuilder_ == null) {
-          stripes_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000004);
-        } else {
-          stripesBuilder_.clear();
-        }
-        if (typesBuilder_ == null) {
-          types_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000008);
-        } else {
-          typesBuilder_.clear();
-        }
-        if (metadataBuilder_ == null) {
-          metadata_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000010);
-        } else {
-          metadataBuilder_.clear();
-        }
-        numberOfRows_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000020);
-        if (statisticsBuilder_ == null) {
-          statistics_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000040);
-        } else {
-          statisticsBuilder_.clear();
-        }
-        rowIndexStride_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000080);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_Footer_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.Footer getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.Footer.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.Footer build() {
-        org.apache.orc.OrcProto.Footer result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.Footer buildPartial() {
-        org.apache.orc.OrcProto.Footer result = new org.apache.orc.OrcProto.Footer(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.headerLength_ = headerLength_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.contentLength_ = contentLength_;
-        if (stripesBuilder_ == null) {
-          if (((bitField0_ & 0x00000004) == 0x00000004)) {
-            stripes_ = java.util.Collections.unmodifiableList(stripes_);
-            bitField0_ = (bitField0_ & ~0x00000004);
-          }
-          result.stripes_ = stripes_;
-        } else {
-          result.stripes_ = stripesBuilder_.build();
-        }
-        if (typesBuilder_ == null) {
-          if (((bitField0_ & 0x00000008) == 0x00000008)) {
-            types_ = java.util.Collections.unmodifiableList(types_);
-            bitField0_ = (bitField0_ & ~0x00000008);
-          }
-          result.types_ = types_;
-        } else {
-          result.types_ = typesBuilder_.build();
-        }
-        if (metadataBuilder_ == null) {
-          if (((bitField0_ & 0x00000010) == 0x00000010)) {
-            metadata_ = java.util.Collections.unmodifiableList(metadata_);
-            bitField0_ = (bitField0_ & ~0x00000010);
-          }
-          result.metadata_ = metadata_;
-        } else {
-          result.metadata_ = metadataBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.numberOfRows_ = numberOfRows_;
-        if (statisticsBuilder_ == null) {
-          if (((bitField0_ & 0x00000040) == 0x00000040)) {
-            statistics_ = java.util.Collections.unmodifiableList(statistics_);
-            bitField0_ = (bitField0_ & ~0x00000040);
-          }
-          result.statistics_ = statistics_;
-        } else {
-          result.statistics_ = statisticsBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        result.rowIndexStride_ = rowIndexStride_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.Footer) {
-          return mergeFrom((org.apache.orc.OrcProto.Footer)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.Footer other) {
-        if (other == org.apache.orc.OrcProto.Footer.getDefaultInstance()) return this;
-        if (other.hasHeaderLength()) {
-          setHeaderLength(other.getHeaderLength());
-        }
-        if (other.hasContentLength()) {
-          setContentLength(other.getContentLength());
-        }
-        if (stripesBuilder_ == null) {
-          if (!other.stripes_.isEmpty()) {
-            if (stripes_.isEmpty()) {
-              stripes_ = other.stripes_;
-              bitField0_ = (bitField0_ & ~0x00000004);
-            } else {
-              ensureStripesIsMutable();
-              stripes_.addAll(other.stripes_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.stripes_.isEmpty()) {
-            if (stripesBuilder_.isEmpty()) {
-              stripesBuilder_.dispose();
-              stripesBuilder_ = null;
-              stripes_ = other.stripes_;
-              bitField0_ = (bitField0_ & ~0x00000004);
-              stripesBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getStripesFieldBuilder() : null;
-            } else {
-              stripesBuilder_.addAllMessages(other.stripes_);
-            }
-          }
-        }
-        if (typesBuilder_ == null) {
-          if (!other.types_.isEmpty()) {
-            if (types_.isEmpty()) {
-              types_ = other.types_;
-              bitField0_ = (bitField0_ & ~0x00000008);
-            } else {
-              ensureTypesIsMutable();
-              types_.addAll(other.types_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.types_.isEmpty()) {
-            if (typesBuilder_.isEmpty()) {
-              typesBuilder_.dispose();
-              typesBuilder_ = null;
-              types_ = other.types_;
-              bitField0_ = (bitField0_ & ~0x00000008);
-              typesBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getTypesFieldBuilder() : null;
-            } else {
-              typesBuilder_.addAllMessages(other.types_);
-            }
-          }
-        }
-        if (metadataBuilder_ == null) {
-          if (!other.metadata_.isEmpty()) {
-            if (metadata_.isEmpty()) {
-              metadata_ = other.metadata_;
-              bitField0_ = (bitField0_ & ~0x00000010);
-            } else {
-              ensureMetadataIsMutable();
-              metadata_.addAll(other.metadata_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.metadata_.isEmpty()) {
-            if (metadataBuilder_.isEmpty()) {
-              metadataBuilder_.dispose();
-              metadataBuilder_ = null;
-              metadata_ = other.metadata_;
-              bitField0_ = (bitField0_ & ~0x00000010);
-              metadataBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getMetadataFieldBuilder() : null;
-            } else {
-              metadataBuilder_.addAllMessages(other.metadata_);
-            }
-          }
-        }
-        if (other.hasNumberOfRows()) {
-          setNumberOfRows(other.getNumberOfRows());
-        }
-        if (statisticsBuilder_ == null) {
-          if (!other.statistics_.isEmpty()) {
-            if (statistics_.isEmpty()) {
-              statistics_ = other.statistics_;
-              bitField0_ = (bitField0_ & ~0x00000040);
-            } else {
-              ensureStatisticsIsMutable();
-              statistics_.addAll(other.statistics_);
-            }
-            onChanged();
-          }
-        } else {
-          if (!other.statistics_.isEmpty()) {
-            if (statisticsBuilder_.isEmpty()) {
-              statisticsBuilder_.dispose();
-              statisticsBuilder_ = null;
-              statistics_ = other.statistics_;
-              bitField0_ = (bitField0_ & ~0x00000040);
-              statisticsBuilder_ = 
-                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
-                   getStatisticsFieldBuilder() : null;
-            } else {
-              statisticsBuilder_.addAllMessages(other.statistics_);
-            }
-          }
-        }
-        if (other.hasRowIndexStride()) {
-          setRowIndexStride(other.getRowIndexStride());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.Footer parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.Footer) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional uint64 headerLength = 1;
-      private long headerLength_ ;
-      /**
-       * <code>optional uint64 headerLength = 1;</code>
-       */
-      public boolean hasHeaderLength() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional uint64 headerLength = 1;</code>
-       */
-      public long getHeaderLength() {
-        return headerLength_;
-      }
-      /**
-       * <code>optional uint64 headerLength = 1;</code>
-       */
-      public Builder setHeaderLength(long value) {
-        bitField0_ |= 0x00000001;
-        headerLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 headerLength = 1;</code>
-       */
-      public Builder clearHeaderLength() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        headerLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 contentLength = 2;
-      private long contentLength_ ;
-      /**
-       * <code>optional uint64 contentLength = 2;</code>
-       */
-      public boolean hasContentLength() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional uint64 contentLength = 2;</code>
-       */
-      public long getContentLength() {
-        return contentLength_;
-      }
-      /**
-       * <code>optional uint64 contentLength = 2;</code>
-       */
-      public Builder setContentLength(long value) {
-        bitField0_ |= 0x00000002;
-        contentLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 contentLength = 2;</code>
-       */
-      public Builder clearContentLength() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        contentLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // repeated .orc.proto.StripeInformation stripes = 3;
-      private java.util.List<org.apache.orc.OrcProto.StripeInformation> stripes_ =
-        java.util.Collections.emptyList();
-      private void ensureStripesIsMutable() {
-        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
-          stripes_ = new java.util.ArrayList<org.apache.orc.OrcProto.StripeInformation>(stripes_);
-          bitField0_ |= 0x00000004;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.StripeInformation, org.apache.orc.OrcProto.StripeInformation.Builder, org.apache.orc.OrcProto.StripeInformationOrBuilder> stripesBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.StripeInformation> getStripesList() {
-        if (stripesBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(stripes_);
-        } else {
-          return stripesBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public int getStripesCount() {
-        if (stripesBuilder_ == null) {
-          return stripes_.size();
-        } else {
-          return stripesBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public org.apache.orc.OrcProto.StripeInformation getStripes(int index) {
-        if (stripesBuilder_ == null) {
-          return stripes_.get(index);
-        } else {
-          return stripesBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder setStripes(
-          int index, org.apache.orc.OrcProto.StripeInformation value) {
-        if (stripesBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStripesIsMutable();
-          stripes_.set(index, value);
-          onChanged();
-        } else {
-          stripesBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder setStripes(
-          int index, org.apache.orc.OrcProto.StripeInformation.Builder builderForValue) {
-        if (stripesBuilder_ == null) {
-          ensureStripesIsMutable();
-          stripes_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          stripesBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder addStripes(org.apache.orc.OrcProto.StripeInformation value) {
-        if (stripesBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStripesIsMutable();
-          stripes_.add(value);
-          onChanged();
-        } else {
-          stripesBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder addStripes(
-          int index, org.apache.orc.OrcProto.StripeInformation value) {
-        if (stripesBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStripesIsMutable();
-          stripes_.add(index, value);
-          onChanged();
-        } else {
-          stripesBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder addStripes(
-          org.apache.orc.OrcProto.StripeInformation.Builder builderForValue) {
-        if (stripesBuilder_ == null) {
-          ensureStripesIsMutable();
-          stripes_.add(builderForValue.build());
-          onChanged();
-        } else {
-          stripesBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder addStripes(
-          int index, org.apache.orc.OrcProto.StripeInformation.Builder builderForValue) {
-        if (stripesBuilder_ == null) {
-          ensureStripesIsMutable();
-          stripes_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          stripesBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder addAllStripes(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.StripeInformation> values) {
-        if (stripesBuilder_ == null) {
-          ensureStripesIsMutable();
-          super.addAll(values, stripes_);
-          onChanged();
-        } else {
-          stripesBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder clearStripes() {
-        if (stripesBuilder_ == null) {
-          stripes_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000004);
-          onChanged();
-        } else {
-          stripesBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public Builder removeStripes(int index) {
-        if (stripesBuilder_ == null) {
-          ensureStripesIsMutable();
-          stripes_.remove(index);
-          onChanged();
-        } else {
-          stripesBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public org.apache.orc.OrcProto.StripeInformation.Builder getStripesBuilder(
-          int index) {
-        return getStripesFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public org.apache.orc.OrcProto.StripeInformationOrBuilder getStripesOrBuilder(
-          int index) {
-        if (stripesBuilder_ == null) {
-          return stripes_.get(index);  } else {
-          return stripesBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.StripeInformationOrBuilder> 
-           getStripesOrBuilderList() {
-        if (stripesBuilder_ != null) {
-          return stripesBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(stripes_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public org.apache.orc.OrcProto.StripeInformation.Builder addStripesBuilder() {
-        return getStripesFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.StripeInformation.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public org.apache.orc.OrcProto.StripeInformation.Builder addStripesBuilder(
-          int index) {
-        return getStripesFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.StripeInformation.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.StripeInformation stripes = 3;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.StripeInformation.Builder> 
-           getStripesBuilderList() {
-        return getStripesFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.StripeInformation, org.apache.orc.OrcProto.StripeInformation.Builder, org.apache.orc.OrcProto.StripeInformationOrBuilder> 
-          getStripesFieldBuilder() {
-        if (stripesBuilder_ == null) {
-          stripesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.StripeInformation, org.apache.orc.OrcProto.StripeInformation.Builder, org.apache.orc.OrcProto.StripeInformationOrBuilder>(
-                  stripes_,
-                  ((bitField0_ & 0x00000004) == 0x00000004),
-                  getParentForChildren(),
-                  isClean());
-          stripes_ = null;
-        }
-        return stripesBuilder_;
-      }
-
-      // repeated .orc.proto.Type types = 4;
-      private java.util.List<org.apache.orc.OrcProto.Type> types_ =
-        java.util.Collections.emptyList();
-      private void ensureTypesIsMutable() {
-        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
-          types_ = new java.util.ArrayList<org.apache.orc.OrcProto.Type>(types_);
-          bitField0_ |= 0x00000008;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.Type, org.apache.orc.OrcProto.Type.Builder, org.apache.orc.OrcProto.TypeOrBuilder> typesBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.Type> getTypesList() {
-        if (typesBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(types_);
-        } else {
-          return typesBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public int getTypesCount() {
-        if (typesBuilder_ == null) {
-          return types_.size();
-        } else {
-          return typesBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public org.apache.orc.OrcProto.Type getTypes(int index) {
-        if (typesBuilder_ == null) {
-          return types_.get(index);
-        } else {
-          return typesBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder setTypes(
-          int index, org.apache.orc.OrcProto.Type value) {
-        if (typesBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureTypesIsMutable();
-          types_.set(index, value);
-          onChanged();
-        } else {
-          typesBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder setTypes(
-          int index, org.apache.orc.OrcProto.Type.Builder builderForValue) {
-        if (typesBuilder_ == null) {
-          ensureTypesIsMutable();
-          types_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          typesBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder addTypes(org.apache.orc.OrcProto.Type value) {
-        if (typesBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureTypesIsMutable();
-          types_.add(value);
-          onChanged();
-        } else {
-          typesBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder addTypes(
-          int index, org.apache.orc.OrcProto.Type value) {
-        if (typesBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureTypesIsMutable();
-          types_.add(index, value);
-          onChanged();
-        } else {
-          typesBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder addTypes(
-          org.apache.orc.OrcProto.Type.Builder builderForValue) {
-        if (typesBuilder_ == null) {
-          ensureTypesIsMutable();
-          types_.add(builderForValue.build());
-          onChanged();
-        } else {
-          typesBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder addTypes(
-          int index, org.apache.orc.OrcProto.Type.Builder builderForValue) {
-        if (typesBuilder_ == null) {
-          ensureTypesIsMutable();
-          types_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          typesBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder addAllTypes(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.Type> values) {
-        if (typesBuilder_ == null) {
-          ensureTypesIsMutable();
-          super.addAll(values, types_);
-          onChanged();
-        } else {
-          typesBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder clearTypes() {
-        if (typesBuilder_ == null) {
-          types_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000008);
-          onChanged();
-        } else {
-          typesBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public Builder removeTypes(int index) {
-        if (typesBuilder_ == null) {
-          ensureTypesIsMutable();
-          types_.remove(index);
-          onChanged();
-        } else {
-          typesBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public org.apache.orc.OrcProto.Type.Builder getTypesBuilder(
-          int index) {
-        return getTypesFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public org.apache.orc.OrcProto.TypeOrBuilder getTypesOrBuilder(
-          int index) {
-        if (typesBuilder_ == null) {
-          return types_.get(index);  } else {
-          return typesBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.TypeOrBuilder> 
-           getTypesOrBuilderList() {
-        if (typesBuilder_ != null) {
-          return typesBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(types_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public org.apache.orc.OrcProto.Type.Builder addTypesBuilder() {
-        return getTypesFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.Type.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public org.apache.orc.OrcProto.Type.Builder addTypesBuilder(
-          int index) {
-        return getTypesFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.Type.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.Type types = 4;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.Type.Builder> 
-           getTypesBuilderList() {
-        return getTypesFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.Type, org.apache.orc.OrcProto.Type.Builder, org.apache.orc.OrcProto.TypeOrBuilder> 
-          getTypesFieldBuilder() {
-        if (typesBuilder_ == null) {
-          typesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.Type, org.apache.orc.OrcProto.Type.Builder, org.apache.orc.OrcProto.TypeOrBuilder>(
-                  types_,
-                  ((bitField0_ & 0x00000008) == 0x00000008),
-                  getParentForChildren(),
-                  isClean());
-          types_ = null;
-        }
-        return typesBuilder_;
-      }
-
-      // repeated .orc.proto.UserMetadataItem metadata = 5;
-      private java.util.List<org.apache.orc.OrcProto.UserMetadataItem> metadata_ =
-        java.util.Collections.emptyList();
-      private void ensureMetadataIsMutable() {
-        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
-          metadata_ = new java.util.ArrayList<org.apache.orc.OrcProto.UserMetadataItem>(metadata_);
-          bitField0_ |= 0x00000010;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.UserMetadataItem, org.apache.orc.OrcProto.UserMetadataItem.Builder, org.apache.orc.OrcProto.UserMetadataItemOrBuilder> metadataBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.UserMetadataItem> getMetadataList() {
-        if (metadataBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(metadata_);
-        } else {
-          return metadataBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public int getMetadataCount() {
-        if (metadataBuilder_ == null) {
-          return metadata_.size();
-        } else {
-          return metadataBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public org.apache.orc.OrcProto.UserMetadataItem getMetadata(int index) {
-        if (metadataBuilder_ == null) {
-          return metadata_.get(index);
-        } else {
-          return metadataBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder setMetadata(
-          int index, org.apache.orc.OrcProto.UserMetadataItem value) {
-        if (metadataBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureMetadataIsMutable();
-          metadata_.set(index, value);
-          onChanged();
-        } else {
-          metadataBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder setMetadata(
-          int index, org.apache.orc.OrcProto.UserMetadataItem.Builder builderForValue) {
-        if (metadataBuilder_ == null) {
-          ensureMetadataIsMutable();
-          metadata_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          metadataBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder addMetadata(org.apache.orc.OrcProto.UserMetadataItem value) {
-        if (metadataBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureMetadataIsMutable();
-          metadata_.add(value);
-          onChanged();
-        } else {
-          metadataBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder addMetadata(
-          int index, org.apache.orc.OrcProto.UserMetadataItem value) {
-        if (metadataBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureMetadataIsMutable();
-          metadata_.add(index, value);
-          onChanged();
-        } else {
-          metadataBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder addMetadata(
-          org.apache.orc.OrcProto.UserMetadataItem.Builder builderForValue) {
-        if (metadataBuilder_ == null) {
-          ensureMetadataIsMutable();
-          metadata_.add(builderForValue.build());
-          onChanged();
-        } else {
-          metadataBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder addMetadata(
-          int index, org.apache.orc.OrcProto.UserMetadataItem.Builder builderForValue) {
-        if (metadataBuilder_ == null) {
-          ensureMetadataIsMutable();
-          metadata_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          metadataBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder addAllMetadata(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.UserMetadataItem> values) {
-        if (metadataBuilder_ == null) {
-          ensureMetadataIsMutable();
-          super.addAll(values, metadata_);
-          onChanged();
-        } else {
-          metadataBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder clearMetadata() {
-        if (metadataBuilder_ == null) {
-          metadata_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000010);
-          onChanged();
-        } else {
-          metadataBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public Builder removeMetadata(int index) {
-        if (metadataBuilder_ == null) {
-          ensureMetadataIsMutable();
-          metadata_.remove(index);
-          onChanged();
-        } else {
-          metadataBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public org.apache.orc.OrcProto.UserMetadataItem.Builder getMetadataBuilder(
-          int index) {
-        return getMetadataFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public org.apache.orc.OrcProto.UserMetadataItemOrBuilder getMetadataOrBuilder(
-          int index) {
-        if (metadataBuilder_ == null) {
-          return metadata_.get(index);  } else {
-          return metadataBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.UserMetadataItemOrBuilder> 
-           getMetadataOrBuilderList() {
-        if (metadataBuilder_ != null) {
-          return metadataBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(metadata_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public org.apache.orc.OrcProto.UserMetadataItem.Builder addMetadataBuilder() {
-        return getMetadataFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.UserMetadataItem.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public org.apache.orc.OrcProto.UserMetadataItem.Builder addMetadataBuilder(
-          int index) {
-        return getMetadataFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.UserMetadataItem.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.UserMetadataItem metadata = 5;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.UserMetadataItem.Builder> 
-           getMetadataBuilderList() {
-        return getMetadataFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.UserMetadataItem, org.apache.orc.OrcProto.UserMetadataItem.Builder, org.apache.orc.OrcProto.UserMetadataItemOrBuilder> 
-          getMetadataFieldBuilder() {
-        if (metadataBuilder_ == null) {
-          metadataBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.UserMetadataItem, org.apache.orc.OrcProto.UserMetadataItem.Builder, org.apache.orc.OrcProto.UserMetadataItemOrBuilder>(
-                  metadata_,
-                  ((bitField0_ & 0x00000010) == 0x00000010),
-                  getParentForChildren(),
-                  isClean());
-          metadata_ = null;
-        }
-        return metadataBuilder_;
-      }
-
-      // optional uint64 numberOfRows = 6;
-      private long numberOfRows_ ;
-      /**
-       * <code>optional uint64 numberOfRows = 6;</code>
-       */
-      public boolean hasNumberOfRows() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
-      }
-      /**
-       * <code>optional uint64 numberOfRows = 6;</code>
-       */
-      public long getNumberOfRows() {
-        return numberOfRows_;
-      }
-      /**
-       * <code>optional uint64 numberOfRows = 6;</code>
-       */
-      public Builder setNumberOfRows(long value) {
-        bitField0_ |= 0x00000020;
-        numberOfRows_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 numberOfRows = 6;</code>
-       */
-      public Builder clearNumberOfRows() {
-        bitField0_ = (bitField0_ & ~0x00000020);
-        numberOfRows_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // repeated .orc.proto.ColumnStatistics statistics = 7;
-      private java.util.List<org.apache.orc.OrcProto.ColumnStatistics> statistics_ =
-        java.util.Collections.emptyList();
-      private void ensureStatisticsIsMutable() {
-        if (!((bitField0_ & 0x00000040) == 0x00000040)) {
-          statistics_ = new java.util.ArrayList<org.apache.orc.OrcProto.ColumnStatistics>(statistics_);
-          bitField0_ |= 0x00000040;
-         }
-      }
-
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> statisticsBuilder_;
-
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.ColumnStatistics> getStatisticsList() {
-        if (statisticsBuilder_ == null) {
-          return java.util.Collections.unmodifiableList(statistics_);
-        } else {
-          return statisticsBuilder_.getMessageList();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public int getStatisticsCount() {
-        if (statisticsBuilder_ == null) {
-          return statistics_.size();
-        } else {
-          return statisticsBuilder_.getCount();
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics getStatistics(int index) {
-        if (statisticsBuilder_ == null) {
-          return statistics_.get(index);
-        } else {
-          return statisticsBuilder_.getMessage(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder setStatistics(
-          int index, org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (statisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStatisticsIsMutable();
-          statistics_.set(index, value);
-          onChanged();
-        } else {
-          statisticsBuilder_.setMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder setStatistics(
-          int index, org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (statisticsBuilder_ == null) {
-          ensureStatisticsIsMutable();
-          statistics_.set(index, builderForValue.build());
-          onChanged();
-        } else {
-          statisticsBuilder_.setMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder addStatistics(org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (statisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStatisticsIsMutable();
-          statistics_.add(value);
-          onChanged();
-        } else {
-          statisticsBuilder_.addMessage(value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder addStatistics(
-          int index, org.apache.orc.OrcProto.ColumnStatistics value) {
-        if (statisticsBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          ensureStatisticsIsMutable();
-          statistics_.add(index, value);
-          onChanged();
-        } else {
-          statisticsBuilder_.addMessage(index, value);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder addStatistics(
-          org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (statisticsBuilder_ == null) {
-          ensureStatisticsIsMutable();
-          statistics_.add(builderForValue.build());
-          onChanged();
-        } else {
-          statisticsBuilder_.addMessage(builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder addStatistics(
-          int index, org.apache.orc.OrcProto.ColumnStatistics.Builder builderForValue) {
-        if (statisticsBuilder_ == null) {
-          ensureStatisticsIsMutable();
-          statistics_.add(index, builderForValue.build());
-          onChanged();
-        } else {
-          statisticsBuilder_.addMessage(index, builderForValue.build());
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder addAllStatistics(
-          java.lang.Iterable<? extends org.apache.orc.OrcProto.ColumnStatistics> values) {
-        if (statisticsBuilder_ == null) {
-          ensureStatisticsIsMutable();
-          super.addAll(values, statistics_);
-          onChanged();
-        } else {
-          statisticsBuilder_.addAllMessages(values);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder clearStatistics() {
-        if (statisticsBuilder_ == null) {
-          statistics_ = java.util.Collections.emptyList();
-          bitField0_ = (bitField0_ & ~0x00000040);
-          onChanged();
-        } else {
-          statisticsBuilder_.clear();
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public Builder removeStatistics(int index) {
-        if (statisticsBuilder_ == null) {
-          ensureStatisticsIsMutable();
-          statistics_.remove(index);
-          onChanged();
-        } else {
-          statisticsBuilder_.remove(index);
-        }
-        return this;
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder getStatisticsBuilder(
-          int index) {
-        return getStatisticsFieldBuilder().getBuilder(index);
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatisticsOrBuilder getStatisticsOrBuilder(
-          int index) {
-        if (statisticsBuilder_ == null) {
-          return statistics_.get(index);  } else {
-          return statisticsBuilder_.getMessageOrBuilder(index);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public java.util.List<? extends org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-           getStatisticsOrBuilderList() {
-        if (statisticsBuilder_ != null) {
-          return statisticsBuilder_.getMessageOrBuilderList();
-        } else {
-          return java.util.Collections.unmodifiableList(statistics_);
-        }
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder addStatisticsBuilder() {
-        return getStatisticsFieldBuilder().addBuilder(
-            org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public org.apache.orc.OrcProto.ColumnStatistics.Builder addStatisticsBuilder(
-          int index) {
-        return getStatisticsFieldBuilder().addBuilder(
-            index, org.apache.orc.OrcProto.ColumnStatistics.getDefaultInstance());
-      }
-      /**
-       * <code>repeated .orc.proto.ColumnStatistics statistics = 7;</code>
-       */
-      public java.util.List<org.apache.orc.OrcProto.ColumnStatistics.Builder> 
-           getStatisticsBuilderList() {
-        return getStatisticsFieldBuilder().getBuilderList();
-      }
-      private com.google.protobuf.RepeatedFieldBuilder<
-          org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder> 
-          getStatisticsFieldBuilder() {
-        if (statisticsBuilder_ == null) {
-          statisticsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
-              org.apache.orc.OrcProto.ColumnStatistics, org.apache.orc.OrcProto.ColumnStatistics.Builder, org.apache.orc.OrcProto.ColumnStatisticsOrBuilder>(
-                  statistics_,
-                  ((bitField0_ & 0x00000040) == 0x00000040),
-                  getParentForChildren(),
-                  isClean());
-          statistics_ = null;
-        }
-        return statisticsBuilder_;
-      }
-
-      // optional uint32 rowIndexStride = 8;
-      private int rowIndexStride_ ;
-      /**
-       * <code>optional uint32 rowIndexStride = 8;</code>
-       */
-      public boolean hasRowIndexStride() {
-        return ((bitField0_ & 0x00000080) == 0x00000080);
-      }
-      /**
-       * <code>optional uint32 rowIndexStride = 8;</code>
-       */
-      public int getRowIndexStride() {
-        return rowIndexStride_;
-      }
-      /**
-       * <code>optional uint32 rowIndexStride = 8;</code>
-       */
-      public Builder setRowIndexStride(int value) {
-        bitField0_ |= 0x00000080;
-        rowIndexStride_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 rowIndexStride = 8;</code>
-       */
-      public Builder clearRowIndexStride() {
-        bitField0_ = (bitField0_ & ~0x00000080);
-        rowIndexStride_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.Footer)
-    }
-
-    static {
-      defaultInstance = new Footer(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.Footer)
-  }
-
-  public interface PostScriptOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional uint64 footerLength = 1;
-    /**
-     * <code>optional uint64 footerLength = 1;</code>
-     */
-    boolean hasFooterLength();
-    /**
-     * <code>optional uint64 footerLength = 1;</code>
-     */
-    long getFooterLength();
-
-    // optional .orc.proto.CompressionKind compression = 2;
-    /**
-     * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-     */
-    boolean hasCompression();
-    /**
-     * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-     */
-    org.apache.orc.OrcProto.CompressionKind getCompression();
-
-    // optional uint64 compressionBlockSize = 3;
-    /**
-     * <code>optional uint64 compressionBlockSize = 3;</code>
-     */
-    boolean hasCompressionBlockSize();
-    /**
-     * <code>optional uint64 compressionBlockSize = 3;</code>
-     */
-    long getCompressionBlockSize();
-
-    // repeated uint32 version = 4 [packed = true];
-    /**
-     * <code>repeated uint32 version = 4 [packed = true];</code>
-     *
-     * <pre>
-     * the version of the file format
-     *   [0, 11] = Hive 0.11
-     *   [0, 12] = Hive 0.12
-     * </pre>
-     */
-    java.util.List<java.lang.Integer> getVersionList();
-    /**
-     * <code>repeated uint32 version = 4 [packed = true];</code>
-     *
-     * <pre>
-     * the version of the file format
-     *   [0, 11] = Hive 0.11
-     *   [0, 12] = Hive 0.12
-     * </pre>
-     */
-    int getVersionCount();
-    /**
-     * <code>repeated uint32 version = 4 [packed = true];</code>
-     *
-     * <pre>
-     * the version of the file format
-     *   [0, 11] = Hive 0.11
-     *   [0, 12] = Hive 0.12
-     * </pre>
-     */
-    int getVersion(int index);
-
-    // optional uint64 metadataLength = 5;
-    /**
-     * <code>optional uint64 metadataLength = 5;</code>
-     */
-    boolean hasMetadataLength();
-    /**
-     * <code>optional uint64 metadataLength = 5;</code>
-     */
-    long getMetadataLength();
-
-    // optional uint32 writerVersion = 6;
-    /**
-     * <code>optional uint32 writerVersion = 6;</code>
-     *
-     * <pre>
-     * Version of the writer:
-     *   0 (or missing) = original
-     *   1 = HIVE-8732 fixed
-     *   2 = HIVE-4243 fixed
-     *   3 = HIVE-12055 fixed
-     *   4 = HIVE-13083 fixed
-     * </pre>
-     */
-    boolean hasWriterVersion();
-    /**
-     * <code>optional uint32 writerVersion = 6;</code>
-     *
-     * <pre>
-     * Version of the writer:
-     *   0 (or missing) = original
-     *   1 = HIVE-8732 fixed
-     *   2 = HIVE-4243 fixed
-     *   3 = HIVE-12055 fixed
-     *   4 = HIVE-13083 fixed
-     * </pre>
-     */
-    int getWriterVersion();
-
-    // optional string magic = 8000;
-    /**
-     * <code>optional string magic = 8000;</code>
-     *
-     * <pre>
-     * Leave this last in the record
-     * </pre>
-     */
-    boolean hasMagic();
-    /**
-     * <code>optional string magic = 8000;</code>
-     *
-     * <pre>
-     * Leave this last in the record
-     * </pre>
-     */
-    java.lang.String getMagic();
-    /**
-     * <code>optional string magic = 8000;</code>
-     *
-     * <pre>
-     * Leave this last in the record
-     * </pre>
-     */
-    com.google.protobuf.ByteString
-        getMagicBytes();
-  }
-  /**
-   * Protobuf type {@code orc.proto.PostScript}
-   *
-   * <pre>
-   * Serialized length must be less that 255 bytes
-   * </pre>
-   */
-  public static final class PostScript extends
-      com.google.protobuf.GeneratedMessage
-      implements PostScriptOrBuilder {
-    // Use PostScript.newBuilder() to construct.
-    private PostScript(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private PostScript(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final PostScript defaultInstance;
-    public static PostScript getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public PostScript getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private PostScript(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 8: {
-              bitField0_ |= 0x00000001;
-              footerLength_ = input.readUInt64();
-              break;
-            }
-            case 16: {
-              int rawValue = input.readEnum();
-              org.apache.orc.OrcProto.CompressionKind value = org.apache.orc.OrcProto.CompressionKind.valueOf(rawValue);
-              if (value == null) {
-                unknownFields.mergeVarintField(2, rawValue);
-              } else {
-                bitField0_ |= 0x00000002;
-                compression_ = value;
-              }
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              compressionBlockSize_ = input.readUInt64();
-              break;
-            }
-            case 32: {
-              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
-                version_ = new java.util.ArrayList<java.lang.Integer>();
-                mutable_bitField0_ |= 0x00000008;
-              }
-              version_.add(input.readUInt32());
-              break;
-            }
-            case 34: {
-              int length = input.readRawVarint32();
-              int limit = input.pushLimit(length);
-              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008) && input.getBytesUntilLimit() > 0) {
-                version_ = new java.util.ArrayList<java.lang.Integer>();
-                mutable_bitField0_ |= 0x00000008;
-              }
-              while (input.getBytesUntilLimit() > 0) {
-                version_.add(input.readUInt32());
-              }
-              input.popLimit(limit);
-              break;
-            }
-            case 40: {
-              bitField0_ |= 0x00000008;
-              metadataLength_ = input.readUInt64();
-              break;
-            }
-            case 48: {
-              bitField0_ |= 0x00000010;
-              writerVersion_ = input.readUInt32();
-              break;
-            }
-            case 64002: {
-              bitField0_ |= 0x00000020;
-              magic_ = input.readBytes();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
-          version_ = java.util.Collections.unmodifiableList(version_);
-        }
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_PostScript_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_PostScript_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.PostScript.class, org.apache.orc.OrcProto.PostScript.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<PostScript> PARSER =
-        new com.google.protobuf.AbstractParser<PostScript>() {
-      public PostScript parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new PostScript(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<PostScript> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional uint64 footerLength = 1;
-    public static final int FOOTERLENGTH_FIELD_NUMBER = 1;
-    private long footerLength_;
-    /**
-     * <code>optional uint64 footerLength = 1;</code>
-     */
-    public boolean hasFooterLength() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional uint64 footerLength = 1;</code>
-     */
-    public long getFooterLength() {
-      return footerLength_;
-    }
-
-    // optional .orc.proto.CompressionKind compression = 2;
-    public static final int COMPRESSION_FIELD_NUMBER = 2;
-    private org.apache.orc.OrcProto.CompressionKind compression_;
-    /**
-     * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-     */
-    public boolean hasCompression() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-     */
-    public org.apache.orc.OrcProto.CompressionKind getCompression() {
-      return compression_;
-    }
-
-    // optional uint64 compressionBlockSize = 3;
-    public static final int COMPRESSIONBLOCKSIZE_FIELD_NUMBER = 3;
-    private long compressionBlockSize_;
-    /**
-     * <code>optional uint64 compressionBlockSize = 3;</code>
-     */
-    public boolean hasCompressionBlockSize() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional uint64 compressionBlockSize = 3;</code>
-     */
-    public long getCompressionBlockSize() {
-      return compressionBlockSize_;
-    }
-
-    // repeated uint32 version = 4 [packed = true];
-    public static final int VERSION_FIELD_NUMBER = 4;
-    private java.util.List<java.lang.Integer> version_;
-    /**
-     * <code>repeated uint32 version = 4 [packed = true];</code>
-     *
-     * <pre>
-     * the version of the file format
-     *   [0, 11] = Hive 0.11
-     *   [0, 12] = Hive 0.12
-     * </pre>
-     */
-    public java.util.List<java.lang.Integer>
-        getVersionList() {
-      return version_;
-    }
-    /**
-     * <code>repeated uint32 version = 4 [packed = true];</code>
-     *
-     * <pre>
-     * the version of the file format
-     *   [0, 11] = Hive 0.11
-     *   [0, 12] = Hive 0.12
-     * </pre>
-     */
-    public int getVersionCount() {
-      return version_.size();
-    }
-    /**
-     * <code>repeated uint32 version = 4 [packed = true];</code>
-     *
-     * <pre>
-     * the version of the file format
-     *   [0, 11] = Hive 0.11
-     *   [0, 12] = Hive 0.12
-     * </pre>
-     */
-    public int getVersion(int index) {
-      return version_.get(index);
-    }
-    private int versionMemoizedSerializedSize = -1;
-
-    // optional uint64 metadataLength = 5;
-    public static final int METADATALENGTH_FIELD_NUMBER = 5;
-    private long metadataLength_;
-    /**
-     * <code>optional uint64 metadataLength = 5;</code>
-     */
-    public boolean hasMetadataLength() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    /**
-     * <code>optional uint64 metadataLength = 5;</code>
-     */
-    public long getMetadataLength() {
-      return metadataLength_;
-    }
-
-    // optional uint32 writerVersion = 6;
-    public static final int WRITERVERSION_FIELD_NUMBER = 6;
-    private int writerVersion_;
-    /**
-     * <code>optional uint32 writerVersion = 6;</code>
-     *
-     * <pre>
-     * Version of the writer:
-     *   0 (or missing) = original
-     *   1 = HIVE-8732 fixed
-     *   2 = HIVE-4243 fixed
-     *   3 = HIVE-12055 fixed
-     *   4 = HIVE-13083 fixed
-     * </pre>
-     */
-    public boolean hasWriterVersion() {
-      return ((bitField0_ & 0x00000010) == 0x00000010);
-    }
-    /**
-     * <code>optional uint32 writerVersion = 6;</code>
-     *
-     * <pre>
-     * Version of the writer:
-     *   0 (or missing) = original
-     *   1 = HIVE-8732 fixed
-     *   2 = HIVE-4243 fixed
-     *   3 = HIVE-12055 fixed
-     *   4 = HIVE-13083 fixed
-     * </pre>
-     */
-    public int getWriterVersion() {
-      return writerVersion_;
-    }
-
-    // optional string magic = 8000;
-    public static final int MAGIC_FIELD_NUMBER = 8000;
-    private java.lang.Object magic_;
-    /**
-     * <code>optional string magic = 8000;</code>
-     *
-     * <pre>
-     * Leave this last in the record
-     * </pre>
-     */
-    public boolean hasMagic() {
-      return ((bitField0_ & 0x00000020) == 0x00000020);
-    }
-    /**
-     * <code>optional string magic = 8000;</code>
-     *
-     * <pre>
-     * Leave this last in the record
-     * </pre>
-     */
-    public java.lang.String getMagic() {
-      java.lang.Object ref = magic_;
-      if (ref instanceof java.lang.String) {
-        return (java.lang.String) ref;
-      } else {
-        com.google.protobuf.ByteString bs = 
-            (com.google.protobuf.ByteString) ref;
-        java.lang.String s = bs.toStringUtf8();
-        if (bs.isValidUtf8()) {
-          magic_ = s;
-        }
-        return s;
-      }
-    }
-    /**
-     * <code>optional string magic = 8000;</code>
-     *
-     * <pre>
-     * Leave this last in the record
-     * </pre>
-     */
-    public com.google.protobuf.ByteString
-        getMagicBytes() {
-      java.lang.Object ref = magic_;
-      if (ref instanceof java.lang.String) {
-        com.google.protobuf.ByteString b = 
-            com.google.protobuf.ByteString.copyFromUtf8(
-                (java.lang.String) ref);
-        magic_ = b;
-        return b;
-      } else {
-        return (com.google.protobuf.ByteString) ref;
-      }
-    }
-
-    private void initFields() {
-      footerLength_ = 0L;
-      compression_ = org.apache.orc.OrcProto.CompressionKind.NONE;
-      compressionBlockSize_ = 0L;
-      version_ = java.util.Collections.emptyList();
-      metadataLength_ = 0L;
-      writerVersion_ = 0;
-      magic_ = "";
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeUInt64(1, footerLength_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeEnum(2, compression_.getNumber());
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt64(3, compressionBlockSize_);
-      }
-      if (getVersionList().size() > 0) {
-        output.writeRawVarint32(34);
-        output.writeRawVarint32(versionMemoizedSerializedSize);
-      }
-      for (int i = 0; i < version_.size(); i++) {
-        output.writeUInt32NoTag(version_.get(i));
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt64(5, metadataLength_);
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        output.writeUInt32(6, writerVersion_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        output.writeBytes(8000, getMagicBytes());
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(1, footerLength_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeEnumSize(2, compression_.getNumber());
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(3, compressionBlockSize_);
-      }
-      {
-        int dataSize = 0;
-        for (int i = 0; i < version_.size(); i++) {
-          dataSize += com.google.protobuf.CodedOutputStream
-            .computeUInt32SizeNoTag(version_.get(i));
-        }
-        size += dataSize;
-        if (!getVersionList().isEmpty()) {
-          size += 1;
-          size += com.google.protobuf.CodedOutputStream
-              .computeInt32SizeNoTag(dataSize);
-        }
-        versionMemoizedSerializedSize = dataSize;
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(5, metadataLength_);
-      }
-      if (((bitField0_ & 0x00000010) == 0x00000010)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt32Size(6, writerVersion_);
-      }
-      if (((bitField0_ & 0x00000020) == 0x00000020)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeBytesSize(8000, getMagicBytes());
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.PostScript parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.PostScript parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.PostScript prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.PostScript}
-     *
-     * <pre>
-     * Serialized length must be less that 255 bytes
-     * </pre>
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.PostScriptOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_PostScript_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_PostScript_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.PostScript.class, org.apache.orc.OrcProto.PostScript.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.PostScript.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        footerLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000001);
-        compression_ = org.apache.orc.OrcProto.CompressionKind.NONE;
-        bitField0_ = (bitField0_ & ~0x00000002);
-        compressionBlockSize_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        version_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000008);
-        metadataLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000010);
-        writerVersion_ = 0;
-        bitField0_ = (bitField0_ & ~0x00000020);
-        magic_ = "";
-        bitField0_ = (bitField0_ & ~0x00000040);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_PostScript_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.PostScript getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.PostScript.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.PostScript build() {
-        org.apache.orc.OrcProto.PostScript result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.PostScript buildPartial() {
-        org.apache.orc.OrcProto.PostScript result = new org.apache.orc.OrcProto.PostScript(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        result.footerLength_ = footerLength_;
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        result.compression_ = compression_;
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.compressionBlockSize_ = compressionBlockSize_;
-        if (((bitField0_ & 0x00000008) == 0x00000008)) {
-          version_ = java.util.Collections.unmodifiableList(version_);
-          bitField0_ = (bitField0_ & ~0x00000008);
-        }
-        result.version_ = version_;
-        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        result.metadataLength_ = metadataLength_;
-        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
-          to_bitField0_ |= 0x00000010;
-        }
-        result.writerVersion_ = writerVersion_;
-        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
-          to_bitField0_ |= 0x00000020;
-        }
-        result.magic_ = magic_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.PostScript) {
-          return mergeFrom((org.apache.orc.OrcProto.PostScript)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.PostScript other) {
-        if (other == org.apache.orc.OrcProto.PostScript.getDefaultInstance()) return this;
-        if (other.hasFooterLength()) {
-          setFooterLength(other.getFooterLength());
-        }
-        if (other.hasCompression()) {
-          setCompression(other.getCompression());
-        }
-        if (other.hasCompressionBlockSize()) {
-          setCompressionBlockSize(other.getCompressionBlockSize());
-        }
-        if (!other.version_.isEmpty()) {
-          if (version_.isEmpty()) {
-            version_ = other.version_;
-            bitField0_ = (bitField0_ & ~0x00000008);
-          } else {
-            ensureVersionIsMutable();
-            version_.addAll(other.version_);
-          }
-          onChanged();
-        }
-        if (other.hasMetadataLength()) {
-          setMetadataLength(other.getMetadataLength());
-        }
-        if (other.hasWriterVersion()) {
-          setWriterVersion(other.getWriterVersion());
-        }
-        if (other.hasMagic()) {
-          bitField0_ |= 0x00000040;
-          magic_ = other.magic_;
-          onChanged();
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.PostScript parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.PostScript) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional uint64 footerLength = 1;
-      private long footerLength_ ;
-      /**
-       * <code>optional uint64 footerLength = 1;</code>
-       */
-      public boolean hasFooterLength() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional uint64 footerLength = 1;</code>
-       */
-      public long getFooterLength() {
-        return footerLength_;
-      }
-      /**
-       * <code>optional uint64 footerLength = 1;</code>
-       */
-      public Builder setFooterLength(long value) {
-        bitField0_ |= 0x00000001;
-        footerLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 footerLength = 1;</code>
-       */
-      public Builder clearFooterLength() {
-        bitField0_ = (bitField0_ & ~0x00000001);
-        footerLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional .orc.proto.CompressionKind compression = 2;
-      private org.apache.orc.OrcProto.CompressionKind compression_ = org.apache.orc.OrcProto.CompressionKind.NONE;
-      /**
-       * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-       */
-      public boolean hasCompression() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-       */
-      public org.apache.orc.OrcProto.CompressionKind getCompression() {
-        return compression_;
-      }
-      /**
-       * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-       */
-      public Builder setCompression(org.apache.orc.OrcProto.CompressionKind value) {
-        if (value == null) {
-          throw new NullPointerException();
-        }
-        bitField0_ |= 0x00000002;
-        compression_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.CompressionKind compression = 2;</code>
-       */
-      public Builder clearCompression() {
-        bitField0_ = (bitField0_ & ~0x00000002);
-        compression_ = org.apache.orc.OrcProto.CompressionKind.NONE;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 compressionBlockSize = 3;
-      private long compressionBlockSize_ ;
-      /**
-       * <code>optional uint64 compressionBlockSize = 3;</code>
-       */
-      public boolean hasCompressionBlockSize() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional uint64 compressionBlockSize = 3;</code>
-       */
-      public long getCompressionBlockSize() {
-        return compressionBlockSize_;
-      }
-      /**
-       * <code>optional uint64 compressionBlockSize = 3;</code>
-       */
-      public Builder setCompressionBlockSize(long value) {
-        bitField0_ |= 0x00000004;
-        compressionBlockSize_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 compressionBlockSize = 3;</code>
-       */
-      public Builder clearCompressionBlockSize() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        compressionBlockSize_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // repeated uint32 version = 4 [packed = true];
-      private java.util.List<java.lang.Integer> version_ = java.util.Collections.emptyList();
-      private void ensureVersionIsMutable() {
-        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
-          version_ = new java.util.ArrayList<java.lang.Integer>(version_);
-          bitField0_ |= 0x00000008;
-         }
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public java.util.List<java.lang.Integer>
-          getVersionList() {
-        return java.util.Collections.unmodifiableList(version_);
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public int getVersionCount() {
-        return version_.size();
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public int getVersion(int index) {
-        return version_.get(index);
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public Builder setVersion(
-          int index, int value) {
-        ensureVersionIsMutable();
-        version_.set(index, value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public Builder addVersion(int value) {
-        ensureVersionIsMutable();
-        version_.add(value);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public Builder addAllVersion(
-          java.lang.Iterable<? extends java.lang.Integer> values) {
-        ensureVersionIsMutable();
-        super.addAll(values, version_);
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>repeated uint32 version = 4 [packed = true];</code>
-       *
-       * <pre>
-       * the version of the file format
-       *   [0, 11] = Hive 0.11
-       *   [0, 12] = Hive 0.12
-       * </pre>
-       */
-      public Builder clearVersion() {
-        version_ = java.util.Collections.emptyList();
-        bitField0_ = (bitField0_ & ~0x00000008);
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 metadataLength = 5;
-      private long metadataLength_ ;
-      /**
-       * <code>optional uint64 metadataLength = 5;</code>
-       */
-      public boolean hasMetadataLength() {
-        return ((bitField0_ & 0x00000010) == 0x00000010);
-      }
-      /**
-       * <code>optional uint64 metadataLength = 5;</code>
-       */
-      public long getMetadataLength() {
-        return metadataLength_;
-      }
-      /**
-       * <code>optional uint64 metadataLength = 5;</code>
-       */
-      public Builder setMetadataLength(long value) {
-        bitField0_ |= 0x00000010;
-        metadataLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 metadataLength = 5;</code>
-       */
-      public Builder clearMetadataLength() {
-        bitField0_ = (bitField0_ & ~0x00000010);
-        metadataLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint32 writerVersion = 6;
-      private int writerVersion_ ;
-      /**
-       * <code>optional uint32 writerVersion = 6;</code>
-       *
-       * <pre>
-       * Version of the writer:
-       *   0 (or missing) = original
-       *   1 = HIVE-8732 fixed
-       *   2 = HIVE-4243 fixed
-       *   3 = HIVE-12055 fixed
-       *   4 = HIVE-13083 fixed
-       * </pre>
-       */
-      public boolean hasWriterVersion() {
-        return ((bitField0_ & 0x00000020) == 0x00000020);
-      }
-      /**
-       * <code>optional uint32 writerVersion = 6;</code>
-       *
-       * <pre>
-       * Version of the writer:
-       *   0 (or missing) = original
-       *   1 = HIVE-8732 fixed
-       *   2 = HIVE-4243 fixed
-       *   3 = HIVE-12055 fixed
-       *   4 = HIVE-13083 fixed
-       * </pre>
-       */
-      public int getWriterVersion() {
-        return writerVersion_;
-      }
-      /**
-       * <code>optional uint32 writerVersion = 6;</code>
-       *
-       * <pre>
-       * Version of the writer:
-       *   0 (or missing) = original
-       *   1 = HIVE-8732 fixed
-       *   2 = HIVE-4243 fixed
-       *   3 = HIVE-12055 fixed
-       *   4 = HIVE-13083 fixed
-       * </pre>
-       */
-      public Builder setWriterVersion(int value) {
-        bitField0_ |= 0x00000020;
-        writerVersion_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint32 writerVersion = 6;</code>
-       *
-       * <pre>
-       * Version of the writer:
-       *   0 (or missing) = original
-       *   1 = HIVE-8732 fixed
-       *   2 = HIVE-4243 fixed
-       *   3 = HIVE-12055 fixed
-       *   4 = HIVE-13083 fixed
-       * </pre>
-       */
-      public Builder clearWriterVersion() {
-        bitField0_ = (bitField0_ & ~0x00000020);
-        writerVersion_ = 0;
-        onChanged();
-        return this;
-      }
-
-      // optional string magic = 8000;
-      private java.lang.Object magic_ = "";
-      /**
-       * <code>optional string magic = 8000;</code>
-       *
-       * <pre>
-       * Leave this last in the record
-       * </pre>
-       */
-      public boolean hasMagic() {
-        return ((bitField0_ & 0x00000040) == 0x00000040);
-      }
-      /**
-       * <code>optional string magic = 8000;</code>
-       *
-       * <pre>
-       * Leave this last in the record
-       * </pre>
-       */
-      public java.lang.String getMagic() {
-        java.lang.Object ref = magic_;
-        if (!(ref instanceof java.lang.String)) {
-          java.lang.String s = ((com.google.protobuf.ByteString) ref)
-              .toStringUtf8();
-          magic_ = s;
-          return s;
-        } else {
-          return (java.lang.String) ref;
-        }
-      }
-      /**
-       * <code>optional string magic = 8000;</code>
-       *
-       * <pre>
-       * Leave this last in the record
-       * </pre>
-       */
-      public com.google.protobuf.ByteString
-          getMagicBytes() {
-        java.lang.Object ref = magic_;
-        if (ref instanceof String) {
-          com.google.protobuf.ByteString b = 
-              com.google.protobuf.ByteString.copyFromUtf8(
-                  (java.lang.String) ref);
-          magic_ = b;
-          return b;
-        } else {
-          return (com.google.protobuf.ByteString) ref;
-        }
-      }
-      /**
-       * <code>optional string magic = 8000;</code>
-       *
-       * <pre>
-       * Leave this last in the record
-       * </pre>
-       */
-      public Builder setMagic(
-          java.lang.String value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000040;
-        magic_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string magic = 8000;</code>
-       *
-       * <pre>
-       * Leave this last in the record
-       * </pre>
-       */
-      public Builder clearMagic() {
-        bitField0_ = (bitField0_ & ~0x00000040);
-        magic_ = getDefaultInstance().getMagic();
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional string magic = 8000;</code>
-       *
-       * <pre>
-       * Leave this last in the record
-       * </pre>
-       */
-      public Builder setMagicBytes(
-          com.google.protobuf.ByteString value) {
-        if (value == null) {
-    throw new NullPointerException();
-  }
-  bitField0_ |= 0x00000040;
-        magic_ = value;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.PostScript)
-    }
-
-    static {
-      defaultInstance = new PostScript(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.PostScript)
-  }
-
-  public interface FileTailOrBuilder
-      extends com.google.protobuf.MessageOrBuilder {
-
-    // optional .orc.proto.PostScript postscript = 1;
-    /**
-     * <code>optional .orc.proto.PostScript postscript = 1;</code>
-     */
-    boolean hasPostscript();
-    /**
-     * <code>optional .orc.proto.PostScript postscript = 1;</code>
-     */
-    org.apache.orc.OrcProto.PostScript getPostscript();
-    /**
-     * <code>optional .orc.proto.PostScript postscript = 1;</code>
-     */
-    org.apache.orc.OrcProto.PostScriptOrBuilder getPostscriptOrBuilder();
-
-    // optional .orc.proto.Footer footer = 2;
-    /**
-     * <code>optional .orc.proto.Footer footer = 2;</code>
-     */
-    boolean hasFooter();
-    /**
-     * <code>optional .orc.proto.Footer footer = 2;</code>
-     */
-    org.apache.orc.OrcProto.Footer getFooter();
-    /**
-     * <code>optional .orc.proto.Footer footer = 2;</code>
-     */
-    org.apache.orc.OrcProto.FooterOrBuilder getFooterOrBuilder();
-
-    // optional uint64 fileLength = 3;
-    /**
-     * <code>optional uint64 fileLength = 3;</code>
-     */
-    boolean hasFileLength();
-    /**
-     * <code>optional uint64 fileLength = 3;</code>
-     */
-    long getFileLength();
-
-    // optional uint64 postscriptLength = 4;
-    /**
-     * <code>optional uint64 postscriptLength = 4;</code>
-     */
-    boolean hasPostscriptLength();
-    /**
-     * <code>optional uint64 postscriptLength = 4;</code>
-     */
-    long getPostscriptLength();
-  }
-  /**
-   * Protobuf type {@code orc.proto.FileTail}
-   *
-   * <pre>
-   * This gets serialized as part of OrcSplit, also used by footer cache.
-   * </pre>
-   */
-  public static final class FileTail extends
-      com.google.protobuf.GeneratedMessage
-      implements FileTailOrBuilder {
-    // Use FileTail.newBuilder() to construct.
-    private FileTail(com.google.protobuf.GeneratedMessage.Builder<?> builder) {
-      super(builder);
-      this.unknownFields = builder.getUnknownFields();
-    }
-    private FileTail(boolean noInit) { this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }
-
-    private static final FileTail defaultInstance;
-    public static FileTail getDefaultInstance() {
-      return defaultInstance;
-    }
-
-    public FileTail getDefaultInstanceForType() {
-      return defaultInstance;
-    }
-
-    private final com.google.protobuf.UnknownFieldSet unknownFields;
-    @java.lang.Override
-    public final com.google.protobuf.UnknownFieldSet
-        getUnknownFields() {
-      return this.unknownFields;
-    }
-    private FileTail(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      initFields();
-      int mutable_bitField0_ = 0;
-      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
-          com.google.protobuf.UnknownFieldSet.newBuilder();
-      try {
-        boolean done = false;
-        while (!done) {
-          int tag = input.readTag();
-          switch (tag) {
-            case 0:
-              done = true;
-              break;
-            default: {
-              if (!parseUnknownField(input, unknownFields,
-                                     extensionRegistry, tag)) {
-                done = true;
-              }
-              break;
-            }
-            case 10: {
-              org.apache.orc.OrcProto.PostScript.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000001) == 0x00000001)) {
-                subBuilder = postscript_.toBuilder();
-              }
-              postscript_ = input.readMessage(org.apache.orc.OrcProto.PostScript.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(postscript_);
-                postscript_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000001;
-              break;
-            }
-            case 18: {
-              org.apache.orc.OrcProto.Footer.Builder subBuilder = null;
-              if (((bitField0_ & 0x00000002) == 0x00000002)) {
-                subBuilder = footer_.toBuilder();
-              }
-              footer_ = input.readMessage(org.apache.orc.OrcProto.Footer.PARSER, extensionRegistry);
-              if (subBuilder != null) {
-                subBuilder.mergeFrom(footer_);
-                footer_ = subBuilder.buildPartial();
-              }
-              bitField0_ |= 0x00000002;
-              break;
-            }
-            case 24: {
-              bitField0_ |= 0x00000004;
-              fileLength_ = input.readUInt64();
-              break;
-            }
-            case 32: {
-              bitField0_ |= 0x00000008;
-              postscriptLength_ = input.readUInt64();
-              break;
-            }
-          }
-        }
-      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-        throw e.setUnfinishedMessage(this);
-      } catch (java.io.IOException e) {
-        throw new com.google.protobuf.InvalidProtocolBufferException(
-            e.getMessage()).setUnfinishedMessage(this);
-      } finally {
-        this.unknownFields = unknownFields.build();
-        makeExtensionsImmutable();
-      }
-    }
-    public static final com.google.protobuf.Descriptors.Descriptor
-        getDescriptor() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_FileTail_descriptor;
-    }
-
-    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-        internalGetFieldAccessorTable() {
-      return org.apache.orc.OrcProto.internal_static_orc_proto_FileTail_fieldAccessorTable
-          .ensureFieldAccessorsInitialized(
-              org.apache.orc.OrcProto.FileTail.class, org.apache.orc.OrcProto.FileTail.Builder.class);
-    }
-
-    public static com.google.protobuf.Parser<FileTail> PARSER =
-        new com.google.protobuf.AbstractParser<FileTail>() {
-      public FileTail parsePartialFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws com.google.protobuf.InvalidProtocolBufferException {
-        return new FileTail(input, extensionRegistry);
-      }
-    };
-
-    @java.lang.Override
-    public com.google.protobuf.Parser<FileTail> getParserForType() {
-      return PARSER;
-    }
-
-    private int bitField0_;
-    // optional .orc.proto.PostScript postscript = 1;
-    public static final int POSTSCRIPT_FIELD_NUMBER = 1;
-    private org.apache.orc.OrcProto.PostScript postscript_;
-    /**
-     * <code>optional .orc.proto.PostScript postscript = 1;</code>
-     */
-    public boolean hasPostscript() {
-      return ((bitField0_ & 0x00000001) == 0x00000001);
-    }
-    /**
-     * <code>optional .orc.proto.PostScript postscript = 1;</code>
-     */
-    public org.apache.orc.OrcProto.PostScript getPostscript() {
-      return postscript_;
-    }
-    /**
-     * <code>optional .orc.proto.PostScript postscript = 1;</code>
-     */
-    public org.apache.orc.OrcProto.PostScriptOrBuilder getPostscriptOrBuilder() {
-      return postscript_;
-    }
-
-    // optional .orc.proto.Footer footer = 2;
-    public static final int FOOTER_FIELD_NUMBER = 2;
-    private org.apache.orc.OrcProto.Footer footer_;
-    /**
-     * <code>optional .orc.proto.Footer footer = 2;</code>
-     */
-    public boolean hasFooter() {
-      return ((bitField0_ & 0x00000002) == 0x00000002);
-    }
-    /**
-     * <code>optional .orc.proto.Footer footer = 2;</code>
-     */
-    public org.apache.orc.OrcProto.Footer getFooter() {
-      return footer_;
-    }
-    /**
-     * <code>optional .orc.proto.Footer footer = 2;</code>
-     */
-    public org.apache.orc.OrcProto.FooterOrBuilder getFooterOrBuilder() {
-      return footer_;
-    }
-
-    // optional uint64 fileLength = 3;
-    public static final int FILELENGTH_FIELD_NUMBER = 3;
-    private long fileLength_;
-    /**
-     * <code>optional uint64 fileLength = 3;</code>
-     */
-    public boolean hasFileLength() {
-      return ((bitField0_ & 0x00000004) == 0x00000004);
-    }
-    /**
-     * <code>optional uint64 fileLength = 3;</code>
-     */
-    public long getFileLength() {
-      return fileLength_;
-    }
-
-    // optional uint64 postscriptLength = 4;
-    public static final int POSTSCRIPTLENGTH_FIELD_NUMBER = 4;
-    private long postscriptLength_;
-    /**
-     * <code>optional uint64 postscriptLength = 4;</code>
-     */
-    public boolean hasPostscriptLength() {
-      return ((bitField0_ & 0x00000008) == 0x00000008);
-    }
-    /**
-     * <code>optional uint64 postscriptLength = 4;</code>
-     */
-    public long getPostscriptLength() {
-      return postscriptLength_;
-    }
-
-    private void initFields() {
-      postscript_ = org.apache.orc.OrcProto.PostScript.getDefaultInstance();
-      footer_ = org.apache.orc.OrcProto.Footer.getDefaultInstance();
-      fileLength_ = 0L;
-      postscriptLength_ = 0L;
-    }
-    private byte memoizedIsInitialized = -1;
-    public final boolean isInitialized() {
-      byte isInitialized = memoizedIsInitialized;
-      if (isInitialized != -1) return isInitialized == 1;
-
-      memoizedIsInitialized = 1;
-      return true;
-    }
-
-    public void writeTo(com.google.protobuf.CodedOutputStream output)
-                        throws java.io.IOException {
-      getSerializedSize();
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        output.writeMessage(1, postscript_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        output.writeMessage(2, footer_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        output.writeUInt64(3, fileLength_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        output.writeUInt64(4, postscriptLength_);
-      }
-      getUnknownFields().writeTo(output);
-    }
-
-    private int memoizedSerializedSize = -1;
-    public int getSerializedSize() {
-      int size = memoizedSerializedSize;
-      if (size != -1) return size;
-
-      size = 0;
-      if (((bitField0_ & 0x00000001) == 0x00000001)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(1, postscript_);
-      }
-      if (((bitField0_ & 0x00000002) == 0x00000002)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeMessageSize(2, footer_);
-      }
-      if (((bitField0_ & 0x00000004) == 0x00000004)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(3, fileLength_);
-      }
-      if (((bitField0_ & 0x00000008) == 0x00000008)) {
-        size += com.google.protobuf.CodedOutputStream
-          .computeUInt64Size(4, postscriptLength_);
-      }
-      size += getUnknownFields().getSerializedSize();
-      memoizedSerializedSize = size;
-      return size;
-    }
-
-    private static final long serialVersionUID = 0L;
-    @java.lang.Override
-    protected java.lang.Object writeReplace()
-        throws java.io.ObjectStreamException {
-      return super.writeReplace();
-    }
-
-    public static org.apache.orc.OrcProto.FileTail parseFrom(
-        com.google.protobuf.ByteString data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(
-        com.google.protobuf.ByteString data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(byte[] data)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(
-        byte[] data,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws com.google.protobuf.InvalidProtocolBufferException {
-      return PARSER.parseFrom(data, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseDelimitedFrom(java.io.InputStream input)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseDelimitedFrom(
-        java.io.InputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseDelimitedFrom(input, extensionRegistry);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(
-        com.google.protobuf.CodedInputStream input)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input);
-    }
-    public static org.apache.orc.OrcProto.FileTail parseFrom(
-        com.google.protobuf.CodedInputStream input,
-        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-        throws java.io.IOException {
-      return PARSER.parseFrom(input, extensionRegistry);
-    }
-
-    public static Builder newBuilder() { return Builder.create(); }
-    public Builder newBuilderForType() { return newBuilder(); }
-    public static Builder newBuilder(org.apache.orc.OrcProto.FileTail prototype) {
-      return newBuilder().mergeFrom(prototype);
-    }
-    public Builder toBuilder() { return newBuilder(this); }
-
-    @java.lang.Override
-    protected Builder newBuilderForType(
-        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-      Builder builder = new Builder(parent);
-      return builder;
-    }
-    /**
-     * Protobuf type {@code orc.proto.FileTail}
-     *
-     * <pre>
-     * This gets serialized as part of OrcSplit, also used by footer cache.
-     * </pre>
-     */
-    public static final class Builder extends
-        com.google.protobuf.GeneratedMessage.Builder<Builder>
-       implements org.apache.orc.OrcProto.FileTailOrBuilder {
-      public static final com.google.protobuf.Descriptors.Descriptor
-          getDescriptor() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_FileTail_descriptor;
-      }
-
-      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
-          internalGetFieldAccessorTable() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_FileTail_fieldAccessorTable
-            .ensureFieldAccessorsInitialized(
-                org.apache.orc.OrcProto.FileTail.class, org.apache.orc.OrcProto.FileTail.Builder.class);
-      }
-
-      // Construct using org.apache.orc.OrcProto.FileTail.newBuilder()
-      private Builder() {
-        maybeForceBuilderInitialization();
-      }
-
-      private Builder(
-          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
-        super(parent);
-        maybeForceBuilderInitialization();
-      }
-      private void maybeForceBuilderInitialization() {
-        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
-          getPostscriptFieldBuilder();
-          getFooterFieldBuilder();
-        }
-      }
-      private static Builder create() {
-        return new Builder();
-      }
-
-      public Builder clear() {
-        super.clear();
-        if (postscriptBuilder_ == null) {
-          postscript_ = org.apache.orc.OrcProto.PostScript.getDefaultInstance();
-        } else {
-          postscriptBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        if (footerBuilder_ == null) {
-          footer_ = org.apache.orc.OrcProto.Footer.getDefaultInstance();
-        } else {
-          footerBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        fileLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000004);
-        postscriptLength_ = 0L;
-        bitField0_ = (bitField0_ & ~0x00000008);
-        return this;
-      }
-
-      public Builder clone() {
-        return create().mergeFrom(buildPartial());
-      }
-
-      public com.google.protobuf.Descriptors.Descriptor
-          getDescriptorForType() {
-        return org.apache.orc.OrcProto.internal_static_orc_proto_FileTail_descriptor;
-      }
-
-      public org.apache.orc.OrcProto.FileTail getDefaultInstanceForType() {
-        return org.apache.orc.OrcProto.FileTail.getDefaultInstance();
-      }
-
-      public org.apache.orc.OrcProto.FileTail build() {
-        org.apache.orc.OrcProto.FileTail result = buildPartial();
-        if (!result.isInitialized()) {
-          throw newUninitializedMessageException(result);
-        }
-        return result;
-      }
-
-      public org.apache.orc.OrcProto.FileTail buildPartial() {
-        org.apache.orc.OrcProto.FileTail result = new org.apache.orc.OrcProto.FileTail(this);
-        int from_bitField0_ = bitField0_;
-        int to_bitField0_ = 0;
-        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
-          to_bitField0_ |= 0x00000001;
-        }
-        if (postscriptBuilder_ == null) {
-          result.postscript_ = postscript_;
-        } else {
-          result.postscript_ = postscriptBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
-          to_bitField0_ |= 0x00000002;
-        }
-        if (footerBuilder_ == null) {
-          result.footer_ = footer_;
-        } else {
-          result.footer_ = footerBuilder_.build();
-        }
-        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
-          to_bitField0_ |= 0x00000004;
-        }
-        result.fileLength_ = fileLength_;
-        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
-          to_bitField0_ |= 0x00000008;
-        }
-        result.postscriptLength_ = postscriptLength_;
-        result.bitField0_ = to_bitField0_;
-        onBuilt();
-        return result;
-      }
-
-      public Builder mergeFrom(com.google.protobuf.Message other) {
-        if (other instanceof org.apache.orc.OrcProto.FileTail) {
-          return mergeFrom((org.apache.orc.OrcProto.FileTail)other);
-        } else {
-          super.mergeFrom(other);
-          return this;
-        }
-      }
-
-      public Builder mergeFrom(org.apache.orc.OrcProto.FileTail other) {
-        if (other == org.apache.orc.OrcProto.FileTail.getDefaultInstance()) return this;
-        if (other.hasPostscript()) {
-          mergePostscript(other.getPostscript());
-        }
-        if (other.hasFooter()) {
-          mergeFooter(other.getFooter());
-        }
-        if (other.hasFileLength()) {
-          setFileLength(other.getFileLength());
-        }
-        if (other.hasPostscriptLength()) {
-          setPostscriptLength(other.getPostscriptLength());
-        }
-        this.mergeUnknownFields(other.getUnknownFields());
-        return this;
-      }
-
-      public final boolean isInitialized() {
-        return true;
-      }
-
-      public Builder mergeFrom(
-          com.google.protobuf.CodedInputStream input,
-          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
-          throws java.io.IOException {
-        org.apache.orc.OrcProto.FileTail parsedMessage = null;
-        try {
-          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
-        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
-          parsedMessage = (org.apache.orc.OrcProto.FileTail) e.getUnfinishedMessage();
-          throw e;
-        } finally {
-          if (parsedMessage != null) {
-            mergeFrom(parsedMessage);
-          }
-        }
-        return this;
-      }
-      private int bitField0_;
-
-      // optional .orc.proto.PostScript postscript = 1;
-      private org.apache.orc.OrcProto.PostScript postscript_ = org.apache.orc.OrcProto.PostScript.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.PostScript, org.apache.orc.OrcProto.PostScript.Builder, org.apache.orc.OrcProto.PostScriptOrBuilder> postscriptBuilder_;
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public boolean hasPostscript() {
-        return ((bitField0_ & 0x00000001) == 0x00000001);
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public org.apache.orc.OrcProto.PostScript getPostscript() {
-        if (postscriptBuilder_ == null) {
-          return postscript_;
-        } else {
-          return postscriptBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public Builder setPostscript(org.apache.orc.OrcProto.PostScript value) {
-        if (postscriptBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          postscript_ = value;
-          onChanged();
-        } else {
-          postscriptBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public Builder setPostscript(
-          org.apache.orc.OrcProto.PostScript.Builder builderForValue) {
-        if (postscriptBuilder_ == null) {
-          postscript_ = builderForValue.build();
-          onChanged();
-        } else {
-          postscriptBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public Builder mergePostscript(org.apache.orc.OrcProto.PostScript value) {
-        if (postscriptBuilder_ == null) {
-          if (((bitField0_ & 0x00000001) == 0x00000001) &&
-              postscript_ != org.apache.orc.OrcProto.PostScript.getDefaultInstance()) {
-            postscript_ =
-              org.apache.orc.OrcProto.PostScript.newBuilder(postscript_).mergeFrom(value).buildPartial();
-          } else {
-            postscript_ = value;
-          }
-          onChanged();
-        } else {
-          postscriptBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000001;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public Builder clearPostscript() {
-        if (postscriptBuilder_ == null) {
-          postscript_ = org.apache.orc.OrcProto.PostScript.getDefaultInstance();
-          onChanged();
-        } else {
-          postscriptBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000001);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public org.apache.orc.OrcProto.PostScript.Builder getPostscriptBuilder() {
-        bitField0_ |= 0x00000001;
-        onChanged();
-        return getPostscriptFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      public org.apache.orc.OrcProto.PostScriptOrBuilder getPostscriptOrBuilder() {
-        if (postscriptBuilder_ != null) {
-          return postscriptBuilder_.getMessageOrBuilder();
-        } else {
-          return postscript_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.PostScript postscript = 1;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.PostScript, org.apache.orc.OrcProto.PostScript.Builder, org.apache.orc.OrcProto.PostScriptOrBuilder> 
-          getPostscriptFieldBuilder() {
-        if (postscriptBuilder_ == null) {
-          postscriptBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.PostScript, org.apache.orc.OrcProto.PostScript.Builder, org.apache.orc.OrcProto.PostScriptOrBuilder>(
-                  postscript_,
-                  getParentForChildren(),
-                  isClean());
-          postscript_ = null;
-        }
-        return postscriptBuilder_;
-      }
-
-      // optional .orc.proto.Footer footer = 2;
-      private org.apache.orc.OrcProto.Footer footer_ = org.apache.orc.OrcProto.Footer.getDefaultInstance();
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.Footer, org.apache.orc.OrcProto.Footer.Builder, org.apache.orc.OrcProto.FooterOrBuilder> footerBuilder_;
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public boolean hasFooter() {
-        return ((bitField0_ & 0x00000002) == 0x00000002);
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public org.apache.orc.OrcProto.Footer getFooter() {
-        if (footerBuilder_ == null) {
-          return footer_;
-        } else {
-          return footerBuilder_.getMessage();
-        }
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public Builder setFooter(org.apache.orc.OrcProto.Footer value) {
-        if (footerBuilder_ == null) {
-          if (value == null) {
-            throw new NullPointerException();
-          }
-          footer_ = value;
-          onChanged();
-        } else {
-          footerBuilder_.setMessage(value);
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public Builder setFooter(
-          org.apache.orc.OrcProto.Footer.Builder builderForValue) {
-        if (footerBuilder_ == null) {
-          footer_ = builderForValue.build();
-          onChanged();
-        } else {
-          footerBuilder_.setMessage(builderForValue.build());
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public Builder mergeFooter(org.apache.orc.OrcProto.Footer value) {
-        if (footerBuilder_ == null) {
-          if (((bitField0_ & 0x00000002) == 0x00000002) &&
-              footer_ != org.apache.orc.OrcProto.Footer.getDefaultInstance()) {
-            footer_ =
-              org.apache.orc.OrcProto.Footer.newBuilder(footer_).mergeFrom(value).buildPartial();
-          } else {
-            footer_ = value;
-          }
-          onChanged();
-        } else {
-          footerBuilder_.mergeFrom(value);
-        }
-        bitField0_ |= 0x00000002;
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public Builder clearFooter() {
-        if (footerBuilder_ == null) {
-          footer_ = org.apache.orc.OrcProto.Footer.getDefaultInstance();
-          onChanged();
-        } else {
-          footerBuilder_.clear();
-        }
-        bitField0_ = (bitField0_ & ~0x00000002);
-        return this;
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public org.apache.orc.OrcProto.Footer.Builder getFooterBuilder() {
-        bitField0_ |= 0x00000002;
-        onChanged();
-        return getFooterFieldBuilder().getBuilder();
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      public org.apache.orc.OrcProto.FooterOrBuilder getFooterOrBuilder() {
-        if (footerBuilder_ != null) {
-          return footerBuilder_.getMessageOrBuilder();
-        } else {
-          return footer_;
-        }
-      }
-      /**
-       * <code>optional .orc.proto.Footer footer = 2;</code>
-       */
-      private com.google.protobuf.SingleFieldBuilder<
-          org.apache.orc.OrcProto.Footer, org.apache.orc.OrcProto.Footer.Builder, org.apache.orc.OrcProto.FooterOrBuilder> 
-          getFooterFieldBuilder() {
-        if (footerBuilder_ == null) {
-          footerBuilder_ = new com.google.protobuf.SingleFieldBuilder<
-              org.apache.orc.OrcProto.Footer, org.apache.orc.OrcProto.Footer.Builder, org.apache.orc.OrcProto.FooterOrBuilder>(
-                  footer_,
-                  getParentForChildren(),
-                  isClean());
-          footer_ = null;
-        }
-        return footerBuilder_;
-      }
-
-      // optional uint64 fileLength = 3;
-      private long fileLength_ ;
-      /**
-       * <code>optional uint64 fileLength = 3;</code>
-       */
-      public boolean hasFileLength() {
-        return ((bitField0_ & 0x00000004) == 0x00000004);
-      }
-      /**
-       * <code>optional uint64 fileLength = 3;</code>
-       */
-      public long getFileLength() {
-        return fileLength_;
-      }
-      /**
-       * <code>optional uint64 fileLength = 3;</code>
-       */
-      public Builder setFileLength(long value) {
-        bitField0_ |= 0x00000004;
-        fileLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 fileLength = 3;</code>
-       */
-      public Builder clearFileLength() {
-        bitField0_ = (bitField0_ & ~0x00000004);
-        fileLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // optional uint64 postscriptLength = 4;
-      private long postscriptLength_ ;
-      /**
-       * <code>optional uint64 postscriptLength = 4;</code>
-       */
-      public boolean hasPostscriptLength() {
-        return ((bitField0_ & 0x00000008) == 0x00000008);
-      }
-      /**
-       * <code>optional uint64 postscriptLength = 4;</code>
-       */
-      public long getPostscriptLength() {
-        return postscriptLength_;
-      }
-      /**
-       * <code>optional uint64 postscriptLength = 4;</code>
-       */
-      public Builder setPostscriptLength(long value) {
-        bitField0_ |= 0x00000008;
-        postscriptLength_ = value;
-        onChanged();
-        return this;
-      }
-      /**
-       * <code>optional uint64 postscriptLength = 4;</code>
-       */
-      public Builder clearPostscriptLength() {
-        bitField0_ = (bitField0_ & ~0x00000008);
-        postscriptLength_ = 0L;
-        onChanged();
-        return this;
-      }
-
-      // @@protoc_insertion_point(builder_scope:orc.proto.FileTail)
-    }
-
-    static {
-      defaultInstance = new FileTail(true);
-      defaultInstance.initFields();
-    }
-
-    // @@protoc_insertion_point(class_scope:orc.proto.FileTail)
-  }
-
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_IntegerStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_IntegerStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_DoubleStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_DoubleStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_StringStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_StringStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_BucketStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_BucketStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_DecimalStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_DecimalStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_DateStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_DateStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_TimestampStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_TimestampStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_BinaryStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_BinaryStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_ColumnStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_ColumnStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_RowIndexEntry_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_RowIndexEntry_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_RowIndex_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_RowIndex_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_BloomFilter_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_BloomFilter_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_BloomFilterIndex_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_BloomFilterIndex_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_Stream_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_Stream_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_ColumnEncoding_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_ColumnEncoding_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_StripeFooter_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_StripeFooter_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_Type_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_Type_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_StripeInformation_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_StripeInformation_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_UserMetadataItem_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_UserMetadataItem_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_StripeStatistics_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_StripeStatistics_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_Metadata_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_Metadata_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_Footer_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_Footer_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_PostScript_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_PostScript_fieldAccessorTable;
-  private static com.google.protobuf.Descriptors.Descriptor
-    internal_static_orc_proto_FileTail_descriptor;
-  private static
-    com.google.protobuf.GeneratedMessage.FieldAccessorTable
-      internal_static_orc_proto_FileTail_fieldAccessorTable;
-
-  public static com.google.protobuf.Descriptors.FileDescriptor
-      getDescriptor() {
-    return descriptor;
-  }
-  private static com.google.protobuf.Descriptors.FileDescriptor
-      descriptor;
-  static {
-    java.lang.String[] descriptorData = {
-      "\n\017orc_proto.proto\022\torc.proto\"B\n\021IntegerS" +
-      "tatistics\022\017\n\007minimum\030\001 \001(\022\022\017\n\007maximum\030\002 " +
-      "\001(\022\022\013\n\003sum\030\003 \001(\022\"A\n\020DoubleStatistics\022\017\n\007" +
-      "minimum\030\001 \001(\001\022\017\n\007maximum\030\002 \001(\001\022\013\n\003sum\030\003 " +
-      "\001(\001\"A\n\020StringStatistics\022\017\n\007minimum\030\001 \001(\t" +
-      "\022\017\n\007maximum\030\002 \001(\t\022\013\n\003sum\030\003 \001(\022\"%\n\020Bucket" +
-      "Statistics\022\021\n\005count\030\001 \003(\004B\002\020\001\"B\n\021Decimal" +
-      "Statistics\022\017\n\007minimum\030\001 \001(\t\022\017\n\007maximum\030\002" +
-      " \001(\t\022\013\n\003sum\030\003 \001(\t\"2\n\016DateStatistics\022\017\n\007m" +
-      "inimum\030\001 \001(\021\022\017\n\007maximum\030\002 \001(\021\"7\n\023Timesta",
-      "mpStatistics\022\017\n\007minimum\030\001 \001(\022\022\017\n\007maximum" +
-      "\030\002 \001(\022\"\037\n\020BinaryStatistics\022\013\n\003sum\030\001 \001(\022\"" +
-      "\365\003\n\020ColumnStatistics\022\026\n\016numberOfValues\030\001" +
-      " \001(\004\0223\n\rintStatistics\030\002 \001(\0132\034.orc.proto." +
-      "IntegerStatistics\0225\n\020doubleStatistics\030\003 " +
-      "\001(\0132\033.orc.proto.DoubleStatistics\0225\n\020stri" +
-      "ngStatistics\030\004 \001(\0132\033.orc.proto.StringSta" +
-      "tistics\0225\n\020bucketStatistics\030\005 \001(\0132\033.orc." +
-      "proto.BucketStatistics\0227\n\021decimalStatist" +
-      "ics\030\006 \001(\0132\034.orc.proto.DecimalStatistics\022",
-      "1\n\016dateStatistics\030\007 \001(\0132\031.orc.proto.Date" +
-      "Statistics\0225\n\020binaryStatistics\030\010 \001(\0132\033.o" +
-      "rc.proto.BinaryStatistics\022;\n\023timestampSt" +
-      "atistics\030\t \001(\0132\036.orc.proto.TimestampStat" +
-      "istics\022\017\n\007hasNull\030\n \001(\010\"W\n\rRowIndexEntry" +
-      "\022\025\n\tpositions\030\001 \003(\004B\002\020\001\022/\n\nstatistics\030\002 " +
-      "\001(\0132\033.orc.proto.ColumnStatistics\"3\n\010RowI" +
-      "ndex\022\'\n\005entry\030\001 \003(\0132\030.orc.proto.RowIndex" +
-      "Entry\"7\n\013BloomFilter\022\030\n\020numHashFunctions" +
-      "\030\001 \001(\r\022\016\n\006bitset\030\002 \003(\006\"?\n\020BloomFilterInd",
-      "ex\022+\n\013bloomFilter\030\001 \003(\0132\026.orc.proto.Bloo" +
-      "mFilter\"\325\001\n\006Stream\022$\n\004kind\030\001 \001(\0162\026.orc.p" +
-      "roto.Stream.Kind\022\016\n\006column\030\002 \001(\r\022\016\n\006leng" +
-      "th\030\003 \001(\004\"\204\001\n\004Kind\022\013\n\007PRESENT\020\000\022\010\n\004DATA\020\001" +
-      "\022\n\n\006LENGTH\020\002\022\023\n\017DICTIONARY_DATA\020\003\022\024\n\020DIC" +
-      "TIONARY_COUNT\020\004\022\r\n\tSECONDARY\020\005\022\r\n\tROW_IN" +
-      "DEX\020\006\022\020\n\014BLOOM_FILTER\020\007\"\234\001\n\016ColumnEncodi" +
-      "ng\022,\n\004kind\030\001 \001(\0162\036.orc.proto.ColumnEncod" +
-      "ing.Kind\022\026\n\016dictionarySize\030\002 \001(\r\"D\n\004Kind" +
-      "\022\n\n\006DIRECT\020\000\022\016\n\nDICTIONARY\020\001\022\r\n\tDIRECT_V",
-      "2\020\002\022\021\n\rDICTIONARY_V2\020\003\"v\n\014StripeFooter\022\"" +
-      "\n\007streams\030\001 \003(\0132\021.orc.proto.Stream\022*\n\007co" +
-      "lumns\030\002 \003(\0132\031.orc.proto.ColumnEncoding\022\026" +
-      "\n\016writerTimezone\030\003 \001(\t\"\341\002\n\004Type\022\"\n\004kind\030" +
-      "\001 \001(\0162\024.orc.proto.Type.Kind\022\024\n\010subtypes\030" +
-      "\002 \003(\rB\002\020\001\022\022\n\nfieldNames\030\003 \003(\t\022\025\n\rmaximum" +
-      "Length\030\004 \001(\r\022\021\n\tprecision\030\005 \001(\r\022\r\n\005scale" +
-      "\030\006 \001(\r\"\321\001\n\004Kind\022\013\n\007BOOLEAN\020\000\022\010\n\004BYTE\020\001\022\t" +
-      "\n\005SHORT\020\002\022\007\n\003INT\020\003\022\010\n\004LONG\020\004\022\t\n\005FLOAT\020\005\022" +
-      "\n\n\006DOUBLE\020\006\022\n\n\006STRING\020\007\022\n\n\006BINARY\020\010\022\r\n\tT",
-      "IMESTAMP\020\t\022\010\n\004LIST\020\n\022\007\n\003MAP\020\013\022\n\n\006STRUCT\020" +
-      "\014\022\t\n\005UNION\020\r\022\013\n\007DECIMAL\020\016\022\010\n\004DATE\020\017\022\013\n\007V" +
-      "ARCHAR\020\020\022\010\n\004CHAR\020\021\"x\n\021StripeInformation\022" +
-      "\016\n\006offset\030\001 \001(\004\022\023\n\013indexLength\030\002 \001(\004\022\022\n\n" +
-      "dataLength\030\003 \001(\004\022\024\n\014footerLength\030\004 \001(\004\022\024" +
-      "\n\014numberOfRows\030\005 \001(\004\"/\n\020UserMetadataItem" +
-      "\022\014\n\004name\030\001 \001(\t\022\r\n\005value\030\002 \001(\014\"A\n\020StripeS" +
-      "tatistics\022-\n\010colStats\030\001 \003(\0132\033.orc.proto." +
-      "ColumnStatistics\"<\n\010Metadata\0220\n\013stripeSt" +
-      "ats\030\001 \003(\0132\033.orc.proto.StripeStatistics\"\222",
-      "\002\n\006Footer\022\024\n\014headerLength\030\001 \001(\004\022\025\n\rconte" +
-      "ntLength\030\002 \001(\004\022-\n\007stripes\030\003 \003(\0132\034.orc.pr" +
-      "oto.StripeInformation\022\036\n\005types\030\004 \003(\0132\017.o" +
-      "rc.proto.Type\022-\n\010metadata\030\005 \003(\0132\033.orc.pr" +
-      "oto.UserMetadataItem\022\024\n\014numberOfRows\030\006 \001" +
-      "(\004\022/\n\nstatistics\030\007 \003(\0132\033.orc.proto.Colum" +
-      "nStatistics\022\026\n\016rowIndexStride\030\010 \001(\r\"\305\001\n\n" +
-      "PostScript\022\024\n\014footerLength\030\001 \001(\004\022/\n\013comp" +
-      "ression\030\002 \001(\0162\032.orc.proto.CompressionKin" +
-      "d\022\034\n\024compressionBlockSize\030\003 \001(\004\022\023\n\007versi",
-      "on\030\004 \003(\rB\002\020\001\022\026\n\016metadataLength\030\005 \001(\004\022\025\n\r" +
-      "writerVersion\030\006 \001(\r\022\016\n\005magic\030\300> \001(\t\"\206\001\n\010" +
-      "FileTail\022)\n\npostscript\030\001 \001(\0132\025.orc.proto" +
-      ".PostScript\022!\n\006footer\030\002 \001(\0132\021.orc.proto." +
-      "Footer\022\022\n\nfileLength\030\003 \001(\004\022\030\n\020postscript" +
-      "Length\030\004 \001(\004*:\n\017CompressionKind\022\010\n\004NONE\020" +
-      "\000\022\010\n\004ZLIB\020\001\022\n\n\006SNAPPY\020\002\022\007\n\003LZO\020\003B\020\n\016org." +
-      "apache.orc"
-    };
-    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
-      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
-        public com.google.protobuf.ExtensionRegistry assignDescriptors(
-            com.google.protobuf.Descriptors.FileDescriptor root) {
-          descriptor = root;
-          internal_static_orc_proto_IntegerStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(0);
-          internal_static_orc_proto_IntegerStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_IntegerStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
-          internal_static_orc_proto_DoubleStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(1);
-          internal_static_orc_proto_DoubleStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_DoubleStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
-          internal_static_orc_proto_StringStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(2);
-          internal_static_orc_proto_StringStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_StringStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
-          internal_static_orc_proto_BucketStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(3);
-          internal_static_orc_proto_BucketStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_BucketStatistics_descriptor,
-              new java.lang.String[] { "Count", });
-          internal_static_orc_proto_DecimalStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(4);
-          internal_static_orc_proto_DecimalStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_DecimalStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", "Sum", });
-          internal_static_orc_proto_DateStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(5);
-          internal_static_orc_proto_DateStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_DateStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", });
-          internal_static_orc_proto_TimestampStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(6);
-          internal_static_orc_proto_TimestampStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_TimestampStatistics_descriptor,
-              new java.lang.String[] { "Minimum", "Maximum", });
-          internal_static_orc_proto_BinaryStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(7);
-          internal_static_orc_proto_BinaryStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_BinaryStatistics_descriptor,
-              new java.lang.String[] { "Sum", });
-          internal_static_orc_proto_ColumnStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(8);
-          internal_static_orc_proto_ColumnStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_ColumnStatistics_descriptor,
-              new java.lang.String[] { "NumberOfValues", "IntStatistics", "DoubleStatistics", "StringStatistics", "BucketStatistics", "DecimalStatistics", "DateStatistics", "BinaryStatistics", "TimestampStatistics", "HasNull", });
-          internal_static_orc_proto_RowIndexEntry_descriptor =
-            getDescriptor().getMessageTypes().get(9);
-          internal_static_orc_proto_RowIndexEntry_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_RowIndexEntry_descriptor,
-              new java.lang.String[] { "Positions", "Statistics", });
-          internal_static_orc_proto_RowIndex_descriptor =
-            getDescriptor().getMessageTypes().get(10);
-          internal_static_orc_proto_RowIndex_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_RowIndex_descriptor,
-              new java.lang.String[] { "Entry", });
-          internal_static_orc_proto_BloomFilter_descriptor =
-            getDescriptor().getMessageTypes().get(11);
-          internal_static_orc_proto_BloomFilter_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_BloomFilter_descriptor,
-              new java.lang.String[] { "NumHashFunctions", "Bitset", });
-          internal_static_orc_proto_BloomFilterIndex_descriptor =
-            getDescriptor().getMessageTypes().get(12);
-          internal_static_orc_proto_BloomFilterIndex_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_BloomFilterIndex_descriptor,
-              new java.lang.String[] { "BloomFilter", });
-          internal_static_orc_proto_Stream_descriptor =
-            getDescriptor().getMessageTypes().get(13);
-          internal_static_orc_proto_Stream_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_Stream_descriptor,
-              new java.lang.String[] { "Kind", "Column", "Length", });
-          internal_static_orc_proto_ColumnEncoding_descriptor =
-            getDescriptor().getMessageTypes().get(14);
-          internal_static_orc_proto_ColumnEncoding_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_ColumnEncoding_descriptor,
-              new java.lang.String[] { "Kind", "DictionarySize", });
-          internal_static_orc_proto_StripeFooter_descriptor =
-            getDescriptor().getMessageTypes().get(15);
-          internal_static_orc_proto_StripeFooter_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_StripeFooter_descriptor,
-              new java.lang.String[] { "Streams", "Columns", "WriterTimezone", });
-          internal_static_orc_proto_Type_descriptor =
-            getDescriptor().getMessageTypes().get(16);
-          internal_static_orc_proto_Type_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_Type_descriptor,
-              new java.lang.String[] { "Kind", "Subtypes", "FieldNames", "MaximumLength", "Precision", "Scale", });
-          internal_static_orc_proto_StripeInformation_descriptor =
-            getDescriptor().getMessageTypes().get(17);
-          internal_static_orc_proto_StripeInformation_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_StripeInformation_descriptor,
-              new java.lang.String[] { "Offset", "IndexLength", "DataLength", "FooterLength", "NumberOfRows", });
-          internal_static_orc_proto_UserMetadataItem_descriptor =
-            getDescriptor().getMessageTypes().get(18);
-          internal_static_orc_proto_UserMetadataItem_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_UserMetadataItem_descriptor,
-              new java.lang.String[] { "Name", "Value", });
-          internal_static_orc_proto_StripeStatistics_descriptor =
-            getDescriptor().getMessageTypes().get(19);
-          internal_static_orc_proto_StripeStatistics_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_StripeStatistics_descriptor,
-              new java.lang.String[] { "ColStats", });
-          internal_static_orc_proto_Metadata_descriptor =
-            getDescriptor().getMessageTypes().get(20);
-          internal_static_orc_proto_Metadata_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_Metadata_descriptor,
-              new java.lang.String[] { "StripeStats", });
-          internal_static_orc_proto_Footer_descriptor =
-            getDescriptor().getMessageTypes().get(21);
-          internal_static_orc_proto_Footer_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_Footer_descriptor,
-              new java.lang.String[] { "HeaderLength", "ContentLength", "Stripes", "Types", "Metadata", "NumberOfRows", "Statistics", "RowIndexStride", });
-          internal_static_orc_proto_PostScript_descriptor =
-            getDescriptor().getMessageTypes().get(22);
-          internal_static_orc_proto_PostScript_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_PostScript_descriptor,
-              new java.lang.String[] { "FooterLength", "Compression", "CompressionBlockSize", "Version", "MetadataLength", "WriterVersion", "Magic", });
-          internal_static_orc_proto_FileTail_descriptor =
-            getDescriptor().getMessageTypes().get(23);
-          internal_static_orc_proto_FileTail_fieldAccessorTable = new
-            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
-              internal_static_orc_proto_FileTail_descriptor,
-              new java.lang.String[] { "Postscript", "Footer", "FileLength", "PostscriptLength", });
-          return null;
-        }
-      };
-    com.google.protobuf.Descriptors.FileDescriptor
-      .internalBuildGeneratedFileFrom(descriptorData,
-        new com.google.protobuf.Descriptors.FileDescriptor[] {
-        }, assigner);
-  }
-
-  // @@protoc_insertion_point(outer_class_scope)
-}
diff --git a/orc/src/java/org/apache/orc/BinaryColumnStatistics.java b/orc/src/java/org/apache/orc/BinaryColumnStatistics.java
deleted file mode 100644
index 19db98aff6..0000000000
--- a/orc/src/java/org/apache/orc/BinaryColumnStatistics.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.orc.ColumnStatistics;
-
-/**
- * Statistics for binary columns.
- */
-public interface BinaryColumnStatistics extends ColumnStatistics {
-  long getSum();
-}
diff --git a/orc/src/java/org/apache/orc/BloomFilterIO.java b/orc/src/java/org/apache/orc/BloomFilterIO.java
deleted file mode 100644
index 14062663f4..0000000000
--- a/orc/src/java/org/apache/orc/BloomFilterIO.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import org.apache.hive.common.util.BloomFilter;
-
-import com.google.common.primitives.Longs;
-
-public class BloomFilterIO extends BloomFilter {
-
-  public BloomFilterIO(long expectedEntries) {
-    super(expectedEntries, DEFAULT_FPP);
-  }
-
-  public BloomFilterIO(long expectedEntries, double fpp) {
-    super(expectedEntries, fpp);
-  }
-
-/**
- * Initializes the BloomFilter from the given Orc BloomFilter
- */
-  public BloomFilterIO(OrcProto.BloomFilter bloomFilter) {
-    this.bitSet = new BitSet(Longs.toArray(bloomFilter.getBitsetList()));
-    this.numHashFunctions = bloomFilter.getNumHashFunctions();
-    this.numBits = (int) this.bitSet.bitSize();
-  }
-}
diff --git a/orc/src/java/org/apache/orc/BooleanColumnStatistics.java b/orc/src/java/org/apache/orc/BooleanColumnStatistics.java
deleted file mode 100644
index af08f06cc2..0000000000
--- a/orc/src/java/org/apache/orc/BooleanColumnStatistics.java
+++ /dev/null
@@ -1,29 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.orc.ColumnStatistics;
-
-/**
- * Statistics for boolean columns.
- */
-public interface BooleanColumnStatistics extends ColumnStatistics {
-  long getFalseCount();
-
-  long getTrueCount();
-}
diff --git a/orc/src/java/org/apache/orc/ColumnStatistics.java b/orc/src/java/org/apache/orc/ColumnStatistics.java
deleted file mode 100644
index 72d8fbfa11..0000000000
--- a/orc/src/java/org/apache/orc/ColumnStatistics.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-/**
- * Statistics that are available for all types of columns.
- */
-public interface ColumnStatistics {
-  /**
-   * Get the number of values in this column. It will differ from the number
-   * of rows because of NULL values and repeated values.
-   * @return the number of values
-   */
-  long getNumberOfValues();
-
-  /**
-   * Returns true if there are nulls in the scope of column statistics.
-   * @return true if null present else false
-   */
-  boolean hasNull();
-}
diff --git a/orc/src/java/org/apache/orc/CompressionCodec.java b/orc/src/java/org/apache/orc/CompressionCodec.java
deleted file mode 100644
index 342196942d..0000000000
--- a/orc/src/java/org/apache/orc/CompressionCodec.java
+++ /dev/null
@@ -1,69 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.EnumSet;
-
-import javax.annotation.Nullable;
-
-public interface CompressionCodec {
-
-  enum Modifier {
-    /* speed/compression tradeoffs */
-    FASTEST,
-    FAST,
-    DEFAULT,
-    /* data sensitivity modifiers */
-    TEXT,
-    BINARY
-  };
-
-  /**
-   * Compress the in buffer to the out buffer.
-   * @param in the bytes to compress
-   * @param out the uncompressed bytes
-   * @param overflow put any additional bytes here
-   * @return true if the output is smaller than input
-   * @throws IOException
-   */
-  boolean compress(ByteBuffer in, ByteBuffer out, ByteBuffer overflow
-                  ) throws IOException;
-
-  /**
-   * Decompress the in buffer to the out buffer.
-   * @param in the bytes to decompress
-   * @param out the decompressed bytes
-   * @throws IOException
-   */
-  void decompress(ByteBuffer in, ByteBuffer out) throws IOException;
-
-  /**
-   * Produce a modified compression codec if the underlying algorithm allows
-   * modification.
-   *
-   * This does not modify the current object, but returns a new object if
-   * modifications are possible. Returns the same object if no modifications
-   * are possible.
-   * @param modifiers compression modifiers
-   * @return codec for use after optional modification
-   */
-  CompressionCodec modify(@Nullable EnumSet<Modifier> modifiers);
-
-}
diff --git a/orc/src/java/org/apache/orc/CompressionKind.java b/orc/src/java/org/apache/orc/CompressionKind.java
deleted file mode 100644
index f684bef817..0000000000
--- a/orc/src/java/org/apache/orc/CompressionKind.java
+++ /dev/null
@@ -1,27 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-/**
- * An enumeration that lists the generic compression algorithms that
- * can be applied to ORC files.
- */
-public enum CompressionKind {
-  NONE, ZLIB, SNAPPY, LZO
-}
diff --git a/orc/src/java/org/apache/orc/DataReader.java b/orc/src/java/org/apache/orc/DataReader.java
deleted file mode 100644
index a5dbb7678e..0000000000
--- a/orc/src/java/org/apache/orc/DataReader.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-import org.apache.hadoop.hive.common.io.DiskRangeList;
-import org.apache.orc.impl.OrcIndex;
-
-/** An abstract data reader that IO formats can use to read bytes from underlying storage. */
-public interface DataReader extends AutoCloseable {
-
-  /** Opens the DataReader, making it ready to use. */
-  void open() throws IOException;
-
-  OrcIndex readRowIndex(StripeInformation stripe,
-                        OrcProto.StripeFooter footer,
-                        boolean[] included, OrcProto.RowIndex[] indexes,
-                        boolean[] sargColumns,
-                        OrcProto.BloomFilterIndex[] bloomFilterIndices
-                        ) throws IOException;
-
-  OrcProto.StripeFooter readStripeFooter(StripeInformation stripe) throws IOException;
-
-  /** Reads the data.
-   *
-   * Note that for the cases such as zero-copy read, caller must release the disk ranges
-   * produced after being done with them. Call isTrackingDiskRanges to find out if this is needed.
-   * @param range List if disk ranges to read. Ranges with data will be ignored.
-   * @param baseOffset Base offset from the start of the file of the ranges in disk range list.
-   * @param doForceDirect Whether the data should be read into direct buffers.
-   * @return New or modified list of DiskRange-s, where all the ranges are filled with data.
-   */
-  DiskRangeList readFileData(
-      DiskRangeList range, long baseOffset, boolean doForceDirect) throws IOException;
-
-
-  /**
-   * Whether the user should release buffers created by readFileData. See readFileData javadoc.
-   */
-  boolean isTrackingDiskRanges();
-
-  /**
-   * Releases buffers created by readFileData. See readFileData javadoc.
-   * @param toRelease The buffer to release.
-   */
-  void releaseBuffer(ByteBuffer toRelease);
-
-  /**
-   * Clone the entire state of the DataReader with the assumption that the
-   * clone will be closed at a different time. Thus, any file handles in the
-   * implementation need to be cloned.
-   * @return a new instance
-   */
-  DataReader clone();
-
-  @Override
-  public void close() throws IOException;
-}
diff --git a/orc/src/java/org/apache/orc/DateColumnStatistics.java b/orc/src/java/org/apache/orc/DateColumnStatistics.java
deleted file mode 100644
index cdd01af588..0000000000
--- a/orc/src/java/org/apache/orc/DateColumnStatistics.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.orc.ColumnStatistics;
-
-import java.util.Date;
-
-/**
- * Statistics for DATE columns.
- */
-public interface DateColumnStatistics extends ColumnStatistics {
-  /**
-   * Get the minimum value for the column.
-   * @return minimum value
-   */
-  Date getMinimum();
-
-  /**
-   * Get the maximum value for the column.
-   * @return maximum value
-   */
-  Date getMaximum();
-}
diff --git a/orc/src/java/org/apache/orc/DecimalColumnStatistics.java b/orc/src/java/org/apache/orc/DecimalColumnStatistics.java
deleted file mode 100644
index 51b6d7d6f6..0000000000
--- a/orc/src/java/org/apache/orc/DecimalColumnStatistics.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.orc.ColumnStatistics;
-
-/**
- * Statistics for decimal columns.
- */
-public interface DecimalColumnStatistics extends ColumnStatistics {
-
-  /**
-   * Get the minimum value for the column.
-   * @return the minimum value
-   */
-  HiveDecimal getMinimum();
-
-  /**
-   * Get the maximum value for the column.
-   * @return the maximum value
-   */
-  HiveDecimal getMaximum();
-
-  /**
-   * Get the sum of the values of the column.
-   * @return the sum
-   */
-  HiveDecimal getSum();
-
-}
diff --git a/orc/src/java/org/apache/orc/DoubleColumnStatistics.java b/orc/src/java/org/apache/orc/DoubleColumnStatistics.java
deleted file mode 100644
index 00c728fc8b..0000000000
--- a/orc/src/java/org/apache/orc/DoubleColumnStatistics.java
+++ /dev/null
@@ -1,46 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.orc.ColumnStatistics;
-
-/**
- * Statistics for float and double columns.
- */
-public interface DoubleColumnStatistics extends ColumnStatistics {
-
-  /**
-   * Get the smallest value in the column. Only defined if getNumberOfValues
-   * is non-zero.
-   * @return the minimum
-   */
-  double getMinimum();
-
-  /**
-   * Get the largest value in the column. Only defined if getNumberOfValues
-   * is non-zero.
-   * @return the maximum
-   */
-  double getMaximum();
-
-  /**
-   * Get the sum of the values in the column.
-   * @return the sum
-   */
-  double getSum();
-}
diff --git a/orc/src/java/org/apache/orc/FileFormatException.java b/orc/src/java/org/apache/orc/FileFormatException.java
deleted file mode 100644
index 2cebea78f9..0000000000
--- a/orc/src/java/org/apache/orc/FileFormatException.java
+++ /dev/null
@@ -1,30 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import java.io.IOException;
-
-/**
- * Thrown when an invalid file format is encountered.
- */
-public class FileFormatException extends IOException {
-
-  public FileFormatException(String errMsg) {
-    super(errMsg);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/FileMetadata.java b/orc/src/java/org/apache/orc/FileMetadata.java
deleted file mode 100644
index 807e696ad2..0000000000
--- a/orc/src/java/org/apache/orc/FileMetadata.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import java.util.List;
-
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcProto;
-import org.apache.orc.StripeInformation;
-
-/**
- * Cached file metadata. Right now, it caches everything; we don't have to store all the
- * protobuf structs actually, we could just store what we need, but that would require that
- * ORC stop depending on them too. Luckily, they shouldn't be very big.
- */
-public interface FileMetadata {
-  boolean isOriginalFormat();
-
-  List<StripeInformation> getStripes();
-
-  CompressionKind getCompressionKind();
-
-  int getCompressionBufferSize();
-
-  int getRowIndexStride();
-
-  int getColumnCount();
-
-  int getFlattenedColumnCount();
-
-  Object getFileKey();
-
-  List<Integer> getVersionList();
-
-  int getMetadataSize();
-
-  int getWriterVersionNum();
-
-  List<OrcProto.Type> getTypes();
-
-  List<OrcProto.StripeStatistics> getStripeStats();
-
-  long getContentLength();
-
-  long getNumberOfRows();
-
-  List<OrcProto.ColumnStatistics> getFileStats();
-}
\ No newline at end of file
diff --git a/orc/src/java/org/apache/orc/IntegerColumnStatistics.java b/orc/src/java/org/apache/orc/IntegerColumnStatistics.java
deleted file mode 100644
index 1a162ffbcd..0000000000
--- a/orc/src/java/org/apache/orc/IntegerColumnStatistics.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.orc.ColumnStatistics;
-
-/**
- * Statistics for all of the integer columns, such as byte, short, int, and
- * long.
- */
-public interface IntegerColumnStatistics extends ColumnStatistics {
-  /**
-   * Get the smallest value in the column. Only defined if getNumberOfValues
-   * is non-zero.
-   * @return the minimum
-   */
-  long getMinimum();
-
-  /**
-   * Get the largest value in the column. Only defined if getNumberOfValues
-   * is non-zero.
-   * @return the maximum
-   */
-  long getMaximum();
-
-  /**
-   * Is the sum defined? If the sum overflowed the counter this will be false.
-   * @return is the sum available
-   */
-  boolean isSumDefined();
-
-  /**
-   * Get the sum of the column. Only valid if isSumDefined returns true.
-   * @return the sum of the column
-   */
-  long getSum();
-}
diff --git a/orc/src/java/org/apache/orc/OrcConf.java b/orc/src/java/org/apache/orc/OrcConf.java
deleted file mode 100644
index 357318d739..0000000000
--- a/orc/src/java/org/apache/orc/OrcConf.java
+++ /dev/null
@@ -1,193 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import org.apache.hadoop.conf.Configuration;
-
-import java.util.Properties;
-
-/**
- * Define the configuration properties that Orc understands.
- */
-public enum OrcConf {
-  STRIPE_SIZE("orc.stripe.size", "hive.exec.orc.default.stripe.size",
-      64L * 1024 * 1024,
-      "Define the default ORC stripe size, in bytes."),
-  BLOCK_SIZE("orc.block.size", "hive.exec.orc.default.block.size",
-      256L * 1024 * 1024,
-      "Define the default file system block size for ORC files."),
-  ENABLE_INDEXES("orc.create.index", "orc.create.index", true,
-      "Should the ORC writer create indexes as part of the file."),
-  ROW_INDEX_STRIDE("orc.row.index.stride",
-      "hive.exec.orc.default.row.index.stride", 10000,
-      "Define the default ORC index stride in number of rows. (Stride is the\n"+
-          " number of rows n index entry represents.)"),
-  BUFFER_SIZE("orc.compress.size", "hive.exec.orc.default.buffer.size",
-      256 * 1024, "Define the default ORC buffer size, in bytes."),
-  BASE_DELTA_RATIO("orc.base.delta.ratio", "hive.exec.orc.base.delta.ratio", 8,
-      "The ratio of base writer and delta writer in terms of STRIPE_SIZE and BUFFER_SIZE."),
-  BLOCK_PADDING("orc.block.padding", "hive.exec.orc.default.block.padding",
-      true,
-      "Define whether stripes should be padded to the HDFS block boundaries."),
-  COMPRESS("orc.compress", "hive.exec.orc.default.compress", "ZLIB",
-      "Define the default compression codec for ORC file"),
-  WRITE_FORMAT("orc.write.format", "hive.exec.orc.write.format", "0.12",
-      "Define the version of the file to write. Possible values are 0.11 and\n"+
-          " 0.12. If this parameter is not defined, ORC will use the run\n" +
-          " length encoding (RLE) introduced in Hive 0.12."),
-  ENCODING_STRATEGY("orc.encoding.strategy", "hive.exec.orc.encoding.strategy",
-      "SPEED",
-      "Define the encoding strategy to use while writing data. Changing this\n"+
-          "will only affect the light weight encoding for integers. This\n" +
-          "flag will not change the compression level of higher level\n" +
-          "compression codec (like ZLIB)."),
-  COMPRESSION_STRATEGY("orc.compression.strategy",
-      "hive.exec.orc.compression.strategy", "SPEED",
-      "Define the compression strategy to use while writing data.\n" +
-          "This changes the compression level of higher level compression\n" +
-          "codec (like ZLIB)."),
-  BLOCK_PADDING_TOLERANCE("orc.block.padding.tolerance",
-      "hive.exec.orc.block.padding.tolerance", 0.05,
-      "Define the tolerance for block padding as a decimal fraction of\n" +
-          "stripe size (for example, the default value 0.05 is 5% of the\n" +
-          "stripe size). For the defaults of 64Mb ORC stripe and 256Mb HDFS\n" +
-          "blocks, the default block padding tolerance of 5% will\n" +
-          "reserve a maximum of 3.2Mb for padding within the 256Mb block.\n" +
-          "In that case, if the available size within the block is more than\n"+
-          "3.2Mb, a new smaller stripe will be inserted to fit within that\n" +
-          "space. This will make sure that no stripe written will block\n" +
-          " boundaries and cause remote reads within a node local task."),
-  BLOOM_FILTER_FPP("orc.bloom.filter.fpp", "orc.default.bloom.fpp", 0.05,
-      "Define the default false positive probability for bloom filters."),
-  USE_ZEROCOPY("orc.use.zerocopy", "hive.exec.orc.zerocopy", false,
-      "Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)"),
-  SKIP_CORRUPT_DATA("orc.skip.corrupt.data", "hive.exec.orc.skip.corrupt.data",
-      false,
-      "If ORC reader encounters corrupt data, this value will be used to\n" +
-          "determine whether to skip the corrupt data or throw exception.\n" +
-          "The default behavior is to throw exception."),
-  MEMORY_POOL("orc.memory.pool", "hive.exec.orc.memory.pool", 0.5,
-      "Maximum fraction of heap that can be used by ORC file writers"),
-  DICTIONARY_KEY_SIZE_THRESHOLD("orc.dictionary.key.threshold",
-      "hive.exec.orc.dictionary.key.size.threshold",
-      0.8,
-      "If the number of distinct keys in a dictionary is greater than this\n" +
-          "fraction of the total number of non-null rows, turn off \n" +
-          "dictionary encoding.  Use 1 to always use dictionary encoding."),
-  ROW_INDEX_STRIDE_DICTIONARY_CHECK("orc.dictionary.early.check",
-      "hive.orc.row.index.stride.dictionary.check",
-      true,
-      "If enabled dictionary check will happen after first row index stride\n" +
-          "(default 10000 rows) else dictionary check will happen before\n" +
-          "writing first stripe. In both cases, the decision to use\n" +
-          "dictionary or not will be retained thereafter."),
-  BLOOM_FILTER_COLUMNS("orc.bloom.filter.columns", "orc.bloom.filter.columns",
-      "", "List of columns to create bloom filters for when writing.")
-  ;
-
-  private final String attribute;
-  private final String hiveConfName;
-  private final Object defaultValue;
-  private final String description;
-
-  OrcConf(String attribute,
-          String hiveConfName,
-          Object defaultValue,
-          String description) {
-    this.attribute = attribute;
-    this.hiveConfName = hiveConfName;
-    this.defaultValue = defaultValue;
-    this.description = description;
-  }
-
-  public String getAttribute() {
-    return attribute;
-  }
-
-  public String getHiveConfName() {
-    return hiveConfName;
-  }
-
-  public Object getDefaultValue() {
-    return defaultValue;
-  }
-
-  public String getDescription() {
-    return description;
-  }
-
-  private String lookupValue(Properties tbl, Configuration conf) {
-    String result = null;
-    if (tbl != null) {
-      result = tbl.getProperty(attribute);
-    }
-    if (result == null && conf != null) {
-      result = conf.get(attribute);
-      if (result == null) {
-        result = conf.get(hiveConfName);
-      }
-    }
-    return result;
-  }
-
-  public long getLong(Properties tbl, Configuration conf) {
-    String value = lookupValue(tbl, conf);
-    if (value != null) {
-      return Long.parseLong(value);
-    }
-    return ((Number) defaultValue).longValue();
-  }
-
-  public long getLong(Configuration conf) {
-    return getLong(null, conf);
-  }
-
-  public String getString(Properties tbl, Configuration conf) {
-    String value = lookupValue(tbl, conf);
-    return value == null ? (String) defaultValue : value;
-  }
-
-  public String getString(Configuration conf) {
-    return getString(null, conf);
-  }
-
-  public boolean getBoolean(Properties tbl, Configuration conf) {
-    String value = lookupValue(tbl, conf);
-    if (value != null) {
-      return Boolean.parseBoolean(value);
-    }
-    return (Boolean) defaultValue;
-  }
-
-  public boolean getBoolean(Configuration conf) {
-    return getBoolean(null, conf);
-  }
-
-  public double getDouble(Properties tbl, Configuration conf) {
-    String value = lookupValue(tbl, conf);
-    if (value != null) {
-      return Double.parseDouble(value);
-    }
-    return ((Number) defaultValue).doubleValue();
-  }
-
-  public double getDouble(Configuration conf) {
-    return getDouble(null, conf);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/OrcFile.java b/orc/src/java/org/apache/orc/OrcFile.java
deleted file mode 100644
index ddfa9f7a7d..0000000000
--- a/orc/src/java/org/apache/orc/OrcFile.java
+++ /dev/null
@@ -1,566 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import java.io.IOException;
-import java.util.Properties;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.impl.MemoryManager;
-import org.apache.orc.impl.OrcTail;
-import org.apache.orc.impl.ReaderImpl;
-import org.apache.orc.impl.WriterImpl;
-
-/**
- * Contains factory methods to read or write ORC files.
- */
-public class OrcFile {
-  public static final String MAGIC = "ORC";
-
-  /**
-   * Create a version number for the ORC file format, so that we can add
-   * non-forward compatible changes in the future. To make it easier for users
-   * to understand the version numbers, we use the Hive release number that
-   * first wrote that version of ORC files.
-   *
-   * Thus, if you add new encodings or other non-forward compatible changes
-   * to ORC files, which prevent the old reader from reading the new format,
-   * you should change these variable to reflect the next Hive release number.
-   * Non-forward compatible changes should never be added in patch releases.
-   *
-   * Do not make any changes that break backwards compatibility, which would
-   * prevent the new reader from reading ORC files generated by any released
-   * version of Hive.
-   */
-  public enum Version {
-    V_0_11("0.11", 0, 11),
-    V_0_12("0.12", 0, 12);
-
-    public static final Version CURRENT = V_0_12;
-
-    private final String name;
-    private final int major;
-    private final int minor;
-
-    Version(String name, int major, int minor) {
-      this.name = name;
-      this.major = major;
-      this.minor = minor;
-    }
-
-    public static Version byName(String name) {
-      for(Version version: values()) {
-        if (version.name.equals(name)) {
-          return version;
-        }
-      }
-      throw new IllegalArgumentException("Unknown ORC version " + name);
-    }
-
-    /**
-     * Get the human readable name for the version.
-     */
-    public String getName() {
-      return name;
-    }
-
-    /**
-     * Get the major version number.
-     */
-    public int getMajor() {
-      return major;
-    }
-
-    /**
-     * Get the minor version number.
-     */
-    public int getMinor() {
-      return minor;
-    }
-  }
-
-  /**
-   * Records the version of the writer in terms of which bugs have been fixed.
-   * For bugs in the writer, but the old readers already read the new data
-   * correctly, bump this version instead of the Version.
-   */
-  public enum WriterVersion {
-    ORIGINAL(0),
-    HIVE_8732(1), // corrupted stripe/file maximum column statistics
-    HIVE_4243(2), // use real column names from Hive tables
-    HIVE_12055(3), // vectorized writer
-    HIVE_13083(4), // decimal writer updating present stream wrongly
-
-    // Don't use any magic numbers here except for the below:
-    FUTURE(Integer.MAX_VALUE); // a version from a future writer
-
-    private final int id;
-
-    public int getId() {
-      return id;
-    }
-
-    WriterVersion(int id) {
-      this.id = id;
-    }
-
-    private static final WriterVersion[] values;
-    static {
-      // Assumes few non-negative values close to zero.
-      int max = Integer.MIN_VALUE;
-      for (WriterVersion v : WriterVersion.values()) {
-        if (v.id < 0) throw new AssertionError();
-        if (v.id > max && FUTURE.id != v.id) {
-          max = v.id;
-        }
-      }
-      values = new WriterVersion[max + 1];
-      for (WriterVersion v : WriterVersion.values()) {
-        if (v.id < values.length) {
-          values[v.id] = v;
-        }
-      }
-    }
-
-    public static WriterVersion from(int val) {
-      if (val == FUTURE.id) return FUTURE; // Special handling for the magic value.
-      return values[val];
-    }
-  }
-  public static final WriterVersion CURRENT_WRITER = WriterVersion.HIVE_13083;
-
-  public enum EncodingStrategy {
-    SPEED, COMPRESSION
-  }
-
-  public enum CompressionStrategy {
-    SPEED, COMPRESSION
-  }
-
-  // unused
-  protected OrcFile() {}
-
-  public static class ReaderOptions {
-    private final Configuration conf;
-    private FileSystem filesystem;
-    private long maxLength = Long.MAX_VALUE;
-    private OrcTail orcTail;
-    // TODO: We can generalize FileMetada interface. Make OrcTail implement FileMetadata interface
-    // and remove this class altogether. Both footer caching and llap caching just needs OrcTail.
-    // For now keeping this around to avoid complex surgery
-    private FileMetadata fileMetadata;
-
-    public ReaderOptions(Configuration conf) {
-      this.conf = conf;
-    }
-
-    public ReaderOptions filesystem(FileSystem fs) {
-      this.filesystem = fs;
-      return this;
-    }
-
-    public ReaderOptions maxLength(long val) {
-      maxLength = val;
-      return this;
-    }
-
-    public ReaderOptions orcTail(OrcTail tail) {
-      this.orcTail = tail;
-      return this;
-    }
-
-    public Configuration getConfiguration() {
-      return conf;
-    }
-
-    public FileSystem getFilesystem() {
-      return filesystem;
-    }
-
-    public long getMaxLength() {
-      return maxLength;
-    }
-
-    public OrcTail getOrcTail() {
-      return orcTail;
-    }
-
-    public ReaderOptions fileMetadata(final FileMetadata metadata) {
-      fileMetadata = metadata;
-      return this;
-    }
-
-    public FileMetadata getFileMetadata() {
-      return fileMetadata;
-    }
-  }
-
-  public static ReaderOptions readerOptions(Configuration conf) {
-    return new ReaderOptions(conf);
-  }
-
-  public static Reader createReader(Path path,
-                                    ReaderOptions options) throws IOException {
-    return new ReaderImpl(path, options);
-  }
-
-  public interface WriterContext {
-    Writer getWriter();
-  }
-
-  public interface WriterCallback {
-    void preStripeWrite(WriterContext context) throws IOException;
-    void preFooterWrite(WriterContext context) throws IOException;
-  }
-
-  /**
-   * Options for creating ORC file writers.
-   */
-  public static class WriterOptions {
-    private final Configuration configuration;
-    private FileSystem fileSystemValue = null;
-    private TypeDescription schema = null;
-    private long stripeSizeValue;
-    private long blockSizeValue;
-    private int rowIndexStrideValue;
-    private int bufferSizeValue;
-    private boolean enforceBufferSize = false;
-    private boolean blockPaddingValue;
-    private CompressionKind compressValue;
-    private MemoryManager memoryManagerValue;
-    private Version versionValue;
-    private WriterCallback callback;
-    private EncodingStrategy encodingStrategy;
-    private CompressionStrategy compressionStrategy;
-    private double paddingTolerance;
-    private String bloomFilterColumns;
-    private double bloomFilterFpp;
-
-    protected WriterOptions(Properties tableProperties, Configuration conf) {
-      configuration = conf;
-      memoryManagerValue = getStaticMemoryManager(conf);
-      stripeSizeValue = OrcConf.STRIPE_SIZE.getLong(tableProperties, conf);
-      blockSizeValue = OrcConf.BLOCK_SIZE.getLong(tableProperties, conf);
-      rowIndexStrideValue =
-          (int) OrcConf.ROW_INDEX_STRIDE.getLong(tableProperties, conf);
-      bufferSizeValue = (int) OrcConf.BUFFER_SIZE.getLong(tableProperties,
-          conf);
-      blockPaddingValue =
-          OrcConf.BLOCK_PADDING.getBoolean(tableProperties, conf);
-      compressValue =
-          CompressionKind.valueOf(OrcConf.COMPRESS.getString(tableProperties,
-              conf).toUpperCase());
-      String versionName = OrcConf.WRITE_FORMAT.getString(tableProperties,
-          conf);
-      versionValue = Version.byName(versionName);
-      String enString = OrcConf.ENCODING_STRATEGY.getString(tableProperties,
-          conf);
-      encodingStrategy = EncodingStrategy.valueOf(enString);
-
-      String compString =
-          OrcConf.COMPRESSION_STRATEGY.getString(tableProperties, conf);
-      compressionStrategy = CompressionStrategy.valueOf(compString);
-
-      paddingTolerance =
-          OrcConf.BLOCK_PADDING_TOLERANCE.getDouble(tableProperties, conf);
-
-      bloomFilterColumns = OrcConf.BLOOM_FILTER_COLUMNS.getString(tableProperties,
-          conf);
-      bloomFilterFpp = OrcConf.BLOOM_FILTER_FPP.getDouble(tableProperties,
-          conf);
-    }
-
-    /**
-     * Provide the filesystem for the path, if the client has it available.
-     * If it is not provided, it will be found from the path.
-     */
-    public WriterOptions fileSystem(FileSystem value) {
-      fileSystemValue = value;
-      return this;
-    }
-
-    /**
-     * Set the stripe size for the file. The writer stores the contents of the
-     * stripe in memory until this memory limit is reached and the stripe
-     * is flushed to the HDFS file and the next stripe started.
-     */
-    public WriterOptions stripeSize(long value) {
-      stripeSizeValue = value;
-      return this;
-    }
-
-    /**
-     * Set the file system block size for the file. For optimal performance,
-     * set the block size to be multiple factors of stripe size.
-     */
-    public WriterOptions blockSize(long value) {
-      blockSizeValue = value;
-      return this;
-    }
-
-    /**
-     * Set the distance between entries in the row index. The minimum value is
-     * 1000 to prevent the index from overwhelming the data. If the stride is
-     * set to 0, no indexes will be included in the file.
-     */
-    public WriterOptions rowIndexStride(int value) {
-      rowIndexStrideValue = value;
-      return this;
-    }
-
-    /**
-     * The size of the memory buffers used for compressing and storing the
-     * stripe in memory. NOTE: ORC writer may choose to use smaller buffer
-     * size based on stripe size and number of columns for efficient stripe
-     * writing and memory utilization. To enforce writer to use the requested
-     * buffer size use enforceBufferSize().
-     */
-    public WriterOptions bufferSize(int value) {
-      bufferSizeValue = value;
-      return this;
-    }
-
-    /**
-     * Enforce writer to use requested buffer size instead of estimating
-     * buffer size based on stripe size and number of columns.
-     * See bufferSize() method for more info.
-     * Default: false
-     */
-    public WriterOptions enforceBufferSize() {
-      enforceBufferSize = true;
-      return this;
-    }
-
-    /**
-     * Sets whether the HDFS blocks are padded to prevent stripes from
-     * straddling blocks. Padding improves locality and thus the speed of
-     * reading, but costs space.
-     */
-    public WriterOptions blockPadding(boolean value) {
-      blockPaddingValue = value;
-      return this;
-    }
-
-    /**
-     * Sets the encoding strategy that is used to encode the data.
-     */
-    public WriterOptions encodingStrategy(EncodingStrategy strategy) {
-      encodingStrategy = strategy;
-      return this;
-    }
-
-    /**
-     * Sets the tolerance for block padding as a percentage of stripe size.
-     */
-    public WriterOptions paddingTolerance(double value) {
-      paddingTolerance = value;
-      return this;
-    }
-
-    /**
-     * Comma separated values of column names for which bloom filter is to be created.
-     */
-    public WriterOptions bloomFilterColumns(String columns) {
-      bloomFilterColumns = columns;
-      return this;
-    }
-
-    /**
-     * Specify the false positive probability for bloom filter.
-     * @param fpp - false positive probability
-     * @return this
-     */
-    public WriterOptions bloomFilterFpp(double fpp) {
-      bloomFilterFpp = fpp;
-      return this;
-    }
-
-    /**
-     * Sets the generic compression that is used to compress the data.
-     */
-    public WriterOptions compress(CompressionKind value) {
-      compressValue = value;
-      return this;
-    }
-
-    /**
-     * Set the schema for the file. This is a required parameter.
-     * @param schema the schema for the file.
-     * @return this
-     */
-    public WriterOptions setSchema(TypeDescription schema) {
-      this.schema = schema;
-      return this;
-    }
-
-    /**
-     * Sets the version of the file that will be written.
-     */
-    public WriterOptions version(Version value) {
-      versionValue = value;
-      return this;
-    }
-
-    /**
-     * Add a listener for when the stripe and file are about to be closed.
-     * @param callback the object to be called when the stripe is closed
-     * @return this
-     */
-    public WriterOptions callback(WriterCallback callback) {
-      this.callback = callback;
-      return this;
-    }
-
-    /**
-     * A package local option to set the memory manager.
-     */
-    protected WriterOptions memory(MemoryManager value) {
-      memoryManagerValue = value;
-      return this;
-    }
-
-    public boolean getBlockPadding() {
-      return blockPaddingValue;
-    }
-
-    public long getBlockSize() {
-      return blockSizeValue;
-    }
-
-    public String getBloomFilterColumns() {
-      return bloomFilterColumns;
-    }
-
-    public FileSystem getFileSystem() {
-      return fileSystemValue;
-    }
-
-    public Configuration getConfiguration() {
-      return configuration;
-    }
-
-    public TypeDescription getSchema() {
-      return schema;
-    }
-
-    public long getStripeSize() {
-      return stripeSizeValue;
-    }
-
-    public CompressionKind getCompress() {
-      return compressValue;
-    }
-
-    public WriterCallback getCallback() {
-      return callback;
-    }
-
-    public Version getVersion() {
-      return versionValue;
-    }
-
-    public MemoryManager getMemoryManager() {
-      return memoryManagerValue;
-    }
-
-    public int getBufferSize() {
-      return bufferSizeValue;
-    }
-
-    public boolean isEnforceBufferSize() {
-      return enforceBufferSize;
-    }
-
-    public int getRowIndexStride() {
-      return rowIndexStrideValue;
-    }
-
-    public CompressionStrategy getCompressionStrategy() {
-      return compressionStrategy;
-    }
-
-    public EncodingStrategy getEncodingStrategy() {
-      return encodingStrategy;
-    }
-
-    public double getPaddingTolerance() {
-      return paddingTolerance;
-    }
-
-    public double getBloomFilterFpp() {
-      return bloomFilterFpp;
-    }
-  }
-
-  /**
-   * Create a set of writer options based on a configuration.
-   * @param conf the configuration to use for values
-   * @return A WriterOptions object that can be modified
-   */
-  public static WriterOptions writerOptions(Configuration conf) {
-    return new WriterOptions(null, conf);
-  }
-
-  /**
-   * Create a set of write options based on a set of table properties and
-   * configuration.
-   * @param tableProperties the properties of the table
-   * @param conf the configuration of the query
-   * @return a WriterOptions object that can be modified
-   */
-  public static WriterOptions writerOptions(Properties tableProperties,
-                                            Configuration conf) {
-    return new WriterOptions(tableProperties, conf);
-  }
-
-  private static ThreadLocal<MemoryManager> memoryManager = null;
-
-  private static synchronized MemoryManager getStaticMemoryManager(
-      final Configuration conf) {
-    if (memoryManager == null) {
-      memoryManager = new ThreadLocal<MemoryManager>() {
-        @Override
-        protected MemoryManager initialValue() {
-          return new MemoryManager(conf);
-        }
-      };
-    }
-    return memoryManager.get();
-  }
-
-  /**
-   * Create an ORC file writer. This is the public interface for creating
-   * writers going forward and new options will only be added to this method.
-   * @param path filename to write to
-   * @param opts the options
-   * @return a new ORC file writer
-   * @throws IOException
-   */
-  public static Writer createWriter(Path path,
-                                    WriterOptions opts
-                                    ) throws IOException {
-    FileSystem fs = opts.getFileSystem() == null ?
-        path.getFileSystem(opts.getConfiguration()) : opts.getFileSystem();
-
-    return new WriterImpl(fs, path, opts);
-  }
-
-}
diff --git a/orc/src/java/org/apache/orc/OrcUtils.java b/orc/src/java/org/apache/orc/OrcUtils.java
deleted file mode 100644
index 4f02926177..0000000000
--- a/orc/src/java/org/apache/orc/OrcUtils.java
+++ /dev/null
@@ -1,624 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.orc.OrcProto.Type.Builder;
-import org.apache.orc.impl.ReaderImpl;
-
-import com.google.common.collect.Lists;
-
-public class OrcUtils {
-
-  /**
-   * Returns selected columns as a boolean array with true value set for specified column names.
-   * The result will contain number of elements equal to flattened number of columns.
-   * For example:
-   * selectedColumns - a,b,c
-   * allColumns - a,b,c,d
-   * If column c is a complex type, say list<string> and other types are primitives then result will
-   * be [false, true, true, true, true, true, false]
-   * Index 0 is the root element of the struct which is set to false by default, index 1,2
-   * corresponds to columns a and b. Index 3,4 correspond to column c which is list<string> and
-   * index 5 correspond to column d. After flattening list<string> gets 2 columns.
-   *
-   * @param selectedColumns - comma separated list of selected column names
-   * @param schema       - object schema
-   * @return - boolean array with true value set for the specified column names
-   */
-  public static boolean[] includeColumns(String selectedColumns,
-                                         TypeDescription schema) {
-    int numFlattenedCols = schema.getMaximumId();
-    boolean[] results = new boolean[numFlattenedCols + 1];
-    if ("*".equals(selectedColumns)) {
-      Arrays.fill(results, true);
-      return results;
-    }
-    if (selectedColumns != null &&
-        schema.getCategory() == TypeDescription.Category.STRUCT) {
-      List<String> fieldNames = schema.getFieldNames();
-      List<TypeDescription> fields = schema.getChildren();
-      for (String column: selectedColumns.split((","))) {
-        TypeDescription col = findColumn(column, fieldNames, fields);
-        if (col != null) {
-          for(int i=col.getId(); i <= col.getMaximumId(); ++i) {
-            results[i] = true;
-          }
-        }
-      }
-    }
-    return results;
-  }
-
-  private static TypeDescription findColumn(String columnName,
-                                            List<String> fieldNames,
-                                            List<TypeDescription> fields) {
-    int i = 0;
-    for(String fieldName: fieldNames) {
-      if (fieldName.equalsIgnoreCase(columnName)) {
-        return fields.get(i);
-      } else {
-        i += 1;
-      }
-    }
-    return null;
-  }
-
-  public static List<OrcProto.Type> getOrcTypes(TypeDescription typeDescr) {
-    List<OrcProto.Type> result = Lists.newArrayList();
-    appendOrcTypes(result, typeDescr);
-    return result;
-  }
-
-  private static void appendOrcTypes(List<OrcProto.Type> result, TypeDescription typeDescr) {
-    OrcProto.Type.Builder type = OrcProto.Type.newBuilder();
-    List<TypeDescription> children = typeDescr.getChildren();
-    switch (typeDescr.getCategory()) {
-    case BOOLEAN:
-      type.setKind(OrcProto.Type.Kind.BOOLEAN);
-      break;
-    case BYTE:
-      type.setKind(OrcProto.Type.Kind.BYTE);
-      break;
-    case SHORT:
-      type.setKind(OrcProto.Type.Kind.SHORT);
-      break;
-    case INT:
-      type.setKind(OrcProto.Type.Kind.INT);
-      break;
-    case LONG:
-      type.setKind(OrcProto.Type.Kind.LONG);
-      break;
-    case FLOAT:
-      type.setKind(OrcProto.Type.Kind.FLOAT);
-      break;
-    case DOUBLE:
-      type.setKind(OrcProto.Type.Kind.DOUBLE);
-      break;
-    case STRING:
-      type.setKind(OrcProto.Type.Kind.STRING);
-      break;
-    case CHAR:
-      type.setKind(OrcProto.Type.Kind.CHAR);
-      type.setMaximumLength(typeDescr.getMaxLength());
-      break;
-    case VARCHAR:
-      type.setKind(OrcProto.Type.Kind.VARCHAR);
-      type.setMaximumLength(typeDescr.getMaxLength());
-      break;
-    case BINARY:
-      type.setKind(OrcProto.Type.Kind.BINARY);
-      break;
-    case TIMESTAMP:
-      type.setKind(OrcProto.Type.Kind.TIMESTAMP);
-      break;
-    case DATE:
-      type.setKind(OrcProto.Type.Kind.DATE);
-      break;
-    case DECIMAL:
-      type.setKind(OrcProto.Type.Kind.DECIMAL);
-      type.setPrecision(typeDescr.getPrecision());
-      type.setScale(typeDescr.getScale());
-      break;
-    case LIST:
-      type.setKind(OrcProto.Type.Kind.LIST);
-      type.addSubtypes(children.get(0).getId());
-      break;
-    case MAP:
-      type.setKind(OrcProto.Type.Kind.MAP);
-      for(TypeDescription t: children) {
-        type.addSubtypes(t.getId());
-      }
-      break;
-    case STRUCT:
-      type.setKind(OrcProto.Type.Kind.STRUCT);
-      for(TypeDescription t: children) {
-        type.addSubtypes(t.getId());
-      }
-      for(String field: typeDescr.getFieldNames()) {
-        type.addFieldNames(field);
-      }
-      break;
-    case UNION:
-      type.setKind(OrcProto.Type.Kind.UNION);
-      for(TypeDescription t: children) {
-        type.addSubtypes(t.getId());
-      }
-      break;
-    default:
-      throw new IllegalArgumentException("Unknown category: " +
-          typeDescr.getCategory());
-    }
-    result.add(type.build());
-    if (children != null) {
-      for(TypeDescription child: children) {
-        appendOrcTypes(result, child);
-      }
-    }
-  }
-
-  /**
-   * NOTE: This method ignores the subtype numbers in the TypeDescription rebuilds the subtype
-   * numbers based on the length of the result list being appended.
-   *
-   * @param result
-   * @param typeDescr
-   */
-  public static void appendOrcTypesRebuildSubtypes(List<OrcProto.Type> result,
-      TypeDescription typeDescr) {
-
-    int subtype = result.size();
-    OrcProto.Type.Builder type = OrcProto.Type.newBuilder();
-    boolean needsAdd = true;
-    List<TypeDescription> children = typeDescr.getChildren();
-    switch (typeDescr.getCategory()) {
-    case BOOLEAN:
-      type.setKind(OrcProto.Type.Kind.BOOLEAN);
-      break;
-    case BYTE:
-      type.setKind(OrcProto.Type.Kind.BYTE);
-      break;
-    case SHORT:
-      type.setKind(OrcProto.Type.Kind.SHORT);
-      break;
-    case INT:
-      type.setKind(OrcProto.Type.Kind.INT);
-      break;
-    case LONG:
-      type.setKind(OrcProto.Type.Kind.LONG);
-      break;
-    case FLOAT:
-      type.setKind(OrcProto.Type.Kind.FLOAT);
-      break;
-    case DOUBLE:
-      type.setKind(OrcProto.Type.Kind.DOUBLE);
-      break;
-    case STRING:
-      type.setKind(OrcProto.Type.Kind.STRING);
-      break;
-    case CHAR:
-      type.setKind(OrcProto.Type.Kind.CHAR);
-      type.setMaximumLength(typeDescr.getMaxLength());
-      break;
-    case VARCHAR:
-      type.setKind(OrcProto.Type.Kind.VARCHAR);
-      type.setMaximumLength(typeDescr.getMaxLength());
-      break;
-    case BINARY:
-      type.setKind(OrcProto.Type.Kind.BINARY);
-      break;
-    case TIMESTAMP:
-      type.setKind(OrcProto.Type.Kind.TIMESTAMP);
-      break;
-    case DATE:
-      type.setKind(OrcProto.Type.Kind.DATE);
-      break;
-    case DECIMAL:
-      type.setKind(OrcProto.Type.Kind.DECIMAL);
-      type.setPrecision(typeDescr.getPrecision());
-      type.setScale(typeDescr.getScale());
-      break;
-    case LIST:
-      type.setKind(OrcProto.Type.Kind.LIST);
-      type.addSubtypes(++subtype);
-      result.add(type.build());
-      needsAdd = false;
-      appendOrcTypesRebuildSubtypes(result, children.get(0));
-      break;
-    case MAP:
-      {
-        // Make room for MAP type.
-        result.add(null);
-  
-        // Add MAP type pair in order to determine their subtype values.
-        appendOrcTypesRebuildSubtypes(result, children.get(0));
-        int subtype2 = result.size();
-        appendOrcTypesRebuildSubtypes(result, children.get(1));
-        type.setKind(OrcProto.Type.Kind.MAP);
-        type.addSubtypes(subtype + 1);
-        type.addSubtypes(subtype2);
-        result.set(subtype, type.build());
-        needsAdd = false;
-      }
-      break;
-    case STRUCT:
-      {
-        List<String> fieldNames = typeDescr.getFieldNames();
-
-        // Make room for STRUCT type.
-        result.add(null);
-
-        List<Integer> fieldSubtypes = new ArrayList<Integer>(fieldNames.size());
-        for(TypeDescription child: children) {
-          int fieldSubtype = result.size();
-          fieldSubtypes.add(fieldSubtype);
-          appendOrcTypesRebuildSubtypes(result, child);
-        }
-
-        type.setKind(OrcProto.Type.Kind.STRUCT);
-
-        for (int i = 0 ; i < fieldNames.size(); i++) {
-          type.addSubtypes(fieldSubtypes.get(i));
-          type.addFieldNames(fieldNames.get(i));
-        }
-        result.set(subtype, type.build());
-        needsAdd = false;
-      }
-      break;
-    case UNION:
-      {
-        // Make room for UNION type.
-        result.add(null);
-
-        List<Integer> unionSubtypes = new ArrayList<Integer>(children.size());
-        for(TypeDescription child: children) {
-          int unionSubtype = result.size();
-          unionSubtypes.add(unionSubtype);
-          appendOrcTypesRebuildSubtypes(result, child);
-        }
-
-        type.setKind(OrcProto.Type.Kind.UNION);
-        for (int i = 0 ; i < children.size(); i++) {
-          type.addSubtypes(unionSubtypes.get(i));
-        }
-        result.set(subtype, type.build());
-        needsAdd = false;
-      }
-      break;
-    default:
-      throw new IllegalArgumentException("Unknown category: " + typeDescr.getCategory());
-    }
-    if (needsAdd) {
-      result.add(type.build());
-    }
-  }
-
-  /**
-   * NOTE: This method ignores the subtype numbers in the OrcProto.Type rebuilds the subtype
-   * numbers based on the length of the result list being appended.
-   *
-   * @param result
-   * @param types
-   * @param columnId
-   */
-  public static int appendOrcTypesRebuildSubtypes(List<OrcProto.Type> result,
-      List<OrcProto.Type> types, int columnId) {
-
-    OrcProto.Type oldType = types.get(columnId++);
-
-    int subtype = result.size();
-    OrcProto.Type.Builder builder = OrcProto.Type.newBuilder();
-    boolean needsAdd = true;
-    switch (oldType.getKind()) {
-    case BOOLEAN:
-      builder.setKind(OrcProto.Type.Kind.BOOLEAN);
-      break;
-    case BYTE:
-      builder.setKind(OrcProto.Type.Kind.BYTE);
-      break;
-    case SHORT:
-      builder.setKind(OrcProto.Type.Kind.SHORT);
-      break;
-    case INT:
-      builder.setKind(OrcProto.Type.Kind.INT);
-      break;
-    case LONG:
-      builder.setKind(OrcProto.Type.Kind.LONG);
-      break;
-    case FLOAT:
-      builder.setKind(OrcProto.Type.Kind.FLOAT);
-      break;
-    case DOUBLE:
-      builder.setKind(OrcProto.Type.Kind.DOUBLE);
-      break;
-    case STRING:
-      builder.setKind(OrcProto.Type.Kind.STRING);
-      break;
-    case CHAR:
-      builder.setKind(OrcProto.Type.Kind.CHAR);
-      builder.setMaximumLength(oldType.getMaximumLength());
-      break;
-    case VARCHAR:
-      builder.setKind(OrcProto.Type.Kind.VARCHAR);
-      builder.setMaximumLength(oldType.getMaximumLength());
-      break;
-    case BINARY:
-      builder.setKind(OrcProto.Type.Kind.BINARY);
-      break;
-    case TIMESTAMP:
-      builder.setKind(OrcProto.Type.Kind.TIMESTAMP);
-      break;
-    case DATE:
-      builder.setKind(OrcProto.Type.Kind.DATE);
-      break;
-    case DECIMAL:
-      builder.setKind(OrcProto.Type.Kind.DECIMAL);
-      builder.setPrecision(oldType.getPrecision());
-      builder.setScale(oldType.getScale());
-      break;
-    case LIST:
-      builder.setKind(OrcProto.Type.Kind.LIST);
-      builder.addSubtypes(++subtype);
-      result.add(builder.build());
-      needsAdd = false;
-      columnId = appendOrcTypesRebuildSubtypes(result, types, columnId);
-      break;
-    case MAP:
-      {
-        // Make room for MAP type.
-        result.add(null);
-  
-        // Add MAP type pair in order to determine their subtype values.
-        columnId = appendOrcTypesRebuildSubtypes(result, types, columnId);
-        int subtype2 = result.size();
-        columnId = appendOrcTypesRebuildSubtypes(result, types, columnId);
-        builder.setKind(OrcProto.Type.Kind.MAP);
-        builder.addSubtypes(subtype + 1);
-        builder.addSubtypes(subtype2);
-        result.set(subtype, builder.build());
-        needsAdd = false;
-      }
-      break;
-    case STRUCT:
-      {
-        List<String> fieldNames = oldType.getFieldNamesList();
-
-        // Make room for STRUCT type.
-        result.add(null);
-
-        List<Integer> fieldSubtypes = new ArrayList<Integer>(fieldNames.size());
-        for(int i = 0 ; i < fieldNames.size(); i++) {
-          int fieldSubtype = result.size();
-          fieldSubtypes.add(fieldSubtype);
-          columnId = appendOrcTypesRebuildSubtypes(result, types, columnId);
-        }
-
-        builder.setKind(OrcProto.Type.Kind.STRUCT);
-
-        for (int i = 0 ; i < fieldNames.size(); i++) {
-          builder.addSubtypes(fieldSubtypes.get(i));
-          builder.addFieldNames(fieldNames.get(i));
-        }
-        result.set(subtype, builder.build());
-        needsAdd = false;
-      }
-      break;
-    case UNION:
-      {
-        int subtypeCount = oldType.getSubtypesCount();
-
-        // Make room for UNION type.
-        result.add(null);
-
-        List<Integer> unionSubtypes = new ArrayList<Integer>(subtypeCount);
-        for(int i = 0 ; i < subtypeCount; i++) {
-          int unionSubtype = result.size();
-          unionSubtypes.add(unionSubtype);
-          columnId = appendOrcTypesRebuildSubtypes(result, types, columnId);
-        }
-
-        builder.setKind(OrcProto.Type.Kind.UNION);
-        for (int i = 0 ; i < subtypeCount; i++) {
-          builder.addSubtypes(unionSubtypes.get(i));
-        }
-        result.set(subtype, builder.build());
-        needsAdd = false;
-      }
-      break;
-    default:
-      throw new IllegalArgumentException("Unknown category: " + oldType.getKind());
-    }
-    if (needsAdd) {
-      result.add(builder.build());
-    }
-    return columnId;
-  }
-
-  /**
-   * Translate the given rootColumn from the list of types to a TypeDescription.
-   * @param types all of the types
-   * @param rootColumn translate this type
-   * @return a new TypeDescription that matches the given rootColumn
-   */
-  public static
-        TypeDescription convertTypeFromProtobuf(List<OrcProto.Type> types,
-                                                int rootColumn) {
-    OrcProto.Type type = types.get(rootColumn);
-    switch (type.getKind()) {
-      case BOOLEAN:
-        return TypeDescription.createBoolean();
-      case BYTE:
-        return TypeDescription.createByte();
-      case SHORT:
-        return TypeDescription.createShort();
-      case INT:
-        return TypeDescription.createInt();
-      case LONG:
-        return TypeDescription.createLong();
-      case FLOAT:
-        return TypeDescription.createFloat();
-      case DOUBLE:
-        return TypeDescription.createDouble();
-      case STRING:
-        return TypeDescription.createString();
-      case CHAR:
-      case VARCHAR: {
-        TypeDescription result = type.getKind() == OrcProto.Type.Kind.CHAR ?
-            TypeDescription.createChar() : TypeDescription.createVarchar();
-        if (type.hasMaximumLength()) {
-          result.withMaxLength(type.getMaximumLength());
-        }
-        return result;
-      }
-      case BINARY:
-        return TypeDescription.createBinary();
-      case TIMESTAMP:
-        return TypeDescription.createTimestamp();
-      case DATE:
-        return TypeDescription.createDate();
-      case DECIMAL: {
-        TypeDescription result = TypeDescription.createDecimal();
-        if (type.hasScale()) {
-          result.withScale(type.getScale());
-        }
-        if (type.hasPrecision()) {
-          result.withPrecision(type.getPrecision());
-        }
-        return result;
-      }
-      case LIST:
-        return TypeDescription.createList(
-            convertTypeFromProtobuf(types, type.getSubtypes(0)));
-      case MAP:
-        return TypeDescription.createMap(
-            convertTypeFromProtobuf(types, type.getSubtypes(0)),
-            convertTypeFromProtobuf(types, type.getSubtypes(1)));
-      case STRUCT: {
-        TypeDescription result = TypeDescription.createStruct();
-        for(int f=0; f < type.getSubtypesCount(); ++f) {
-          result.addField(type.getFieldNames(f),
-              convertTypeFromProtobuf(types, type.getSubtypes(f)));
-        }
-        return result;
-      }
-      case UNION: {
-        TypeDescription result = TypeDescription.createUnion();
-        for(int f=0; f < type.getSubtypesCount(); ++f) {
-          result.addUnionChild(
-              convertTypeFromProtobuf(types, type.getSubtypes(f)));
-        }
-        return result;
-      }
-    }
-    throw new IllegalArgumentException("Unknown ORC type " + type.getKind());
-  }
-
-  public static List<StripeInformation> convertProtoStripesToStripes(
-      List<OrcProto.StripeInformation> stripes) {
-    List<StripeInformation> result = new ArrayList<StripeInformation>(stripes.size());
-    for (OrcProto.StripeInformation info : stripes) {
-      result.add(new ReaderImpl.StripeInformationImpl(info));
-    }
-    return result;
-  }
-
-  public static List<TypeDescription> setTypeBuilderFromSchema(
-      OrcProto.Type.Builder type, TypeDescription schema) {
-    List<TypeDescription> children = schema.getChildren();
-    switch (schema.getCategory()) {
-      case BOOLEAN:
-        type.setKind(OrcProto.Type.Kind.BOOLEAN);
-        break;
-      case BYTE:
-        type.setKind(OrcProto.Type.Kind.BYTE);
-        break;
-      case SHORT:
-        type.setKind(OrcProto.Type.Kind.SHORT);
-        break;
-      case INT:
-        type.setKind(OrcProto.Type.Kind.INT);
-        break;
-      case LONG:
-        type.setKind(OrcProto.Type.Kind.LONG);
-        break;
-      case FLOAT:
-        type.setKind(OrcProto.Type.Kind.FLOAT);
-        break;
-      case DOUBLE:
-        type.setKind(OrcProto.Type.Kind.DOUBLE);
-        break;
-      case STRING:
-        type.setKind(OrcProto.Type.Kind.STRING);
-        break;
-      case CHAR:
-        type.setKind(OrcProto.Type.Kind.CHAR);
-        type.setMaximumLength(schema.getMaxLength());
-        break;
-      case VARCHAR:
-        type.setKind(OrcProto.Type.Kind.VARCHAR);
-        type.setMaximumLength(schema.getMaxLength());
-        break;
-      case BINARY:
-        type.setKind(OrcProto.Type.Kind.BINARY);
-        break;
-      case TIMESTAMP:
-        type.setKind(OrcProto.Type.Kind.TIMESTAMP);
-        break;
-      case DATE:
-        type.setKind(OrcProto.Type.Kind.DATE);
-        break;
-      case DECIMAL:
-        type.setKind(OrcProto.Type.Kind.DECIMAL);
-        type.setPrecision(schema.getPrecision());
-        type.setScale(schema.getScale());
-        break;
-      case LIST:
-        type.setKind(OrcProto.Type.Kind.LIST);
-        type.addSubtypes(children.get(0).getId());
-        break;
-      case MAP:
-        type.setKind(OrcProto.Type.Kind.MAP);
-        for(TypeDescription t: children) {
-          type.addSubtypes(t.getId());
-        }
-        break;
-      case STRUCT:
-        type.setKind(OrcProto.Type.Kind.STRUCT);
-        for(TypeDescription t: children) {
-          type.addSubtypes(t.getId());
-        }
-        for(String field: schema.getFieldNames()) {
-          type.addFieldNames(field);
-        }
-        break;
-      case UNION:
-        type.setKind(OrcProto.Type.Kind.UNION);
-        for(TypeDescription t: children) {
-          type.addSubtypes(t.getId());
-        }
-        break;
-      default:
-        throw new IllegalArgumentException("Unknown category: " +
-          schema.getCategory());
-    }
-    return children;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/Reader.java b/orc/src/java/org/apache/orc/Reader.java
deleted file mode 100644
index c2d5235dca..0000000000
--- a/orc/src/java/org/apache/orc/Reader.java
+++ /dev/null
@@ -1,375 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.List;
-
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
-
-/**
- * The interface for reading ORC files.
- *
- * One Reader can support multiple concurrent RecordReader.
- */
-public interface Reader {
-
-  /**
-   * Get the number of rows in the file.
-   * @return the number of rows
-   */
-  long getNumberOfRows();
-
-  /**
-   * Get the deserialized data size of the file
-   * @return raw data size
-   */
-  long getRawDataSize();
-
-  /**
-   * Get the deserialized data size of the specified columns
-   * @param colNames
-   * @return raw data size of columns
-   */
-  long getRawDataSizeOfColumns(List<String> colNames);
-
-  /**
-   * Get the deserialized data size of the specified columns ids
-   * @param colIds - internal column id (check orcfiledump for column ids)
-   * @return raw data size of columns
-   */
-  long getRawDataSizeFromColIndices(List<Integer> colIds);
-
-  /**
-   * Get the user metadata keys.
-   * @return the set of metadata keys
-   */
-  List<String> getMetadataKeys();
-
-  /**
-   * Get a user metadata value.
-   * @param key a key given by the user
-   * @return the bytes associated with the given key
-   */
-  ByteBuffer getMetadataValue(String key);
-
-  /**
-   * Did the user set the given metadata value.
-   * @param key the key to check
-   * @return true if the metadata value was set
-   */
-  boolean hasMetadataValue(String key);
-
-  /**
-   * Get the compression kind.
-   * @return the kind of compression in the file
-   */
-  CompressionKind getCompressionKind();
-
-  /**
-   * Get the buffer size for the compression.
-   * @return number of bytes to buffer for the compression codec.
-   */
-  int getCompressionSize();
-
-  /**
-   * Get the number of rows per a entry in the row index.
-   * @return the number of rows per an entry in the row index or 0 if there
-   * is no row index.
-   */
-  int getRowIndexStride();
-
-  /**
-   * Get the list of stripes.
-   * @return the information about the stripes in order
-   */
-  List<StripeInformation> getStripes();
-
-  /**
-   * Get the length of the file.
-   * @return the number of bytes in the file
-   */
-  long getContentLength();
-
-  /**
-   * Get the statistics about the columns in the file.
-   * @return the information about the column
-   */
-  ColumnStatistics[] getStatistics();
-
-  /**
-   * Get the type of rows in this ORC file.
-   */
-  TypeDescription getSchema();
-
-  /**
-   * Get the list of types contained in the file. The root type is the first
-   * type in the list.
-   * @return the list of flattened types
-   * @deprecated use getSchema instead
-   */
-  List<OrcProto.Type> getTypes();
-
-  /**
-   * Get the file format version.
-   */
-  OrcFile.Version getFileVersion();
-
-  /**
-   * Get the version of the writer of this file.
-   */
-  OrcFile.WriterVersion getWriterVersion();
-
-  /**
-   * Get the file tail (footer + postscript)
-   *
-   * @return - file tail
-   */
-  OrcProto.FileTail getFileTail();
-
-  /**
-   * Options for creating a RecordReader.
-   */
-  public static class Options {
-    private boolean[] include;
-    private long offset = 0;
-    private long length = Long.MAX_VALUE;
-    private SearchArgument sarg = null;
-    private String[] columnNames = null;
-    private Boolean useZeroCopy = null;
-    private Boolean skipCorruptRecords = null;
-    private TypeDescription schema = null;
-    private DataReader dataReader = null;
-
-    /**
-     * Set the list of columns to read.
-     * @param include a list of columns to read
-     * @return this
-     */
-    public Options include(boolean[] include) {
-      this.include = include;
-      return this;
-    }
-
-    /**
-     * Set the range of bytes to read
-     * @param offset the starting byte offset
-     * @param length the number of bytes to read
-     * @return this
-     */
-    public Options range(long offset, long length) {
-      this.offset = offset;
-      this.length = length;
-      return this;
-    }
-
-    /**
-     * Set the schema on read type description.
-     */
-    public Options schema(TypeDescription schema) {
-      this.schema = schema;
-      return this;
-    }
-
-    /**
-     * Set search argument for predicate push down.
-     * @param sarg the search argument
-     * @param columnNames the column names for
-     * @return this
-     */
-    public Options searchArgument(SearchArgument sarg, String[] columnNames) {
-      this.sarg = sarg;
-      this.columnNames = columnNames;
-      return this;
-    }
-
-    /**
-     * Set whether to use zero copy from HDFS.
-     * @param value the new zero copy flag
-     * @return this
-     */
-    public Options useZeroCopy(boolean value) {
-      this.useZeroCopy = value;
-      return this;
-    }
-
-    public Options dataReader(DataReader value) {
-      this.dataReader = value;
-      return this;
-    }
-
-    /**
-     * Set whether to skip corrupt records.
-     * @param value the new skip corrupt records flag
-     * @return this
-     */
-    public Options skipCorruptRecords(boolean value) {
-      this.skipCorruptRecords = value;
-      return this;
-    }
-
-    public boolean[] getInclude() {
-      return include;
-    }
-
-    public long getOffset() {
-      return offset;
-    }
-
-    public long getLength() {
-      return length;
-    }
-
-    public TypeDescription getSchema() {
-      return schema;
-    }
-
-    public SearchArgument getSearchArgument() {
-      return sarg;
-    }
-
-    public String[] getColumnNames() {
-      return columnNames;
-    }
-
-    public long getMaxOffset() {
-      long result = offset + length;
-      if (result < 0) {
-        result = Long.MAX_VALUE;
-      }
-      return result;
-    }
-
-    public Boolean getUseZeroCopy() {
-      return useZeroCopy;
-    }
-
-    public Boolean getSkipCorruptRecords() {
-      return skipCorruptRecords;
-    }
-
-    public DataReader getDataReader() {
-      return dataReader;
-    }
-
-    public Options clone() {
-      Options result = new Options();
-      result.include = include;
-      result.offset = offset;
-      result.length = length;
-      result.sarg = sarg;
-      result.schema = schema;
-      result.columnNames = columnNames;
-      result.useZeroCopy = useZeroCopy;
-      result.skipCorruptRecords = skipCorruptRecords;
-      result.dataReader = dataReader == null ? null : dataReader.clone();
-      return result;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buffer = new StringBuilder();
-      buffer.append("{include: ");
-      if (include == null) {
-        buffer.append("null");
-      } else {
-        buffer.append("[");
-        for(int i=0; i < include.length; ++i) {
-          if (i != 0) {
-            buffer.append(", ");
-          }
-          buffer.append(include[i]);
-        }
-        buffer.append("]");
-      }
-      buffer.append(", offset: ");
-      buffer.append(offset);
-      buffer.append(", length: ");
-      buffer.append(length);
-      if (sarg != null) {
-        buffer.append(", sarg: ");
-        buffer.append(sarg.toString());
-        buffer.append(", columns: [");
-        for(int i=0; i < columnNames.length; ++i) {
-          if (i != 0) {
-            buffer.append(", ");
-          }
-          buffer.append("'");
-          buffer.append(columnNames[i]);
-          buffer.append("'");
-        }
-        buffer.append("]");
-      }
-      if (schema != null) {
-        buffer.append(", schema: ");
-        schema.printToBuffer(buffer);
-      }
-      buffer.append("}");
-      return buffer.toString();
-    }
-  }
-
-  /**
-   * Create a RecordReader that reads everything with the default options.
-   * @return a new RecordReader
-   * @throws IOException
-   */
-  RecordReader rows() throws IOException;
-
-  /**
-   * Create a RecordReader that uses the options given.
-   * This method can't be named rows, because many callers used rows(null)
-   * before the rows() method was introduced.
-   * @param options the options to read with
-   * @return a new RecordReader
-   * @throws IOException
-   */
-  RecordReader rows(Options options) throws IOException;
-
-  /**
-   * @return List of integers representing version of the file, in order from major to minor.
-   */
-  List<Integer> getVersionList();
-
-  /**
-   * @return Gets the size of metadata, in bytes.
-   */
-  int getMetadataSize();
-
-  /**
-   * @return Stripe statistics, in original protobuf form.
-   */
-  List<OrcProto.StripeStatistics> getOrcProtoStripeStatistics();
-
-  /**
-   * @return Stripe statistics.
-   */
-  List<StripeStatistics> getStripeStatistics() throws IOException;
-
-  /**
-   * @return File statistics, in original protobuf form.
-   */
-  List<OrcProto.ColumnStatistics> getOrcProtoFileStatistics();
-
-  /**
-   * @return Serialized file metadata read from disk for the purposes of caching, etc.
-   */
-  ByteBuffer getSerializedFileFooter();
-}
diff --git a/orc/src/java/org/apache/orc/RecordReader.java b/orc/src/java/org/apache/orc/RecordReader.java
deleted file mode 100644
index 09ba0f0b67..0000000000
--- a/orc/src/java/org/apache/orc/RecordReader.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import java.io.IOException;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-
-/**
- * A row-by-row iterator for ORC files.
- */
-public interface RecordReader {
-  /**
-   * Read the next row batch. The size of the batch to read cannot be
-   * controlled by the callers. Caller need to look at
-   * VectorizedRowBatch.size of the retunred object to know the batch
-   * size read.
-   * @param batch a row batch object to read into
-   * @return were more rows available to read?
-   * @throws java.io.IOException
-   */
-  boolean nextBatch(VectorizedRowBatch batch) throws IOException;
-
-  /**
-   * Get the row number of the row that will be returned by the following
-   * call to next().
-   * @return the row number from 0 to the number of rows in the file
-   * @throws java.io.IOException
-   */
-  long getRowNumber() throws IOException;
-
-  /**
-   * Get the progress of the reader through the rows.
-   * @return a fraction between 0.0 and 1.0 of rows read
-   * @throws java.io.IOException
-   */
-  float getProgress() throws IOException;
-
-  /**
-   * Release the resources associated with the given reader.
-   * @throws java.io.IOException
-   */
-  void close() throws IOException;
-
-  /**
-   * Seek to a particular row number.
-   */
-  void seekToRow(long rowCount) throws IOException;
-}
diff --git a/orc/src/java/org/apache/orc/StringColumnStatistics.java b/orc/src/java/org/apache/orc/StringColumnStatistics.java
deleted file mode 100644
index 5a868d0eb3..0000000000
--- a/orc/src/java/org/apache/orc/StringColumnStatistics.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import org.apache.orc.ColumnStatistics;
-
-/**
- * Statistics for string columns.
- */
-public interface StringColumnStatistics extends ColumnStatistics {
-  /**
-   * Get the minimum string.
-   * @return the minimum
-   */
-  String getMinimum();
-
-  /**
-   * Get the maximum string.
-   * @return the maximum
-   */
-  String getMaximum();
-
-  /**
-   * Get the total length of all strings
-   * @return the sum (total length)
-   */
-  long getSum();
-}
diff --git a/orc/src/java/org/apache/orc/StripeInformation.java b/orc/src/java/org/apache/orc/StripeInformation.java
deleted file mode 100644
index 38f7eba31e..0000000000
--- a/orc/src/java/org/apache/orc/StripeInformation.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-/**
- * Information about the stripes in an ORC file that is provided by the Reader.
- */
-public interface StripeInformation {
-  /**
-   * Get the byte offset of the start of the stripe.
-   * @return the bytes from the start of the file
-   */
-  long getOffset();
-
-  /**
-   * Get the total length of the stripe in bytes.
-   * @return the number of bytes in the stripe
-   */
-  long getLength();
-
-  /**
-   * Get the length of the stripe's indexes.
-   * @return the number of bytes in the index
-   */
-  long getIndexLength();
-
-  /**
-   * Get the length of the stripe's data.
-   * @return the number of bytes in the stripe
-   */
-  long getDataLength();
-
-  /**
-   * Get the length of the stripe's tail section, which contains its index.
-   * @return the number of bytes in the tail
-   */
-  long getFooterLength();
-
-  /**
-   * Get the number of rows in the stripe.
-   * @return a count of the number of rows
-   */
-  long getNumberOfRows();
-}
diff --git a/orc/src/java/org/apache/orc/StripeStatistics.java b/orc/src/java/org/apache/orc/StripeStatistics.java
deleted file mode 100644
index 8fc91cb787..0000000000
--- a/orc/src/java/org/apache/orc/StripeStatistics.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import org.apache.orc.impl.ColumnStatisticsImpl;
-
-import java.util.List;
-
-public class StripeStatistics {
-  private final List<OrcProto.ColumnStatistics> cs;
-
-  public StripeStatistics(List<OrcProto.ColumnStatistics> list) {
-    this.cs = list;
-  }
-
-  /**
-   * Return list of column statistics
-   *
-   * @return column stats
-   */
-  public ColumnStatistics[] getColumnStatistics() {
-    ColumnStatistics[] result = new ColumnStatistics[cs.size()];
-    for (int i = 0; i < result.length; ++i) {
-      result[i] = ColumnStatisticsImpl.deserialize(cs.get(i));
-    }
-    return result;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/TimestampColumnStatistics.java b/orc/src/java/org/apache/orc/TimestampColumnStatistics.java
deleted file mode 100644
index 27dc49f6c3..0000000000
--- a/orc/src/java/org/apache/orc/TimestampColumnStatistics.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import java.sql.Timestamp;
-
-/**
- * Statistics for Timestamp columns.
- */
-public interface TimestampColumnStatistics extends ColumnStatistics {
-  /**
-   * Get the minimum value for the column.
-   * @return minimum value
-   */
-  Timestamp getMinimum();
-
-  /**
-   * Get the maximum value for the column.
-   * @return maximum value
-   */
-  Timestamp getMaximum();
-}
diff --git a/orc/src/java/org/apache/orc/TypeDescription.java b/orc/src/java/org/apache/orc/TypeDescription.java
deleted file mode 100644
index 2e9328be8a..0000000000
--- a/orc/src/java/org/apache/orc/TypeDescription.java
+++ /dev/null
@@ -1,870 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-/**
- * This is the description of the types in an ORC file.
- */
-public class TypeDescription
-    implements Comparable<TypeDescription>, Serializable {
-  private static final int MAX_PRECISION = 38;
-  private static final int MAX_SCALE = 38;
-  private static final int DEFAULT_PRECISION = 38;
-  private static final int DEFAULT_SCALE = 10;
-  private static final int DEFAULT_LENGTH = 256;
-
-  @Override
-  public int compareTo(TypeDescription other) {
-    if (this == other) {
-      return 0;
-    } else if (other == null) {
-      return -1;
-    } else {
-      int result = category.compareTo(other.category);
-      if (result == 0) {
-        switch (category) {
-          case CHAR:
-          case VARCHAR:
-            return maxLength - other.maxLength;
-          case DECIMAL:
-            if (precision != other.precision) {
-              return precision - other.precision;
-            }
-            return scale - other.scale;
-          case UNION:
-          case LIST:
-          case MAP:
-            if (children.size() != other.children.size()) {
-              return children.size() - other.children.size();
-            }
-            for(int c=0; result == 0 && c < children.size(); ++c) {
-              result = children.get(c).compareTo(other.children.get(c));
-            }
-            break;
-          case STRUCT:
-            if (children.size() != other.children.size()) {
-              return children.size() - other.children.size();
-            }
-            for(int c=0; result == 0 && c < children.size(); ++c) {
-              result = fieldNames.get(c).compareTo(other.fieldNames.get(c));
-              if (result == 0) {
-                result = children.get(c).compareTo(other.children.get(c));
-              }
-            }
-            break;
-          default:
-            // PASS
-        }
-      }
-      return result;
-    }
-  }
-
-  public enum Category {
-    BOOLEAN("boolean", true),
-    BYTE("tinyint", true),
-    SHORT("smallint", true),
-    INT("int", true),
-    LONG("bigint", true),
-    FLOAT("float", true),
-    DOUBLE("double", true),
-    STRING("string", true),
-    DATE("date", true),
-    TIMESTAMP("timestamp", true),
-    BINARY("binary", true),
-    DECIMAL("decimal", true),
-    VARCHAR("varchar", true),
-    CHAR("char", true),
-    LIST("array", false),
-    MAP("map", false),
-    STRUCT("struct", false),
-    UNION("uniontype", false);
-
-    Category(String name, boolean isPrimitive) {
-      this.name = name;
-      this.isPrimitive = isPrimitive;
-    }
-
-    final boolean isPrimitive;
-    final String name;
-
-    public boolean isPrimitive() {
-      return isPrimitive;
-    }
-
-    public String getName() {
-      return name;
-    }
-  }
-
-  public static TypeDescription createBoolean() {
-    return new TypeDescription(Category.BOOLEAN);
-  }
-
-  public static TypeDescription createByte() {
-    return new TypeDescription(Category.BYTE);
-  }
-
-  public static TypeDescription createShort() {
-    return new TypeDescription(Category.SHORT);
-  }
-
-  public static TypeDescription createInt() {
-    return new TypeDescription(Category.INT);
-  }
-
-  public static TypeDescription createLong() {
-    return new TypeDescription(Category.LONG);
-  }
-
-  public static TypeDescription createFloat() {
-    return new TypeDescription(Category.FLOAT);
-  }
-
-  public static TypeDescription createDouble() {
-    return new TypeDescription(Category.DOUBLE);
-  }
-
-  public static TypeDescription createString() {
-    return new TypeDescription(Category.STRING);
-  }
-
-  public static TypeDescription createDate() {
-    return new TypeDescription(Category.DATE);
-  }
-
-  public static TypeDescription createTimestamp() {
-    return new TypeDescription(Category.TIMESTAMP);
-  }
-
-  public static TypeDescription createBinary() {
-    return new TypeDescription(Category.BINARY);
-  }
-
-  public static TypeDescription createDecimal() {
-    return new TypeDescription(Category.DECIMAL);
-  }
-
-  static class StringPosition {
-    final String value;
-    int position;
-    final int length;
-
-    StringPosition(String value) {
-      this.value = value;
-      position = 0;
-      length = value.length();
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buffer = new StringBuilder();
-      buffer.append('\'');
-      buffer.append(value.substring(0, position));
-      buffer.append('^');
-      buffer.append(value.substring(position));
-      buffer.append('\'');
-      return buffer.toString();
-    }
-  }
-
-  static Category parseCategory(StringPosition source) {
-    int start = source.position;
-    while (source.position < source.length) {
-      char ch = source.value.charAt(source.position);
-      if (!Character.isLetter(ch)) {
-        break;
-      }
-      source.position += 1;
-    }
-    if (source.position != start) {
-      String word = source.value.substring(start, source.position).toLowerCase();
-      for (Category cat : Category.values()) {
-        if (cat.getName().equals(word)) {
-          return cat;
-        }
-      }
-    }
-    throw new IllegalArgumentException("Can't parse category at " + source);
-  }
-
-  static int parseInt(StringPosition source) {
-    int start = source.position;
-    int result = 0;
-    while (source.position < source.length) {
-      char ch = source.value.charAt(source.position);
-      if (!Character.isDigit(ch)) {
-        break;
-      }
-      result = result * 10 + (ch - '0');
-      source.position += 1;
-    }
-    if (source.position == start) {
-      throw new IllegalArgumentException("Missing integer at " + source);
-    }
-    return result;
-  }
-
-  static String parseName(StringPosition source) {
-    int start = source.position;
-    while (source.position < source.length) {
-      char ch = source.value.charAt(source.position);
-      if (!Character.isLetterOrDigit(ch) && ch != '.' && ch != '_') {
-        break;
-      }
-      source.position += 1;
-    }
-    if (source.position == start) {
-      throw new IllegalArgumentException("Missing name at " + source);
-    }
-    return source.value.substring(start, source.position);
-  }
-
-  static void requireChar(StringPosition source, char required) {
-    if (source.position >= source.length ||
-        source.value.charAt(source.position) != required) {
-      throw new IllegalArgumentException("Missing required char '" +
-          required + "' at " + source);
-    }
-    source.position += 1;
-  }
-
-  static boolean consumeChar(StringPosition source, char ch) {
-    boolean result = source.position < source.length &&
-        source.value.charAt(source.position) == ch;
-    if (result) {
-      source.position += 1;
-    }
-    return result;
-  }
-
-  static void parseUnion(TypeDescription type, StringPosition source) {
-    requireChar(source, '<');
-    do {
-      type.addUnionChild(parseType(source));
-    } while (consumeChar(source, ','));
-    requireChar(source, '>');
-  }
-
-  static void parseStruct(TypeDescription type, StringPosition source) {
-    requireChar(source, '<');
-    do {
-      String fieldName = parseName(source);
-      requireChar(source, ':');
-      type.addField(fieldName, parseType(source));
-    } while (consumeChar(source, ','));
-    requireChar(source, '>');
-  }
-
-  static TypeDescription parseType(StringPosition source) {
-    TypeDescription result = new TypeDescription(parseCategory(source));
-    switch (result.getCategory()) {
-      case BINARY:
-      case BOOLEAN:
-      case BYTE:
-      case DATE:
-      case DOUBLE:
-      case FLOAT:
-      case INT:
-      case LONG:
-      case SHORT:
-      case STRING:
-      case TIMESTAMP:
-        break;
-      case CHAR:
-      case VARCHAR:
-        requireChar(source, '(');
-        result.withMaxLength(parseInt(source));
-        requireChar(source, ')');
-        break;
-      case DECIMAL: {
-        requireChar(source, '(');
-        int precision = parseInt(source);
-        requireChar(source, ',');
-        result.withScale(parseInt(source));
-        result.withPrecision(precision);
-        requireChar(source, ')');
-        break;
-      }
-      case LIST:
-        requireChar(source, '<');
-        result.children.add(parseType(source));
-        requireChar(source, '>');
-        break;
-      case MAP:
-        requireChar(source, '<');
-        result.children.add(parseType(source));
-        requireChar(source, ',');
-        result.children.add(parseType(source));
-        requireChar(source, '>');
-        break;
-      case UNION:
-        parseUnion(result, source);
-        break;
-      case STRUCT:
-        parseStruct(result, source);
-        break;
-      default:
-        throw new IllegalArgumentException("Unknown type " +
-            result.getCategory() + " at " + source);
-    }
-    return result;
-  }
-
-  /**
-   * Parse TypeDescription from the Hive type names. This is the inverse
-   * of TypeDescription.toString()
-   * @param typeName the name of the type
-   * @return a new TypeDescription or null if typeName was null
-   * @throws IllegalArgumentException if the string is badly formed
-   */
-  public static TypeDescription fromString(String typeName) {
-    if (typeName == null) {
-      return null;
-    }
-    StringPosition source = new StringPosition(typeName);
-    TypeDescription result = parseType(source);
-    if (source.position != source.length) {
-      throw new IllegalArgumentException("Extra characters at " + source);
-    }
-    return result;
-  }
-
-  /**
-   * For decimal types, set the precision.
-   * @param precision the new precision
-   * @return this
-   */
-  public TypeDescription withPrecision(int precision) {
-    if (category != Category.DECIMAL) {
-      throw new IllegalArgumentException("precision is only allowed on decimal"+
-         " and not " + category.name);
-    } else if (precision < 1 || precision > MAX_PRECISION || scale > precision){
-      throw new IllegalArgumentException("precision " + precision +
-          " is out of range 1 .. " + scale);
-    }
-    this.precision = precision;
-    return this;
-  }
-
-  /**
-   * For decimal types, set the scale.
-   * @param scale the new scale
-   * @return this
-   */
-  public TypeDescription withScale(int scale) {
-    if (category != Category.DECIMAL) {
-      throw new IllegalArgumentException("scale is only allowed on decimal"+
-          " and not " + category.name);
-    } else if (scale < 0 || scale > MAX_SCALE || scale > precision) {
-      throw new IllegalArgumentException("scale is out of range at " + scale);
-    }
-    this.scale = scale;
-    return this;
-  }
-
-  public static TypeDescription createVarchar() {
-    return new TypeDescription(Category.VARCHAR);
-  }
-
-  public static TypeDescription createChar() {
-    return new TypeDescription(Category.CHAR);
-  }
-
-  /**
-   * Set the maximum length for char and varchar types.
-   * @param maxLength the maximum value
-   * @return this
-   */
-  public TypeDescription withMaxLength(int maxLength) {
-    if (category != Category.VARCHAR && category != Category.CHAR) {
-      throw new IllegalArgumentException("maxLength is only allowed on char" +
-                   " and varchar and not " + category.name);
-    }
-    this.maxLength = maxLength;
-    return this;
-  }
-
-  public static TypeDescription createList(TypeDescription childType) {
-    TypeDescription result = new TypeDescription(Category.LIST);
-    result.children.add(childType);
-    childType.parent = result;
-    return result;
-  }
-
-  public static TypeDescription createMap(TypeDescription keyType,
-                                          TypeDescription valueType) {
-    TypeDescription result = new TypeDescription(Category.MAP);
-    result.children.add(keyType);
-    result.children.add(valueType);
-    keyType.parent = result;
-    valueType.parent = result;
-    return result;
-  }
-
-  public static TypeDescription createUnion() {
-    return new TypeDescription(Category.UNION);
-  }
-
-  public static TypeDescription createStruct() {
-    return new TypeDescription(Category.STRUCT);
-  }
-
-  /**
-   * Add a child to a union type.
-   * @param child a new child type to add
-   * @return the union type.
-   */
-  public TypeDescription addUnionChild(TypeDescription child) {
-    if (category != Category.UNION) {
-      throw new IllegalArgumentException("Can only add types to union type" +
-          " and not " + category);
-    }
-    children.add(child);
-    child.parent = this;
-    return this;
-  }
-
-  /**
-   * Add a field to a struct type as it is built.
-   * @param field the field name
-   * @param fieldType the type of the field
-   * @return the struct type
-   */
-  public TypeDescription addField(String field, TypeDescription fieldType) {
-    if (category != Category.STRUCT) {
-      throw new IllegalArgumentException("Can only add fields to struct type" +
-          " and not " + category);
-    }
-    fieldNames.add(field);
-    children.add(fieldType);
-    fieldType.parent = this;
-    return this;
-  }
-
-  /**
-   * Get the id for this type.
-   * The first call will cause all of the the ids in tree to be assigned, so
-   * it should not be called before the type is completely built.
-   * @return the sequential id
-   */
-  public int getId() {
-    // if the id hasn't been assigned, assign all of the ids from the root
-    if (id == -1) {
-      TypeDescription root = this;
-      while (root.parent != null) {
-        root = root.parent;
-      }
-      root.assignIds(0);
-    }
-    return id;
-  }
-
-  public TypeDescription clone() {
-    TypeDescription result = new TypeDescription(category);
-    result.maxLength = maxLength;
-    result.precision = precision;
-    result.scale = scale;
-    if (fieldNames != null) {
-      result.fieldNames.addAll(fieldNames);
-    }
-    if (children != null) {
-      for(TypeDescription child: children) {
-        TypeDescription clone = child.clone();
-        clone.parent = result;
-        result.children.add(clone);
-      }
-    }
-    return result;
-  }
-
-  @Override
-  public int hashCode() {
-    long result = category.ordinal() * 4241 + maxLength + precision * 13 + scale;
-    if (children != null) {
-      for(TypeDescription child: children) {
-        result = result * 6959 + child.hashCode();
-      }
-    }
-    return (int) result;
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (other == null || !(other instanceof TypeDescription)) {
-      return false;
-    }
-    if (other == this) {
-      return true;
-    }
-    TypeDescription castOther = (TypeDescription) other;
-    if (category != castOther.category ||
-        maxLength != castOther.maxLength ||
-        scale != castOther.scale ||
-        precision != castOther.precision) {
-      return false;
-    }
-    if (children != null) {
-      if (children.size() != castOther.children.size()) {
-        return false;
-      }
-      for (int i = 0; i < children.size(); ++i) {
-        if (!children.get(i).equals(castOther.children.get(i))) {
-          return false;
-        }
-      }
-    }
-    if (category == Category.STRUCT) {
-      for(int i=0; i < fieldNames.size(); ++i) {
-        if (!fieldNames.get(i).equals(castOther.fieldNames.get(i))) {
-          return false;
-        }
-      }
-    }
-    return true;
-  }
-
-  /**
-   * Get the maximum id assigned to this type or its children.
-   * The first call will cause all of the the ids in tree to be assigned, so
-   * it should not be called before the type is completely built.
-   * @return the maximum id assigned under this type
-   */
-  public int getMaximumId() {
-    // if the id hasn't been assigned, assign all of the ids from the root
-    if (maxId == -1) {
-      TypeDescription root = this;
-      while (root.parent != null) {
-        root = root.parent;
-      }
-      root.assignIds(0);
-    }
-    return maxId;
-  }
-
-  private ColumnVector createColumn(int maxSize) {
-    switch (category) {
-      case BOOLEAN:
-      case BYTE:
-      case SHORT:
-      case INT:
-      case LONG:
-      case DATE:
-        return new LongColumnVector(maxSize);
-      case TIMESTAMP:
-        return new TimestampColumnVector(maxSize);
-      case FLOAT:
-      case DOUBLE:
-        return new DoubleColumnVector(maxSize);
-      case DECIMAL:
-        return new DecimalColumnVector(maxSize, precision, scale);
-      case STRING:
-      case BINARY:
-      case CHAR:
-      case VARCHAR:
-        return new BytesColumnVector(maxSize);
-      case STRUCT: {
-        ColumnVector[] fieldVector = new ColumnVector[children.size()];
-        for(int i=0; i < fieldVector.length; ++i) {
-          fieldVector[i] = children.get(i).createColumn(maxSize);
-        }
-        return new StructColumnVector(maxSize,
-                fieldVector);
-      }
-      case UNION: {
-        ColumnVector[] fieldVector = new ColumnVector[children.size()];
-        for(int i=0; i < fieldVector.length; ++i) {
-          fieldVector[i] = children.get(i).createColumn(maxSize);
-        }
-        return new UnionColumnVector(maxSize,
-            fieldVector);
-      }
-      case LIST:
-        return new ListColumnVector(maxSize,
-            children.get(0).createColumn(maxSize));
-      case MAP:
-        return new MapColumnVector(maxSize,
-            children.get(0).createColumn(maxSize),
-            children.get(1).createColumn(maxSize));
-      default:
-        throw new IllegalArgumentException("Unknown type " + category);
-    }
-  }
-
-  public VectorizedRowBatch createRowBatch(int maxSize) {
-    VectorizedRowBatch result;
-    if (category == Category.STRUCT) {
-      result = new VectorizedRowBatch(children.size(), maxSize);
-      for(int i=0; i < result.cols.length; ++i) {
-        result.cols[i] = children.get(i).createColumn(maxSize);
-      }
-    } else {
-      result = new VectorizedRowBatch(1, maxSize);
-      result.cols[0] = createColumn(maxSize);
-    }
-    result.reset();
-    return result;
-  }
-
-  public VectorizedRowBatch createRowBatch() {
-    return createRowBatch(VectorizedRowBatch.DEFAULT_SIZE);
-  }
-
-  /**
-   * Get the kind of this type.
-   * @return get the category for this type.
-   */
-  public Category getCategory() {
-    return category;
-  }
-
-  /**
-   * Get the maximum length of the type. Only used for char and varchar types.
-   * @return the maximum length of the string type
-   */
-  public int getMaxLength() {
-    return maxLength;
-  }
-
-  /**
-   * Get the precision of the decimal type.
-   * @return the number of digits for the precision.
-   */
-  public int getPrecision() {
-    return precision;
-  }
-
-  /**
-   * Get the scale of the decimal type.
-   * @return the number of digits for the scale.
-   */
-  public int getScale() {
-    return scale;
-  }
-
-  /**
-   * For struct types, get the list of field names.
-   * @return the list of field names.
-   */
-  public List<String> getFieldNames() {
-    return Collections.unmodifiableList(fieldNames);
-  }
-
-  /**
-   * Get the subtypes of this type.
-   * @return the list of children types
-   */
-  public List<TypeDescription> getChildren() {
-    return children == null ? null : Collections.unmodifiableList(children);
-  }
-
-  /**
-   * Assign ids to all of the nodes under this one.
-   * @param startId the lowest id to assign
-   * @return the next available id
-   */
-  private int assignIds(int startId) {
-    id = startId++;
-    if (children != null) {
-      for (TypeDescription child : children) {
-        startId = child.assignIds(startId);
-      }
-    }
-    maxId = startId - 1;
-    return startId;
-  }
-
-  private TypeDescription(Category category) {
-    this.category = category;
-    if (category.isPrimitive) {
-      children = null;
-    } else {
-      children = new ArrayList<>();
-    }
-    if (category == Category.STRUCT) {
-      fieldNames = new ArrayList<>();
-    } else {
-      fieldNames = null;
-    }
-  }
-
-  private int id = -1;
-  private int maxId = -1;
-  private TypeDescription parent;
-  private final Category category;
-  private final List<TypeDescription> children;
-  private final List<String> fieldNames;
-  private int maxLength = DEFAULT_LENGTH;
-  private int precision = DEFAULT_PRECISION;
-  private int scale = DEFAULT_SCALE;
-
-  public void printToBuffer(StringBuilder buffer) {
-    buffer.append(category.name);
-    switch (category) {
-      case DECIMAL:
-        buffer.append('(');
-        buffer.append(precision);
-        buffer.append(',');
-        buffer.append(scale);
-        buffer.append(')');
-        break;
-      case CHAR:
-      case VARCHAR:
-        buffer.append('(');
-        buffer.append(maxLength);
-        buffer.append(')');
-        break;
-      case LIST:
-      case MAP:
-      case UNION:
-        buffer.append('<');
-        for(int i=0; i < children.size(); ++i) {
-          if (i != 0) {
-            buffer.append(',');
-          }
-          children.get(i).printToBuffer(buffer);
-        }
-        buffer.append('>');
-        break;
-      case STRUCT:
-        buffer.append('<');
-        for(int i=0; i < children.size(); ++i) {
-          if (i != 0) {
-            buffer.append(',');
-          }
-          buffer.append(fieldNames.get(i));
-          buffer.append(':');
-          children.get(i).printToBuffer(buffer);
-        }
-        buffer.append('>');
-        break;
-      default:
-        break;
-    }
-  }
-
-  public String toString() {
-    StringBuilder buffer = new StringBuilder();
-    printToBuffer(buffer);
-    return buffer.toString();
-  }
-
-  private void printJsonToBuffer(String prefix, StringBuilder buffer,
-                                 int indent) {
-    for(int i=0; i < indent; ++i) {
-      buffer.append(' ');
-    }
-    buffer.append(prefix);
-    buffer.append("{\"category\": \"");
-    buffer.append(category.name);
-    buffer.append("\", \"id\": ");
-    buffer.append(getId());
-    buffer.append(", \"max\": ");
-    buffer.append(maxId);
-    switch (category) {
-      case DECIMAL:
-        buffer.append(", \"precision\": ");
-        buffer.append(precision);
-        buffer.append(", \"scale\": ");
-        buffer.append(scale);
-        break;
-      case CHAR:
-      case VARCHAR:
-        buffer.append(", \"length\": ");
-        buffer.append(maxLength);
-        break;
-      case LIST:
-      case MAP:
-      case UNION:
-        buffer.append(", \"children\": [");
-        for(int i=0; i < children.size(); ++i) {
-          buffer.append('\n');
-          children.get(i).printJsonToBuffer("", buffer, indent + 2);
-          if (i != children.size() - 1) {
-            buffer.append(',');
-          }
-        }
-        buffer.append("]");
-        break;
-      case STRUCT:
-        buffer.append(", \"fields\": [");
-        for(int i=0; i < children.size(); ++i) {
-          buffer.append('\n');
-          children.get(i).printJsonToBuffer("\"" + fieldNames.get(i) + "\": ",
-              buffer, indent + 2);
-          if (i != children.size() - 1) {
-            buffer.append(',');
-          }
-        }
-        buffer.append(']');
-        break;
-      default:
-        break;
-    }
-    buffer.append('}');
-  }
-
-  public String toJson() {
-    StringBuilder buffer = new StringBuilder();
-    printJsonToBuffer("", buffer, 0);
-    return buffer.toString();
-  }
-
-  /**
-   * Locate a subtype by its id.
-   * @param goal the column id to look for
-   * @return the subtype
-   */
-  public TypeDescription findSubtype(int goal) {
-    // call getId method to make sure the ids are assigned
-    int id = getId();
-    if (goal < id || goal > maxId) {
-      throw new IllegalArgumentException("Unknown type id " + id + " in " +
-          toJson());
-    }
-    if (goal == id) {
-      return this;
-    } else {
-      TypeDescription prev = null;
-      for(TypeDescription next: children) {
-        if (next.id > goal) {
-          return prev.findSubtype(goal);
-        }
-        prev = next;
-      }
-      return prev.findSubtype(goal);
-    }
-  }}
diff --git a/orc/src/java/org/apache/orc/Writer.java b/orc/src/java/org/apache/orc/Writer.java
deleted file mode 100644
index 4492062f31..0000000000
--- a/orc/src/java/org/apache/orc/Writer.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.List;
-
-import org.apache.orc.OrcProto;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.TypeDescription;
-
-/**
- * The interface for writing ORC files.
- */
-public interface Writer {
-
-  /**
-   * Get the schema for this writer
-   * @return the file schema
-   */
-  TypeDescription getSchema();
-
-  /**
-   * Add arbitrary meta-data to the ORC file. This may be called at any point
-   * until the Writer is closed. If the same key is passed a second time, the
-   * second value will replace the first.
-   * @param key a key to label the data with.
-   * @param value the contents of the metadata.
-   */
-  void addUserMetadata(String key, ByteBuffer value);
-
-  /**
-   * Add a row batch to the ORC file.
-   * @param batch the rows to add
-   */
-  void addRowBatch(VectorizedRowBatch batch) throws IOException;
-
-  /**
-   * Flush all of the buffers and close the file. No methods on this writer
-   * should be called afterwards.
-   * @throws IOException
-   */
-  void close() throws IOException;
-
-  /**
-   * Return the deserialized data size. Raw data size will be compute when
-   * writing the file footer. Hence raw data size value will be available only
-   * after closing the writer.
-   *
-   * @return raw data size
-   */
-  long getRawDataSize();
-
-  /**
-   * Return the number of rows in file. Row count gets updated when flushing
-   * the stripes. To get accurate row count this method should be called after
-   * closing the writer.
-   *
-   * @return row count
-   */
-  long getNumberOfRows();
-
-  /**
-   * Write an intermediate footer on the file such that if the file is
-   * truncated to the returned offset, it would be a valid ORC file.
-   * @return the offset that would be a valid end location for an ORC file
-   */
-  long writeIntermediateFooter() throws IOException;
-
-  /**
-   * Fast stripe append to ORC file. This interface is used for fast ORC file
-   * merge with other ORC files. When merging, the file to be merged should pass
-   * stripe in binary form along with stripe information and stripe statistics.
-   * After appending last stripe of a file, use appendUserMetadata() to append
-   * any user metadata.
-   * @param stripe - stripe as byte array
-   * @param offset - offset within byte array
-   * @param length - length of stripe within byte array
-   * @param stripeInfo - stripe information
-   * @param stripeStatistics - stripe statistics (Protobuf objects can be
-   *                         merged directly)
-   * @throws IOException
-   */
-  public void appendStripe(byte[] stripe, int offset, int length,
-      StripeInformation stripeInfo,
-      OrcProto.StripeStatistics stripeStatistics) throws IOException;
-
-  /**
-   * When fast stripe append is used for merging ORC stripes, after appending
-   * the last stripe from a file, this interface must be used to merge any
-   * user metadata.
-   * @param userMetadata - user metadata
-   */
-  public void appendUserMetadata(List<OrcProto.UserMetadataItem> userMetadata);
-}
diff --git a/orc/src/java/org/apache/orc/impl/AcidStats.java b/orc/src/java/org/apache/orc/impl/AcidStats.java
deleted file mode 100644
index 6657fe9be8..0000000000
--- a/orc/src/java/org/apache/orc/impl/AcidStats.java
+++ /dev/null
@@ -1,60 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-/**
- * Statistics about the ACID operations in an ORC file
- */
-public class AcidStats {
-  public long inserts;
-  public long updates;
-  public long deletes;
-
-  public AcidStats() {
-    inserts = 0;
-    updates = 0;
-    deletes = 0;
-  }
-
-  public AcidStats(String serialized) {
-    String[] parts = serialized.split(",");
-    inserts = Long.parseLong(parts[0]);
-    updates = Long.parseLong(parts[1]);
-    deletes = Long.parseLong(parts[2]);
-  }
-
-  public String serialize() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(inserts);
-    builder.append(",");
-    builder.append(updates);
-    builder.append(",");
-    builder.append(deletes);
-    return builder.toString();
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(" inserts: ").append(inserts);
-    builder.append(" updates: ").append(updates);
-    builder.append(" deletes: ").append(deletes);
-    return builder.toString();
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/BitFieldReader.java b/orc/src/java/org/apache/orc/impl/BitFieldReader.java
deleted file mode 100644
index dda7355af8..0000000000
--- a/orc/src/java/org/apache/orc/impl/BitFieldReader.java
+++ /dev/null
@@ -1,217 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.orc.impl.InStream;
-import org.apache.orc.impl.PositionProvider;
-import org.apache.orc.impl.RunLengthByteReader;
-
-public class BitFieldReader {
-  private final RunLengthByteReader input;
-  /** The number of bits in one item. Non-test code always uses 1. */
-  private final int bitSize;
-  private int current;
-  private int bitsLeft;
-  private final int mask;
-
-  public BitFieldReader(InStream input,
-      int bitSize) throws IOException {
-    this.input = new RunLengthByteReader(input);
-    this.bitSize = bitSize;
-    mask = (1 << bitSize) - 1;
-  }
-
-  public void setInStream(InStream inStream) {
-    this.input.setInStream(inStream);
-  }
-
-  private void readByte() throws IOException {
-    if (input.hasNext()) {
-      current = 0xff & input.next();
-      bitsLeft = 8;
-    } else {
-      throw new EOFException("Read past end of bit field from " + this);
-    }
-  }
-
-  public int next() throws IOException {
-    int result = 0;
-    int bitsLeftToRead = bitSize;
-    while (bitsLeftToRead > bitsLeft) {
-      result <<= bitsLeft;
-      result |= current & ((1 << bitsLeft) - 1);
-      bitsLeftToRead -= bitsLeft;
-      readByte();
-    }
-    if (bitsLeftToRead > 0) {
-      result <<= bitsLeftToRead;
-      bitsLeft -= bitsLeftToRead;
-      result |= (current >>> bitsLeft) & ((1 << bitsLeftToRead) - 1);
-    }
-    return result & mask;
-  }
-
-  /**
-   * Unlike integer readers, where runs are encoded explicitly, in this one we have to read ahead
-   * to figure out whether we have a run. Given that runs in booleans are likely it's worth it.
-   * However it means we'd need to keep track of how many bytes we read, and next/nextVector won't
-   * work anymore once this is called. These is trivial to fix, but these are never interspersed.
-   */
-  private boolean lastRunValue;
-  private int lastRunLength = -1;
-  private void readNextRun(int maxRunLength) throws IOException {
-    assert bitSize == 1;
-    if (lastRunLength > 0) return; // last run is not exhausted yet
-    if (bitsLeft == 0) {
-      readByte();
-    }
-    // First take care of the partial bits.
-    boolean hasVal = false;
-    int runLength = 0;
-    if (bitsLeft != 8) {
-      int partialBitsMask = (1 << bitsLeft) - 1;
-      int partialBits = current & partialBitsMask;
-      if (partialBits == partialBitsMask || partialBits == 0) {
-        lastRunValue = (partialBits == partialBitsMask);
-        if (maxRunLength <= bitsLeft) {
-          lastRunLength = maxRunLength;
-          return;
-        }
-        maxRunLength -= bitsLeft;
-        hasVal = true;
-        runLength = bitsLeft;
-        bitsLeft = 0;
-      } else {
-        // There's no run in partial bits. Return whatever we have.
-        int prefixBitsCount = 32 - bitsLeft;
-        runLength = Integer.numberOfLeadingZeros(partialBits) - prefixBitsCount;
-        lastRunValue = (runLength > 0);
-        lastRunLength = Math.min(maxRunLength, lastRunValue ? runLength :
-          (Integer.numberOfLeadingZeros(~(partialBits | ~partialBitsMask)) - prefixBitsCount));
-        return;
-      }
-      assert bitsLeft == 0;
-      readByte();
-    }
-    if (!hasVal) {
-      lastRunValue = ((current >> 7) == 1);
-      hasVal = true;
-    }
-    // Read full bytes until the run ends.
-    assert bitsLeft == 8;
-    while (maxRunLength >= 8
-        && ((lastRunValue && (current == 0xff)) || (!lastRunValue && (current == 0)))) {
-      runLength += 8;
-      maxRunLength -= 8;
-      readByte();
-    }
-    if (maxRunLength > 0) {
-      int extraBits = Integer.numberOfLeadingZeros(
-          lastRunValue ? (~(current | ~255)) : current) - 24;
-      bitsLeft -= extraBits;
-      runLength += extraBits;
-    }
-    lastRunLength = runLength;
-  }
-
-  public void nextVector(LongColumnVector previous,
-                         long previousLen) throws IOException {
-    previous.isRepeating = true;
-    for (int i = 0; i < previousLen; i++) {
-      if (previous.noNulls || !previous.isNull[i]) {
-        previous.vector[i] = next();
-      } else {
-        // The default value of null for int types in vectorized
-        // processing is 1, so set that if the value is null
-        previous.vector[i] = 1;
-      }
-
-      // The default value for nulls in Vectorization for int types is 1
-      // and given that non null value can also be 1, we need to check for isNull also
-      // when determining the isRepeating flag.
-      if (previous.isRepeating
-          && i > 0
-          && ((previous.vector[0] != previous.vector[i]) ||
-          (previous.isNull[0] != previous.isNull[i]))) {
-        previous.isRepeating = false;
-      }
-    }
-  }
-
-  public void seek(PositionProvider index) throws IOException {
-    input.seek(index);
-    int consumed = (int) index.getNext();
-    if (consumed > 8) {
-      throw new IllegalArgumentException("Seek past end of byte at " +
-          consumed + " in " + input);
-    } else if (consumed != 0) {
-      readByte();
-      bitsLeft = 8 - consumed;
-    } else {
-      bitsLeft = 0;
-    }
-  }
-
-  public void skip(long items) throws IOException {
-    long totalBits = bitSize * items;
-    if (bitsLeft >= totalBits) {
-      bitsLeft -= totalBits;
-    } else {
-      totalBits -= bitsLeft;
-      input.skip(totalBits / 8);
-      current = input.next();
-      bitsLeft = (int) (8 - (totalBits % 8));
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "bit reader current: " + current + " bits left: " + bitsLeft +
-        " bit size: " + bitSize + " from " + input;
-  }
-
-  boolean hasFullByte() {
-    return bitsLeft == 8 || bitsLeft == 0;
-  }
-
-  int peekOneBit() throws IOException {
-    assert bitSize == 1;
-    if (bitsLeft == 0) {
-      readByte();
-    }
-    return (current >>> (bitsLeft - 1)) & 1;
-  }
-
-  int peekFullByte() throws IOException {
-    assert bitSize == 1;
-    assert bitsLeft == 8 || bitsLeft == 0;
-    if (bitsLeft == 0) {
-      readByte();
-    }
-    return current;
-  }
-
-  void skipInCurrentByte(int bits) throws IOException {
-    assert bitsLeft >= bits;
-    bitsLeft -= bits;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/BitFieldWriter.java b/orc/src/java/org/apache/orc/impl/BitFieldWriter.java
deleted file mode 100644
index aa5f886e65..0000000000
--- a/orc/src/java/org/apache/orc/impl/BitFieldWriter.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import org.apache.orc.impl.PositionRecorder;
-import org.apache.orc.impl.PositionedOutputStream;
-import org.apache.orc.impl.RunLengthByteWriter;
-
-import java.io.IOException;
-
-public class BitFieldWriter {
-  private RunLengthByteWriter output;
-  private final int bitSize;
-  private byte current = 0;
-  private int bitsLeft = 8;
-
-  public BitFieldWriter(PositionedOutputStream output,
-                 int bitSize) throws IOException {
-    this.output = new RunLengthByteWriter(output);
-    this.bitSize = bitSize;
-  }
-
-  private void writeByte() throws IOException {
-    output.write(current);
-    current = 0;
-    bitsLeft = 8;
-  }
-
-  public void flush() throws IOException {
-    if (bitsLeft != 8) {
-      writeByte();
-    }
-    output.flush();
-  }
-
-  public void write(int value) throws IOException {
-    int bitsToWrite = bitSize;
-    while (bitsToWrite > bitsLeft) {
-      // add the bits to the bottom of the current word
-      current |= value >>> (bitsToWrite - bitsLeft);
-      // subtract out the bits we just added
-      bitsToWrite -= bitsLeft;
-      // zero out the bits above bitsToWrite
-      value &= (1 << bitsToWrite) - 1;
-      writeByte();
-    }
-    bitsLeft -= bitsToWrite;
-    current |= value << bitsLeft;
-    if (bitsLeft == 0) {
-      writeByte();
-    }
-  }
-
-  public void getPosition(PositionRecorder recorder) throws IOException {
-    output.getPosition(recorder);
-    recorder.addPosition(8 - bitsLeft);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/BufferChunk.java b/orc/src/java/org/apache/orc/impl/BufferChunk.java
deleted file mode 100644
index da43b96c5b..0000000000
--- a/orc/src/java/org/apache/orc/impl/BufferChunk.java
+++ /dev/null
@@ -1,85 +0,0 @@
-package org.apache.orc.impl;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.hadoop.hive.common.io.DiskRange;
-import org.apache.hadoop.hive.common.io.DiskRangeList;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.nio.ByteBuffer;
-
-/**
- * The sections of stripe that we have read.
- * This might not match diskRange - 1 disk range can be multiple buffer chunks,
- * depending on DFS block boundaries.
- */
-public class BufferChunk extends DiskRangeList {
-
-  private static final Logger LOG =
-      LoggerFactory.getLogger(BufferChunk.class);
-  final ByteBuffer chunk;
-
-  public BufferChunk(ByteBuffer chunk, long offset) {
-    super(offset, offset + chunk.remaining());
-    this.chunk = chunk;
-  }
-
-  public ByteBuffer getChunk() {
-    return chunk;
-  }
-
-  @Override
-  public boolean hasData() {
-    return chunk != null;
-  }
-
-  @Override
-  public final String toString() {
-    boolean makesSense = chunk.remaining() == (end - offset);
-    return "data range [" + offset + ", " + end + "), size: " + chunk.remaining()
-        + (makesSense ? "" : "(!)") + " type: " +
-        (chunk.isDirect() ? "direct" : "array-backed");
-  }
-
-  @Override
-  public DiskRange sliceAndShift(long offset, long end, long shiftBy) {
-    assert offset <= end && offset >= this.offset && end <= this.end;
-    assert offset + shiftBy >= 0;
-    ByteBuffer sliceBuf = chunk.slice();
-    int newPos = (int) (offset - this.offset);
-    int newLimit = newPos + (int) (end - offset);
-    try {
-      sliceBuf.position(newPos);
-      sliceBuf.limit(newLimit);
-    } catch (Throwable t) {
-      LOG.error("Failed to slice buffer chunk with range" + " [" + this.offset + ", " + this.end
-          + "), position: " + chunk.position() + " limit: " + chunk.limit() + ", "
-          + (chunk.isDirect() ? "direct" : "array") + "; to [" + offset + ", " + end + ") "
-          + t.getClass());
-      throw new RuntimeException(t);
-    }
-    return new BufferChunk(sliceBuf, offset + shiftBy);
-  }
-
-  @Override
-  public ByteBuffer getData() {
-    return chunk;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java b/orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java
deleted file mode 100644
index 1118c5c40c..0000000000
--- a/orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java
+++ /dev/null
@@ -1,1101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.sql.Date;
-import java.sql.Timestamp;
-
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.orc.BinaryColumnStatistics;
-import org.apache.orc.BooleanColumnStatistics;
-import org.apache.orc.ColumnStatistics;
-import org.apache.orc.DateColumnStatistics;
-import org.apache.orc.DecimalColumnStatistics;
-import org.apache.orc.DoubleColumnStatistics;
-import org.apache.orc.IntegerColumnStatistics;
-import org.apache.orc.OrcProto;
-import org.apache.orc.StringColumnStatistics;
-import org.apache.orc.TimestampColumnStatistics;
-import org.apache.orc.TypeDescription;
-
-public class ColumnStatisticsImpl implements ColumnStatistics {
-
-  private static final class BooleanStatisticsImpl extends ColumnStatisticsImpl
-      implements BooleanColumnStatistics {
-    private long trueCount = 0;
-
-    BooleanStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.BucketStatistics bkt = stats.getBucketStatistics();
-      trueCount = bkt.getCount(0);
-    }
-
-    BooleanStatisticsImpl() {
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      trueCount = 0;
-    }
-
-    @Override
-    public void updateBoolean(boolean value, int repetitions) {
-      if (value) {
-        trueCount += repetitions;
-      }
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof BooleanStatisticsImpl) {
-        BooleanStatisticsImpl bkt = (BooleanStatisticsImpl) other;
-        trueCount += bkt.trueCount;
-      } else {
-        if (isStatsExists() && trueCount != 0) {
-          throw new IllegalArgumentException("Incompatible merging of boolean column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder builder = super.serialize();
-      OrcProto.BucketStatistics.Builder bucket =
-        OrcProto.BucketStatistics.newBuilder();
-      bucket.addCount(trueCount);
-      builder.setBucketStatistics(bucket);
-      return builder;
-    }
-
-    @Override
-    public long getFalseCount() {
-      return getNumberOfValues() - trueCount;
-    }
-
-    @Override
-    public long getTrueCount() {
-      return trueCount;
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " true: " + trueCount;
-    }
-  }
-
-  private static final class IntegerStatisticsImpl extends ColumnStatisticsImpl
-      implements IntegerColumnStatistics {
-
-    private long minimum = Long.MAX_VALUE;
-    private long maximum = Long.MIN_VALUE;
-    private long sum = 0;
-    private boolean hasMinimum = false;
-    private boolean overflow = false;
-
-    IntegerStatisticsImpl() {
-    }
-
-    IntegerStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.IntegerStatistics intStat = stats.getIntStatistics();
-      if (intStat.hasMinimum()) {
-        hasMinimum = true;
-        minimum = intStat.getMinimum();
-      }
-      if (intStat.hasMaximum()) {
-        maximum = intStat.getMaximum();
-      }
-      if (intStat.hasSum()) {
-        sum = intStat.getSum();
-      } else {
-        overflow = true;
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      hasMinimum = false;
-      minimum = Long.MAX_VALUE;
-      maximum = Long.MIN_VALUE;
-      sum = 0;
-      overflow = false;
-    }
-
-    @Override
-    public void updateInteger(long value, int repetitions) {
-      if (!hasMinimum) {
-        hasMinimum = true;
-        minimum = value;
-        maximum = value;
-      } else if (value < minimum) {
-        minimum = value;
-      } else if (value > maximum) {
-        maximum = value;
-      }
-      if (!overflow) {
-        boolean wasPositive = sum >= 0;
-        sum += value * repetitions;
-        if ((value >= 0) == wasPositive) {
-          overflow = (sum >= 0) != wasPositive;
-        }
-      }
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof IntegerStatisticsImpl) {
-        IntegerStatisticsImpl otherInt = (IntegerStatisticsImpl) other;
-        if (!hasMinimum) {
-          hasMinimum = otherInt.hasMinimum;
-          minimum = otherInt.minimum;
-          maximum = otherInt.maximum;
-        } else if (otherInt.hasMinimum) {
-          if (otherInt.minimum < minimum) {
-            minimum = otherInt.minimum;
-          }
-          if (otherInt.maximum > maximum) {
-            maximum = otherInt.maximum;
-          }
-        }
-
-        overflow |= otherInt.overflow;
-        if (!overflow) {
-          boolean wasPositive = sum >= 0;
-          sum += otherInt.sum;
-          if ((otherInt.sum >= 0) == wasPositive) {
-            overflow = (sum >= 0) != wasPositive;
-          }
-        }
-      } else {
-        if (isStatsExists() && hasMinimum) {
-          throw new IllegalArgumentException("Incompatible merging of integer column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder builder = super.serialize();
-      OrcProto.IntegerStatistics.Builder intb =
-        OrcProto.IntegerStatistics.newBuilder();
-      if (hasMinimum) {
-        intb.setMinimum(minimum);
-        intb.setMaximum(maximum);
-      }
-      if (!overflow) {
-        intb.setSum(sum);
-      }
-      builder.setIntStatistics(intb);
-      return builder;
-    }
-
-    @Override
-    public long getMinimum() {
-      return minimum;
-    }
-
-    @Override
-    public long getMaximum() {
-      return maximum;
-    }
-
-    @Override
-    public boolean isSumDefined() {
-      return !overflow;
-    }
-
-    @Override
-    public long getSum() {
-      return sum;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (hasMinimum) {
-        buf.append(" min: ");
-        buf.append(minimum);
-        buf.append(" max: ");
-        buf.append(maximum);
-      }
-      if (!overflow) {
-        buf.append(" sum: ");
-        buf.append(sum);
-      }
-      return buf.toString();
-    }
-  }
-
-  private static final class DoubleStatisticsImpl extends ColumnStatisticsImpl
-       implements DoubleColumnStatistics {
-    private boolean hasMinimum = false;
-    private double minimum = Double.MAX_VALUE;
-    private double maximum = Double.MIN_VALUE;
-    private double sum = 0;
-
-    DoubleStatisticsImpl() {
-    }
-
-    DoubleStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.DoubleStatistics dbl = stats.getDoubleStatistics();
-      if (dbl.hasMinimum()) {
-        hasMinimum = true;
-        minimum = dbl.getMinimum();
-      }
-      if (dbl.hasMaximum()) {
-        maximum = dbl.getMaximum();
-      }
-      if (dbl.hasSum()) {
-        sum = dbl.getSum();
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      hasMinimum = false;
-      minimum = Double.MAX_VALUE;
-      maximum = Double.MIN_VALUE;
-      sum = 0;
-    }
-
-    @Override
-    public void updateDouble(double value) {
-      if (!hasMinimum) {
-        hasMinimum = true;
-        minimum = value;
-        maximum = value;
-      } else if (value < minimum) {
-        minimum = value;
-      } else if (value > maximum) {
-        maximum = value;
-      }
-      sum += value;
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof DoubleStatisticsImpl) {
-        DoubleStatisticsImpl dbl = (DoubleStatisticsImpl) other;
-        if (!hasMinimum) {
-          hasMinimum = dbl.hasMinimum;
-          minimum = dbl.minimum;
-          maximum = dbl.maximum;
-        } else if (dbl.hasMinimum) {
-          if (dbl.minimum < minimum) {
-            minimum = dbl.minimum;
-          }
-          if (dbl.maximum > maximum) {
-            maximum = dbl.maximum;
-          }
-        }
-        sum += dbl.sum;
-      } else {
-        if (isStatsExists() && hasMinimum) {
-          throw new IllegalArgumentException("Incompatible merging of double column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder builder = super.serialize();
-      OrcProto.DoubleStatistics.Builder dbl =
-        OrcProto.DoubleStatistics.newBuilder();
-      if (hasMinimum) {
-        dbl.setMinimum(minimum);
-        dbl.setMaximum(maximum);
-      }
-      dbl.setSum(sum);
-      builder.setDoubleStatistics(dbl);
-      return builder;
-    }
-
-    @Override
-    public double getMinimum() {
-      return minimum;
-    }
-
-    @Override
-    public double getMaximum() {
-      return maximum;
-    }
-
-    @Override
-    public double getSum() {
-      return sum;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (hasMinimum) {
-        buf.append(" min: ");
-        buf.append(minimum);
-        buf.append(" max: ");
-        buf.append(maximum);
-      }
-      buf.append(" sum: ");
-      buf.append(sum);
-      return buf.toString();
-    }
-  }
-
-  protected static final class StringStatisticsImpl extends ColumnStatisticsImpl
-      implements StringColumnStatistics {
-    private Text minimum = null;
-    private Text maximum = null;
-    private long sum = 0;
-
-    StringStatisticsImpl() {
-    }
-
-    StringStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.StringStatistics str = stats.getStringStatistics();
-      if (str.hasMaximum()) {
-        maximum = new Text(str.getMaximum());
-      }
-      if (str.hasMinimum()) {
-        minimum = new Text(str.getMinimum());
-      }
-      if(str.hasSum()) {
-        sum = str.getSum();
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      minimum = null;
-      maximum = null;
-      sum = 0;
-    }
-
-    @Override
-    public void updateString(Text value) {
-      if (minimum == null) {
-        maximum = minimum = new Text(value);
-      } else if (minimum.compareTo(value) > 0) {
-        minimum = new Text(value);
-      } else if (maximum.compareTo(value) < 0) {
-        maximum = new Text(value);
-      }
-      sum += value.getLength();
-    }
-
-    @Override
-    public void updateString(byte[] bytes, int offset, int length,
-                             int repetitions) {
-      if (minimum == null) {
-        maximum = minimum = new Text();
-        maximum.set(bytes, offset, length);
-      } else if (WritableComparator.compareBytes(minimum.getBytes(), 0,
-          minimum.getLength(), bytes, offset, length) > 0) {
-        minimum = new Text();
-        minimum.set(bytes, offset, length);
-      } else if (WritableComparator.compareBytes(maximum.getBytes(), 0,
-          maximum.getLength(), bytes, offset, length) < 0) {
-        maximum = new Text();
-        maximum.set(bytes, offset, length);
-      }
-      sum += length * repetitions;
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof StringStatisticsImpl) {
-        StringStatisticsImpl str = (StringStatisticsImpl) other;
-        if (minimum == null) {
-          if (str.minimum != null) {
-            maximum = new Text(str.getMaximum());
-            minimum = new Text(str.getMinimum());
-          } else {
-          /* both are empty */
-            maximum = minimum = null;
-          }
-        } else if (str.minimum != null) {
-          if (minimum.compareTo(str.minimum) > 0) {
-            minimum = new Text(str.getMinimum());
-          }
-          if (maximum.compareTo(str.maximum) < 0) {
-            maximum = new Text(str.getMaximum());
-          }
-        }
-        sum += str.sum;
-      } else {
-        if (isStatsExists() && minimum != null) {
-          throw new IllegalArgumentException("Incompatible merging of string column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder result = super.serialize();
-      OrcProto.StringStatistics.Builder str =
-        OrcProto.StringStatistics.newBuilder();
-      if (getNumberOfValues() != 0) {
-        str.setMinimum(getMinimum());
-        str.setMaximum(getMaximum());
-        str.setSum(sum);
-      }
-      result.setStringStatistics(str);
-      return result;
-    }
-
-    @Override
-    public String getMinimum() {
-      return minimum == null ? null : minimum.toString();
-    }
-
-    @Override
-    public String getMaximum() {
-      return maximum == null ? null : maximum.toString();
-    }
-
-    @Override
-    public long getSum() {
-      return sum;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (getNumberOfValues() != 0) {
-        buf.append(" min: ");
-        buf.append(getMinimum());
-        buf.append(" max: ");
-        buf.append(getMaximum());
-        buf.append(" sum: ");
-        buf.append(sum);
-      }
-      return buf.toString();
-    }
-  }
-
-  protected static final class BinaryStatisticsImpl extends ColumnStatisticsImpl implements
-      BinaryColumnStatistics {
-
-    private long sum = 0;
-
-    BinaryStatisticsImpl() {
-    }
-
-    BinaryStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.BinaryStatistics binStats = stats.getBinaryStatistics();
-      if (binStats.hasSum()) {
-        sum = binStats.getSum();
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      sum = 0;
-    }
-
-    @Override
-    public void updateBinary(BytesWritable value) {
-      sum += value.getLength();
-    }
-
-    @Override
-    public void updateBinary(byte[] bytes, int offset, int length,
-                             int repetitions) {
-      sum += length * repetitions;
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof BinaryColumnStatistics) {
-        BinaryStatisticsImpl bin = (BinaryStatisticsImpl) other;
-        sum += bin.sum;
-      } else {
-        if (isStatsExists() && sum != 0) {
-          throw new IllegalArgumentException("Incompatible merging of binary column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public long getSum() {
-      return sum;
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder result = super.serialize();
-      OrcProto.BinaryStatistics.Builder bin = OrcProto.BinaryStatistics.newBuilder();
-      bin.setSum(sum);
-      result.setBinaryStatistics(bin);
-      return result;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (getNumberOfValues() != 0) {
-        buf.append(" sum: ");
-        buf.append(sum);
-      }
-      return buf.toString();
-    }
-  }
-
-  private static final class DecimalStatisticsImpl extends ColumnStatisticsImpl
-      implements DecimalColumnStatistics {
-
-    // These objects are mutable for better performance.
-    private HiveDecimalWritable minimum = null;
-    private HiveDecimalWritable maximum = null;
-    private HiveDecimalWritable sum = new HiveDecimalWritable(0);
-
-    DecimalStatisticsImpl() {
-    }
-
-    DecimalStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.DecimalStatistics dec = stats.getDecimalStatistics();
-      if (dec.hasMaximum()) {
-        maximum = new HiveDecimalWritable(dec.getMaximum());
-      }
-      if (dec.hasMinimum()) {
-        minimum = new HiveDecimalWritable(dec.getMinimum());
-      }
-      if (dec.hasSum()) {
-        sum = new HiveDecimalWritable(dec.getSum());
-      } else {
-        sum = null;
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      minimum = null;
-      maximum = null;
-      sum = new HiveDecimalWritable(0);
-    }
-
-    @Override
-    public void updateDecimal(HiveDecimalWritable value) {
-      if (minimum == null) {
-        minimum = new HiveDecimalWritable(value);
-        maximum = new HiveDecimalWritable(value);
-      } else if (minimum.compareTo(value) > 0) {
-        minimum.set(value);
-      } else if (maximum.compareTo(value) < 0) {
-        maximum.set(value);
-      }
-      if (sum != null) {
-        sum.mutateAdd(value);
-      }
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof DecimalStatisticsImpl) {
-        DecimalStatisticsImpl dec = (DecimalStatisticsImpl) other;
-        if (minimum == null) {
-          minimum = (dec.minimum != null ? new HiveDecimalWritable(dec.minimum) : null);
-          maximum = (dec.maximum != null ? new HiveDecimalWritable(dec.maximum) : null);
-          sum = dec.sum;
-        } else if (dec.minimum != null) {
-          if (minimum.compareTo(dec.minimum) > 0) {
-            minimum.set(dec.minimum);
-          }
-          if (maximum.compareTo(dec.maximum) < 0) {
-            maximum.set(dec.maximum);
-          }
-          if (sum == null || dec.sum == null) {
-            sum = null;
-          } else {
-            sum.mutateAdd(dec.sum);
-          }
-        }
-      } else {
-        if (isStatsExists() && minimum != null) {
-          throw new IllegalArgumentException("Incompatible merging of decimal column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder result = super.serialize();
-      OrcProto.DecimalStatistics.Builder dec =
-          OrcProto.DecimalStatistics.newBuilder();
-      if (getNumberOfValues() != 0 && minimum != null) {
-        dec.setMinimum(minimum.toString());
-        dec.setMaximum(maximum.toString());
-      }
-      // Check isSet for overflow.
-      if (sum != null && sum.isSet()) {
-        dec.setSum(sum.toString());
-      }
-      result.setDecimalStatistics(dec);
-      return result;
-    }
-
-    @Override
-    public HiveDecimal getMinimum() {
-      return minimum.getHiveDecimal();
-    }
-
-    @Override
-    public HiveDecimal getMaximum() {
-      return maximum.getHiveDecimal();
-    }
-
-    @Override
-    public HiveDecimal getSum() {
-      return sum.getHiveDecimal();
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (getNumberOfValues() != 0) {
-        buf.append(" min: ");
-        buf.append(minimum);
-        buf.append(" max: ");
-        buf.append(maximum);
-        if (sum != null) {
-          buf.append(" sum: ");
-          buf.append(sum);
-        }
-      }
-      return buf.toString();
-    }
-  }
-
-  private static final class DateStatisticsImpl extends ColumnStatisticsImpl
-      implements DateColumnStatistics {
-    private Integer minimum = null;
-    private Integer maximum = null;
-
-    DateStatisticsImpl() {
-    }
-
-    DateStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.DateStatistics dateStats = stats.getDateStatistics();
-      // min,max values serialized/deserialized as int (days since epoch)
-      if (dateStats.hasMaximum()) {
-        maximum = dateStats.getMaximum();
-      }
-      if (dateStats.hasMinimum()) {
-        minimum = dateStats.getMinimum();
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      minimum = null;
-      maximum = null;
-    }
-
-    @Override
-    public void updateDate(DateWritable value) {
-      if (minimum == null) {
-        minimum = value.getDays();
-        maximum = value.getDays();
-      } else if (minimum > value.getDays()) {
-        minimum = value.getDays();
-      } else if (maximum < value.getDays()) {
-        maximum = value.getDays();
-      }
-    }
-
-    @Override
-    public void updateDate(int value) {
-      if (minimum == null) {
-        minimum = value;
-        maximum = value;
-      } else if (minimum > value) {
-        minimum = value;
-      } else if (maximum < value) {
-        maximum = value;
-      }
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof DateStatisticsImpl) {
-        DateStatisticsImpl dateStats = (DateStatisticsImpl) other;
-        if (minimum == null) {
-          minimum = dateStats.minimum;
-          maximum = dateStats.maximum;
-        } else if (dateStats.minimum != null) {
-          if (minimum > dateStats.minimum) {
-            minimum = dateStats.minimum;
-          }
-          if (maximum < dateStats.maximum) {
-            maximum = dateStats.maximum;
-          }
-        }
-      } else {
-        if (isStatsExists() && minimum != null) {
-          throw new IllegalArgumentException("Incompatible merging of date column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder result = super.serialize();
-      OrcProto.DateStatistics.Builder dateStats =
-          OrcProto.DateStatistics.newBuilder();
-      if (getNumberOfValues() != 0 && minimum != null) {
-        dateStats.setMinimum(minimum);
-        dateStats.setMaximum(maximum);
-      }
-      result.setDateStatistics(dateStats);
-      return result;
-    }
-
-    private transient final DateWritable minDate = new DateWritable();
-    private transient final DateWritable maxDate = new DateWritable();
-
-    @Override
-    public Date getMinimum() {
-      if (minimum == null) {
-        return null;
-      }
-      minDate.set(minimum);
-      return minDate.get();
-    }
-
-    @Override
-    public Date getMaximum() {
-      if (maximum == null) {
-        return null;
-      }
-      maxDate.set(maximum);
-      return maxDate.get();
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (getNumberOfValues() != 0) {
-        buf.append(" min: ");
-        buf.append(getMinimum());
-        buf.append(" max: ");
-        buf.append(getMaximum());
-      }
-      return buf.toString();
-    }
-  }
-
-  private static final class TimestampStatisticsImpl extends ColumnStatisticsImpl
-      implements TimestampColumnStatistics {
-    private Long minimum = null;
-    private Long maximum = null;
-
-    TimestampStatisticsImpl() {
-    }
-
-    TimestampStatisticsImpl(OrcProto.ColumnStatistics stats) {
-      super(stats);
-      OrcProto.TimestampStatistics timestampStats = stats.getTimestampStatistics();
-      // min,max values serialized/deserialized as int (milliseconds since epoch)
-      if (timestampStats.hasMaximum()) {
-        maximum = timestampStats.getMaximum();
-      }
-      if (timestampStats.hasMinimum()) {
-        minimum = timestampStats.getMinimum();
-      }
-    }
-
-    @Override
-    public void reset() {
-      super.reset();
-      minimum = null;
-      maximum = null;
-    }
-
-    @Override
-    public void updateTimestamp(Timestamp value) {
-      if (minimum == null) {
-        minimum = value.getTime();
-        maximum = value.getTime();
-      } else if (minimum > value.getTime()) {
-        minimum = value.getTime();
-      } else if (maximum < value.getTime()) {
-        maximum = value.getTime();
-      }
-    }
-
-    @Override
-    public void updateTimestamp(long value) {
-      if (minimum == null) {
-        minimum = value;
-        maximum = value;
-      } else if (minimum > value) {
-        minimum = value;
-      } else if (maximum < value) {
-        maximum = value;
-      }
-    }
-
-    @Override
-    public void merge(ColumnStatisticsImpl other) {
-      if (other instanceof TimestampStatisticsImpl) {
-        TimestampStatisticsImpl timestampStats = (TimestampStatisticsImpl) other;
-        if (minimum == null) {
-          minimum = timestampStats.minimum;
-          maximum = timestampStats.maximum;
-        } else if (timestampStats.minimum != null) {
-          if (minimum > timestampStats.minimum) {
-            minimum = timestampStats.minimum;
-          }
-          if (maximum < timestampStats.maximum) {
-            maximum = timestampStats.maximum;
-          }
-        }
-      } else {
-        if (isStatsExists() && minimum != null) {
-          throw new IllegalArgumentException("Incompatible merging of timestamp column statistics");
-        }
-      }
-      super.merge(other);
-    }
-
-    @Override
-    public OrcProto.ColumnStatistics.Builder serialize() {
-      OrcProto.ColumnStatistics.Builder result = super.serialize();
-      OrcProto.TimestampStatistics.Builder timestampStats = OrcProto.TimestampStatistics
-          .newBuilder();
-      if (getNumberOfValues() != 0 && minimum != null) {
-        timestampStats.setMinimum(minimum);
-        timestampStats.setMaximum(maximum);
-      }
-      result.setTimestampStatistics(timestampStats);
-      return result;
-    }
-
-    @Override
-    public Timestamp getMinimum() {
-      return minimum == null ? null : new Timestamp(minimum);
-    }
-
-    @Override
-    public Timestamp getMaximum() {
-      return maximum == null ? null : new Timestamp(maximum);
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder buf = new StringBuilder(super.toString());
-      if (getNumberOfValues() != 0) {
-        buf.append(" min: ");
-        buf.append(getMinimum());
-        buf.append(" max: ");
-        buf.append(getMaximum());
-      }
-      return buf.toString();
-    }
-  }
-
-  private long count = 0;
-  private boolean hasNull = false;
-
-  ColumnStatisticsImpl(OrcProto.ColumnStatistics stats) {
-    if (stats.hasNumberOfValues()) {
-      count = stats.getNumberOfValues();
-    }
-
-    if (stats.hasHasNull()) {
-      hasNull = stats.getHasNull();
-    } else {
-      hasNull = true;
-    }
-  }
-
-  ColumnStatisticsImpl() {
-  }
-
-  public void increment() {
-    count += 1;
-  }
-
-  public void increment(int count) {
-    this.count += count;
-  }
-
-  public void setNull() {
-    hasNull = true;
-  }
-
-  public void updateBoolean(boolean value, int repetitions) {
-    throw new UnsupportedOperationException("Can't update boolean");
-  }
-
-  public void updateInteger(long value, int repetitions) {
-    throw new UnsupportedOperationException("Can't update integer");
-  }
-
-  public void updateDouble(double value) {
-    throw new UnsupportedOperationException("Can't update double");
-  }
-
-  public void updateString(Text value) {
-    throw new UnsupportedOperationException("Can't update string");
-  }
-
-  public void updateString(byte[] bytes, int offset, int length,
-                           int repetitions) {
-    throw new UnsupportedOperationException("Can't update string");
-  }
-
-  public void updateBinary(BytesWritable value) {
-    throw new UnsupportedOperationException("Can't update binary");
-  }
-
-  public void updateBinary(byte[] bytes, int offset, int length,
-                           int repetitions) {
-    throw new UnsupportedOperationException("Can't update string");
-  }
-
-  public void updateDecimal(HiveDecimalWritable value) {
-    throw new UnsupportedOperationException("Can't update decimal");
-  }
-
-  public void updateDate(DateWritable value) {
-    throw new UnsupportedOperationException("Can't update date");
-  }
-
-  public void updateDate(int value) {
-    throw new UnsupportedOperationException("Can't update date");
-  }
-
-  public void updateTimestamp(Timestamp value) {
-    throw new UnsupportedOperationException("Can't update timestamp");
-  }
-
-  public void updateTimestamp(long value) {
-    throw new UnsupportedOperationException("Can't update timestamp");
-  }
-
-  public boolean isStatsExists() {
-    return (count > 0 || hasNull == true);
-  }
-
-  public void merge(ColumnStatisticsImpl stats) {
-    count += stats.count;
-    hasNull |= stats.hasNull;
-  }
-
-  public void reset() {
-    count = 0;
-    hasNull = false;
-  }
-
-  @Override
-  public long getNumberOfValues() {
-    return count;
-  }
-
-  @Override
-  public boolean hasNull() {
-    return hasNull;
-  }
-
-  @Override
-  public String toString() {
-    return "count: " + count + " hasNull: " + hasNull;
-  }
-
-  public OrcProto.ColumnStatistics.Builder serialize() {
-    OrcProto.ColumnStatistics.Builder builder =
-      OrcProto.ColumnStatistics.newBuilder();
-    builder.setNumberOfValues(count);
-    builder.setHasNull(hasNull);
-    return builder;
-  }
-
-  public static ColumnStatisticsImpl create(TypeDescription schema) {
-    switch (schema.getCategory()) {
-      case BOOLEAN:
-        return new BooleanStatisticsImpl();
-      case BYTE:
-      case SHORT:
-      case INT:
-      case LONG:
-        return new IntegerStatisticsImpl();
-      case FLOAT:
-      case DOUBLE:
-        return new DoubleStatisticsImpl();
-      case STRING:
-      case CHAR:
-      case VARCHAR:
-        return new StringStatisticsImpl();
-      case DECIMAL:
-        return new DecimalStatisticsImpl();
-      case DATE:
-        return new DateStatisticsImpl();
-      case TIMESTAMP:
-        return new TimestampStatisticsImpl();
-      case BINARY:
-        return new BinaryStatisticsImpl();
-      default:
-        return new ColumnStatisticsImpl();
-    }
-  }
-
-  public static ColumnStatisticsImpl deserialize(OrcProto.ColumnStatistics stats) {
-    if (stats.hasBucketStatistics()) {
-      return new BooleanStatisticsImpl(stats);
-    } else if (stats.hasIntStatistics()) {
-      return new IntegerStatisticsImpl(stats);
-    } else if (stats.hasDoubleStatistics()) {
-      return new DoubleStatisticsImpl(stats);
-    } else if (stats.hasStringStatistics()) {
-      return new StringStatisticsImpl(stats);
-    } else if (stats.hasDecimalStatistics()) {
-      return new DecimalStatisticsImpl(stats);
-    } else if (stats.hasDateStatistics()) {
-      return new DateStatisticsImpl(stats);
-    } else if (stats.hasTimestampStatistics()) {
-      return new TimestampStatisticsImpl(stats);
-    } else if(stats.hasBinaryStatistics()) {
-      return new BinaryStatisticsImpl(stats);
-    } else {
-      return new ColumnStatisticsImpl(stats);
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java b/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java
deleted file mode 100644
index e60075f9b6..0000000000
--- a/orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java
+++ /dev/null
@@ -1,2930 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.sql.Date;
-import java.sql.Timestamp;
-import java.util.EnumMap;
-import java.util.Map;
-
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
-import org.apache.hadoop.hive.ql.util.TimestampUtils;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.orc.OrcProto;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.TypeDescription.Category;
-
-/**
- * Convert ORC tree readers.
- */
-public class ConvertTreeReaderFactory extends TreeReaderFactory {
-
-  /**
-   * Override methods like checkEncoding to pass-thru to the convert TreeReader.
-   */
-  public static class ConvertTreeReader extends TreeReader {
-
-    private TreeReader convertTreeReader;
-
-    ConvertTreeReader(int columnId) throws IOException {
-      super(columnId);
-    }
-
-    // The ordering of types here is used to determine which numeric types
-    // are common/convertible to one another. Probably better to rely on the
-    // ordering explicitly defined here than to assume that the enum values
-    // that were arbitrarily assigned in PrimitiveCategory work for our purposes.
-    private static EnumMap<TypeDescription.Category, Integer> numericTypes =
-        new EnumMap<>(TypeDescription.Category.class);
-
-    static {
-      registerNumericType(TypeDescription.Category.BOOLEAN, 1);
-      registerNumericType(TypeDescription.Category.BYTE, 2);
-      registerNumericType(TypeDescription.Category.SHORT, 3);
-      registerNumericType(TypeDescription.Category.INT, 4);
-      registerNumericType(TypeDescription.Category.LONG, 5);
-      registerNumericType(TypeDescription.Category.FLOAT, 6);
-      registerNumericType(TypeDescription.Category.DOUBLE, 7);
-      registerNumericType(TypeDescription.Category.DECIMAL, 8);
-    }
-
-    private static void registerNumericType(TypeDescription.Category kind, int level) {
-      numericTypes.put(kind, level);
-    }
-
-    protected void setConvertTreeReader(TreeReader convertTreeReader) {
-      this.convertTreeReader = convertTreeReader;
-    }
-
-    protected TreeReader getStringGroupTreeReader(int columnId,
-        TypeDescription fileType) throws IOException {
-      switch (fileType.getCategory()) {
-      case STRING:
-        return new StringTreeReader(columnId);
-      case CHAR:
-        return new CharTreeReader(columnId, fileType.getMaxLength());
-      case VARCHAR:
-        return new VarcharTreeReader(columnId, fileType.getMaxLength());
-      default:
-        throw new RuntimeException("Unexpected type kind " + fileType.getCategory().name());
-      }
-    }
-
-    protected void assignStringGroupVectorEntry(BytesColumnVector bytesColVector,
-        int elementNum, TypeDescription readerType, byte[] bytes) {
-      assignStringGroupVectorEntry(bytesColVector,
-          elementNum, readerType, bytes, 0, bytes.length);
-    }
-
-    /*
-     * Assign a BytesColumnVector entry when we have a byte array, start, and
-     * length for the string group which can be (STRING, CHAR, VARCHAR).
-     */
-    protected void assignStringGroupVectorEntry(BytesColumnVector bytesColVector,
-        int elementNum, TypeDescription readerType, byte[] bytes, int start, int length) {
-      switch (readerType.getCategory()) {
-      case STRING:
-        bytesColVector.setVal(elementNum, bytes, start, length);
-        break;
-      case CHAR:
-        {
-          int adjustedDownLen =
-              StringExpr.rightTrimAndTruncate(bytes, start, length, readerType.getMaxLength());
-          bytesColVector.setVal(elementNum, bytes, start, adjustedDownLen);
-        }
-        break;
-      case VARCHAR:
-        {
-          int adjustedDownLen =
-              StringExpr.truncate(bytes, start, length, readerType.getMaxLength());
-          bytesColVector.setVal(elementNum, bytes, start, adjustedDownLen);
-        }
-        break;
-      default:
-        throw new RuntimeException("Unexpected type kind " + readerType.getCategory().name());
-      }
-    }
-
-    protected void convertStringGroupVectorElement(BytesColumnVector bytesColVector,
-        int elementNum, TypeDescription readerType) {
-      switch (readerType.getCategory()) {
-      case STRING:
-        // No conversion needed.
-        break;
-      case CHAR:
-        {
-          int length = bytesColVector.length[elementNum];
-          int adjustedDownLen = StringExpr
-            .rightTrimAndTruncate(bytesColVector.vector[elementNum],
-                bytesColVector.start[elementNum], length,
-                readerType.getMaxLength());
-          if (adjustedDownLen < length) {
-            bytesColVector.length[elementNum] = adjustedDownLen;
-          }
-        }
-        break;
-      case VARCHAR:
-        {
-          int length = bytesColVector.length[elementNum];
-          int adjustedDownLen = StringExpr
-            .truncate(bytesColVector.vector[elementNum],
-                bytesColVector.start[elementNum], length,
-                readerType.getMaxLength());
-          if (adjustedDownLen < length) {
-            bytesColVector.length[elementNum] = adjustedDownLen;
-          }
-        }
-        break;
-      default:
-        throw new RuntimeException("Unexpected type kind " + readerType.getCategory().name());
-      }
-    }
-
-    private boolean isParseError;
-
-    /*
-     * We do this because we want the various parse methods return a primitive.
-     *
-     * @return true if there was a parse error in the last call to
-     * parseLongFromString, etc.
-     */
-    protected boolean getIsParseError() {
-      return isParseError;
-    }
-
-    protected long parseLongFromString(String string) {
-      try {
-        long longValue = Long.parseLong(string);
-        isParseError = false;
-        return longValue;
-      } catch (NumberFormatException e) {
-        isParseError = true;
-        return 0;
-      }
-    }
-
-    protected float parseFloatFromString(String string) {
-      try {
-        float floatValue = Float.parseFloat(string);
-        isParseError = false;
-        return floatValue;
-      } catch (NumberFormatException e) {
-        isParseError = true;
-        return Float.NaN;
-      }
-    }
-
-    protected double parseDoubleFromString(String string) {
-      try {
-        double value = Double.parseDouble(string);
-        isParseError = false;
-        return value;
-      } catch (NumberFormatException e) {
-        isParseError = true;
-        return Double.NaN;
-      }
-    }
-
-    /**
-     * @param string
-     * @return the HiveDecimal parsed, or null if there was a parse error.
-     */
-    protected HiveDecimal parseDecimalFromString(String string) {
-      try {
-        HiveDecimal value = HiveDecimal.create(string);
-        return value;
-      } catch (NumberFormatException e) {
-        return null;
-      }
-    }
-
-    /**
-     * @param string
-     * @return the Timestamp parsed, or null if there was a parse error.
-     */
-    protected Timestamp parseTimestampFromString(String string) {
-      try {
-        Timestamp value = Timestamp.valueOf(string);
-        return value;
-      } catch (IllegalArgumentException e) {
-        return null;
-      }
-    }
-
-    /**
-     * @param string
-     * @return the Date parsed, or null if there was a parse error.
-     */
-    protected Date parseDateFromString(String string) {
-      try {
-        Date value = Date.valueOf(string);
-        return value;
-      } catch (IllegalArgumentException e) {
-        return null;
-      }
-    }
-
-    protected String stringFromBytesColumnVectorEntry(
-        BytesColumnVector bytesColVector, int elementNum) {
-      String string;
-
-      string = new String(
-          bytesColVector.vector[elementNum],
-          bytesColVector.start[elementNum], bytesColVector.length[elementNum],
-          StandardCharsets.UTF_8);
-
-      return string;
-    }
-
-    private static final double MIN_LONG_AS_DOUBLE = -0x1p63;
-    /*
-     * We cannot store Long.MAX_VALUE as a double without losing precision. Instead, we store
-     * Long.MAX_VALUE + 1 == -Long.MIN_VALUE, and then offset all comparisons by 1.
-     */
-    private static final double MAX_LONG_AS_DOUBLE_PLUS_ONE = 0x1p63;
-
-    public boolean doubleCanFitInLong(double doubleValue) {
-
-      // Borrowed from Guava DoubleMath.roundToLong except do not want dependency on Guava and we
-      // don't want to catch an exception.
-
-      return ((MIN_LONG_AS_DOUBLE - doubleValue < 1.0) &&
-              (doubleValue < MAX_LONG_AS_DOUBLE_PLUS_ONE));
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      // Pass-thru.
-      convertTreeReader.checkEncoding(encoding);
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      // Pass-thru.
-      convertTreeReader.startStripe(streams, stripeFooter);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-     // Pass-thru.
-      convertTreeReader.seek(index);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      // Pass-thru.
-      convertTreeReader.seek(index);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      // Pass-thru.
-      convertTreeReader.skipRows(items);
-    }
-
-    /**
-     * Override this to use convertVector.
-     * Source and result are member variables in the subclass with the right
-     * type.
-     * @param elementNum
-     * @throws IOException
-     */
-    // Override this to use convertVector.
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      throw new RuntimeException("Expected this method to be overriden");
-    }
-
-    // Common code used by the conversion.
-    public void convertVector(ColumnVector fromColVector,
-        ColumnVector resultColVector, final int batchSize) throws IOException {
-
-      resultColVector.reset();
-      if (fromColVector.isRepeating) {
-        resultColVector.isRepeating = true;
-        if (fromColVector.noNulls || !fromColVector.isNull[0]) {
-          setConvertVectorElement(0);
-        } else {
-          resultColVector.noNulls = false;
-          resultColVector.isNull[0] = true;
-        }
-      } else if (fromColVector.noNulls){
-        for (int i = 0; i < batchSize; i++) {
-          setConvertVectorElement(i);
-        }
-      } else {
-        for (int i = 0; i < batchSize; i++) {
-          if (!fromColVector.isNull[i]) {
-            setConvertVectorElement(i);
-          } else {
-            resultColVector.noNulls = false;
-            resultColVector.isNull[i] = true;
-          }
-        }
-      }
-    }
-
-    public void downCastAnyInteger(LongColumnVector longColVector, int elementNum,
-        TypeDescription readerType) {
-      downCastAnyInteger(longColVector, elementNum, longColVector.vector[elementNum], readerType);
-    }
-
-    public void downCastAnyInteger(LongColumnVector longColVector, int elementNum, long inputLong,
-        TypeDescription readerType) {
-      long[] vector = longColVector.vector;
-      long outputLong;
-      Category readerCategory = readerType.getCategory();
-      switch (readerCategory) {
-      case BOOLEAN:
-        // No data loss for boolean.
-        vector[elementNum] = inputLong == 0 ? 0 : 1;
-        return;
-      case BYTE:
-        outputLong = (byte) inputLong;
-        break;
-      case SHORT:
-        outputLong = (short) inputLong;
-        break;
-      case INT:
-        outputLong = (int) inputLong;
-        break;
-      case LONG:
-        // No data loss for long.
-        vector[elementNum] = inputLong;
-        return;
-      default:
-        throw new RuntimeException("Unexpected type kind " + readerCategory.name());
-      }
-
-      if (outputLong != inputLong) {
-        // Data loss.
-        longColVector.isNull[elementNum] = true;
-        longColVector.noNulls = false;
-      } else {
-        vector[elementNum] = outputLong;
-      }
-    }
-
-    protected boolean integerDownCastNeeded(TypeDescription fileType, TypeDescription readerType) {
-      Integer fileLevel = numericTypes.get(fileType.getCategory());
-      Integer schemaLevel = numericTypes.get(readerType.getCategory());
-      return (schemaLevel.intValue() < fileLevel.intValue());
-    }
-  }
-
-  public static class AnyIntegerTreeReader extends ConvertTreeReader {
-
-    private TypeDescription.Category fileTypeCategory;
-    private TreeReader anyIntegerTreeReader;
-
-    private long longValue;
-
-    AnyIntegerTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.fileTypeCategory = fileType.getCategory();
-      switch (fileTypeCategory) {
-      case BOOLEAN:
-        anyIntegerTreeReader = new BooleanTreeReader(columnId);
-        break;
-      case BYTE:
-        anyIntegerTreeReader = new ByteTreeReader(columnId);
-        break;
-      case SHORT:
-        anyIntegerTreeReader = new ShortTreeReader(columnId);
-        break;
-      case INT:
-        anyIntegerTreeReader = new IntTreeReader(columnId);
-        break;
-      case LONG:
-        anyIntegerTreeReader = new LongTreeReader(columnId, skipCorrupt);
-        break;
-      default:
-        throw new RuntimeException("Unexpected type kind " + fileType.getCategory().name());
-      }
-      setConvertTreeReader(anyIntegerTreeReader);
-    }
-
-    protected long getLong() throws IOException {
-      return longValue;
-    }
-
-    protected String getString(long longValue) {
-      if (fileTypeCategory == TypeDescription.Category.BOOLEAN) {
-        return longValue == 0 ? "FALSE" : "TRUE";
-      } else {
-        return Long.toString(longValue);
-      }
-    }
-
-    protected String getString() {
-      return getString(longValue);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      anyIntegerTreeReader.nextVector(previousVector, isNull, batchSize);
-    }
-  }
-
-  public static class AnyIntegerFromAnyIntegerTreeReader extends ConvertTreeReader {
-
-    private AnyIntegerTreeReader anyIntegerAsLongTreeReader;
-
-    private final TypeDescription readerType;
-    private final boolean downCastNeeded;
-
-    AnyIntegerFromAnyIntegerTreeReader(int columnId, TypeDescription fileType, TypeDescription readerType, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      anyIntegerAsLongTreeReader = new AnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-      setConvertTreeReader(anyIntegerAsLongTreeReader);
-      downCastNeeded = integerDownCastNeeded(fileType, readerType);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      anyIntegerAsLongTreeReader.nextVector(previousVector, isNull, batchSize);
-      LongColumnVector resultColVector = (LongColumnVector) previousVector;
-      if (downCastNeeded) {
-        if (resultColVector.isRepeating) {
-          if (resultColVector.noNulls || !resultColVector.isNull[0]) {
-            downCastAnyInteger(resultColVector, 0, readerType);
-          } else {
-            // Result remains null.
-          }
-        } else if (resultColVector.noNulls){
-          for (int i = 0; i < batchSize; i++) {
-            downCastAnyInteger(resultColVector, i, readerType);
-          }
-        } else {
-          for (int i = 0; i < batchSize; i++) {
-            if (!resultColVector.isNull[i]) {
-              downCastAnyInteger(resultColVector, i, readerType);
-            } else {
-              // Result remains null.
-            }
-          }
-        }
-      }
-    }
-  }
-
-  public static class AnyIntegerFromFloatTreeReader extends ConvertTreeReader {
-
-    private FloatTreeReader floatTreeReader;
-
-    private final TypeDescription readerType;
-    private DoubleColumnVector doubleColVector;
-    private LongColumnVector longColVector;
-
-    AnyIntegerFromFloatTreeReader(int columnId, TypeDescription readerType)
-        throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      floatTreeReader = new FloatTreeReader(columnId);
-      setConvertTreeReader(floatTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      double doubleValue = doubleColVector.vector[elementNum];
-      if (!doubleCanFitInLong(doubleValue)) {
-        longColVector.isNull[elementNum] = true;
-        longColVector.noNulls = false;
-      } else {
-        // UNDONE: Does the overflow check above using double really work here for float?
-        float floatValue = (float) doubleValue;
-        downCastAnyInteger(longColVector, elementNum, (long) floatValue, readerType);
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      floatTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class AnyIntegerFromDoubleTreeReader extends ConvertTreeReader {
-
-    private DoubleTreeReader doubleTreeReader;
-
-    private final TypeDescription readerType;
-    private DoubleColumnVector doubleColVector;
-    private LongColumnVector longColVector;
-
-    AnyIntegerFromDoubleTreeReader(int columnId, TypeDescription readerType)
-        throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      doubleTreeReader = new DoubleTreeReader(columnId);
-      setConvertTreeReader(doubleTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      double doubleValue = doubleColVector.vector[elementNum];
-      if (!doubleCanFitInLong(doubleValue)) {
-        longColVector.isNull[elementNum] = true;
-        longColVector.noNulls = false;
-      } else {
-        downCastAnyInteger(longColVector, elementNum, (long) doubleValue, readerType);
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      doubleTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class AnyIntegerFromDecimalTreeReader extends ConvertTreeReader {
-
-    private DecimalTreeReader decimalTreeReader;
-
-    private final int precision;
-    private final int scale;
-    private final TypeDescription readerType;
-    private DecimalColumnVector decimalColVector;
-    private LongColumnVector longColVector;
-
-    AnyIntegerFromDecimalTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType) throws IOException {
-      super(columnId);
-      this.precision = fileType.getPrecision();
-      this.scale = fileType.getScale();
-      this.readerType = readerType;
-      decimalTreeReader = new DecimalTreeReader(columnId, precision, scale);
-      setConvertTreeReader(decimalTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      HiveDecimalWritable decWritable = decimalColVector.vector[elementNum];
-      long[] vector = longColVector.vector;
-      Category readerCategory = readerType.getCategory();
-
-      // Check to see if the decimal will fit in the Hive integer data type.
-      // If not, set the element to null.
-      boolean isInRange;
-      switch (readerCategory) {
-      case BOOLEAN:
-        // No data loss for boolean.
-        vector[elementNum] = decWritable.signum() == 0 ? 0 : 1;
-        return;
-      case BYTE:
-        isInRange = decWritable.isByte();
-        break;
-      case SHORT:
-        isInRange = decWritable.isShort();
-        break;
-      case INT:
-        isInRange = decWritable.isInt();
-        break;
-      case LONG:
-        isInRange = decWritable.isLong();
-        break;
-      default:
-        throw new RuntimeException("Unexpected type kind " + readerCategory.name());
-      }
-      if (!isInRange) {
-        longColVector.isNull[elementNum] = true;
-        longColVector.noNulls = false;
-        return;
-      }
-      switch (readerCategory) {
-      case BYTE:
-        vector[elementNum] = decWritable.byteValue();
-        break;
-      case SHORT:
-        vector[elementNum] = decWritable.shortValue();
-        break;
-      case INT:
-        vector[elementNum] = decWritable.intValue();
-        break;
-      case LONG:
-        vector[elementNum] = decWritable.longValue();
-        break;
-      default:
-        throw new RuntimeException("Unexpected type kind " + readerCategory.name());
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (decimalColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        decimalColVector = new DecimalColumnVector(precision, scale);
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      decimalTreeReader.nextVector(decimalColVector, isNull, batchSize);
-
-      convertVector(decimalColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class AnyIntegerFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private final TypeDescription readerType;
-    private BytesColumnVector bytesColVector;
-    private LongColumnVector longColVector;
-
-    AnyIntegerFromStringGroupTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String string = stringFromBytesColumnVectorEntry(bytesColVector, elementNum);
-      long longValue = parseLongFromString(string);
-      if (!getIsParseError()) {
-        downCastAnyInteger(longColVector, elementNum, longValue, readerType);
-      } else {
-        longColVector.noNulls = false;
-        longColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (bytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        bytesColVector = new BytesColumnVector();
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      stringGroupTreeReader.nextVector(bytesColVector, isNull, batchSize);
-
-      convertVector(bytesColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class AnyIntegerFromTimestampTreeReader extends ConvertTreeReader {
-
-    private TimestampTreeReader timestampTreeReader;
-
-    private final TypeDescription readerType;
-    private TimestampColumnVector timestampColVector;
-    private LongColumnVector longColVector;
-
-    AnyIntegerFromTimestampTreeReader(int columnId, TypeDescription readerType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      timestampTreeReader = new TimestampTreeReader(columnId, skipCorrupt);
-      setConvertTreeReader(timestampTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      // Use TimestampWritable's getSeconds.
-      long longValue = TimestampUtils.millisToSeconds(
-          timestampColVector.asScratchTimestamp(elementNum).getTime());
-      downCastAnyInteger(longColVector, elementNum, longValue, readerType);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (timestampColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        timestampColVector = new TimestampColumnVector();
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      timestampTreeReader.nextVector(timestampColVector, isNull, batchSize);
-
-      convertVector(timestampColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class FloatFromAnyIntegerTreeReader extends ConvertTreeReader {
-
-    private AnyIntegerTreeReader anyIntegerAsLongTreeReader;
-
-    private LongColumnVector longColVector;
-    private DoubleColumnVector doubleColVector;
-
-    FloatFromAnyIntegerTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      anyIntegerAsLongTreeReader =
-          new AnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-      setConvertTreeReader(anyIntegerAsLongTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      float floatValue = (float) longColVector.vector[elementNum];
-      if (!Float.isNaN(floatValue)) {
-        doubleColVector.vector[elementNum] = floatValue;
-      } else {
-        doubleColVector.vector[elementNum] = Double.NaN;
-        doubleColVector.noNulls = false;
-        doubleColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      anyIntegerAsLongTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class FloatFromDoubleTreeReader extends ConvertTreeReader {
-
-    private DoubleTreeReader doubleTreeReader;
-
-    FloatFromDoubleTreeReader(int columnId) throws IOException {
-      super(columnId);
-      doubleTreeReader = new DoubleTreeReader(columnId);
-      setConvertTreeReader(doubleTreeReader);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      doubleTreeReader.nextVector(previousVector, isNull, batchSize);
-
-      DoubleColumnVector resultColVector = (DoubleColumnVector) previousVector;
-      double[] resultVector = resultColVector.vector;
-      if (resultColVector.isRepeating) {
-        if (resultColVector.noNulls || !resultColVector.isNull[0]) {
-          resultVector[0] = (float) resultVector[0];
-        } else {
-          // Remains null.
-        }
-      } else if (resultColVector.noNulls){
-        for (int i = 0; i < batchSize; i++) {
-          resultVector[i] = (float) resultVector[i];
-        }
-      } else {
-        for (int i = 0; i < batchSize; i++) {
-          if (!resultColVector.isNull[i]) {
-            resultVector[i] = (float) resultVector[i];
-          } else {
-            // Remains null.
-          }
-        }
-      }
-    }
-  }
-
-  public static class FloatFromDecimalTreeReader extends ConvertTreeReader {
-
-    private DecimalTreeReader decimalTreeReader;
-
-    private final int precision;
-    private final int scale;
-    private DecimalColumnVector decimalColVector;
-    private DoubleColumnVector doubleColVector;
-
-    FloatFromDecimalTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType) throws IOException {
-      super(columnId);
-      this.precision = fileType.getPrecision();
-      this.scale = fileType.getScale();
-      decimalTreeReader = new DecimalTreeReader(columnId, precision, scale);
-      setConvertTreeReader(decimalTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      doubleColVector.vector[elementNum] =
-          (float) decimalColVector.vector[elementNum].doubleValue();
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (decimalColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        decimalColVector = new DecimalColumnVector(precision, scale);
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      decimalTreeReader.nextVector(decimalColVector, isNull, batchSize);
-
-      convertVector(decimalColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class FloatFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private BytesColumnVector bytesColVector;
-    private DoubleColumnVector doubleColVector;
-
-    FloatFromStringGroupTreeReader(int columnId, TypeDescription fileType)
-        throws IOException {
-      super(columnId);
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String string = stringFromBytesColumnVectorEntry(bytesColVector, elementNum);
-      float floatValue = parseFloatFromString(string);
-      if (!getIsParseError()) {
-        doubleColVector.vector[elementNum] = floatValue;
-      } else {
-        doubleColVector.vector[elementNum] = Double.NaN;
-        doubleColVector.noNulls = false;
-        doubleColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (bytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        bytesColVector = new BytesColumnVector();
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      stringGroupTreeReader.nextVector(bytesColVector, isNull, batchSize);
-
-      convertVector(bytesColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class FloatFromTimestampTreeReader extends ConvertTreeReader {
-
-    private TimestampTreeReader timestampTreeReader;
-
-    private TimestampColumnVector timestampColVector;
-    private DoubleColumnVector doubleColVector;
-
-    FloatFromTimestampTreeReader(int columnId, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      timestampTreeReader = new TimestampTreeReader(columnId, skipCorrupt);
-      setConvertTreeReader(timestampTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      doubleColVector.vector[elementNum] = (float) TimestampUtils.getDouble(
-          timestampColVector.asScratchTimestamp(elementNum));
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (timestampColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        timestampColVector = new TimestampColumnVector();
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      timestampTreeReader.nextVector(timestampColVector, isNull, batchSize);
-
-      convertVector(timestampColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class DoubleFromAnyIntegerTreeReader extends ConvertTreeReader {
-
-    private AnyIntegerTreeReader anyIntegerAsLongTreeReader;
-
-    private LongColumnVector longColVector;
-    private DoubleColumnVector doubleColVector;
-
-    DoubleFromAnyIntegerTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      anyIntegerAsLongTreeReader =
-          new AnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-      setConvertTreeReader(anyIntegerAsLongTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-
-      double doubleValue = (double) longColVector.vector[elementNum];
-      if (!Double.isNaN(doubleValue)) {
-        doubleColVector.vector[elementNum] = doubleValue;
-      } else {
-        doubleColVector.vector[elementNum] = Double.NaN;
-        doubleColVector.noNulls = false;
-        doubleColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      anyIntegerAsLongTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class DoubleFromFloatTreeReader extends ConvertTreeReader {
-
-    private FloatTreeReader floatTreeReader;
-
-    DoubleFromFloatTreeReader(int columnId) throws IOException {
-      super(columnId);
-      floatTreeReader = new FloatTreeReader(columnId);
-      setConvertTreeReader(floatTreeReader);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      // we get the DoubleColumnVector produced by float tree reader first, then iterate through
-      // the elements and make double -> float -> string -> double conversion to preserve the
-      // precision. When float tree reader reads float and assign it to double, java's widening
-      // conversion adds more precision which will break all comparisons.
-      // Example: float f = 74.72
-      // double d = f ---> 74.72000122070312
-      // Double.parseDouble(String.valueOf(f)) ---> 74.72
-      floatTreeReader.nextVector(previousVector, isNull, batchSize);
-
-      DoubleColumnVector doubleColumnVector = (DoubleColumnVector) previousVector;
-      if (doubleColumnVector.isRepeating) {
-        if (doubleColumnVector.noNulls || !doubleColumnVector.isNull[0]) {
-          final float f = (float) doubleColumnVector.vector[0];
-          doubleColumnVector.vector[0] = Double.parseDouble(String.valueOf(f));
-        }
-      } else if (doubleColumnVector.noNulls){
-        for (int i = 0; i < batchSize; i++) {
-          final float f = (float) doubleColumnVector.vector[i];
-          doubleColumnVector.vector[i] = Double.parseDouble(String.valueOf(f));
-        }
-      } else {
-        for (int i = 0; i < batchSize; i++) {
-          if (!doubleColumnVector.isNull[i]) {
-            final float f = (float) doubleColumnVector.vector[i];
-            doubleColumnVector.vector[i] = Double.parseDouble(String.valueOf(f));
-          }
-        }
-      }
-    }
-  }
-
-  public static class DoubleFromDecimalTreeReader extends ConvertTreeReader {
-
-    private DecimalTreeReader decimalTreeReader;
-
-    private final int precision;
-    private final int scale;
-    private DecimalColumnVector decimalColVector;
-    private DoubleColumnVector doubleColVector;
-
-    DoubleFromDecimalTreeReader(int columnId, TypeDescription fileType) throws IOException {
-      super(columnId);
-      this.precision = fileType.getPrecision();
-      this.scale = fileType.getScale();
-      decimalTreeReader = new DecimalTreeReader(columnId, precision, scale);
-      setConvertTreeReader(decimalTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      doubleColVector.vector[elementNum] =
-          decimalColVector.vector[elementNum].doubleValue();
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (decimalColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        decimalColVector = new DecimalColumnVector(precision, scale);
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      decimalTreeReader.nextVector(decimalColVector, isNull, batchSize);
-
-      convertVector(decimalColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class DoubleFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private BytesColumnVector bytesColVector;
-    private DoubleColumnVector doubleColVector;
-
-    DoubleFromStringGroupTreeReader(int columnId, TypeDescription fileType)
-        throws IOException {
-      super(columnId);
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String string = stringFromBytesColumnVectorEntry(bytesColVector, elementNum);
-      double doubleValue = parseDoubleFromString(string);
-      if (!getIsParseError()) {
-        doubleColVector.vector[elementNum] = doubleValue;
-      } else {
-        doubleColVector.noNulls = false;
-        doubleColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (bytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        bytesColVector = new BytesColumnVector();
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      stringGroupTreeReader.nextVector(bytesColVector, isNull, batchSize);
-
-      convertVector(bytesColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class DoubleFromTimestampTreeReader extends ConvertTreeReader {
-
-    private TimestampTreeReader timestampTreeReader;
-
-    private TimestampColumnVector timestampColVector;
-    private DoubleColumnVector doubleColVector;
-
-    DoubleFromTimestampTreeReader(int columnId, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      timestampTreeReader = new TimestampTreeReader(columnId, skipCorrupt);
-      setConvertTreeReader(timestampTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      doubleColVector.vector[elementNum] = TimestampUtils.getDouble(
-          timestampColVector.asScratchTimestamp(elementNum));
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (timestampColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        timestampColVector = new TimestampColumnVector();
-        doubleColVector = (DoubleColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      timestampTreeReader.nextVector(timestampColVector, isNull, batchSize);
-
-      convertVector(timestampColVector, doubleColVector, batchSize);
-    }
-  }
-
-  public static class DecimalFromAnyIntegerTreeReader extends ConvertTreeReader {
-
-    private AnyIntegerTreeReader anyIntegerAsLongTreeReader;
-
-    private LongColumnVector longColVector;
-    private DecimalColumnVector decimalColVector;
-
-    DecimalFromAnyIntegerTreeReader(int columnId, TypeDescription fileType, boolean skipCorrupt)
-        throws IOException {
-      super(columnId);
-      anyIntegerAsLongTreeReader =
-          new AnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-      setConvertTreeReader(anyIntegerAsLongTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      long longValue = longColVector.vector[elementNum];
-      HiveDecimalWritable hiveDecimalWritable = new HiveDecimalWritable(longValue);
-      // The DecimalColumnVector will enforce precision and scale and set the entry to null when out of bounds.
-      decimalColVector.set(elementNum, hiveDecimalWritable);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-        boolean[] isNull,
-        final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        decimalColVector = (DecimalColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      anyIntegerAsLongTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, decimalColVector, batchSize);
-    }
-  }
-
-  public static class DecimalFromFloatTreeReader extends ConvertTreeReader {
-
-    private FloatTreeReader floatTreeReader;
-
-    private DoubleColumnVector doubleColVector;
-    private DecimalColumnVector decimalColVector;
-
-    DecimalFromFloatTreeReader(int columnId, TypeDescription readerType)
-        throws IOException {
-      super(columnId);
-      floatTreeReader = new FloatTreeReader(columnId);
-      setConvertTreeReader(floatTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      float floatValue = (float) doubleColVector.vector[elementNum];
-      if (!Float.isNaN(floatValue)) {
-        HiveDecimal decimalValue =
-            HiveDecimal.create(Float.toString(floatValue));
-        if (decimalValue != null) {
-          // The DecimalColumnVector will enforce precision and scale and set the entry to null when out of bounds.
-          decimalColVector.set(elementNum, decimalValue);
-        } else {
-          decimalColVector.noNulls = false;
-          decimalColVector.isNull[elementNum] = true;
-        }
-      } else {
-        decimalColVector.noNulls = false;
-        decimalColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        decimalColVector = (DecimalColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      floatTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, decimalColVector, batchSize);
-    }
-  }
-
-  public static class DecimalFromDoubleTreeReader extends ConvertTreeReader {
-
-    private DoubleTreeReader doubleTreeReader;
-
-    private DoubleColumnVector doubleColVector;
-    private DecimalColumnVector decimalColVector;
-
-    DecimalFromDoubleTreeReader(int columnId, TypeDescription readerType)
-        throws IOException {
-      super(columnId);
-      doubleTreeReader = new DoubleTreeReader(columnId);
-      setConvertTreeReader(doubleTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      HiveDecimal value =
-          HiveDecimal.create(Double.toString(doubleColVector.vector[elementNum]));
-      if (value != null) {
-        decimalColVector.set(elementNum, value);
-      } else {
-        decimalColVector.noNulls = false;
-        decimalColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        decimalColVector = (DecimalColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      doubleTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, decimalColVector, batchSize);
-    }
-  }
-
-  public static class DecimalFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private BytesColumnVector bytesColVector;
-    private DecimalColumnVector decimalColVector;
-
-    DecimalFromStringGroupTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType) throws IOException {
-      super(columnId);
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String string = stringFromBytesColumnVectorEntry(bytesColVector, elementNum);
-      HiveDecimal value = parseDecimalFromString(string);
-      if (value != null) {
-        // The DecimalColumnVector will enforce precision and scale and set the entry to null when out of bounds.
-        decimalColVector.set(elementNum, value);
-      } else {
-        decimalColVector.noNulls = false;
-        decimalColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (bytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        bytesColVector = new BytesColumnVector();
-        decimalColVector = (DecimalColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      stringGroupTreeReader.nextVector(bytesColVector, isNull, batchSize);
-
-      convertVector(bytesColVector, decimalColVector, batchSize);
-    }
-  }
-
-  public static class DecimalFromTimestampTreeReader extends ConvertTreeReader {
-
-    private TimestampTreeReader timestampTreeReader;
-
-    private TimestampColumnVector timestampColVector;
-    private DecimalColumnVector decimalColVector;
-
-    DecimalFromTimestampTreeReader(int columnId, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      timestampTreeReader = new TimestampTreeReader(columnId, skipCorrupt);
-      setConvertTreeReader(timestampTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      double doubleValue = TimestampUtils.getDouble(
-          timestampColVector.asScratchTimestamp(elementNum));
-      HiveDecimal value = HiveDecimal.create(Double.toString(doubleValue));
-      if (value != null) {
-        // The DecimalColumnVector will enforce precision and scale and set the entry to null when out of bounds.
-        decimalColVector.set(elementNum, value);
-      } else {
-        decimalColVector.noNulls = false;
-        decimalColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (timestampColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        timestampColVector = new TimestampColumnVector();
-        decimalColVector = (DecimalColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      timestampTreeReader.nextVector(timestampColVector, isNull, batchSize);
-
-      convertVector(timestampColVector, decimalColVector, batchSize);
-    }
-  }
-
-  public static class DecimalFromDecimalTreeReader extends ConvertTreeReader {
-
-    private DecimalTreeReader decimalTreeReader;
-
-    private DecimalColumnVector fileDecimalColVector;
-    private int filePrecision;
-    private int fileScale;
-    private int readerPrecision;
-    private int readerScale;
-    private DecimalColumnVector decimalColVector;
-
-    DecimalFromDecimalTreeReader(int columnId, TypeDescription fileType, TypeDescription readerType)
-        throws IOException {
-      super(columnId);
-      filePrecision = fileType.getPrecision();
-      fileScale = fileType.getScale();
-      readerPrecision = readerType.getPrecision();
-      readerScale = readerType.getScale();
-      decimalTreeReader = new DecimalTreeReader(columnId, filePrecision, fileScale);
-      setConvertTreeReader(decimalTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-
-      decimalColVector.set(elementNum, fileDecimalColVector.vector[elementNum]);
-
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (fileDecimalColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        fileDecimalColVector = new DecimalColumnVector(filePrecision, fileScale);
-        decimalColVector = (DecimalColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      decimalTreeReader.nextVector(fileDecimalColVector, isNull, batchSize);
-
-      convertVector(fileDecimalColVector, decimalColVector, batchSize);
-    }
-  }
-
-  public static class StringGroupFromAnyIntegerTreeReader extends ConvertTreeReader {
-
-    private AnyIntegerTreeReader anyIntegerAsLongTreeReader;
-
-    private final TypeDescription readerType;
-    private LongColumnVector longColVector;
-    private BytesColumnVector bytesColVector;
-
-    StringGroupFromAnyIntegerTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      anyIntegerAsLongTreeReader =
-          new AnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-      setConvertTreeReader(anyIntegerAsLongTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      long longValue = longColVector.vector[elementNum];
-      String string = anyIntegerAsLongTreeReader.getString(longValue);
-      byte[] bytes = string.getBytes();
-      assignStringGroupVectorEntry(bytesColVector, elementNum, readerType, bytes);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        bytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      anyIntegerAsLongTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, bytesColVector, batchSize);
-    }
-  }
-
-  public static class StringGroupFromFloatTreeReader extends ConvertTreeReader {
-
-    private FloatTreeReader floatTreeReader;
-
-    private final TypeDescription readerType;
-    private DoubleColumnVector doubleColVector;
-    private BytesColumnVector bytesColVector;
-
-
-    StringGroupFromFloatTreeReader(int columnId, TypeDescription readerType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      floatTreeReader = new FloatTreeReader(columnId);
-      setConvertTreeReader(floatTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      float floatValue = (float) doubleColVector.vector[elementNum];
-      if (!Float.isNaN(floatValue)) {
-        String string = String.valueOf(floatValue);
-        byte[] bytes = string.getBytes();
-        assignStringGroupVectorEntry(bytesColVector, elementNum, readerType, bytes);
-      } else {
-        bytesColVector.noNulls = false;
-        bytesColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        bytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      floatTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, bytesColVector, batchSize);
-    }
-  }
-
-  public static class StringGroupFromDoubleTreeReader extends ConvertTreeReader {
-
-    private DoubleTreeReader doubleTreeReader;
-
-    private final TypeDescription readerType;
-    private DoubleColumnVector doubleColVector;
-    private BytesColumnVector bytesColVector;
-
-    StringGroupFromDoubleTreeReader(int columnId, TypeDescription readerType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      doubleTreeReader = new DoubleTreeReader(columnId);
-      setConvertTreeReader(doubleTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      double doubleValue = doubleColVector.vector[elementNum];
-      if (!Double.isNaN(doubleValue)) {
-        String string = String.valueOf(doubleValue);
-        byte[] bytes = string.getBytes();
-        assignStringGroupVectorEntry(bytesColVector, elementNum, readerType, bytes);
-      } else {
-        bytesColVector.noNulls = false;
-        bytesColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        bytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      doubleTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, bytesColVector, batchSize);
-    }
-  }
-
-
-
-  public static class StringGroupFromDecimalTreeReader extends ConvertTreeReader {
-
-    private DecimalTreeReader decimalTreeReader;
-
-    private int precision;
-    private int scale;
-    private final TypeDescription readerType;
-    private DecimalColumnVector decimalColVector;
-    private BytesColumnVector bytesColVector;
-    private byte[] scratchBuffer;
-
-    StringGroupFromDecimalTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.precision = fileType.getPrecision();
-      this.scale = fileType.getScale();
-      this.readerType = readerType;
-      decimalTreeReader = new DecimalTreeReader(columnId, precision, scale);
-      setConvertTreeReader(decimalTreeReader);
-      scratchBuffer = new byte[HiveDecimal.SCRATCH_BUFFER_LEN_TO_BYTES];
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      HiveDecimalWritable decWritable = decimalColVector.vector[elementNum];
-
-      // Convert decimal into bytes instead of a String for better performance.
-      final int byteIndex = decWritable.toBytes(scratchBuffer);
-
-      assignStringGroupVectorEntry(
-          bytesColVector, elementNum, readerType,
-          scratchBuffer, byteIndex, HiveDecimal.SCRATCH_BUFFER_LEN_TO_BYTES - byteIndex);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (decimalColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        decimalColVector = new DecimalColumnVector(precision, scale);
-        bytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      decimalTreeReader.nextVector(decimalColVector, isNull, batchSize);
-
-      convertVector(decimalColVector, bytesColVector, batchSize);
-    }
-  }
-
-  public static class StringGroupFromTimestampTreeReader extends ConvertTreeReader {
-
-    private TimestampTreeReader timestampTreeReader;
-
-    private final TypeDescription readerType;
-    private TimestampColumnVector timestampColVector;
-    private BytesColumnVector bytesColVector;
-
-    StringGroupFromTimestampTreeReader(int columnId, TypeDescription readerType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      timestampTreeReader = new TimestampTreeReader(columnId, skipCorrupt);
-      setConvertTreeReader(timestampTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String string =
-          timestampColVector.asScratchTimestamp(elementNum).toString();
-      byte[] bytes = string.getBytes();
-      assignStringGroupVectorEntry(bytesColVector, elementNum, readerType, bytes);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (timestampColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        timestampColVector = new TimestampColumnVector();
-        bytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      timestampTreeReader.nextVector(timestampColVector, isNull, batchSize);
-
-      convertVector(timestampColVector, bytesColVector, batchSize);
-    }
-  }
-
-  public static class StringGroupFromDateTreeReader extends ConvertTreeReader {
-
-    private DateTreeReader dateTreeReader;
-
-    private final TypeDescription readerType;
-    private LongColumnVector longColVector;
-    private BytesColumnVector bytesColVector;
-    private Date date;
-
-    StringGroupFromDateTreeReader(int columnId, TypeDescription readerType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      dateTreeReader = new DateTreeReader(columnId);
-      setConvertTreeReader(dateTreeReader);
-      date = new Date(0);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      date.setTime(DateWritable.daysToMillis((int) longColVector.vector[elementNum]));
-      String string = date.toString();
-      byte[] bytes = string.getBytes();
-      assignStringGroupVectorEntry(bytesColVector, elementNum, readerType, bytes);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        bytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      dateTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, bytesColVector, batchSize);
-    }
-  }
-
-  public static class StringGroupFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private final TypeDescription readerType;
-
-    StringGroupFromStringGroupTreeReader(int columnId, TypeDescription fileType,
-        TypeDescription readerType) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      stringGroupTreeReader.nextVector(previousVector, isNull, batchSize);
-
-      BytesColumnVector resultColVector = (BytesColumnVector) previousVector;
-
-      if (resultColVector.isRepeating) {
-        if (resultColVector.noNulls || !resultColVector.isNull[0]) {
-          convertStringGroupVectorElement(resultColVector, 0, readerType);
-        } else {
-          // Remains null.
-        }
-      } else if (resultColVector.noNulls){
-        for (int i = 0; i < batchSize; i++) {
-          convertStringGroupVectorElement(resultColVector, i, readerType);
-        }
-      } else {
-        for (int i = 0; i < batchSize; i++) {
-          if (!resultColVector.isNull[i]) {
-            convertStringGroupVectorElement(resultColVector, i, readerType);
-          } else {
-            // Remains null.
-          }
-        }
-      }
-    }
-  }
-
-  public static class StringGroupFromBinaryTreeReader extends ConvertTreeReader {
-
-    private BinaryTreeReader binaryTreeReader;
-
-    private final TypeDescription readerType;
-    private BytesColumnVector inBytesColVector;
-    private BytesColumnVector outBytesColVector;
-
-    StringGroupFromBinaryTreeReader(int columnId, TypeDescription readerType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.readerType = readerType;
-      binaryTreeReader = new BinaryTreeReader(columnId);
-      setConvertTreeReader(binaryTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      byte[] bytes = inBytesColVector.vector[elementNum];
-      int start = inBytesColVector.start[elementNum];
-      int length = inBytesColVector.length[elementNum];
-      byte[] string = new byte[length == 0 ? 0 : 3 * length - 1];
-      for(int p = 0; p < string.length; p += 2) {
-        if (p != 0) {
-          string[p++] = ' ';
-        }
-        int num = 0xff & bytes[start++];
-        int digit = num / 16;
-        string[p] = (byte)((digit) + (digit < 10 ? '0' : 'a' - 10));
-        digit = num % 16;
-        string[p + 1] = (byte)((digit) + (digit < 10 ? '0' : 'a' - 10));
-      }
-      assignStringGroupVectorEntry(outBytesColVector, elementNum, readerType,
-          string, 0, string.length);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (inBytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        inBytesColVector = new BytesColumnVector();
-        outBytesColVector = (BytesColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      binaryTreeReader.nextVector(inBytesColVector, isNull, batchSize);
-
-      convertVector(inBytesColVector, outBytesColVector, batchSize);
-    }
-  }
-
-  public static class TimestampFromAnyIntegerTreeReader extends ConvertTreeReader {
-
-    private AnyIntegerTreeReader anyIntegerAsLongTreeReader;
-
-    private LongColumnVector longColVector;
-    private TimestampColumnVector timestampColVector;
-
-    TimestampFromAnyIntegerTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      anyIntegerAsLongTreeReader =
-          new AnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-      setConvertTreeReader(anyIntegerAsLongTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      long longValue = longColVector.vector[elementNum];
-      // UNDONE: What does the boolean setting need to be?
-      timestampColVector.set(elementNum, new Timestamp(longValue));
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        timestampColVector = (TimestampColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      anyIntegerAsLongTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, timestampColVector, batchSize);
-    }
-  }
-
-  public static class TimestampFromFloatTreeReader extends ConvertTreeReader {
-
-    private FloatTreeReader floatTreeReader;
-
-    private DoubleColumnVector doubleColVector;
-    private TimestampColumnVector timestampColVector;
-
-    TimestampFromFloatTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      floatTreeReader = new FloatTreeReader(columnId);
-      setConvertTreeReader(floatTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      float floatValue = (float) doubleColVector.vector[elementNum];
-      Timestamp timestampValue = TimestampUtils.doubleToTimestamp(floatValue);
-      // The TimestampColumnVector will set the entry to null when a null timestamp is passed in.
-      timestampColVector.set(elementNum, timestampValue);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        timestampColVector = (TimestampColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      floatTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, timestampColVector, batchSize);
-    }
-  }
-
-  public static class TimestampFromDoubleTreeReader extends ConvertTreeReader {
-
-    private DoubleTreeReader doubleTreeReader;
-
-    private DoubleColumnVector doubleColVector;
-    private TimestampColumnVector timestampColVector;
-
-    TimestampFromDoubleTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      doubleTreeReader = new DoubleTreeReader(columnId);
-      setConvertTreeReader(doubleTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      double doubleValue = doubleColVector.vector[elementNum];
-      Timestamp timestampValue = TimestampUtils.doubleToTimestamp(doubleValue);
-      // The TimestampColumnVector will set the entry to null when a null timestamp is passed in.
-      timestampColVector.set(elementNum, timestampValue);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (doubleColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        doubleColVector = new DoubleColumnVector();
-        timestampColVector = (TimestampColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      doubleTreeReader.nextVector(doubleColVector, isNull, batchSize);
-
-      convertVector(doubleColVector, timestampColVector, batchSize);
-    }
-  }
-
-  public static class TimestampFromDecimalTreeReader extends ConvertTreeReader {
-
-    private DecimalTreeReader decimalTreeReader;
-
-    private final int precision;
-    private final int scale;
-    private DecimalColumnVector decimalColVector;
-    private TimestampColumnVector timestampColVector;
-
-    TimestampFromDecimalTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      this.precision = fileType.getPrecision();
-      this.scale = fileType.getScale();
-      decimalTreeReader = new DecimalTreeReader(columnId, precision, scale);
-      setConvertTreeReader(decimalTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      Timestamp timestampValue =
-            TimestampUtils.decimalToTimestamp(
-                decimalColVector.vector[elementNum].getHiveDecimal());
-      // The TimestampColumnVector will set the entry to null when a null timestamp is passed in.
-      timestampColVector.set(elementNum, timestampValue);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (decimalColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        decimalColVector = new DecimalColumnVector(precision, scale);
-        timestampColVector = (TimestampColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      decimalTreeReader.nextVector(decimalColVector, isNull, batchSize);
-
-      convertVector(decimalColVector, timestampColVector, batchSize);
-    }
-  }
-
-  public static class TimestampFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private BytesColumnVector bytesColVector;
-    private TimestampColumnVector timestampColVector;
-
-    TimestampFromStringGroupTreeReader(int columnId, TypeDescription fileType)
-        throws IOException {
-      super(columnId);
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String stringValue =
-          stringFromBytesColumnVectorEntry(bytesColVector, elementNum);
-      Timestamp timestampValue = parseTimestampFromString(stringValue);
-      if (timestampValue != null) {
-        timestampColVector.set(elementNum, timestampValue);
-      } else {
-        timestampColVector.noNulls = false;
-        timestampColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (bytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        bytesColVector = new BytesColumnVector();
-        timestampColVector = (TimestampColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      stringGroupTreeReader.nextVector(bytesColVector, isNull, batchSize);
-
-      convertVector(bytesColVector, timestampColVector, batchSize);
-    }
-  }
-
-  public static class TimestampFromDateTreeReader extends ConvertTreeReader {
-
-    private DateTreeReader dateTreeReader;
-
-    private LongColumnVector longColVector;
-    private TimestampColumnVector timestampColVector;
-
-    TimestampFromDateTreeReader(int columnId, TypeDescription fileType,
-        boolean skipCorrupt) throws IOException {
-      super(columnId);
-      dateTreeReader = new DateTreeReader(columnId);
-      setConvertTreeReader(dateTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) {
-      long millis =
-          DateWritable.daysToMillis((int) longColVector.vector[elementNum]);
-      timestampColVector.set(elementNum, new Timestamp(millis));
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (longColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        longColVector = new LongColumnVector();
-        timestampColVector = (TimestampColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      dateTreeReader.nextVector(longColVector, isNull, batchSize);
-
-      convertVector(longColVector, timestampColVector, batchSize);
-    }
-  }
-
-  public static class DateFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    private BytesColumnVector bytesColVector;
-    private LongColumnVector longColVector;
-
-    DateFromStringGroupTreeReader(int columnId, TypeDescription fileType)
-        throws IOException {
-      super(columnId);
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      String stringValue =
-          stringFromBytesColumnVectorEntry(bytesColVector, elementNum);
-      Date dateValue = parseDateFromString(stringValue);
-      if (dateValue != null) {
-        longColVector.vector[elementNum] = DateWritable.dateToDays(dateValue);
-      } else {
-        longColVector.noNulls = false;
-        longColVector.isNull[elementNum] = true;
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (bytesColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        bytesColVector = new BytesColumnVector();
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      stringGroupTreeReader.nextVector(bytesColVector, isNull, batchSize);
-
-      convertVector(bytesColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class DateFromTimestampTreeReader extends ConvertTreeReader {
-
-    private TimestampTreeReader timestampTreeReader;
-
-    private TimestampColumnVector timestampColVector;
-    private LongColumnVector longColVector;
-
-    DateFromTimestampTreeReader(int columnId, boolean skipCorrupt) throws IOException {
-      super(columnId);
-      timestampTreeReader = new TimestampTreeReader(columnId, skipCorrupt);
-      setConvertTreeReader(timestampTreeReader);
-    }
-
-    @Override
-    public void setConvertVectorElement(int elementNum) throws IOException {
-      Date dateValue =
-          DateWritable.timeToDate(TimestampUtils.millisToSeconds(
-              timestampColVector.asScratchTimestamp(elementNum).getTime()));
-      longColVector.vector[elementNum] = DateWritable.dateToDays(dateValue);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (timestampColVector == null) {
-        // Allocate column vector for file; cast column vector for reader.
-        timestampColVector = new TimestampColumnVector();
-        longColVector = (LongColumnVector) previousVector;
-      }
-      // Read present/isNull stream
-      timestampTreeReader.nextVector(timestampColVector, isNull, batchSize);
-
-      convertVector(timestampColVector, longColVector, batchSize);
-    }
-  }
-
-  public static class BinaryFromStringGroupTreeReader extends ConvertTreeReader {
-
-    private TreeReader stringGroupTreeReader;
-
-    BinaryFromStringGroupTreeReader(int columnId, TypeDescription fileType)
-        throws IOException {
-      super(columnId);
-      stringGroupTreeReader = getStringGroupTreeReader(columnId, fileType);
-      setConvertTreeReader(stringGroupTreeReader);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      super.nextVector(previousVector, isNull, batchSize);
-    }
-  }
-
-  private static TreeReader createAnyIntegerConvertTreeReader(int columnId,
-                                                              TypeDescription fileType,
-                                                              TypeDescription readerType,
-                                                              SchemaEvolution evolution,
-                                                              boolean[] included,
-                                                              boolean skipCorrupt) throws IOException {
-
-    // CONVERT from (BOOLEAN, BYTE, SHORT, INT, LONG) to schema type.
-    //
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      if (fileType.getCategory() == readerType.getCategory()) {
-        throw new IllegalArgumentException("No conversion of type " +
-            readerType.getCategory() + " to self needed");
-      }
-      return new AnyIntegerFromAnyIntegerTreeReader(columnId, fileType, readerType,
-          skipCorrupt);
-
-    case FLOAT:
-      return new FloatFromAnyIntegerTreeReader(columnId, fileType,
-          skipCorrupt);
-
-    case DOUBLE:
-      return new DoubleFromAnyIntegerTreeReader(columnId, fileType,
-          skipCorrupt);
-
-    case DECIMAL:
-      return new DecimalFromAnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromAnyIntegerTreeReader(columnId, fileType, readerType,
-          skipCorrupt);
-
-    case TIMESTAMP:
-      return new TimestampFromAnyIntegerTreeReader(columnId, fileType, skipCorrupt);
-
-    // Not currently supported conversion(s):
-    case BINARY:
-    case DATE:
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createFloatConvertTreeReader(int columnId,
-                                                         TypeDescription fileType,
-                                                         TypeDescription readerType,
-                                                         SchemaEvolution evolution,
-                                                         boolean[] included,
-                                                         boolean skipCorrupt) throws IOException {
-
-    // CONVERT from FLOAT to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromFloatTreeReader(columnId, readerType);
-
-    case FLOAT:
-      throw new IllegalArgumentException("No conversion of type " +
-        readerType.getCategory() + " to self needed");
-
-    case DOUBLE:
-      return new DoubleFromFloatTreeReader(columnId);
-
-    case DECIMAL:
-      return new DecimalFromFloatTreeReader(columnId, readerType);
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromFloatTreeReader(columnId, readerType, skipCorrupt);
-
-    case TIMESTAMP:
-      return new TimestampFromFloatTreeReader(columnId, readerType, skipCorrupt);
-
-    // Not currently supported conversion(s):
-    case BINARY:
-    case DATE:
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createDoubleConvertTreeReader(int columnId,
-                                                          TypeDescription fileType,
-                                                          TypeDescription readerType,
-                                                          SchemaEvolution evolution,
-                                                          boolean[] included,
-                                                          boolean skipCorrupt) throws IOException {
-
-    // CONVERT from DOUBLE to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromDoubleTreeReader(columnId, readerType);
-
-    case FLOAT:
-      return new FloatFromDoubleTreeReader(columnId);
-
-    case DOUBLE:
-      throw new IllegalArgumentException("No conversion of type " +
-        readerType.getCategory() + " to self needed");
-
-    case DECIMAL:
-      return new DecimalFromDoubleTreeReader(columnId, readerType);
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromDoubleTreeReader(columnId, readerType, skipCorrupt);
-
-    case TIMESTAMP:
-      return new TimestampFromDoubleTreeReader(columnId, readerType, skipCorrupt);
-
-    // Not currently supported conversion(s):
-    case BINARY:
-    case DATE:
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createDecimalConvertTreeReader(int columnId,
-                                                           TypeDescription fileType,
-                                                           TypeDescription readerType,
-                                                           SchemaEvolution evolution,
-                                                           boolean[] included,
-                                                           boolean skipCorrupt) throws IOException {
-
-    // CONVERT from DECIMAL to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromDecimalTreeReader(columnId, fileType, readerType);
-
-    case FLOAT:
-      return new FloatFromDecimalTreeReader(columnId, fileType, readerType);
-
-    case DOUBLE:
-      return new DoubleFromDecimalTreeReader(columnId, fileType);
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromDecimalTreeReader(columnId, fileType, readerType, skipCorrupt);
-
-    case TIMESTAMP:
-      return new TimestampFromDecimalTreeReader(columnId, fileType, skipCorrupt);
-
-    case DECIMAL:
-      return new DecimalFromDecimalTreeReader(columnId, fileType, readerType);
-
-    // Not currently supported conversion(s):
-    case BINARY:
-    case DATE:
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createStringConvertTreeReader(int columnId,
-                                                          TypeDescription fileType,
-                                                          TypeDescription readerType,
-                                                          SchemaEvolution evolution,
-                                                          boolean[] included,
-                                                          boolean skipCorrupt) throws IOException {
-
-    // CONVERT from STRING to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case FLOAT:
-      return new FloatFromStringGroupTreeReader(columnId, fileType);
-
-    case DOUBLE:
-      return new DoubleFromStringGroupTreeReader(columnId, fileType);
-
-    case DECIMAL:
-      return new DecimalFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case CHAR:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case VARCHAR:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case STRING:
-      throw new IllegalArgumentException("No conversion of type " +
-          readerType.getCategory() + " to self needed");
-
-    case BINARY:
-      return new BinaryFromStringGroupTreeReader(columnId, fileType);
-
-    case TIMESTAMP:
-      return new TimestampFromStringGroupTreeReader(columnId, fileType);
-
-    case DATE:
-      return new DateFromStringGroupTreeReader(columnId, fileType);
-
-    // Not currently supported conversion(s):
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createCharConvertTreeReader(int columnId,
-                                                        TypeDescription fileType,
-                                                        TypeDescription readerType,
-                                                        SchemaEvolution evolution,
-                                                        boolean[] included,
-                                                        boolean skipCorrupt) throws IOException {
-
-    // CONVERT from CHAR to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case FLOAT:
-      return new FloatFromStringGroupTreeReader(columnId, fileType);
-
-    case DOUBLE:
-      return new DoubleFromStringGroupTreeReader(columnId, fileType);
-
-    case DECIMAL:
-      return new DecimalFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case STRING:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case VARCHAR:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case CHAR:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case BINARY:
-      return new BinaryFromStringGroupTreeReader(columnId, fileType);
-
-    case TIMESTAMP:
-      return new TimestampFromStringGroupTreeReader(columnId, fileType);
-
-    case DATE:
-      return new DateFromStringGroupTreeReader(columnId, fileType);
-
-    // Not currently supported conversion(s):
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createVarcharConvertTreeReader(int columnId,
-                                                           TypeDescription fileType,
-                                                           TypeDescription readerType,
-                                                           SchemaEvolution evolution,
-                                                           boolean[] included,
-                                                           boolean skipCorrupt) throws IOException {
-
-    // CONVERT from VARCHAR to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case FLOAT:
-      return new FloatFromStringGroupTreeReader(columnId, fileType);
-
-    case DOUBLE:
-      return new DoubleFromStringGroupTreeReader(columnId, fileType);
-
-    case DECIMAL:
-      return new DecimalFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case STRING:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case CHAR:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case VARCHAR:
-      return new StringGroupFromStringGroupTreeReader(columnId, fileType, readerType);
-
-    case BINARY:
-      return new BinaryFromStringGroupTreeReader(columnId, fileType);
-
-    case TIMESTAMP:
-      return new TimestampFromStringGroupTreeReader(columnId, fileType);
-
-    case DATE:
-      return new DateFromStringGroupTreeReader(columnId, fileType);
-
-    // Not currently supported conversion(s):
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createTimestampConvertTreeReader(int columnId,
-                                                             TypeDescription fileType,
-                                                             TypeDescription readerType,
-                                                             SchemaEvolution evolution,
-                                                             boolean[] included,
-                                                             boolean skipCorrupt) throws IOException {
-
-    // CONVERT from TIMESTAMP to schema type.
-    switch (readerType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return new AnyIntegerFromTimestampTreeReader(columnId, readerType, skipCorrupt);
-
-    case FLOAT:
-      return new FloatFromTimestampTreeReader(columnId, skipCorrupt);
-
-    case DOUBLE:
-      return new DoubleFromTimestampTreeReader(columnId, skipCorrupt);
-
-    case DECIMAL:
-      return new DecimalFromTimestampTreeReader(columnId, skipCorrupt);
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromTimestampTreeReader(columnId, readerType, skipCorrupt);
-
-    case TIMESTAMP:
-      throw new IllegalArgumentException("No conversion of type " +
-          readerType.getCategory() + " to self needed");
-
-    case DATE:
-      return new DateFromTimestampTreeReader(columnId, skipCorrupt);
-
-    // Not currently supported conversion(s):
-    case BINARY:
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createDateConvertTreeReader(int columnId,
-                                                        TypeDescription fileType,
-                                                        TypeDescription readerType,
-                                                        SchemaEvolution evolution,
-                                                        boolean[] included,
-                                                        boolean skipCorrupt) throws IOException {
-
-    // CONVERT from DATE to schema type.
-    switch (readerType.getCategory()) {
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromDateTreeReader(columnId, readerType, skipCorrupt);
-
-    case TIMESTAMP:
-      return new TimestampFromDateTreeReader(columnId, readerType, skipCorrupt);
-
-    case DATE:
-      throw new IllegalArgumentException("No conversion of type " +
-          readerType.getCategory() + " to self needed");
-
-      // Not currently supported conversion(s):
-    case BOOLEAN:
-    case BYTE:
-    case FLOAT:
-    case SHORT:
-    case INT:
-    case LONG:
-    case DOUBLE:
-    case BINARY:
-    case DECIMAL:
-
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  private static TreeReader createBinaryConvertTreeReader(int columnId,
-                                                          TypeDescription fileType,
-                                                          TypeDescription readerType,
-                                                          SchemaEvolution evolution,
-                                                          boolean[] included,
-                                                          boolean skipCorrupt) throws IOException {
-
-    // CONVERT from DATE to schema type.
-    switch (readerType.getCategory()) {
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      return new StringGroupFromBinaryTreeReader(columnId, readerType, skipCorrupt);
-
-    case BINARY:
-      throw new IllegalArgumentException("No conversion of type " +
-          readerType.getCategory() + " to self needed");
-
-      // Not currently supported conversion(s):
-    case BOOLEAN:
-    case BYTE:
-    case FLOAT:
-    case SHORT:
-    case INT:
-    case LONG:
-    case DOUBLE:
-    case TIMESTAMP:
-    case DECIMAL:
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          readerType.getCategory());
-    }
-  }
-
-  /**
-   * (Rules from Hive's PrimitiveObjectInspectorUtils conversion)
-   *
-   * To BOOLEAN, BYTE, SHORT, INT, LONG:
-   *   Convert from (BOOLEAN, BYTE, SHORT, INT, LONG) with down cast if necessary.
-   *   Convert from (FLOAT, DOUBLE) using type cast to long and down cast if necessary.
-   *   Convert from DECIMAL from longValue and down cast if necessary.
-   *   Convert from STRING using LazyLong.parseLong and down cast if necessary.
-   *   Convert from (CHAR, VARCHAR) from Integer.parseLong and down cast if necessary.
-   *   Convert from TIMESTAMP using timestamp getSeconds and down cast if necessary.
-   *
-   *   AnyIntegerFromAnyIntegerTreeReader (written)
-   *   AnyIntegerFromFloatTreeReader (written)
-   *   AnyIntegerFromDoubleTreeReader (written)
-   *   AnyIntegerFromDecimalTreeReader (written)
-   *   AnyIntegerFromStringGroupTreeReader (written)
-   *   AnyIntegerFromTimestampTreeReader (written)
-   *
-   * To FLOAT/DOUBLE:
-   *   Convert from (BOOLEAN, BYTE, SHORT, INT, LONG) using cast
-   *   Convert from FLOAT using cast
-   *   Convert from DECIMAL using getDouble
-   *   Convert from (STRING, CHAR, VARCHAR) using Double.parseDouble
-   *   Convert from TIMESTAMP using timestamp getDouble
-   *
-   *   FloatFromAnyIntegerTreeReader (existing)
-   *   FloatFromDoubleTreeReader (written)
-   *   FloatFromDecimalTreeReader (written)
-   *   FloatFromStringGroupTreeReader (written)
-   *
-   *   DoubleFromAnyIntegerTreeReader (existing)
-   *   DoubleFromFloatTreeReader (existing)
-   *   DoubleFromDecimalTreeReader (written)
-   *   DoubleFromStringGroupTreeReader (written)
-   *
-   * To DECIMAL:
-   *   Convert from (BOOLEAN, BYTE, SHORT, INT, LONG) using to HiveDecimal.create()
-   *   Convert from (FLOAT, DOUBLE) using to HiveDecimal.create(string value)
-   *   Convert from (STRING, CHAR, VARCHAR) using HiveDecimal.create(string value)
-   *   Convert from TIMESTAMP using HiveDecimal.create(string value of timestamp getDouble)
-   *
-   *   DecimalFromAnyIntegerTreeReader (existing)
-   *   DecimalFromFloatTreeReader (existing)
-   *   DecimalFromDoubleTreeReader (existing)
-   *   DecimalFromStringGroupTreeReader (written)
-   *
-   * To STRING, CHAR, VARCHAR:
-   *   Convert from (BOOLEAN, BYTE, SHORT, INT, LONG) using to string conversion
-   *   Convert from (FLOAT, DOUBLE) using to string conversion
-   *   Convert from DECIMAL using HiveDecimal.toString
-   *   Convert from CHAR by stripping pads
-   *   Convert from VARCHAR with value
-   *   Convert from TIMESTAMP using Timestamp.toString
-   *   Convert from DATE using Date.toString
-   *   Convert from BINARY using Text.decode
-   *
-   *   StringGroupFromAnyIntegerTreeReader (written)
-   *   StringGroupFromFloatTreeReader (written)
-   *   StringGroupFromDoubleTreeReader (written)
-   *   StringGroupFromDecimalTreeReader (written)
-   *
-   *   String from Char/Varchar conversion
-   *   Char from String/Varchar conversion
-   *   Varchar from String/Char conversion
-   *
-   *   StringGroupFromTimestampTreeReader (written)
-   *   StringGroupFromDateTreeReader (written)
-   *   StringGroupFromBinaryTreeReader *****
-   *
-   * To TIMESTAMP:
-   *   Convert from (BOOLEAN, BYTE, SHORT, INT, LONG) using TimestampWritable.longToTimestamp
-   *   Convert from (FLOAT, DOUBLE) using TimestampWritable.doubleToTimestamp
-   *   Convert from DECIMAL using TimestampWritable.decimalToTimestamp
-   *   Convert from (STRING, CHAR, VARCHAR) using string conversion
-   *   Or, from DATE
-   *
-   *   TimestampFromAnyIntegerTreeReader (written)
-   *   TimestampFromFloatTreeReader (written)
-   *   TimestampFromDoubleTreeReader (written)
-   *   TimestampFromDecimalTreeeReader (written)
-   *   TimestampFromStringGroupTreeReader (written)
-   *   TimestampFromDateTreeReader
-   *
-   *
-   * To DATE:
-   *   Convert from (STRING, CHAR, VARCHAR) using string conversion.
-   *   Or, from TIMESTAMP.
-   *
-   *  DateFromStringGroupTreeReader (written)
-   *  DateFromTimestampTreeReader (written)
-   *
-   * To BINARY:
-   *   Convert from (STRING, CHAR, VARCHAR) using getBinaryFromText
-   *
-   *  BinaryFromStringGroupTreeReader (written)
-   *
-   * (Notes from StructConverter)
-   *
-   * To STRUCT:
-   *   Input must be data type STRUCT
-   *   minFields = Math.min(numSourceFields, numTargetFields)
-   *   Convert those fields
-   *   Extra targetFields --> NULL
-   *
-   * (Notes from ListConverter)
-   *
-   * To LIST:
-   *   Input must be data type LIST
-   *   Convert elements
-   *
-   * (Notes from MapConverter)
-   *
-   * To MAP:
-   *   Input must be data type MAP
-   *   Convert keys and values
-   *
-   * (Notes from UnionConverter)
-   *
-   * To UNION:
-   *   Input must be data type UNION
-   *   Convert value for tag
-   *
-   * @param readerType
-   * @param evolution
-   * @param included
-   * @param skipCorrupt
-   * @return
-   * @throws IOException
-   */
-  public static TreeReader createConvertTreeReader(TypeDescription readerType,
-                                                   SchemaEvolution evolution,
-                                                   boolean[] included,
-                                                   boolean skipCorrupt
-                                                   ) throws IOException {
-
-    int columnId = readerType.getId();
-    TypeDescription fileType = evolution.getFileType(readerType);
-
-    switch (fileType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-      return createAnyIntegerConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case FLOAT:
-      return createFloatConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case DOUBLE:
-      return createDoubleConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case DECIMAL:
-      return createDecimalConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case STRING:
-      return createStringConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case CHAR:
-      return createCharConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case VARCHAR:
-      return createVarcharConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case TIMESTAMP:
-      return createTimestampConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case DATE:
-      return createDateConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    case BINARY:
-      return createBinaryConvertTreeReader(columnId, fileType, readerType, evolution,
-          included, skipCorrupt);
-
-    // UNDONE: Complex conversions...
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          fileType.getCategory());
-    }
-  }
-
-  public static boolean canConvert(TypeDescription fileType, TypeDescription readerType) {
-
-    Category readerTypeCategory = readerType.getCategory();
-
-    // We don't convert from any to complex.
-    switch (readerTypeCategory) {
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-      return false;
-
-    default:
-      // Fall through.
-    }
-
-    // Now look for the few cases we don't convert from
-    switch (fileType.getCategory()) {
-
-    case BOOLEAN:
-    case BYTE:
-    case SHORT:
-    case INT:
-    case LONG:
-    case FLOAT:
-    case DOUBLE:
-    case DECIMAL:
-      switch (readerType.getCategory()) {
-      // Not currently supported conversion(s):
-      case BINARY:
-      case DATE:
-        return false;
-      default:
-        return true;
-      }
-
-
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      switch (readerType.getCategory()) {
-      // Not currently supported conversion(s):
-        // (None)
-      default:
-        return true;
-      }
-
-    case TIMESTAMP:
-      switch (readerType.getCategory()) {
-      // Not currently supported conversion(s):
-      case BINARY:
-        return false;
-      default:
-        return true;
-      }
-
-    case DATE:
-      switch (readerType.getCategory()) {
-      // Not currently supported conversion(s):
-      case BOOLEAN:
-      case BYTE:
-      case FLOAT:
-      case SHORT:
-      case INT:
-      case LONG:
-      case DOUBLE:
-      case BINARY:
-      case DECIMAL:
-        return false;
-      default:
-        return true;
-      }
-
-    case BINARY:
-      switch (readerType.getCategory()) {
-      // Not currently supported conversion(s):
-      case BOOLEAN:
-      case BYTE:
-      case FLOAT:
-      case SHORT:
-      case INT:
-      case LONG:
-      case DOUBLE:
-      case TIMESTAMP:
-      case DECIMAL:
-        return false;
-      default:
-        return true;
-      }
-
-    // We don't convert from complex to any.
-    case STRUCT:
-    case LIST:
-    case MAP:
-    case UNION:
-      return false;
-
-    default:
-      throw new IllegalArgumentException("Unsupported type " +
-          fileType.getCategory());
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/DataReaderProperties.java b/orc/src/java/org/apache/orc/impl/DataReaderProperties.java
deleted file mode 100644
index 22301e8a60..0000000000
--- a/orc/src/java/org/apache/orc/impl/DataReaderProperties.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import com.google.common.base.Preconditions;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.CompressionKind;
-
-import javax.annotation.Nullable;
-
-public final class DataReaderProperties {
-
-  private final FileSystem fileSystem;
-  private final Path path;
-  private final CompressionKind compression;
-  private final boolean zeroCopy;
-  private final int typeCount;
-  private final int bufferSize;
-
-  private DataReaderProperties(Builder builder) {
-    this.fileSystem = builder.fileSystem;
-    this.path = builder.path;
-    this.compression = builder.compression;
-    this.zeroCopy = builder.zeroCopy;
-    this.typeCount = builder.typeCount;
-    this.bufferSize = builder.bufferSize;
-  }
-
-  public FileSystem getFileSystem() {
-    return fileSystem;
-  }
-
-  public Path getPath() {
-    return path;
-  }
-
-  public CompressionKind getCompression() {
-    return compression;
-  }
-
-  public boolean getZeroCopy() {
-    return zeroCopy;
-  }
-
-  public int getTypeCount() {
-    return typeCount;
-  }
-
-  public int getBufferSize() {
-    return bufferSize;
-  }
-
-  public static Builder builder() {
-    return new Builder();
-  }
-
-  public static class Builder {
-
-    private FileSystem fileSystem;
-    private Path path;
-    private CompressionKind compression;
-    private boolean zeroCopy;
-    private int typeCount;
-    private int bufferSize;
-
-    private Builder() {
-
-    }
-
-    public Builder withFileSystem(FileSystem fileSystem) {
-      this.fileSystem = fileSystem;
-      return this;
-    }
-
-    public Builder withPath(Path path) {
-      this.path = path;
-      return this;
-    }
-
-    public Builder withCompression(CompressionKind value) {
-      this.compression = value;
-      return this;
-    }
-
-    public Builder withZeroCopy(boolean zeroCopy) {
-      this.zeroCopy = zeroCopy;
-      return this;
-    }
-
-    public Builder withTypeCount(int value) {
-      this.typeCount = value;
-      return this;
-    }
-
-    public Builder withBufferSize(int value) {
-      this.bufferSize = value;
-      return this;
-    }
-
-    public DataReaderProperties build() {
-      Preconditions.checkNotNull(fileSystem);
-      Preconditions.checkNotNull(path);
-
-      return new DataReaderProperties(this);
-    }
-
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/DirectDecompressionCodec.java b/orc/src/java/org/apache/orc/impl/DirectDecompressionCodec.java
deleted file mode 100644
index 7e0110d433..0000000000
--- a/orc/src/java/org/apache/orc/impl/DirectDecompressionCodec.java
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import org.apache.orc.CompressionCodec;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-public interface DirectDecompressionCodec extends CompressionCodec {
-  public boolean isAvailable();
-  public void directDecompress(ByteBuffer in, ByteBuffer out) throws IOException;
-}
diff --git a/orc/src/java/org/apache/orc/impl/DynamicByteArray.java b/orc/src/java/org/apache/orc/impl/DynamicByteArray.java
deleted file mode 100644
index 986c2ac81b..0000000000
--- a/orc/src/java/org/apache/orc/impl/DynamicByteArray.java
+++ /dev/null
@@ -1,303 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.nio.ByteBuffer;
-
-import org.apache.hadoop.io.Text;
-
-/**
- * A class that is a growable array of bytes. Growth is managed in terms of
- * chunks that are allocated when needed.
- */
-public final class DynamicByteArray {
-  static final int DEFAULT_CHUNKSIZE = 32 * 1024;
-  static final int DEFAULT_NUM_CHUNKS = 128;
-
-  private final int chunkSize;        // our allocation sizes
-  private byte[][] data;              // the real data
-  private int length;                 // max set element index +1
-  private int initializedChunks = 0;  // the number of chunks created
-
-  public DynamicByteArray() {
-    this(DEFAULT_NUM_CHUNKS, DEFAULT_CHUNKSIZE);
-  }
-
-  public DynamicByteArray(int numChunks, int chunkSize) {
-    if (chunkSize == 0) {
-      throw new IllegalArgumentException("bad chunksize");
-    }
-    this.chunkSize = chunkSize;
-    data = new byte[numChunks][];
-  }
-
-  /**
-   * Ensure that the given index is valid.
-   */
-  private void grow(int chunkIndex) {
-    if (chunkIndex >= initializedChunks) {
-      if (chunkIndex >= data.length) {
-        int newSize = Math.max(chunkIndex + 1, 2 * data.length);
-        byte[][] newChunk = new byte[newSize][];
-        System.arraycopy(data, 0, newChunk, 0, data.length);
-        data = newChunk;
-      }
-      for(int i=initializedChunks; i <= chunkIndex; ++i) {
-        data[i] = new byte[chunkSize];
-      }
-      initializedChunks = chunkIndex + 1;
-    }
-  }
-
-  public byte get(int index) {
-    if (index >= length) {
-      throw new IndexOutOfBoundsException("Index " + index +
-                                            " is outside of 0.." +
-                                            (length - 1));
-    }
-    int i = index / chunkSize;
-    int j = index % chunkSize;
-    return data[i][j];
-  }
-
-  public void set(int index, byte value) {
-    int i = index / chunkSize;
-    int j = index % chunkSize;
-    grow(i);
-    if (index >= length) {
-      length = index + 1;
-    }
-    data[i][j] = value;
-  }
-
-  public int add(byte value) {
-    int i = length / chunkSize;
-    int j = length % chunkSize;
-    grow(i);
-    data[i][j] = value;
-    int result = length;
-    length += 1;
-    return result;
-  }
-
-  /**
-   * Copy a slice of a byte array into our buffer.
-   * @param value the array to copy from
-   * @param valueOffset the first location to copy from value
-   * @param valueLength the number of bytes to copy from value
-   * @return the offset of the start of the value
-   */
-  public int add(byte[] value, int valueOffset, int valueLength) {
-    int i = length / chunkSize;
-    int j = length % chunkSize;
-    grow((length + valueLength) / chunkSize);
-    int remaining = valueLength;
-    while (remaining > 0) {
-      int size = Math.min(remaining, chunkSize - j);
-      System.arraycopy(value, valueOffset, data[i], j, size);
-      remaining -= size;
-      valueOffset += size;
-      i += 1;
-      j = 0;
-    }
-    int result = length;
-    length += valueLength;
-    return result;
-  }
-
-  /**
-   * Read the entire stream into this array.
-   * @param in the stream to read from
-   * @throws IOException
-   */
-  public void readAll(InputStream in) throws IOException {
-    int currentChunk = length / chunkSize;
-    int currentOffset = length % chunkSize;
-    grow(currentChunk);
-    int currentLength = in.read(data[currentChunk], currentOffset,
-      chunkSize - currentOffset);
-    while (currentLength > 0) {
-      length += currentLength;
-      currentOffset = length % chunkSize;
-      if (currentOffset == 0) {
-        currentChunk = length / chunkSize;
-        grow(currentChunk);
-      }
-      currentLength = in.read(data[currentChunk], currentOffset,
-        chunkSize - currentOffset);
-    }
-  }
-
-  /**
-   * Byte compare a set of bytes against the bytes in this dynamic array.
-   * @param other source of the other bytes
-   * @param otherOffset start offset in the other array
-   * @param otherLength number of bytes in the other array
-   * @param ourOffset the offset in our array
-   * @param ourLength the number of bytes in our array
-   * @return negative for less, 0 for equal, positive for greater
-   */
-  public int compare(byte[] other, int otherOffset, int otherLength,
-                     int ourOffset, int ourLength) {
-    int currentChunk = ourOffset / chunkSize;
-    int currentOffset = ourOffset % chunkSize;
-    int maxLength = Math.min(otherLength, ourLength);
-    while (maxLength > 0 &&
-      other[otherOffset] == data[currentChunk][currentOffset]) {
-      otherOffset += 1;
-      currentOffset += 1;
-      if (currentOffset == chunkSize) {
-        currentChunk += 1;
-        currentOffset = 0;
-      }
-      maxLength -= 1;
-    }
-    if (maxLength == 0) {
-      return otherLength - ourLength;
-    }
-    int otherByte = 0xff & other[otherOffset];
-    int ourByte = 0xff & data[currentChunk][currentOffset];
-    return otherByte > ourByte ? 1 : -1;
-  }
-
-  /**
-   * Get the size of the array.
-   * @return the number of bytes in the array
-   */
-  public int size() {
-    return length;
-  }
-
-  /**
-   * Clear the array to its original pristine state.
-   */
-  public void clear() {
-    length = 0;
-    for(int i=0; i < data.length; ++i) {
-      data[i] = null;
-    }
-    initializedChunks = 0;
-  }
-
-  /**
-   * Set a text value from the bytes in this dynamic array.
-   * @param result the value to set
-   * @param offset the start of the bytes to copy
-   * @param length the number of bytes to copy
-   */
-  public void setText(Text result, int offset, int length) {
-    result.clear();
-    int currentChunk = offset / chunkSize;
-    int currentOffset = offset % chunkSize;
-    int currentLength = Math.min(length, chunkSize - currentOffset);
-    while (length > 0) {
-      result.append(data[currentChunk], currentOffset, currentLength);
-      length -= currentLength;
-      currentChunk += 1;
-      currentOffset = 0;
-      currentLength = Math.min(length, chunkSize - currentOffset);
-    }
-  }
-
-  /**
-   * Write out a range of this dynamic array to an output stream.
-   * @param out the stream to write to
-   * @param offset the first offset to write
-   * @param length the number of bytes to write
-   * @throws IOException
-   */
-  public void write(OutputStream out, int offset,
-                    int length) throws IOException {
-    int currentChunk = offset / chunkSize;
-    int currentOffset = offset % chunkSize;
-    while (length > 0) {
-      int currentLength = Math.min(length, chunkSize - currentOffset);
-      out.write(data[currentChunk], currentOffset, currentLength);
-      length -= currentLength;
-      currentChunk += 1;
-      currentOffset = 0;
-    }
-  }
-
-  @Override
-  public String toString() {
-    int i;
-    StringBuilder sb = new StringBuilder(length * 3);
-
-    sb.append('{');
-    int l = length - 1;
-    for (i=0; i<l; i++) {
-      sb.append(Integer.toHexString(get(i)));
-      sb.append(',');
-    }
-    sb.append(get(i));
-    sb.append('}');
-
-    return sb.toString();
-  }
-
-  public void setByteBuffer(ByteBuffer result, int offset, int length) {
-    result.clear();
-    int currentChunk = offset / chunkSize;
-    int currentOffset = offset % chunkSize;
-    int currentLength = Math.min(length, chunkSize - currentOffset);
-    while (length > 0) {
-      result.put(data[currentChunk], currentOffset, currentLength);
-      length -= currentLength;
-      currentChunk += 1;
-      currentOffset = 0;
-      currentLength = Math.min(length, chunkSize - currentOffset);
-    }
-  }
-
-  /**
-   * Gets all the bytes of the array.
-   *
-   * @return Bytes of the array
-   */
-  public byte[] get() {
-    byte[] result = null;
-    if (length > 0) {
-      int currentChunk = 0;
-      int currentOffset = 0;
-      int currentLength = Math.min(length, chunkSize);
-      int destOffset = 0;
-      result = new byte[length];
-      int totalLength = length;
-      while (totalLength > 0) {
-        System.arraycopy(data[currentChunk], currentOffset, result, destOffset, currentLength);
-        destOffset += currentLength;
-        totalLength -= currentLength;
-        currentChunk += 1;
-        currentOffset = 0;
-        currentLength = Math.min(totalLength, chunkSize - currentOffset);
-      }
-    }
-    return result;
-  }
-
-  /**
-   * Get the size of the buffers.
-   */
-  public long getSizeInBytes() {
-    return initializedChunks * chunkSize;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/DynamicIntArray.java b/orc/src/java/org/apache/orc/impl/DynamicIntArray.java
deleted file mode 100644
index 3b2884b64e..0000000000
--- a/orc/src/java/org/apache/orc/impl/DynamicIntArray.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-/**
- * Dynamic int array that uses primitive types and chunks to avoid copying
- * large number of integers when it resizes.
- *
- * The motivation for this class is memory optimization, i.e. space efficient
- * storage of potentially huge arrays without good a-priori size guesses.
- *
- * The API of this class is between a primitive array and a AbstractList. It's
- * not a Collection implementation because it handles primitive types, but the
- * API could be extended to support iterators and the like.
- *
- * NOTE: Like standard Collection implementations/arrays, this class is not
- * synchronized.
- */
-public final class DynamicIntArray {
-  static final int DEFAULT_CHUNKSIZE = 8 * 1024;
-  static final int INIT_CHUNKS = 128;
-
-  private final int chunkSize;       // our allocation size
-  private int[][] data;              // the real data
-  private int length;                // max set element index +1
-  private int initializedChunks = 0; // the number of created chunks
-
-  public DynamicIntArray() {
-    this(DEFAULT_CHUNKSIZE);
-  }
-
-  public DynamicIntArray(int chunkSize) {
-    this.chunkSize = chunkSize;
-
-    data = new int[INIT_CHUNKS][];
-  }
-
-  /**
-   * Ensure that the given index is valid.
-   */
-  private void grow(int chunkIndex) {
-    if (chunkIndex >= initializedChunks) {
-      if (chunkIndex >= data.length) {
-        int newSize = Math.max(chunkIndex + 1, 2 * data.length);
-        int[][] newChunk = new int[newSize][];
-        System.arraycopy(data, 0, newChunk, 0, data.length);
-        data = newChunk;
-      }
-      for (int i=initializedChunks; i <= chunkIndex; ++i) {
-        data[i] = new int[chunkSize];
-      }
-      initializedChunks = chunkIndex + 1;
-    }
-  }
-
-  public int get(int index) {
-    if (index >= length) {
-      throw new IndexOutOfBoundsException("Index " + index +
-                                            " is outside of 0.." +
-                                            (length - 1));
-    }
-    int i = index / chunkSize;
-    int j = index % chunkSize;
-    return data[i][j];
-  }
-
-  public void set(int index, int value) {
-    int i = index / chunkSize;
-    int j = index % chunkSize;
-    grow(i);
-    if (index >= length) {
-      length = index + 1;
-    }
-    data[i][j] = value;
-  }
-
-  public void increment(int index, int value) {
-    int i = index / chunkSize;
-    int j = index % chunkSize;
-    grow(i);
-    if (index >= length) {
-      length = index + 1;
-    }
-    data[i][j] += value;
-  }
-
-  public void add(int value) {
-    int i = length / chunkSize;
-    int j = length % chunkSize;
-    grow(i);
-    data[i][j] = value;
-    length += 1;
-  }
-
-  public int size() {
-    return length;
-  }
-
-  public void clear() {
-    length = 0;
-    for(int i=0; i < data.length; ++i) {
-      data[i] = null;
-    }
-    initializedChunks = 0;
-  }
-
-  public String toString() {
-    int i;
-    StringBuilder sb = new StringBuilder(length * 4);
-
-    sb.append('{');
-    int l = length - 1;
-    for (i=0; i<l; i++) {
-      sb.append(get(i));
-      sb.append(',');
-    }
-    sb.append(get(i));
-    sb.append('}');
-
-    return sb.toString();
-  }
-
-  public int getSizeInBytes() {
-    return 4 * initializedChunks * chunkSize;
-  }
-}
-
diff --git a/orc/src/java/org/apache/orc/impl/HadoopShims.java b/orc/src/java/org/apache/orc/impl/HadoopShims.java
deleted file mode 100644
index ef7d70fb40..0000000000
--- a/orc/src/java/org/apache/orc/impl/HadoopShims.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.util.VersionInfo;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.ByteBuffer;
-
-public interface HadoopShims {
-
-  enum DirectCompressionType {
-    NONE,
-    ZLIB_NOHEADER,
-    ZLIB,
-    SNAPPY,
-  }
-
-  interface DirectDecompressor {
-    void decompress(ByteBuffer var1, ByteBuffer var2) throws IOException;
-  }
-
-  /**
-   * Get a direct decompressor codec, if it is available
-   * @param codec
-   * @return
-   */
-  DirectDecompressor getDirectDecompressor(DirectCompressionType codec);
-
-  /**
-   * a hadoop.io ByteBufferPool shim.
-   */
-  public interface ByteBufferPoolShim {
-    /**
-     * Get a new ByteBuffer from the pool.  The pool can provide this from
-     * removing a buffer from its internal cache, or by allocating a
-     * new buffer.
-     *
-     * @param direct     Whether the buffer should be direct.
-     * @param length     The minimum length the buffer will have.
-     * @return           A new ByteBuffer. Its capacity can be less
-     *                   than what was requested, but must be at
-     *                   least 1 byte.
-     */
-    ByteBuffer getBuffer(boolean direct, int length);
-
-    /**
-     * Release a buffer back to the pool.
-     * The pool may choose to put this buffer into its cache/free it.
-     *
-     * @param buffer    a direct bytebuffer
-     */
-    void putBuffer(ByteBuffer buffer);
-  }
-
-  /**
-   * Provides an HDFS ZeroCopyReader shim.
-   * @param in FSDataInputStream to read from (where the cached/mmap buffers are tied to)
-   * @param in ByteBufferPoolShim to allocate fallback buffers with
-   *
-   * @return returns null if not supported
-   */
-  public ZeroCopyReaderShim getZeroCopyReader(FSDataInputStream in, ByteBufferPoolShim pool) throws IOException;
-
-  public interface ZeroCopyReaderShim extends Closeable {
-    /**
-     * Get a ByteBuffer from the FSDataInputStream - this can be either a HeapByteBuffer or an MappedByteBuffer.
-     * Also move the in stream by that amount. The data read can be small than maxLength.
-     *
-     * @return ByteBuffer read from the stream,
-     */
-    public ByteBuffer readBuffer(int maxLength, boolean verifyChecksums) throws IOException;
-    /**
-     * Release a ByteBuffer obtained from a read on the
-     * Also move the in stream by that amount. The data read can be small than maxLength.
-     *
-     */
-    public void releaseBuffer(ByteBuffer buffer);
-
-    /**
-     * Close the underlying stream.
-     * @throws IOException
-     */
-    public void close() throws IOException;
-  }
-  /**
-   * Read data into a Text object in the fastest way possible
-   */
-  public interface TextReaderShim {
-    /**
-     * @param txt
-     * @param size
-     * @return bytes read
-     * @throws IOException
-     */
-    void read(Text txt, int size) throws IOException;
-  }
-
-  /**
-   * Wrap a TextReaderShim around an input stream. The reader shim will not
-   * buffer any reads from the underlying stream and will only consume bytes
-   * which are required for TextReaderShim.read() input.
-   */
-  public TextReaderShim getTextReaderShim(InputStream input) throws IOException;
-
-  class Factory {
-    private static HadoopShims SHIMS = null;
-
-    public static synchronized HadoopShims get() {
-      if (SHIMS == null) {
-        String[] versionParts = VersionInfo.getVersion().split("[.]");
-        int major = Integer.parseInt(versionParts[0]);
-        int minor = Integer.parseInt(versionParts[1]);
-        if (major < 2 || (major == 2 && minor < 3)) {
-          SHIMS = new HadoopShims_2_2();
-        } else {
-          SHIMS = new HadoopShimsCurrent();
-        }
-      }
-      return SHIMS;
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/HadoopShimsCurrent.java b/orc/src/java/org/apache/orc/impl/HadoopShimsCurrent.java
deleted file mode 100644
index 5c53f744c7..0000000000
--- a/orc/src/java/org/apache/orc/impl/HadoopShimsCurrent.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.compress.snappy.SnappyDecompressor;
-import org.apache.hadoop.io.compress.zlib.ZlibDecompressor;
-
-import java.io.DataInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.ByteBuffer;
-
-/**
- * Shims for recent versions of Hadoop
- */
-public class HadoopShimsCurrent implements HadoopShims {
-
-  private static class DirectDecompressWrapper implements DirectDecompressor {
-    private final org.apache.hadoop.io.compress.DirectDecompressor root;
-
-    DirectDecompressWrapper(org.apache.hadoop.io.compress.DirectDecompressor root) {
-      this.root = root;
-    }
-
-    public void decompress(ByteBuffer input,
-                           ByteBuffer output) throws IOException {
-      root.decompress(input, output);
-    }
-  }
-
-  public DirectDecompressor getDirectDecompressor(
-      DirectCompressionType codec) {
-    switch (codec) {
-      case ZLIB:
-        return new DirectDecompressWrapper
-            (new ZlibDecompressor.ZlibDirectDecompressor());
-      case ZLIB_NOHEADER:
-        return new DirectDecompressWrapper
-            (new ZlibDecompressor.ZlibDirectDecompressor
-                (ZlibDecompressor.CompressionHeader.NO_HEADER, 0));
-      case SNAPPY:
-        return new DirectDecompressWrapper
-            (new SnappyDecompressor.SnappyDirectDecompressor());
-      default:
-        return null;
-    }
-  }
-
-  @Override
-  public ZeroCopyReaderShim getZeroCopyReader(FSDataInputStream in,
-                                              ByteBufferPoolShim pool
-                                              ) throws IOException {
-    return ZeroCopyShims.getZeroCopyReader(in, pool);
-  }
-
-  private final class FastTextReaderShim implements TextReaderShim {
-    private final DataInputStream din;
-
-    public FastTextReaderShim(InputStream in) {
-      this.din = new DataInputStream(in);
-    }
-
-    @Override
-    public void read(Text txt, int len) throws IOException {
-      txt.readWithKnownLength(din, len);
-    }
-  }
-
-  @Override
-  public TextReaderShim getTextReaderShim(InputStream in) throws IOException {
-    return new FastTextReaderShim(in);
-  }
-
-}
diff --git a/orc/src/java/org/apache/orc/impl/HadoopShims_2_2.java b/orc/src/java/org/apache/orc/impl/HadoopShims_2_2.java
deleted file mode 100644
index 3f65e74478..0000000000
--- a/orc/src/java/org/apache/orc/impl/HadoopShims_2_2.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.io.Text;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InputStream;
-import java.lang.reflect.Method;
-
-/**
- * Shims for versions of Hadoop up to and including 2.2.x
- */
-public class HadoopShims_2_2 implements HadoopShims {
-
-  final boolean zeroCopy;
-  final boolean fastRead;
-
-  HadoopShims_2_2() {
-    boolean zcr = false;
-    try {
-      Class.forName("org.apache.hadoop.fs.CacheFlag", false,
-        HadoopShims_2_2.class.getClassLoader());
-      zcr = true;
-    } catch (ClassNotFoundException ce) {
-    }
-    zeroCopy = zcr;
-    boolean fastRead = false;
-    if (zcr) {
-      for (Method m : Text.class.getMethods()) {
-        if ("readWithKnownLength".equals(m.getName())) {
-          fastRead = true;
-        }
-      }
-    }
-    this.fastRead = fastRead;
-  }
-
-  public DirectDecompressor getDirectDecompressor(
-      DirectCompressionType codec) {
-    return null;
-  }
-
-  @Override
-  public ZeroCopyReaderShim getZeroCopyReader(FSDataInputStream in,
-                                              ByteBufferPoolShim pool
-                                              ) throws IOException {
-    if(zeroCopy) {
-      return ZeroCopyShims.getZeroCopyReader(in, pool);
-    }
-    /* not supported */
-    return null;
-  }
-
-  private final class BasicTextReaderShim implements TextReaderShim {
-    private final InputStream in;
-
-    public BasicTextReaderShim(InputStream in) {
-      this.in = in;
-    }
-
-    @Override
-    public void read(Text txt, int len) throws IOException {
-      int offset = 0;
-      byte[] bytes = new byte[len];
-      while (len > 0) {
-        int written = in.read(bytes, offset, len);
-        if (written < 0) {
-          throw new EOFException("Can't finish read from " + in + " read "
-              + (offset) + " bytes out of " + bytes.length);
-        }
-        len -= written;
-        offset += written;
-      }
-      txt.set(bytes);
-    }
-  }
-
-  @Override
-  public TextReaderShim getTextReaderShim(InputStream in) throws IOException {
-    return new BasicTextReaderShim(in);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/InStream.java b/orc/src/java/org/apache/orc/impl/InStream.java
deleted file mode 100644
index 851f6455dc..0000000000
--- a/orc/src/java/org/apache/orc/impl/InStream.java
+++ /dev/null
@@ -1,498 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.ListIterator;
-
-import org.apache.orc.CompressionCodec;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.hive.common.io.DiskRange;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.protobuf.CodedInputStream;
-
-public abstract class InStream extends InputStream {
-
-  private static final Logger LOG = LoggerFactory.getLogger(InStream.class);
-  public static final int PROTOBUF_MESSAGE_MAX_LIMIT = 1024 << 20; // 1GB
-
-  protected final String name;
-  protected long length;
-
-  public InStream(String name, long length) {
-    this.name = name;
-    this.length = length;
-  }
-
-  public String getStreamName() {
-    return name;
-  }
-
-  public long getStreamLength() {
-    return length;
-  }
-
-  @Override
-  public abstract void close();
-
-  public static class UncompressedStream extends InStream {
-    private List<DiskRange> bytes;
-    private long length;
-    protected long currentOffset;
-    private ByteBuffer range;
-    private int currentRange;
-
-    public UncompressedStream(String name, List<DiskRange> input, long length) {
-      super(name, length);
-      reset(input, length);
-    }
-
-    protected void reset(List<DiskRange> input, long length) {
-      this.bytes = input;
-      this.length = length;
-      currentRange = 0;
-      currentOffset = 0;
-      range = null;
-    }
-
-    @Override
-    public int read() {
-      if (range == null || range.remaining() == 0) {
-        if (currentOffset == length) {
-          return -1;
-        }
-        seek(currentOffset);
-      }
-      currentOffset += 1;
-      return 0xff & range.get();
-    }
-
-    @Override
-    public int read(byte[] data, int offset, int length) {
-      if (range == null || range.remaining() == 0) {
-        if (currentOffset == this.length) {
-          return -1;
-        }
-        seek(currentOffset);
-      }
-      int actualLength = Math.min(length, range.remaining());
-      range.get(data, offset, actualLength);
-      currentOffset += actualLength;
-      return actualLength;
-    }
-
-    @Override
-    public int available() {
-      if (range != null && range.remaining() > 0) {
-        return range.remaining();
-      }
-      return (int) (length - currentOffset);
-    }
-
-    @Override
-    public void close() {
-      currentRange = bytes.size();
-      currentOffset = length;
-      // explicit de-ref of bytes[]
-      bytes.clear();
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      seek(index.getNext());
-    }
-
-    public void seek(long desired) {
-      if (desired == 0 && bytes.isEmpty()) {
-        logEmptySeek(name);
-        return;
-      }
-      int i = 0;
-      for (DiskRange curRange : bytes) {
-        if (desired == 0 && curRange.getData().remaining() == 0) {
-          logEmptySeek(name);
-          return;
-        }
-        if (curRange.getOffset() <= desired &&
-            (desired - curRange.getOffset()) < curRange.getLength()) {
-          currentOffset = desired;
-          currentRange = i;
-          this.range = curRange.getData().duplicate();
-          int pos = range.position();
-          pos += (int)(desired - curRange.getOffset()); // this is why we duplicate
-          this.range.position(pos);
-          return;
-        }
-        ++i;
-      }
-      // if they are seeking to the precise end, go ahead and let them go there
-      int segments = bytes.size();
-      if (segments != 0 && desired == bytes.get(segments - 1).getEnd()) {
-        currentOffset = desired;
-        currentRange = segments - 1;
-        DiskRange curRange = bytes.get(currentRange);
-        this.range = curRange.getData().duplicate();
-        int pos = range.position();
-        pos += (int)(desired - curRange.getOffset()); // this is why we duplicate
-        this.range.position(pos);
-        return;
-      }
-      throw new IllegalArgumentException("Seek in " + name + " to " +
-        desired + " is outside of the data");
-    }
-
-    @Override
-    public String toString() {
-      return "uncompressed stream " + name + " position: " + currentOffset +
-          " length: " + length + " range: " + currentRange +
-          " offset: " + (range == null ? 0 : range.position()) + " limit: " + (range == null ? 0 : range.limit());
-    }
-  }
-
-  private static ByteBuffer allocateBuffer(int size, boolean isDirect) {
-    // TODO: use the same pool as the ORC readers
-    if (isDirect) {
-      return ByteBuffer.allocateDirect(size);
-    } else {
-      return ByteBuffer.allocate(size);
-    }
-  }
-
-  private static class CompressedStream extends InStream {
-    private final List<DiskRange> bytes;
-    private final int bufferSize;
-    private ByteBuffer uncompressed;
-    private final CompressionCodec codec;
-    private ByteBuffer compressed;
-    private long currentOffset;
-    private int currentRange;
-    private boolean isUncompressedOriginal;
-
-    public CompressedStream(String name, List<DiskRange> input, long length,
-                            CompressionCodec codec, int bufferSize) {
-      super(name, length);
-      this.bytes = input;
-      this.codec = codec;
-      this.bufferSize = bufferSize;
-      currentOffset = 0;
-      currentRange = 0;
-    }
-
-    private void allocateForUncompressed(int size, boolean isDirect) {
-      uncompressed = allocateBuffer(size, isDirect);
-    }
-
-    private void readHeader() throws IOException {
-      if (compressed == null || compressed.remaining() <= 0) {
-        seek(currentOffset);
-      }
-      if (compressed.remaining() > OutStream.HEADER_SIZE) {
-        int b0 = compressed.get() & 0xff;
-        int b1 = compressed.get() & 0xff;
-        int b2 = compressed.get() & 0xff;
-        boolean isOriginal = (b0 & 0x01) == 1;
-        int chunkLength = (b2 << 15) | (b1 << 7) | (b0 >> 1);
-
-        if (chunkLength > bufferSize) {
-          throw new IllegalArgumentException("Buffer size too small. size = " +
-              bufferSize + " needed = " + chunkLength);
-        }
-        // read 3 bytes, which should be equal to OutStream.HEADER_SIZE always
-        assert OutStream.HEADER_SIZE == 3 : "The Orc HEADER_SIZE must be the same in OutStream and InStream";
-        currentOffset += OutStream.HEADER_SIZE;
-
-        ByteBuffer slice = this.slice(chunkLength);
-
-        if (isOriginal) {
-          uncompressed = slice;
-          isUncompressedOriginal = true;
-        } else {
-          if (isUncompressedOriginal) {
-            allocateForUncompressed(bufferSize, slice.isDirect());
-            isUncompressedOriginal = false;
-          } else if (uncompressed == null) {
-            allocateForUncompressed(bufferSize, slice.isDirect());
-          } else {
-            uncompressed.clear();
-          }
-          codec.decompress(slice, uncompressed);
-         }
-      } else {
-        throw new IllegalStateException("Can't read header at " + this);
-      }
-    }
-
-    @Override
-    public int read() throws IOException {
-      if (uncompressed == null || uncompressed.remaining() == 0) {
-        if (currentOffset == length) {
-          return -1;
-        }
-        readHeader();
-      }
-      return 0xff & uncompressed.get();
-    }
-
-    @Override
-    public int read(byte[] data, int offset, int length) throws IOException {
-      if (uncompressed == null || uncompressed.remaining() == 0) {
-        if (currentOffset == this.length) {
-          return -1;
-        }
-        readHeader();
-      }
-      int actualLength = Math.min(length, uncompressed.remaining());
-      uncompressed.get(data, offset, actualLength);
-      return actualLength;
-    }
-
-    @Override
-    public int available() throws IOException {
-      if (uncompressed == null || uncompressed.remaining() == 0) {
-        if (currentOffset == length) {
-          return 0;
-        }
-        readHeader();
-      }
-      return uncompressed.remaining();
-    }
-
-    @Override
-    public void close() {
-      uncompressed = null;
-      compressed = null;
-      currentRange = bytes.size();
-      currentOffset = length;
-      bytes.clear();
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      seek(index.getNext());
-      long uncompressedBytes = index.getNext();
-      if (uncompressedBytes != 0) {
-        readHeader();
-        uncompressed.position(uncompressed.position() +
-                              (int) uncompressedBytes);
-      } else if (uncompressed != null) {
-        // mark the uncompressed buffer as done
-        uncompressed.position(uncompressed.limit());
-      }
-    }
-
-    /* slices a read only contiguous buffer of chunkLength */
-    private ByteBuffer slice(int chunkLength) throws IOException {
-      int len = chunkLength;
-      final long oldOffset = currentOffset;
-      ByteBuffer slice;
-      if (compressed.remaining() >= len) {
-        slice = compressed.slice();
-        // simple case
-        slice.limit(len);
-        currentOffset += len;
-        compressed.position(compressed.position() + len);
-        return slice;
-      } else if (currentRange >= (bytes.size() - 1)) {
-        // nothing has been modified yet
-        throw new IOException("EOF in " + this + " while trying to read " +
-            chunkLength + " bytes");
-      }
-
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(String.format(
-            "Crossing into next BufferChunk because compressed only has %d bytes (needs %d)",
-            compressed.remaining(), len));
-      }
-
-      // we need to consolidate 2 or more buffers into 1
-      // first copy out compressed buffers
-      ByteBuffer copy = allocateBuffer(chunkLength, compressed.isDirect());
-      currentOffset += compressed.remaining();
-      len -= compressed.remaining();
-      copy.put(compressed);
-      ListIterator<DiskRange> iter = bytes.listIterator(currentRange);
-
-      while (len > 0 && iter.hasNext()) {
-        ++currentRange;
-        if (LOG.isDebugEnabled()) {
-          LOG.debug(String.format("Read slow-path, >1 cross block reads with %s", this.toString()));
-        }
-        DiskRange range = iter.next();
-        compressed = range.getData().duplicate();
-        if (compressed.remaining() >= len) {
-          slice = compressed.slice();
-          slice.limit(len);
-          copy.put(slice);
-          currentOffset += len;
-          compressed.position(compressed.position() + len);
-          return copy;
-        }
-        currentOffset += compressed.remaining();
-        len -= compressed.remaining();
-        copy.put(compressed);
-      }
-
-      // restore offsets for exception clarity
-      seek(oldOffset);
-      throw new IOException("EOF in " + this + " while trying to read " +
-          chunkLength + " bytes");
-    }
-
-    private void seek(long desired) throws IOException {
-      if (desired == 0 && bytes.isEmpty()) {
-        logEmptySeek(name);
-        return;
-      }
-      int i = 0;
-      for (DiskRange range : bytes) {
-        if (range.getOffset() <= desired && desired < range.getEnd()) {
-          currentRange = i;
-          compressed = range.getData().duplicate();
-          int pos = compressed.position();
-          pos += (int)(desired - range.getOffset());
-          compressed.position(pos);
-          currentOffset = desired;
-          return;
-        }
-        ++i;
-      }
-      // if they are seeking to the precise end, go ahead and let them go there
-      int segments = bytes.size();
-      if (segments != 0 && desired == bytes.get(segments - 1).getEnd()) {
-        DiskRange range = bytes.get(segments - 1);
-        currentRange = segments - 1;
-        compressed = range.getData().duplicate();
-        compressed.position(compressed.limit());
-        currentOffset = desired;
-        return;
-      }
-      throw new IOException("Seek outside of data in " + this + " to " + desired);
-    }
-
-    private String rangeString() {
-      StringBuilder builder = new StringBuilder();
-      int i = 0;
-      for (DiskRange range : bytes) {
-        if (i != 0) {
-          builder.append("; ");
-        }
-        builder.append(" range " + i + " = " + range.getOffset()
-            + " to " + (range.getEnd() - range.getOffset()));
-        ++i;
-      }
-      return builder.toString();
-    }
-
-    @Override
-    public String toString() {
-      return "compressed stream " + name + " position: " + currentOffset +
-          " length: " + length + " range: " + currentRange +
-          " offset: " + (compressed == null ? 0 : compressed.position()) + " limit: " + (compressed == null ? 0 : compressed.limit()) +
-          rangeString() +
-          (uncompressed == null ? "" :
-              " uncompressed: " + uncompressed.position() + " to " +
-                  uncompressed.limit());
-    }
-  }
-
-  public abstract void seek(PositionProvider index) throws IOException;
-
-  private static void logEmptySeek(String name) {
-    if (LOG.isWarnEnabled()) {
-      LOG.warn("Attempting seek into empty stream (" + name + ") Skipping stream.");
-    }
-  }
-
-  /**
-   * Create an input stream from a list of buffers.
-   * @param streamName the name of the stream
-   * @param buffers the list of ranges of bytes for the stream
-   * @param offsets a list of offsets (the same length as input) that must
-   *                contain the first offset of the each set of bytes in input
-   * @param length the length in bytes of the stream
-   * @param codec the compression codec
-   * @param bufferSize the compression buffer size
-   * @return an input stream
-   * @throws IOException
-   */
-  @VisibleForTesting
-  @Deprecated
-  public static InStream create(String streamName,
-                                ByteBuffer[] buffers,
-                                long[] offsets,
-                                long length,
-                                CompressionCodec codec,
-                                int bufferSize) throws IOException {
-    List<DiskRange> input = new ArrayList<DiskRange>(buffers.length);
-    for (int i = 0; i < buffers.length; ++i) {
-      input.add(new BufferChunk(buffers[i], offsets[i]));
-    }
-    return create(streamName, input, length, codec, bufferSize);
-  }
-
-  /**
-   * Create an input stream from a list of disk ranges with data.
-   * @param name the name of the stream
-   * @param input the list of ranges of bytes for the stream; from disk or cache
-   * @param length the length in bytes of the stream
-   * @param codec the compression codec
-   * @param bufferSize the compression buffer size
-   * @return an input stream
-   * @throws IOException
-   */
-  public static InStream create(String name,
-                                List<DiskRange> input,
-                                long length,
-                                CompressionCodec codec,
-                                int bufferSize) throws IOException {
-    if (codec == null) {
-      return new UncompressedStream(name, input, length);
-    } else {
-      return new CompressedStream(name, input, length, codec, bufferSize);
-    }
-  }
-
-  /**
-   * Creates coded input stream (used for protobuf message parsing) with higher message size limit.
-   *
-   * @param name       the name of the stream
-   * @param input      the list of ranges of bytes for the stream; from disk or cache
-   * @param length     the length in bytes of the stream
-   * @param codec      the compression codec
-   * @param bufferSize the compression buffer size
-   * @return coded input stream
-   * @throws IOException
-   */
-  public static CodedInputStream createCodedInputStream(
-      String name,
-      List<DiskRange> input,
-      long length,
-      CompressionCodec codec,
-      int bufferSize) throws IOException {
-    InStream inStream = create(name, input, length, codec, bufferSize);
-    CodedInputStream codedInputStream = CodedInputStream.newInstance(inStream);
-    codedInputStream.setSizeLimit(PROTOBUF_MESSAGE_MAX_LIMIT);
-    return codedInputStream;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/IntegerReader.java b/orc/src/java/org/apache/orc/impl/IntegerReader.java
deleted file mode 100644
index 3e64d548c7..0000000000
--- a/orc/src/java/org/apache/orc/impl/IntegerReader.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.io.IOException;
-
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-
-/**
- * Interface for reading integers.
- */
-public interface IntegerReader {
-
-  /**
-   * Seek to the position provided by index.
-   * @param index
-   * @throws IOException
-   */
-  void seek(PositionProvider index) throws IOException;
-
-  /**
-   * Skip number of specified rows.
-   * @param numValues
-   * @throws IOException
-   */
-  void skip(long numValues) throws IOException;
-
-  /**
-   * Check if there are any more values left.
-   * @return
-   * @throws IOException
-   */
-  boolean hasNext() throws IOException;
-
-  /**
-   * Return the next available value.
-   * @return
-   * @throws IOException
-   */
-  long next() throws IOException;
-
-  /**
-   * Return the next available vector for values.
-   * @param column the column being read
-   * @param data the vector to read into
-   * @param length the number of numbers to read
-   * @throws IOException
-   */
-   void nextVector(ColumnVector column,
-                   long[] data,
-                   int length
-                   ) throws IOException;
-
-  /**
-   * Return the next available vector for values. Does not change the
-   * value of column.isRepeating.
-   * @param column the column being read
-   * @param data the vector to read into
-   * @param length the number of numbers to read
-   * @throws IOException
-   */
-  void nextVector(ColumnVector column,
-                  int[] data,
-                  int length
-                  ) throws IOException;
-}
diff --git a/orc/src/java/org/apache/orc/impl/IntegerWriter.java b/orc/src/java/org/apache/orc/impl/IntegerWriter.java
deleted file mode 100644
index 419054f7ea..0000000000
--- a/orc/src/java/org/apache/orc/impl/IntegerWriter.java
+++ /dev/null
@@ -1,47 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.io.IOException;
-
-/**
- * Interface for writing integers.
- */
-public interface IntegerWriter {
-
-  /**
-   * Get position from the stream.
-   * @param recorder
-   * @throws IOException
-   */
-  void getPosition(PositionRecorder recorder) throws IOException;
-
-  /**
-   * Write the integer value
-   * @param value
-   * @throws IOException
-   */
-  void write(long value) throws IOException;
-
-  /**
-   * Flush the buffer
-   * @throws IOException
-   */
-  void flush() throws IOException;
-}
diff --git a/orc/src/java/org/apache/orc/impl/MemoryManager.java b/orc/src/java/org/apache/orc/impl/MemoryManager.java
deleted file mode 100644
index 757c0b4c82..0000000000
--- a/orc/src/java/org/apache/orc/impl/MemoryManager.java
+++ /dev/null
@@ -1,214 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.OrcConf;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-
-import com.google.common.base.Preconditions;
-
-import java.io.IOException;
-import java.lang.management.ManagementFactory;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.locks.ReentrantLock;
-
-/**
- * Implements a memory manager that keeps a global context of how many ORC
- * writers there are and manages the memory between them. For use cases with
- * dynamic partitions, it is easy to end up with many writers in the same task.
- * By managing the size of each allocation, we try to cut down the size of each
- * allocation and keep the task from running out of memory.
- * 
- * This class is not thread safe, but is re-entrant - ensure creation and all
- * invocations are triggered from the same thread.
- */
-public class MemoryManager {
-
-  private static final Logger LOG = LoggerFactory.getLogger(MemoryManager.class);
-
-  /**
-   * How often should we check the memory sizes? Measured in rows added
-   * to all of the writers.
-   */
-  private static final int ROWS_BETWEEN_CHECKS = 5000;
-  private final long totalMemoryPool;
-  private final Map<Path, WriterInfo> writerList =
-      new HashMap<Path, WriterInfo>();
-  private long totalAllocation = 0;
-  private double currentScale = 1;
-  private int rowsAddedSinceCheck = 0;
-  private final OwnedLock ownerLock = new OwnedLock();
-
-  @SuppressWarnings("serial")
-  private static class OwnedLock extends ReentrantLock {
-    public Thread getOwner() {
-      return super.getOwner();
-    }
-  }
-
-  private static class WriterInfo {
-    long allocation;
-    Callback callback;
-    WriterInfo(long allocation, Callback callback) {
-      this.allocation = allocation;
-      this.callback = callback;
-    }
-  }
-
-  public interface Callback {
-    /**
-     * The writer needs to check its memory usage
-     * @param newScale the current scale factor for memory allocations
-     * @return true if the writer was over the limit
-     * @throws IOException
-     */
-    boolean checkMemory(double newScale) throws IOException;
-  }
-
-  /**
-   * Create the memory manager.
-   * @param conf use the configuration to find the maximum size of the memory
-   *             pool.
-   */
-  public MemoryManager(Configuration conf) {
-    double maxLoad = OrcConf.MEMORY_POOL.getDouble(conf);
-    totalMemoryPool = Math.round(ManagementFactory.getMemoryMXBean().
-        getHeapMemoryUsage().getMax() * maxLoad);
-    ownerLock.lock();
-  }
-
-  /**
-   * Light weight thread-safety check for multi-threaded access patterns
-   */
-  private void checkOwner() {
-    if (!ownerLock.isHeldByCurrentThread()) {
-      LOG.warn("Owner thread expected {}, got {}",
-          ownerLock.getOwner(), Thread.currentThread());
-    }
-  }
-
-  /**
-   * Add a new writer's memory allocation to the pool. We use the path
-   * as a unique key to ensure that we don't get duplicates.
-   * @param path the file that is being written
-   * @param requestedAllocation the requested buffer size
-   */
-  public void addWriter(Path path, long requestedAllocation,
-                              Callback callback) throws IOException {
-    checkOwner();
-    WriterInfo oldVal = writerList.get(path);
-    // this should always be null, but we handle the case where the memory
-    // manager wasn't told that a writer wasn't still in use and the task
-    // starts writing to the same path.
-    if (oldVal == null) {
-      oldVal = new WriterInfo(requestedAllocation, callback);
-      writerList.put(path, oldVal);
-      totalAllocation += requestedAllocation;
-    } else {
-      // handle a new writer that is writing to the same path
-      totalAllocation += requestedAllocation - oldVal.allocation;
-      oldVal.allocation = requestedAllocation;
-      oldVal.callback = callback;
-    }
-    updateScale(true);
-  }
-
-  /**
-   * Remove the given writer from the pool.
-   * @param path the file that has been closed
-   */
-  public void removeWriter(Path path) throws IOException {
-    checkOwner();
-    WriterInfo val = writerList.get(path);
-    if (val != null) {
-      writerList.remove(path);
-      totalAllocation -= val.allocation;
-      if (writerList.isEmpty()) {
-        rowsAddedSinceCheck = 0;
-      }
-      updateScale(false);
-    }
-    if(writerList.isEmpty()) {
-      rowsAddedSinceCheck = 0;
-    }
-  }
-
-  /**
-   * Get the total pool size that is available for ORC writers.
-   * @return the number of bytes in the pool
-   */
-  public long getTotalMemoryPool() {
-    return totalMemoryPool;
-  }
-
-  /**
-   * The scaling factor for each allocation to ensure that the pool isn't
-   * oversubscribed.
-   * @return a fraction between 0.0 and 1.0 of the requested size that is
-   * available for each writer.
-   */
-  public double getAllocationScale() {
-    return currentScale;
-  }
-
-  /**
-   * Give the memory manager an opportunity for doing a memory check.
-   * @param rows number of rows added
-   * @throws IOException
-   */
-  public void addedRow(int rows) throws IOException {
-    rowsAddedSinceCheck += rows;
-    if (rowsAddedSinceCheck >= ROWS_BETWEEN_CHECKS) {
-      notifyWriters();
-    }
-  }
-
-  /**
-   * Notify all of the writers that they should check their memory usage.
-   * @throws IOException
-   */
-  public void notifyWriters() throws IOException {
-    checkOwner();
-    LOG.debug("Notifying writers after " + rowsAddedSinceCheck);
-    for(WriterInfo writer: writerList.values()) {
-      boolean flushed = writer.callback.checkMemory(currentScale);
-      if (LOG.isDebugEnabled() && flushed) {
-        LOG.debug("flushed " + writer.toString());
-      }
-    }
-    rowsAddedSinceCheck = 0;
-  }
-
-  /**
-   * Update the currentScale based on the current allocation and pool size.
-   * This also updates the notificationTrigger.
-   * @param isAllocate is this an allocation?
-   */
-  private void updateScale(boolean isAllocate) throws IOException {
-    if (totalAllocation <= totalMemoryPool) {
-      currentScale = 1;
-    } else {
-      currentScale = (double) totalMemoryPool / totalAllocation;
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/OrcAcidUtils.java b/orc/src/java/org/apache/orc/impl/OrcAcidUtils.java
deleted file mode 100644
index 7ca9e1dced..0000000000
--- a/orc/src/java/org/apache/orc/impl/OrcAcidUtils.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.Reader;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.nio.charset.CharacterCodingException;
-import java.nio.charset.Charset;
-import java.nio.charset.CharsetDecoder;
-
-public class OrcAcidUtils {
-  public static final String ACID_STATS = "hive.acid.stats";
-  public static final String DELTA_SIDE_FILE_SUFFIX = "_flush_length";
-
-  /**
-   * Get the filename of the ORC ACID side file that contains the lengths
-   * of the intermediate footers.
-   * @param main the main ORC filename
-   * @return the name of the side file
-   */
-  public static Path getSideFile(Path main) {
-    return new Path(main + DELTA_SIDE_FILE_SUFFIX);
-  }
-
-  /**
-   * Read the side file to get the last flush length.
-   * @param fs the file system to use
-   * @param deltaFile the path of the delta file
-   * @return the maximum size of the file to use
-   * @throws IOException
-   */
-  public static long getLastFlushLength(FileSystem fs,
-                                        Path deltaFile) throws IOException {
-    Path lengths = getSideFile(deltaFile);
-    long result = Long.MAX_VALUE;
-    if(!fs.exists(lengths)) {
-      return result;
-    }
-    try (FSDataInputStream stream = fs.open(lengths)) {
-      result = -1;
-      while (stream.available() > 0) {
-        result = stream.readLong();
-      }
-      return result;
-    } catch (IOException ioe) {
-      return result;
-    }
-  }
-
-  private static final Charset utf8 = Charset.forName("UTF-8");
-  private static final CharsetDecoder utf8Decoder = utf8.newDecoder();
-
-  public static AcidStats parseAcidStats(Reader reader) {
-    if (reader.hasMetadataValue(ACID_STATS)) {
-      try {
-        ByteBuffer val = reader.getMetadataValue(ACID_STATS).duplicate();
-        return new AcidStats(utf8Decoder.decode(val).toString());
-      } catch (CharacterCodingException e) {
-        throw new IllegalArgumentException("Bad string encoding for " +
-            ACID_STATS, e);
-      }
-    } else {
-      return null;
-    }
-  }
-
-}
diff --git a/orc/src/java/org/apache/orc/impl/OrcIndex.java b/orc/src/java/org/apache/orc/impl/OrcIndex.java
deleted file mode 100644
index 50a15f2985..0000000000
--- a/orc/src/java/org/apache/orc/impl/OrcIndex.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.OrcProto;
-
-public final class OrcIndex {
-  OrcProto.RowIndex[] rowGroupIndex;
-  OrcProto.BloomFilterIndex[] bloomFilterIndex;
-
-  public OrcIndex(OrcProto.RowIndex[] rgIndex, OrcProto.BloomFilterIndex[] bfIndex) {
-    this.rowGroupIndex = rgIndex;
-    this.bloomFilterIndex = bfIndex;
-  }
-
-  public OrcProto.RowIndex[] getRowGroupIndex() {
-    return rowGroupIndex;
-  }
-
-  public OrcProto.BloomFilterIndex[] getBloomFilterIndex() {
-    return bloomFilterIndex;
-  }
-
-  public void setRowGroupIndex(OrcProto.RowIndex[] rowGroupIndex) {
-    this.rowGroupIndex = rowGroupIndex;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/OrcTail.java b/orc/src/java/org/apache/orc/impl/OrcTail.java
deleted file mode 100644
index f0956035db..0000000000
--- a/orc/src/java/org/apache/orc/impl/OrcTail.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * http://www.apache.org/licenses/LICENSE-2.0
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import static org.apache.orc.impl.ReaderImpl.extractMetadata;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.orc.CompressionCodec;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcFile;
-import org.apache.orc.OrcProto;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.StripeStatistics;
-
-// TODO: Make OrcTail implement FileMetadata or Reader interface
-public final class OrcTail {
-  // postscript + footer - Serialized in OrcSplit
-  private final OrcProto.FileTail fileTail;
-  // serialized representation of metadata, footer and postscript
-  private final ByteBuffer serializedTail;
-  // used to invalidate cache entries
-  private final long fileModificationTime;
-  // lazily deserialized
-  private OrcProto.Metadata metadata;
-
-  public OrcTail(OrcProto.FileTail fileTail, ByteBuffer serializedTail) {
-    this(fileTail, serializedTail, -1);
-  }
-
-  public OrcTail(OrcProto.FileTail fileTail, ByteBuffer serializedTail, long fileModificationTime) {
-    this.fileTail = fileTail;
-    this.serializedTail = serializedTail;
-    this.fileModificationTime = fileModificationTime;
-    this.metadata = null;
-  }
-
-  public ByteBuffer getSerializedTail() {
-    return serializedTail;
-  }
-
-  public long getFileModificationTime() {
-    return fileModificationTime;
-  }
-
-  public OrcProto.Footer getFooter() {
-    return fileTail.getFooter();
-  }
-
-  public OrcProto.PostScript getPostScript() {
-    return fileTail.getPostscript();
-  }
-
-  public OrcFile.WriterVersion getWriterVersion() {
-    OrcProto.PostScript ps = fileTail.getPostscript();
-    return (ps.hasWriterVersion()
-        ? OrcFile.WriterVersion.from(ps.getWriterVersion()) : OrcFile.WriterVersion.ORIGINAL);
-  }
-
-  public List<StripeInformation> getStripes() {
-    List<StripeInformation> result = new ArrayList<>(fileTail.getFooter().getStripesCount());
-    for (OrcProto.StripeInformation stripeProto : fileTail.getFooter().getStripesList()) {
-      result.add(new ReaderImpl.StripeInformationImpl(stripeProto));
-    }
-    return result;
-  }
-
-  public CompressionKind getCompressionKind() {
-    return CompressionKind.valueOf(fileTail.getPostscript().getCompression().name());
-  }
-
-  public CompressionCodec getCompressionCodec() {
-    return PhysicalFsWriter.createCodec(getCompressionKind());
-  }
-
-  public int getCompressionBufferSize() {
-    return (int) fileTail.getPostscript().getCompressionBlockSize();
-  }
-
-  public List<StripeStatistics> getStripeStatistics() throws IOException {
-    List<StripeStatistics> result = new ArrayList<>();
-    List<OrcProto.StripeStatistics> ssProto = getStripeStatisticsProto();
-    if (ssProto != null) {
-      for (OrcProto.StripeStatistics ss : ssProto) {
-        result.add(new StripeStatistics(ss.getColStatsList()));
-      }
-    }
-    return result;
-  }
-
-  public List<OrcProto.StripeStatistics> getStripeStatisticsProto() throws IOException {
-    if (serializedTail == null) return null;
-    if (metadata == null) {
-      metadata = extractMetadata(serializedTail, 0,
-          (int) fileTail.getPostscript().getMetadataLength(),
-          getCompressionCodec(), getCompressionBufferSize());
-      // clear does not clear the contents but sets position to 0 and limit = capacity
-      serializedTail.clear();
-    }
-    return metadata.getStripeStatsList();
-  }
-
-  public int getMetadataSize() {
-    return (int) getPostScript().getMetadataLength();
-  }
-
-  public List<OrcProto.Type> getTypes() {
-    return getFooter().getTypesList();
-  }
-
-  public OrcProto.FileTail getFileTail() {
-    return fileTail;
-  }
-
-  public OrcProto.FileTail getMinimalFileTail() {
-    OrcProto.FileTail.Builder fileTailBuilder = OrcProto.FileTail.newBuilder(fileTail);
-    OrcProto.Footer.Builder footerBuilder = OrcProto.Footer.newBuilder(fileTail.getFooter());
-    footerBuilder.clearStatistics();
-    fileTailBuilder.setFooter(footerBuilder.build());
-    OrcProto.FileTail result = fileTailBuilder.build();
-    return result;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/OutStream.java b/orc/src/java/org/apache/orc/impl/OutStream.java
deleted file mode 100644
index 81662cc31f..0000000000
--- a/orc/src/java/org/apache/orc/impl/OutStream.java
+++ /dev/null
@@ -1,289 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import org.apache.orc.CompressionCodec;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-public class OutStream extends PositionedOutputStream {
-
-  public interface OutputReceiver {
-    /**
-     * Output the given buffer to the final destination
-     * @param buffer the buffer to output
-     * @throws IOException
-     */
-    void output(ByteBuffer buffer) throws IOException;
-  }
-
-  public static final int HEADER_SIZE = 3;
-  private final String name;
-  private final OutputReceiver receiver;
-  // if enabled the stream will be suppressed when writing stripe
-  private boolean suppress;
-
-  /**
-   * Stores the uncompressed bytes that have been serialized, but not
-   * compressed yet. When this fills, we compress the entire buffer.
-   */
-  private ByteBuffer current = null;
-
-  /**
-   * Stores the compressed bytes until we have a full buffer and then outputs
-   * them to the receiver. If no compression is being done, this (and overflow)
-   * will always be null and the current buffer will be sent directly to the
-   * receiver.
-   */
-  private ByteBuffer compressed = null;
-
-  /**
-   * Since the compressed buffer may start with contents from previous
-   * compression blocks, we allocate an overflow buffer so that the
-   * output of the codec can be split between the two buffers. After the
-   * compressed buffer is sent to the receiver, the overflow buffer becomes
-   * the new compressed buffer.
-   */
-  private ByteBuffer overflow = null;
-  private final int bufferSize;
-  private final CompressionCodec codec;
-  private long compressedBytes = 0;
-  private long uncompressedBytes = 0;
-
-  public OutStream(String name,
-                   int bufferSize,
-                   CompressionCodec codec,
-                   OutputReceiver receiver) throws IOException {
-    this.name = name;
-    this.bufferSize = bufferSize;
-    this.codec = codec;
-    this.receiver = receiver;
-    this.suppress = false;
-  }
-
-  public void clear() throws IOException {
-    flush();
-    suppress = false;
-  }
-
-  /**
-   * Write the length of the compressed bytes. Life is much easier if the
-   * header is constant length, so just use 3 bytes. Considering most of the
-   * codecs want between 32k (snappy) and 256k (lzo, zlib), 3 bytes should
-   * be plenty. We also use the low bit for whether it is the original or
-   * compressed bytes.
-   * @param buffer the buffer to write the header to
-   * @param position the position in the buffer to write at
-   * @param val the size in the file
-   * @param original is it uncompressed
-   */
-  private static void writeHeader(ByteBuffer buffer,
-                                  int position,
-                                  int val,
-                                  boolean original) {
-    buffer.put(position, (byte) ((val << 1) + (original ? 1 : 0)));
-    buffer.put(position + 1, (byte) (val >> 7));
-    buffer.put(position + 2, (byte) (val >> 15));
-  }
-
-  private void getNewInputBuffer() throws IOException {
-    if (codec == null) {
-      current = ByteBuffer.allocate(bufferSize);
-    } else {
-      current = ByteBuffer.allocate(bufferSize + HEADER_SIZE);
-      writeHeader(current, 0, bufferSize, true);
-      current.position(HEADER_SIZE);
-    }
-  }
-
-  /**
-   * Allocate a new output buffer if we are compressing.
-   */
-  private ByteBuffer getNewOutputBuffer() throws IOException {
-    return ByteBuffer.allocate(bufferSize + HEADER_SIZE);
-  }
-
-  private void flip() throws IOException {
-    current.limit(current.position());
-    current.position(codec == null ? 0 : HEADER_SIZE);
-  }
-
-  @Override
-  public void write(int i) throws IOException {
-    if (current == null) {
-      getNewInputBuffer();
-    }
-    if (current.remaining() < 1) {
-      spill();
-    }
-    uncompressedBytes += 1;
-    current.put((byte) i);
-  }
-
-  @Override
-  public void write(byte[] bytes, int offset, int length) throws IOException {
-    if (current == null) {
-      getNewInputBuffer();
-    }
-    int remaining = Math.min(current.remaining(), length);
-    current.put(bytes, offset, remaining);
-    uncompressedBytes += remaining;
-    length -= remaining;
-    while (length != 0) {
-      spill();
-      offset += remaining;
-      remaining = Math.min(current.remaining(), length);
-      current.put(bytes, offset, remaining);
-      uncompressedBytes += remaining;
-      length -= remaining;
-    }
-  }
-
-  private void spill() throws java.io.IOException {
-    // if there isn't anything in the current buffer, don't spill
-    if (current == null ||
-        current.position() == (codec == null ? 0 : HEADER_SIZE)) {
-      return;
-    }
-    flip();
-    if (codec == null) {
-      receiver.output(current);
-      getNewInputBuffer();
-    } else {
-      if (compressed == null) {
-        compressed = getNewOutputBuffer();
-      } else if (overflow == null) {
-        overflow = getNewOutputBuffer();
-      }
-      int sizePosn = compressed.position();
-      compressed.position(compressed.position() + HEADER_SIZE);
-      if (codec.compress(current, compressed, overflow)) {
-        uncompressedBytes = 0;
-        // move position back to after the header
-        current.position(HEADER_SIZE);
-        current.limit(current.capacity());
-        // find the total bytes in the chunk
-        int totalBytes = compressed.position() - sizePosn - HEADER_SIZE;
-        if (overflow != null) {
-          totalBytes += overflow.position();
-        }
-        compressedBytes += totalBytes + HEADER_SIZE;
-        writeHeader(compressed, sizePosn, totalBytes, false);
-        // if we have less than the next header left, spill it.
-        if (compressed.remaining() < HEADER_SIZE) {
-          compressed.flip();
-          receiver.output(compressed);
-          compressed = overflow;
-          overflow = null;
-        }
-      } else {
-        compressedBytes += uncompressedBytes + HEADER_SIZE;
-        uncompressedBytes = 0;
-        // we are using the original, but need to spill the current
-        // compressed buffer first. So back up to where we started,
-        // flip it and add it to done.
-        if (sizePosn != 0) {
-          compressed.position(sizePosn);
-          compressed.flip();
-          receiver.output(compressed);
-          compressed = null;
-          // if we have an overflow, clear it and make it the new compress
-          // buffer
-          if (overflow != null) {
-            overflow.clear();
-            compressed = overflow;
-            overflow = null;
-          }
-        } else {
-          compressed.clear();
-          if (overflow != null) {
-            overflow.clear();
-          }
-        }
-
-        // now add the current buffer into the done list and get a new one.
-        current.position(0);
-        // update the header with the current length
-        writeHeader(current, 0, current.limit() - HEADER_SIZE, true);
-        receiver.output(current);
-        getNewInputBuffer();
-      }
-    }
-  }
-
-  @Override
-  public void getPosition(PositionRecorder recorder) throws IOException {
-    if (codec == null) {
-      recorder.addPosition(uncompressedBytes);
-    } else {
-      recorder.addPosition(compressedBytes);
-      recorder.addPosition(uncompressedBytes);
-    }
-  }
-
-  @Override
-  public void flush() throws IOException {
-    spill();
-    if (compressed != null && compressed.position() != 0) {
-      compressed.flip();
-      receiver.output(compressed);
-    }
-    compressed = null;
-    uncompressedBytes = 0;
-    compressedBytes = 0;
-    overflow = null;
-    current = null;
-  }
-
-  @Override
-  public String toString() {
-    return name;
-  }
-
-  @Override
-  public long getBufferSize() {
-    long result = 0;
-    if (current != null) {
-      result += current.capacity();
-    }
-    if (compressed != null) {
-      result += compressed.capacity();
-    }
-    if (overflow != null) {
-      result += overflow.capacity();
-    }
-    return result;
-  }
-
-  /**
-   * Set suppress flag
-   */
-  public void suppress() {
-    suppress = true;
-  }
-
-  /**
-   * Returns the state of suppress flag
-   * @return value of suppress flag
-   */
-  public boolean isSuppressed() {
-    return suppress;
-  }
-}
-
diff --git a/orc/src/java/org/apache/orc/impl/PhysicalFsWriter.java b/orc/src/java/org/apache/orc/impl/PhysicalFsWriter.java
deleted file mode 100644
index ba8c13fd29..0000000000
--- a/orc/src/java/org/apache/orc/impl/PhysicalFsWriter.java
+++ /dev/null
@@ -1,529 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.io.OutputStream;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.EnumSet;
-import java.util.List;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.CompressionCodec;
-import org.apache.orc.CompressionCodec.Modifier;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcFile;
-import org.apache.orc.OrcFile.CompressionStrategy;
-import org.apache.orc.OrcProto;
-import org.apache.orc.OrcProto.BloomFilterIndex;
-import org.apache.orc.OrcProto.Footer;
-import org.apache.orc.OrcProto.Metadata;
-import org.apache.orc.OrcProto.PostScript;
-import org.apache.orc.OrcProto.Stream.Kind;
-import org.apache.orc.OrcProto.StripeFooter;
-import org.apache.orc.OrcProto.StripeInformation;
-import org.apache.orc.OrcProto.RowIndex.Builder;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.protobuf.CodedOutputStream;
-
-public class PhysicalFsWriter implements PhysicalWriter {
-  private static final Logger LOG = LoggerFactory.getLogger(PhysicalFsWriter.class);
-
-  private static final int HDFS_BUFFER_SIZE = 256 * 1024;
-
-  private FSDataOutputStream rawWriter = null;
-  // the compressed metadata information outStream
-  private OutStream writer = null;
-  // a protobuf outStream around streamFactory
-  private CodedOutputStream protobufWriter = null;
-
-  private final FileSystem fs;
-  private final Path path;
-  private final long blockSize;
-  private final int bufferSize;
-  private final CompressionCodec codec;
-  private final double paddingTolerance;
-  private final long defaultStripeSize;
-  private final CompressionKind compress;
-  private final boolean addBlockPadding;
-  private final CompressionStrategy compressionStrategy;
-
-  // the streams that make up the current stripe
-  private final Map<StreamName, BufferedStream> streams =
-    new TreeMap<StreamName, BufferedStream>();
-
-  private long adjustedStripeSize;
-  private long headerLength;
-  private long stripeStart;
-  private int metadataLength;
-  private int footerLength;
-
-  public PhysicalFsWriter(FileSystem fs, Path path, int numColumns, OrcFile.WriterOptions opts) {
-    this.fs = fs;
-    this.path = path;
-    this.defaultStripeSize = this.adjustedStripeSize = opts.getStripeSize();
-    this.addBlockPadding = opts.getBlockPadding();
-    if (opts.isEnforceBufferSize()) {
-      this.bufferSize = opts.getBufferSize();
-    } else {
-      this.bufferSize = getEstimatedBufferSize(defaultStripeSize, numColumns, opts.getBufferSize());
-    }
-    this.compress = opts.getCompress();
-    this.compressionStrategy = opts.getCompressionStrategy();
-    codec = createCodec(compress);
-    this.paddingTolerance = opts.getPaddingTolerance();
-    this.blockSize = opts.getBlockSize();
-    LOG.info("ORC writer created for path: {} with stripeSize: {} blockSize: {}" +
-        " compression: {} bufferSize: {}", path, defaultStripeSize, blockSize,
-        compress, bufferSize);
-  }
-
-  @Override
-  public void initialize() throws IOException {
-    if (rawWriter != null) return;
-    rawWriter = fs.create(path, false, HDFS_BUFFER_SIZE,
-                          fs.getDefaultReplication(path), blockSize);
-    rawWriter.writeBytes(OrcFile.MAGIC);
-    headerLength = rawWriter.getPos();
-    writer = new OutStream("metadata", bufferSize, codec,
-                           new DirectStream(rawWriter));
-    protobufWriter = CodedOutputStream.newInstance(writer);
-  }
-
-  private void padStripe(long indexSize, long dataSize, int footerSize) throws IOException {
-    this.stripeStart = rawWriter.getPos();
-    final long currentStripeSize = indexSize + dataSize + footerSize;
-    final long available = blockSize - (stripeStart % blockSize);
-    final long overflow = currentStripeSize - adjustedStripeSize;
-    final float availRatio = (float) available / (float) defaultStripeSize;
-
-    if (availRatio > 0.0f && availRatio < 1.0f
-        && availRatio > paddingTolerance) {
-      // adjust default stripe size to fit into remaining space, also adjust
-      // the next stripe for correction based on the current stripe size
-      // and user specified padding tolerance. Since stripe size can overflow
-      // the default stripe size we should apply this correction to avoid
-      // writing portion of last stripe to next hdfs block.
-      double correction = overflow > 0 ? (double) overflow
-          / (double) adjustedStripeSize : 0.0;
-
-      // correction should not be greater than user specified padding
-      // tolerance
-      correction = correction > paddingTolerance ? paddingTolerance
-          : correction;
-
-      // adjust next stripe size based on current stripe estimate correction
-      adjustedStripeSize = (long) ((1.0f - correction) * (availRatio * defaultStripeSize));
-    } else if (availRatio >= 1.0) {
-      adjustedStripeSize = defaultStripeSize;
-    }
-
-    if (availRatio < paddingTolerance && addBlockPadding) {
-      long padding = blockSize - (stripeStart % blockSize);
-      byte[] pad = new byte[(int) Math.min(HDFS_BUFFER_SIZE, padding)];
-      LOG.info(String.format("Padding ORC by %d bytes (<=  %.2f * %d)", 
-          padding, availRatio, defaultStripeSize));
-      stripeStart += padding;
-      while (padding > 0) {
-        int writeLen = (int) Math.min(padding, pad.length);
-        rawWriter.write(pad, 0, writeLen);
-        padding -= writeLen;
-      }
-      adjustedStripeSize = defaultStripeSize;
-    } else if (currentStripeSize < blockSize
-        && (stripeStart % blockSize) + currentStripeSize > blockSize) {
-      // even if you don't pad, reset the default stripe size when crossing a
-      // block boundary
-      adjustedStripeSize = defaultStripeSize;
-    }
-  }
-
-  /**
-   * An output receiver that writes the ByteBuffers to the output stream
-   * as they are received.
-   */
-  private class DirectStream implements OutStream.OutputReceiver {
-    private final FSDataOutputStream output;
-
-    DirectStream(FSDataOutputStream output) {
-      this.output = output;
-    }
-
-    @Override
-    public void output(ByteBuffer buffer) throws IOException {
-      output.write(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());
-    }
-  }
-
-  @Override
-  public long getPhysicalStripeSize() {
-    return adjustedStripeSize;
-  }
-
-  @Override
-  public boolean isCompressed() {
-    return codec != null;
-  }
-
-
-  public static CompressionCodec createCodec(CompressionKind kind) {
-    switch (kind) {
-      case NONE:
-        return null;
-      case ZLIB:
-        return new ZlibCodec();
-      case SNAPPY:
-        return new SnappyCodec();
-      case LZO:
-        try {
-          ClassLoader loader = Thread.currentThread().getContextClassLoader();
-          if (loader == null) {
-            loader = WriterImpl.class.getClassLoader();
-          }
-          @SuppressWarnings("unchecked")
-          Class<? extends CompressionCodec> lzo =
-              (Class<? extends CompressionCodec>)
-              loader.loadClass("org.apache.hadoop.hive.ql.io.orc.LzoCodec");
-          return lzo.newInstance();
-        } catch (ClassNotFoundException e) {
-          throw new IllegalArgumentException("LZO is not available.", e);
-        } catch (InstantiationException e) {
-          throw new IllegalArgumentException("Problem initializing LZO", e);
-        } catch (IllegalAccessException e) {
-          throw new IllegalArgumentException("Insufficient access to LZO", e);
-        }
-      default:
-        throw new IllegalArgumentException("Unknown compression codec: " +
-            kind);
-    }
-  }
-
-  private void writeStripeFooter(StripeFooter footer, long dataSize, long indexSize,
-      StripeInformation.Builder dirEntry) throws IOException {
-    footer.writeTo(protobufWriter);
-    protobufWriter.flush();
-    writer.flush();
-    dirEntry.setOffset(stripeStart);
-    dirEntry.setFooterLength(rawWriter.getPos() - stripeStart - dataSize - indexSize);
-  }
-
-  @VisibleForTesting
-  public static int getEstimatedBufferSize(long stripeSize, int numColumns,
-                                           int bs) {
-    // The worst case is that there are 2 big streams per a column and
-    // we want to guarantee that each stream gets ~10 buffers.
-    // This keeps buffers small enough that we don't get really small stripe
-    // sizes.
-    int estBufferSize = (int) (stripeSize / (20 * numColumns));
-    estBufferSize = getClosestBufferSize(estBufferSize);
-    return estBufferSize > bs ? bs : estBufferSize;
-  }
-
-  private static int getClosestBufferSize(int estBufferSize) {
-    final int kb4 = 4 * 1024;
-    final int kb8 = 8 * 1024;
-    final int kb16 = 16 * 1024;
-    final int kb32 = 32 * 1024;
-    final int kb64 = 64 * 1024;
-    final int kb128 = 128 * 1024;
-    final int kb256 = 256 * 1024;
-    if (estBufferSize <= kb4) {
-      return kb4;
-    } else if (estBufferSize > kb4 && estBufferSize <= kb8) {
-      return kb8;
-    } else if (estBufferSize > kb8 && estBufferSize <= kb16) {
-      return kb16;
-    } else if (estBufferSize > kb16 && estBufferSize <= kb32) {
-      return kb32;
-    } else if (estBufferSize > kb32 && estBufferSize <= kb64) {
-      return kb64;
-    } else if (estBufferSize > kb64 && estBufferSize <= kb128) {
-      return kb128;
-    } else {
-      return kb256;
-    }
-  }
-
-  @Override
-  public void writeFileMetadata(Metadata.Builder builder) throws IOException {
-    long startPosn = rawWriter.getPos();
-    Metadata metadata = builder.build();
-    metadata.writeTo(protobufWriter);
-    protobufWriter.flush();
-    writer.flush();
-    this.metadataLength = (int) (rawWriter.getPos() - startPosn);
-  }
-
-  @Override
-  public void writeFileFooter(Footer.Builder builder) throws IOException {
-    long bodyLength = rawWriter.getPos() - metadataLength;
-    builder.setContentLength(bodyLength);
-    builder.setHeaderLength(headerLength);
-    long startPosn = rawWriter.getPos();
-    Footer footer = builder.build();
-    footer.writeTo(protobufWriter);
-    protobufWriter.flush();
-    writer.flush();
-    this.footerLength = (int) (rawWriter.getPos() - startPosn);
-  }
-
-  @Override
-  public void writePostScript(PostScript.Builder builder) throws IOException {
-    builder.setCompression(writeCompressionKind(compress));
-    builder.setFooterLength(footerLength);
-    builder.setMetadataLength(metadataLength);
-    if (compress != CompressionKind.NONE) {
-      builder.setCompressionBlockSize(bufferSize);
-    }
-    PostScript ps = builder.build();
-    // need to write this uncompressed
-    long startPosn = rawWriter.getPos();
-    ps.writeTo(rawWriter);
-    long length = rawWriter.getPos() - startPosn;
-    if (length > 255) {
-      throw new IllegalArgumentException("PostScript too large at " + length);
-    }
-    rawWriter.writeByte((int)length);
-  }
-
-  @Override
-  public void close() throws IOException {
-    rawWriter.close();
-  }
-
-  private OrcProto.CompressionKind writeCompressionKind(CompressionKind kind) {
-    switch (kind) {
-      case NONE: return OrcProto.CompressionKind.NONE;
-      case ZLIB: return OrcProto.CompressionKind.ZLIB;
-      case SNAPPY: return OrcProto.CompressionKind.SNAPPY;
-      case LZO: return OrcProto.CompressionKind.LZO;
-      default:
-        throw new IllegalArgumentException("Unknown compression " + kind);
-    }
-  }
-
-  @Override
-  public void flush() throws IOException {
-    rawWriter.hflush();
-    // TODO: reset?
-  }
-
-  @Override
-  public long getRawWriterPosition() throws IOException {
-    return rawWriter.getPos();
-  }
-
-  @Override
-  public void appendRawStripe(byte[] stripe, int offset, int length,
-      StripeInformation.Builder dirEntry) throws IOException {
-    long start = rawWriter.getPos();
-    long availBlockSpace = blockSize - (start % blockSize);
-
-    // see if stripe can fit in the current hdfs block, else pad the remaining
-    // space in the block
-    if (length < blockSize && length > availBlockSpace &&
-        addBlockPadding) {
-      byte[] pad = new byte[(int) Math.min(HDFS_BUFFER_SIZE, availBlockSpace)];
-      LOG.info(String.format("Padding ORC by %d bytes while merging..",
-          availBlockSpace));
-      start += availBlockSpace;
-      while (availBlockSpace > 0) {
-        int writeLen = (int) Math.min(availBlockSpace, pad.length);
-        rawWriter.write(pad, 0, writeLen);
-        availBlockSpace -= writeLen;
-      }
-    }
-
-    rawWriter.write(stripe);
-    dirEntry.setOffset(start);
-  }
-
-
-  /**
-   * This class is used to hold the contents of streams as they are buffered.
-   * The TreeWriters write to the outStream and the codec compresses the
-   * data as buffers fill up and stores them in the output list. When the
-   * stripe is being written, the whole stream is written to the file.
-   */
-  private class BufferedStream implements OutStream.OutputReceiver {
-    private final OutStream outStream;
-    private final List<ByteBuffer> output = new ArrayList<ByteBuffer>();
-
-    BufferedStream(String name, int bufferSize,
-                   CompressionCodec codec) throws IOException {
-      outStream = new OutStream(name, bufferSize, codec, this);
-    }
-
-    /**
-     * Receive a buffer from the compression codec.
-     * @param buffer the buffer to save
-     */
-    @Override
-    public void output(ByteBuffer buffer) {
-      output.add(buffer);
-    }
-
-    /**
-     * @return the number of bytes in buffers that are allocated to this stream.
-     */
-    public long getBufferSize() {
-      long result = 0;
-      for (ByteBuffer buf: output) {
-        result += buf.capacity();
-      }
-      return outStream.getBufferSize() + result;
-    }
-
-    /**
-     * Write any saved buffers to the OutputStream if needed, and clears all the buffers.
-     */
-    public void spillToDiskAndClear() throws IOException {
-      if (!outStream.isSuppressed()) {
-        for (ByteBuffer buffer: output) {
-          rawWriter.write(buffer.array(), buffer.arrayOffset() + buffer.position(),
-            buffer.remaining());
-        }
-      }
-      outStream.clear();
-      output.clear();
-    }
-
-    /**
-     * @return The number of bytes that will be written to the output. Assumes the stream writing
-     *         into this receiver has already been flushed.
-     */
-    public long getOutputSize() {
-      long result = 0;
-      for (ByteBuffer buffer: output) {
-        result += buffer.remaining();
-      }
-      return result;
-    }
-
-    @Override
-    public String toString() {
-      return outStream.toString();
-    }
-  }
-
-  @Override
-  public OutStream getOrCreatePhysicalStream(StreamName name) throws IOException {
-    BufferedStream result = streams.get(name);
-    if (result == null) {
-      EnumSet<Modifier> modifiers = createCompressionModifiers(name.getKind());
-      result = new BufferedStream(name.toString(), bufferSize,
-          codec == null ? null : codec.modify(modifiers));
-      streams.put(name, result);
-    }
-    return result.outStream;
-  }
-
-  private EnumSet<Modifier> createCompressionModifiers(Kind kind) {
-    switch (kind) {
-      case BLOOM_FILTER:
-      case DATA:
-      case DICTIONARY_DATA:
-        return EnumSet.of(Modifier.TEXT,
-            compressionStrategy == CompressionStrategy.SPEED ? Modifier.FAST : Modifier.DEFAULT);
-      case LENGTH:
-      case DICTIONARY_COUNT:
-      case PRESENT:
-      case ROW_INDEX:
-      case SECONDARY:
-        // easily compressed using the fastest modes
-        return EnumSet.of(CompressionCodec.Modifier.FASTEST, CompressionCodec.Modifier.BINARY);
-      default:
-        LOG.warn("Missing ORC compression modifiers for " + kind);
-        return null;
-    }
-  }
-
-  @Override
-  public void finalizeStripe(StripeFooter.Builder footerBuilder,
-      StripeInformation.Builder dirEntry) throws IOException {
-    long indexSize = 0;
-    long dataSize = 0;
-    for (Map.Entry<StreamName, BufferedStream> pair: streams.entrySet()) {
-      BufferedStream receiver = pair.getValue();
-      OutStream outStream = receiver.outStream;
-      if (!outStream.isSuppressed()) {
-        outStream.flush();
-        long streamSize = receiver.getOutputSize();
-        StreamName name = pair.getKey();
-        footerBuilder.addStreams(OrcProto.Stream.newBuilder().setColumn(name.getColumn())
-            .setKind(name.getKind()).setLength(streamSize));
-        if (StreamName.Area.INDEX == name.getArea()) {
-          indexSize += streamSize;
-        } else {
-          dataSize += streamSize;
-        }
-      }
-    }
-    dirEntry.setIndexLength(indexSize).setDataLength(dataSize);
-
-    OrcProto.StripeFooter footer = footerBuilder.build();
-    // Do we need to pad the file so the stripe doesn't straddle a block boundary?
-    padStripe(indexSize, dataSize, footer.getSerializedSize());
-
-    // write out the data streams
-    for (Map.Entry<StreamName, BufferedStream> pair : streams.entrySet()) {
-      pair.getValue().spillToDiskAndClear();
-    }
-    // Write out the footer.
-    writeStripeFooter(footer, dataSize, indexSize, dirEntry);
-  }
-
-  @Override
-  public long estimateMemory() {
-    long result = 0;
-    for (BufferedStream stream: streams.values()) {
-      result += stream.getBufferSize();
-    }
-    return result;
-  }
-
-  @Override
-  public void writeIndexStream(StreamName name, Builder rowIndex) throws IOException {
-    OutStream stream = getOrCreatePhysicalStream(name);
-    rowIndex.build().writeTo(stream);
-    stream.flush();
-  }
-
-  @Override
-  public void writeBloomFilterStream(
-      StreamName name, BloomFilterIndex.Builder bloomFilterIndex) throws IOException {
-    OutStream stream = getOrCreatePhysicalStream(name);
-    bloomFilterIndex.build().writeTo(stream);
-    stream.flush();
-  }
-
-  @VisibleForTesting
-  public OutputStream getStream() throws IOException {
-    initialize();
-    return rawWriter;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/PhysicalWriter.java b/orc/src/java/org/apache/orc/impl/PhysicalWriter.java
deleted file mode 100644
index 5ba1b9b7b1..0000000000
--- a/orc/src/java/org/apache/orc/impl/PhysicalWriter.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.io.IOException;
-
-import org.apache.orc.OrcProto.BloomFilterIndex;
-import org.apache.orc.OrcProto.Footer;
-import org.apache.orc.OrcProto.Metadata;
-import org.apache.orc.OrcProto.PostScript;
-import org.apache.orc.OrcProto.RowIndex;
-import org.apache.orc.OrcProto.StripeFooter;
-import org.apache.orc.OrcProto.StripeInformation;
-
-public interface PhysicalWriter {
-
-  /**
-   * Creates all the streams/connections/etc. necessary to write.
-   */
-  void initialize() throws IOException;
-
-  /**
-   * Writes out the file metadata.
-   * @param builder Metadata builder to finalize and write.
-   */
-  void writeFileMetadata(Metadata.Builder builder) throws IOException;
-
-  /**
-   * Writes out the file footer.
-   * @param builder Footer builder to finalize and write.
-   */
-  void writeFileFooter(Footer.Builder builder) throws IOException;
-
-  /**
-   * Writes out the postscript (including the size byte if needed).
-   * @param builder Postscript builder to finalize and write.
-   */
-  void writePostScript(PostScript.Builder builder) throws IOException;
-
-  /**
-   * Creates physical stream to write data to.
-   * @param name Stream name.
-   * @return The output stream.
-   */
-  OutStream getOrCreatePhysicalStream(StreamName name) throws IOException;
-
-  /**
-   * Flushes the data in all the streams, spills them to disk, write out stripe footer.
-   * @param footer Stripe footer to be updated with relevant data and written out.
-   * @param dirEntry File metadata entry for the stripe, to be updated with relevant data.
-   */
-  void finalizeStripe(StripeFooter.Builder footer,
-      StripeInformation.Builder dirEntry) throws IOException;
-
-  /**
-   * Writes out the index for the stripe column.
-   * @param streamName Stream name.
-   * @param rowIndex Row index entries to write.
-   */
-  void writeIndexStream(StreamName name, RowIndex.Builder rowIndex) throws IOException;
-
-  /**
-   * Writes out the index for the stripe column.
-   * @param streamName Stream name.
-   * @param bloomFilterIndex Bloom filter index to write.
-   */
-  void writeBloomFilterStream(StreamName streamName,
-      BloomFilterIndex.Builder bloomFilterIndex) throws IOException;
-
-  /**
-   * Closes the writer.
-   */
-  void close() throws IOException;
-
-  /**
-   * Force-flushes the writer.
-   */
-  void flush() throws IOException;
-
-  /**
-   * @return the physical writer position (e.g. for updater).
-   */
-  long getRawWriterPosition() throws IOException;
-
-  /** @return physical stripe size, taking padding into account. */
-  long getPhysicalStripeSize();
-
-  /** @return whether the writer is compressed. */
-  boolean isCompressed();
-
-  /**
-   * Appends raw stripe data (e.g. for file merger).
-   * @param stripe Stripe data buffer.
-   * @param offset Stripe data buffer offset.
-   * @param length Stripe data buffer length.
-   * @param dirEntry File metadata entry for the stripe, to be updated with relevant data.
-   * @throws IOException
-   */
-  void appendRawStripe(byte[] stripe, int offset, int length,
-      StripeInformation.Builder dirEntry) throws IOException;
-
-  /**
-   * @return the estimated memory usage for the stripe.
-   */
-  long estimateMemory();
-}
diff --git a/orc/src/java/org/apache/orc/impl/PositionProvider.java b/orc/src/java/org/apache/orc/impl/PositionProvider.java
deleted file mode 100644
index 47cf4811c0..0000000000
--- a/orc/src/java/org/apache/orc/impl/PositionProvider.java
+++ /dev/null
@@ -1,26 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-/**
- * An interface used for seeking to a row index.
- */
-public interface PositionProvider {
-  long getNext();
-}
diff --git a/orc/src/java/org/apache/orc/impl/PositionRecorder.java b/orc/src/java/org/apache/orc/impl/PositionRecorder.java
deleted file mode 100644
index 1fff760b12..0000000000
--- a/orc/src/java/org/apache/orc/impl/PositionRecorder.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-/**
- * An interface for recording positions in a stream.
- */
-public interface PositionRecorder {
-  void addPosition(long offset);
-}
diff --git a/orc/src/java/org/apache/orc/impl/PositionedOutputStream.java b/orc/src/java/org/apache/orc/impl/PositionedOutputStream.java
deleted file mode 100644
index d412939dba..0000000000
--- a/orc/src/java/org/apache/orc/impl/PositionedOutputStream.java
+++ /dev/null
@@ -1,39 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.io.OutputStream;
-
-public abstract class PositionedOutputStream extends OutputStream {
-
-  /**
-   * Record the current position to the recorder.
-   * @param recorder the object that receives the position
-   * @throws IOException
-   */
-  public abstract void getPosition(PositionRecorder recorder
-                                   ) throws IOException;
-
-  /**
-   * Get the memory size currently allocated as buffer associated with this
-   * stream.
-   * @return the number of bytes used by buffers.
-   */
-  public abstract long getBufferSize();
-}
diff --git a/orc/src/java/org/apache/orc/impl/ReaderImpl.java b/orc/src/java/org/apache/orc/impl/ReaderImpl.java
deleted file mode 100644
index 70fa628a52..0000000000
--- a/orc/src/java/org/apache/orc/impl/ReaderImpl.java
+++ /dev/null
@@ -1,764 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.FileMetadata;
-import org.apache.orc.OrcFile;
-import org.apache.orc.OrcUtils;
-import org.apache.orc.Reader;
-import org.apache.orc.RecordReader;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.ColumnStatistics;
-import org.apache.orc.CompressionCodec;
-import org.apache.orc.FileFormatException;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.StripeStatistics;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.io.DiskRange;
-import org.apache.hadoop.hive.ql.util.JavaDataModel;
-import org.apache.hadoop.io.Text;
-import org.apache.orc.OrcProto;
-
-import com.google.common.collect.Lists;
-import com.google.protobuf.CodedInputStream;
-
-public class ReaderImpl implements Reader {
-
-  private static final Logger LOG = LoggerFactory.getLogger(ReaderImpl.class);
-
-  private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;
-
-  protected final FileSystem fileSystem;
-  private final long maxLength;
-  protected final Path path;
-  protected final org.apache.orc.CompressionKind compressionKind;
-  protected CompressionCodec codec;
-  protected int bufferSize;
-  protected OrcProto.Metadata metadata;
-  private List<OrcProto.StripeStatistics> stripeStats;
-  private final int metadataSize;
-  protected final List<OrcProto.Type> types;
-  private TypeDescription schema;
-  private final List<OrcProto.UserMetadataItem> userMetadata;
-  private final List<OrcProto.ColumnStatistics> fileStats;
-  private final List<StripeInformation> stripes;
-  protected final int rowIndexStride;
-  private final long contentLength, numberOfRows;
-
-  private long deserializedSize = -1;
-  protected final Configuration conf;
-  private final List<Integer> versionList;
-  private final OrcFile.WriterVersion writerVersion;
-
-  protected OrcTail tail;
-
-  public static class StripeInformationImpl
-      implements StripeInformation {
-    private final OrcProto.StripeInformation stripe;
-
-    public StripeInformationImpl(OrcProto.StripeInformation stripe) {
-      this.stripe = stripe;
-    }
-
-    @Override
-    public long getOffset() {
-      return stripe.getOffset();
-    }
-
-    @Override
-    public long getLength() {
-      return stripe.getDataLength() + getIndexLength() + getFooterLength();
-    }
-
-    @Override
-    public long getDataLength() {
-      return stripe.getDataLength();
-    }
-
-    @Override
-    public long getFooterLength() {
-      return stripe.getFooterLength();
-    }
-
-    @Override
-    public long getIndexLength() {
-      return stripe.getIndexLength();
-    }
-
-    @Override
-    public long getNumberOfRows() {
-      return stripe.getNumberOfRows();
-    }
-
-    @Override
-    public String toString() {
-      return "offset: " + getOffset() + " data: " + getDataLength() +
-        " rows: " + getNumberOfRows() + " tail: " + getFooterLength() +
-        " index: " + getIndexLength();
-    }
-  }
-
-  @Override
-  public long getNumberOfRows() {
-    return numberOfRows;
-  }
-
-  @Override
-  public List<String> getMetadataKeys() {
-    List<String> result = new ArrayList<String>();
-    for(OrcProto.UserMetadataItem item: userMetadata) {
-      result.add(item.getName());
-    }
-    return result;
-  }
-
-  @Override
-  public ByteBuffer getMetadataValue(String key) {
-    for(OrcProto.UserMetadataItem item: userMetadata) {
-      if (item.hasName() && item.getName().equals(key)) {
-        return item.getValue().asReadOnlyByteBuffer();
-      }
-    }
-    throw new IllegalArgumentException("Can't find user metadata " + key);
-  }
-
-  public boolean hasMetadataValue(String key) {
-    for(OrcProto.UserMetadataItem item: userMetadata) {
-      if (item.hasName() && item.getName().equals(key)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  @Override
-  public org.apache.orc.CompressionKind getCompressionKind() {
-    return compressionKind;
-  }
-
-  @Override
-  public int getCompressionSize() {
-    return bufferSize;
-  }
-
-  @Override
-  public List<StripeInformation> getStripes() {
-    return stripes;
-  }
-
-  @Override
-  public long getContentLength() {
-    return contentLength;
-  }
-
-  @Override
-  public List<OrcProto.Type> getTypes() {
-    return types;
-  }
-
-  @Override
-  public OrcFile.Version getFileVersion() {
-    for (OrcFile.Version version: OrcFile.Version.values()) {
-      if ((versionList != null && !versionList.isEmpty()) &&
-          version.getMajor() == versionList.get(0) &&
-          version.getMinor() == versionList.get(1)) {
-        return version;
-      }
-    }
-    return OrcFile.Version.V_0_11;
-  }
-
-  @Override
-  public OrcFile.WriterVersion getWriterVersion() {
-    return writerVersion;
-  }
-
-  @Override
-  public OrcProto.FileTail getFileTail() {
-    return tail.getFileTail();
-  }
-
-  @Override
-  public int getRowIndexStride() {
-    return rowIndexStride;
-  }
-
-  @Override
-  public ColumnStatistics[] getStatistics() {
-    ColumnStatistics[] result = new ColumnStatistics[types.size()];
-    for(int i=0; i < result.length; ++i) {
-      result[i] = ColumnStatisticsImpl.deserialize(fileStats.get(i));
-    }
-    return result;
-  }
-
-  @Override
-  public TypeDescription getSchema() {
-    return schema;
-  }
-
-  /**
-   * Ensure this is an ORC file to prevent users from trying to read text
-   * files or RC files as ORC files.
-   * @param in the file being read
-   * @param path the filename for error messages
-   * @param psLen the postscript length
-   * @param buffer the tail of the file
-   * @throws IOException
-   */
-  protected static void ensureOrcFooter(FSDataInputStream in,
-                                        Path path,
-                                        int psLen,
-                                        ByteBuffer buffer) throws IOException {
-    int magicLength = OrcFile.MAGIC.length();
-    int fullLength = magicLength + 1;
-    if (psLen < fullLength || buffer.remaining() < fullLength) {
-      throw new FileFormatException("Malformed ORC file " + path +
-          ". Invalid postscript length " + psLen);
-    }
-    int offset = buffer.arrayOffset() + buffer.position() + buffer.limit() - fullLength;
-    byte[] array = buffer.array();
-    // now look for the magic string at the end of the postscript.
-    if (!Text.decode(array, offset, magicLength).equals(OrcFile.MAGIC)) {
-      // If it isn't there, this may be the 0.11.0 version of ORC.
-      // Read the first 3 bytes of the file to check for the header
-      byte[] header = new byte[magicLength];
-      in.readFully(0, header, 0, magicLength);
-      // if it isn't there, this isn't an ORC file
-      if (!Text.decode(header, 0 , magicLength).equals(OrcFile.MAGIC)) {
-        throw new FileFormatException("Malformed ORC file " + path +
-            ". Invalid postscript.");
-      }
-    }
-  }
-
-  /**
-   * Ensure this is an ORC file to prevent users from trying to read text
-   * files or RC files as ORC files.
-   * @param psLen the postscript length
-   * @param buffer the tail of the file
-   * @throws IOException
-   */
-  protected static void ensureOrcFooter(ByteBuffer buffer, int psLen) throws IOException {
-    int magicLength = OrcFile.MAGIC.length();
-    int fullLength = magicLength + 1;
-    if (psLen < fullLength || buffer.remaining() < fullLength) {
-      throw new FileFormatException("Malformed ORC file. Invalid postscript length " + psLen);
-    }
-
-    int offset = buffer.arrayOffset() + buffer.position() + buffer.limit() - fullLength;
-    byte[] array = buffer.array();
-    // now look for the magic string at the end of the postscript.
-    if (!Text.decode(array, offset, magicLength).equals(OrcFile.MAGIC)) {
-      // if it isn't there, this may be 0.11.0 version of the ORC file.
-      // Read the first 3 bytes from the buffer to check for the header
-      if (!Text.decode(buffer.array(), 0, magicLength).equals(OrcFile.MAGIC)) {
-        throw new FileFormatException("Malformed ORC file. Invalid postscript length " + psLen);
-      }
-    }
-  }
-
-  /**
-   * Build a version string out of an array.
-   * @param version the version number as a list
-   * @return the human readable form of the version string
-   */
-  private static String versionString(List<Integer> version) {
-    StringBuilder buffer = new StringBuilder();
-    for(int i=0; i < version.size(); ++i) {
-      if (i != 0) {
-        buffer.append('.');
-      }
-      buffer.append(version.get(i));
-    }
-    return buffer.toString();
-  }
-
-  /**
-   * Check to see if this ORC file is from a future version and if so,
-   * warn the user that we may not be able to read all of the column encodings.
-   * @param log the logger to write any error message to
-   * @param path the data source path for error messages
-   * @param version the version of hive that wrote the file.
-   */
-  protected static void checkOrcVersion(Logger log, Path path,
-                                        List<Integer> version) {
-    if (version.size() >= 1) {
-      int major = version.get(0);
-      int minor = 0;
-      if (version.size() >= 2) {
-        minor = version.get(1);
-      }
-      if (major > OrcFile.Version.CURRENT.getMajor() ||
-          (major == OrcFile.Version.CURRENT.getMajor() &&
-           minor > OrcFile.Version.CURRENT.getMinor())) {
-        log.warn(path + " was written by a future Hive version " +
-                 versionString(version) +
-                 ". This file may not be readable by this version of Hive.");
-      }
-    }
-  }
-
-  /**
-  * Constructor that let's the user specify additional options.
-   * @param path pathname for file
-   * @param options options for reading
-   * @throws IOException
-   */
-  public ReaderImpl(Path path, OrcFile.ReaderOptions options) throws IOException {
-    FileSystem fs = options.getFilesystem();
-    if (fs == null) {
-      fs = path.getFileSystem(options.getConfiguration());
-    }
-    this.fileSystem = fs;
-    this.path = path;
-    this.conf = options.getConfiguration();
-    this.maxLength = options.getMaxLength();
-    FileMetadata fileMetadata = options.getFileMetadata();
-    if (fileMetadata != null) {
-      this.compressionKind = fileMetadata.getCompressionKind();
-      this.bufferSize = fileMetadata.getCompressionBufferSize();
-      this.codec = PhysicalFsWriter.createCodec(compressionKind);
-      this.metadataSize = fileMetadata.getMetadataSize();
-      this.stripeStats = fileMetadata.getStripeStats();
-      this.versionList = fileMetadata.getVersionList();
-      this.writerVersion =
-          OrcFile.WriterVersion.from(fileMetadata.getWriterVersionNum());
-      this.types = fileMetadata.getTypes();
-      this.rowIndexStride = fileMetadata.getRowIndexStride();
-      this.contentLength = fileMetadata.getContentLength();
-      this.numberOfRows = fileMetadata.getNumberOfRows();
-      this.fileStats = fileMetadata.getFileStats();
-      this.stripes = fileMetadata.getStripes();
-      this.userMetadata = null; // not cached and not needed here
-    } else {
-      OrcTail orcTail = options.getOrcTail();
-      if (orcTail == null) {
-        tail = extractFileTail(fs, path, options.getMaxLength());
-        options.orcTail(tail);
-      } else {
-        tail = orcTail;
-      }
-      this.compressionKind = tail.getCompressionKind();
-      this.codec = tail.getCompressionCodec();
-      this.bufferSize = tail.getCompressionBufferSize();
-      this.metadataSize = tail.getMetadataSize();
-      this.versionList = tail.getPostScript().getVersionList();
-      this.types = tail.getFooter().getTypesList();
-      this.rowIndexStride = tail.getFooter().getRowIndexStride();
-      this.contentLength = tail.getFooter().getContentLength();
-      this.numberOfRows = tail.getFooter().getNumberOfRows();
-      this.userMetadata = tail.getFooter().getMetadataList();
-      this.fileStats = tail.getFooter().getStatisticsList();
-      this.writerVersion = tail.getWriterVersion();
-      this.stripes = tail.getStripes();
-      this.stripeStats = tail.getStripeStatisticsProto();
-    }
-    this.schema = OrcUtils.convertTypeFromProtobuf(this.types, 0);
-  }
-
-  /**
-   * Get the WriterVersion based on the ORC file postscript.
-   * @param writerVersion the integer writer version
-   * @return the version of the software that produced the file
-   */
-  public static OrcFile.WriterVersion getWriterVersion(int writerVersion) {
-    for(OrcFile.WriterVersion version: OrcFile.WriterVersion.values()) {
-      if (version.getId() == writerVersion) {
-        return version;
-      }
-    }
-    return OrcFile.WriterVersion.FUTURE;
-  }
-
-  private static OrcProto.Footer extractFooter(ByteBuffer bb, int footerAbsPos,
-      int footerSize, CompressionCodec codec, int bufferSize) throws IOException {
-    bb.position(footerAbsPos);
-    bb.limit(footerAbsPos + footerSize);
-    return OrcProto.Footer.parseFrom(InStream.createCodedInputStream("footer",
-        Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)), footerSize, codec, bufferSize));
-  }
-
-  public static OrcProto.Metadata extractMetadata(ByteBuffer bb, int metadataAbsPos,
-      int metadataSize, CompressionCodec codec, int bufferSize) throws IOException {
-    bb.position(metadataAbsPos);
-    bb.limit(metadataAbsPos + metadataSize);
-    return OrcProto.Metadata.parseFrom(InStream.createCodedInputStream("metadata",
-        Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)), metadataSize, codec, bufferSize));
-  }
-
-  private static OrcProto.PostScript extractPostScript(ByteBuffer bb, Path path,
-      int psLen, int psAbsOffset) throws IOException {
-    // TODO: when PB is upgraded to 2.6, newInstance(ByteBuffer) method should be used here.
-    assert bb.hasArray();
-    CodedInputStream in = CodedInputStream.newInstance(
-        bb.array(), bb.arrayOffset() + psAbsOffset, psLen);
-    OrcProto.PostScript ps = OrcProto.PostScript.parseFrom(in);
-    checkOrcVersion(LOG, path, ps.getVersionList());
-
-    // Check compression codec.
-    switch (ps.getCompression()) {
-      case NONE:
-        break;
-      case ZLIB:
-        break;
-      case SNAPPY:
-        break;
-      case LZO:
-        break;
-      default:
-        throw new IllegalArgumentException("Unknown compression");
-    }
-    return ps;
-  }
-
-  public static OrcTail extractFileTail(ByteBuffer buffer)
-      throws IOException {
-    return extractFileTail(buffer, -1, -1);
-  }
-
-  public static OrcTail extractFileTail(ByteBuffer buffer, long fileLength, long modificationTime)
-      throws IOException {
-    int readSize = buffer.limit();
-    int psLen = buffer.get(readSize - 1) & 0xff;
-    int psOffset = readSize - 1 - psLen;
-    ensureOrcFooter(buffer, psLen);
-    byte[] psBuffer = new byte[psLen];
-    System.arraycopy(buffer.array(), psOffset, psBuffer, 0, psLen);
-    OrcProto.PostScript ps = OrcProto.PostScript.parseFrom(psBuffer);
-    int footerSize = (int) ps.getFooterLength();
-    CompressionCodec codec = PhysicalFsWriter
-        .createCodec(CompressionKind.valueOf(ps.getCompression().name()));
-    OrcProto.Footer footer = extractFooter(buffer,
-        (int) (buffer.position() + ps.getMetadataLength()),
-        footerSize, codec, (int) ps.getCompressionBlockSize());
-    OrcProto.FileTail.Builder fileTailBuilder = OrcProto.FileTail.newBuilder()
-        .setPostscriptLength(psLen)
-        .setPostscript(ps)
-        .setFooter(footer)
-        .setFileLength(fileLength);
-    // clear does not clear the contents but sets position to 0 and limit = capacity
-    buffer.clear();
-    return new OrcTail(fileTailBuilder.build(), buffer.slice(), modificationTime);
-  }
-
-  protected OrcTail extractFileTail(FileSystem fs, Path path,
-      long maxFileLength) throws IOException {
-    FSDataInputStream file = fs.open(path);
-    ByteBuffer buffer;
-    OrcProto.PostScript ps;
-    OrcProto.FileTail.Builder fileTailBuilder = OrcProto.FileTail.newBuilder();
-    long modificationTime;
-    try {
-      // figure out the size of the file using the option or filesystem
-      long size;
-      if (maxFileLength == Long.MAX_VALUE) {
-        FileStatus fileStatus = fs.getFileStatus(path);
-        size = fileStatus.getLen();
-        modificationTime = fileStatus.getModificationTime();
-      } else {
-        size = maxFileLength;
-        modificationTime = -1;
-      }
-      fileTailBuilder.setFileLength(size);
-
-      //read last bytes into buffer to get PostScript
-      int readSize = (int) Math.min(size, DIRECTORY_SIZE_GUESS);
-      buffer = ByteBuffer.allocate(readSize);
-      assert buffer.position() == 0;
-      file.readFully((size - readSize),
-          buffer.array(), buffer.arrayOffset(), readSize);
-      buffer.position(0);
-
-      //read the PostScript
-      //get length of PostScript
-      int psLen = buffer.get(readSize - 1) & 0xff;
-      ensureOrcFooter(file, path, psLen, buffer);
-      int psOffset = readSize - 1 - psLen;
-      ps = extractPostScript(buffer, path, psLen, psOffset);
-      bufferSize = (int) ps.getCompressionBlockSize();
-      codec = PhysicalFsWriter.createCodec(CompressionKind.valueOf(ps.getCompression().name()));
-      fileTailBuilder.setPostscriptLength(psLen).setPostscript(ps);
-
-      int footerSize = (int) ps.getFooterLength();
-      int metadataSize = (int) ps.getMetadataLength();
-
-      //check if extra bytes need to be read
-      int extra = Math.max(0, psLen + 1 + footerSize + metadataSize - readSize);
-      int tailSize = 1 + psLen + footerSize + metadataSize;
-      if (extra > 0) {
-        //more bytes need to be read, seek back to the right place and read extra bytes
-        ByteBuffer extraBuf = ByteBuffer.allocate(extra + readSize);
-        file.readFully((size - readSize - extra), extraBuf.array(),
-            extraBuf.arrayOffset() + extraBuf.position(), extra);
-        extraBuf.position(extra);
-        //append with already read bytes
-        extraBuf.put(buffer);
-        buffer = extraBuf;
-        buffer.position(0);
-        buffer.limit(tailSize);
-        readSize += extra;
-        psOffset = readSize - 1 - psLen;
-      } else {
-        //footer is already in the bytes in buffer, just adjust position, length
-        buffer.position(psOffset - footerSize - metadataSize);
-        buffer.limit(buffer.position() + tailSize);
-      }
-
-      buffer.mark();
-      int footerOffset = psOffset - footerSize;
-      buffer.position(footerOffset);
-      ByteBuffer footerBuffer = buffer.slice();
-      buffer.reset();
-      OrcProto.Footer footer = extractFooter(footerBuffer, 0, footerSize,
-          codec, bufferSize);
-      fileTailBuilder.setFooter(footer);
-    } finally {
-      try {
-        file.close();
-      } catch (IOException ex) {
-        LOG.error("Failed to close the file after another error", ex);
-      }
-    }
-
-    ByteBuffer serializedTail = ByteBuffer.allocate(buffer.remaining());
-    serializedTail.put(buffer.slice());
-    serializedTail.rewind();
-    return new OrcTail(fileTailBuilder.build(), serializedTail, modificationTime);
-  }
-
-  @Override
-  public ByteBuffer getSerializedFileFooter() {
-    return tail.getSerializedTail();
-  }
-
-  @Override
-  public RecordReader rows() throws IOException {
-    return rows(new Options());
-  }
-
-  @Override
-  public RecordReader rows(Options options) throws IOException {
-    LOG.info("Reading ORC rows from " + path + " with " + options);
-    return new RecordReaderImpl(this, options);
-  }
-
-
-  @Override
-  public long getRawDataSize() {
-    // if the deserializedSize is not computed, then compute it, else
-    // return the already computed size. since we are reading from the footer
-    // we don't have to compute deserialized size repeatedly
-    if (deserializedSize == -1) {
-      List<Integer> indices = Lists.newArrayList();
-      for (int i = 0; i < fileStats.size(); ++i) {
-        indices.add(i);
-      }
-      deserializedSize = getRawDataSizeFromColIndices(indices);
-    }
-    return deserializedSize;
-  }
-
-  @Override
-  public long getRawDataSizeFromColIndices(List<Integer> colIndices) {
-    return getRawDataSizeFromColIndices(colIndices, types, fileStats);
-  }
-
-  public static long getRawDataSizeFromColIndices(
-      List<Integer> colIndices, List<OrcProto.Type> types,
-      List<OrcProto.ColumnStatistics> stats) {
-    long result = 0;
-    for (int colIdx : colIndices) {
-      result += getRawDataSizeOfColumn(colIdx, types, stats);
-    }
-    return result;
-  }
-
-  private static long getRawDataSizeOfColumn(int colIdx, List<OrcProto.Type> types,
-      List<OrcProto.ColumnStatistics> stats) {
-    OrcProto.ColumnStatistics colStat = stats.get(colIdx);
-    long numVals = colStat.getNumberOfValues();
-    OrcProto.Type type = types.get(colIdx);
-
-    switch (type.getKind()) {
-    case BINARY:
-      // old orc format doesn't support binary statistics. checking for binary
-      // statistics is not required as protocol buffers takes care of it.
-      return colStat.getBinaryStatistics().getSum();
-    case STRING:
-    case CHAR:
-    case VARCHAR:
-      // old orc format doesn't support sum for string statistics. checking for
-      // existence is not required as protocol buffers takes care of it.
-
-      // ORC strings are deserialized to java strings. so use java data model's
-      // string size
-      numVals = numVals == 0 ? 1 : numVals;
-      int avgStrLen = (int) (colStat.getStringStatistics().getSum() / numVals);
-      return numVals * JavaDataModel.get().lengthForStringOfLength(avgStrLen);
-    case TIMESTAMP:
-      return numVals * JavaDataModel.get().lengthOfTimestamp();
-    case DATE:
-      return numVals * JavaDataModel.get().lengthOfDate();
-    case DECIMAL:
-      return numVals * JavaDataModel.get().lengthOfDecimal();
-    case DOUBLE:
-    case LONG:
-      return numVals * JavaDataModel.get().primitive2();
-    case FLOAT:
-    case INT:
-    case SHORT:
-    case BOOLEAN:
-    case BYTE:
-      return numVals * JavaDataModel.get().primitive1();
-    default:
-      LOG.debug("Unknown primitive category: " + type.getKind());
-      break;
-    }
-
-    return 0;
-  }
-
-  @Override
-  public long getRawDataSizeOfColumns(List<String> colNames) {
-    List<Integer> colIndices = getColumnIndicesFromNames(colNames);
-    return getRawDataSizeFromColIndices(colIndices);
-  }
-
-  private List<Integer> getColumnIndicesFromNames(List<String> colNames) {
-    // top level struct
-    OrcProto.Type type = types.get(0);
-    List<Integer> colIndices = Lists.newArrayList();
-    List<String> fieldNames = type.getFieldNamesList();
-    int fieldIdx;
-    for (String colName : colNames) {
-      if (fieldNames.contains(colName)) {
-        fieldIdx = fieldNames.indexOf(colName);
-      } else {
-        String s = "Cannot find field for: " + colName + " in ";
-        for (String fn : fieldNames) {
-          s += fn + ", ";
-        }
-        LOG.warn(s);
-        continue;
-      }
-
-      // a single field may span multiple columns. find start and end column
-      // index for the requested field
-      int idxStart = type.getSubtypes(fieldIdx);
-
-      int idxEnd;
-
-      // if the specified is the last field and then end index will be last
-      // column index
-      if (fieldIdx + 1 > fieldNames.size() - 1) {
-        idxEnd = getLastIdx() + 1;
-      } else {
-        idxEnd = type.getSubtypes(fieldIdx + 1);
-      }
-
-      // if start index and end index are same then the field is a primitive
-      // field else complex field (like map, list, struct, union)
-      if (idxStart == idxEnd) {
-        // simple field
-        colIndices.add(idxStart);
-      } else {
-        // complex fields spans multiple columns
-        for (int i = idxStart; i < idxEnd; i++) {
-          colIndices.add(i);
-        }
-      }
-    }
-    return colIndices;
-  }
-
-  private int getLastIdx() {
-    Set<Integer> indices = new HashSet<>();
-    for (OrcProto.Type type : types) {
-      indices.addAll(type.getSubtypesList());
-    }
-    return Collections.max(indices);
-  }
-
-  @Override
-  public List<OrcProto.StripeStatistics> getOrcProtoStripeStatistics() {
-    return stripeStats;
-  }
-
-  @Override
-  public List<OrcProto.ColumnStatistics> getOrcProtoFileStatistics() {
-    return fileStats;
-  }
-
-  @Override
-  public List<StripeStatistics> getStripeStatistics() throws IOException {
-    if (stripeStats == null && metadata == null) {
-      metadata = extractMetadata(tail.getSerializedTail(), 0, metadataSize, codec, bufferSize);
-      stripeStats = metadata.getStripeStatsList();
-    }
-    List<StripeStatistics> result = new ArrayList<>();
-    for (OrcProto.StripeStatistics ss : stripeStats) {
-      result.add(new StripeStatistics(ss.getColStatsList()));
-    }
-    return result;
-  }
-
-  public List<OrcProto.UserMetadataItem> getOrcProtoUserMetadata() {
-    return userMetadata;
-  }
-
-  @Override
-  public List<Integer> getVersionList() {
-    return versionList;
-  }
-
-  @Override
-  public int getMetadataSize() {
-    return metadataSize;
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder buffer = new StringBuilder();
-    buffer.append("ORC Reader(");
-    buffer.append(path);
-    if (maxLength != -1) {
-      buffer.append(", ");
-      buffer.append(maxLength);
-    }
-    buffer.append(")");
-    return buffer.toString();
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RecordReaderImpl.java b/orc/src/java/org/apache/orc/impl/RecordReaderImpl.java
deleted file mode 100644
index 9433e546b8..0000000000
--- a/orc/src/java/org/apache/orc/impl/RecordReaderImpl.java
+++ /dev/null
@@ -1,1230 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.math.BigDecimal;
-import java.sql.Date;
-import java.sql.Timestamp;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.orc.BooleanColumnStatistics;
-import org.apache.orc.Reader;
-import org.apache.orc.RecordReader;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.ColumnStatistics;
-import org.apache.orc.CompressionCodec;
-import org.apache.orc.DataReader;
-import org.apache.orc.DateColumnStatistics;
-import org.apache.orc.DecimalColumnStatistics;
-import org.apache.orc.DoubleColumnStatistics;
-import org.apache.orc.IntegerColumnStatistics;
-import org.apache.orc.OrcConf;
-import org.apache.orc.StringColumnStatistics;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.TimestampColumnStatistics;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.io.DiskRange;
-import org.apache.hadoop.hive.common.io.DiskRangeList;
-import org.apache.hadoop.hive.common.io.DiskRangeList.CreateHelper;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.orc.BloomFilterIO;
-import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.hive.ql.util.TimestampUtils;
-import org.apache.hadoop.io.Text;
-import org.apache.orc.OrcProto;
-
-public class RecordReaderImpl implements RecordReader {
-  static final Logger LOG = LoggerFactory.getLogger(RecordReaderImpl.class);
-  private static final boolean isLogDebugEnabled = LOG.isDebugEnabled();
-  private static final Object UNKNOWN_VALUE = new Object();
-  protected final Path path;
-  private final long firstRow;
-  private final List<StripeInformation> stripes =
-      new ArrayList<StripeInformation>();
-  private OrcProto.StripeFooter stripeFooter;
-  private final long totalRowCount;
-  private final CompressionCodec codec;
-  protected final TypeDescription schema;
-  private final List<OrcProto.Type> types;
-  private final int bufferSize;
-  private final SchemaEvolution evolution;
-  // the file included columns indexed by the file's column ids.
-  private final boolean[] included;
-  private final long rowIndexStride;
-  private long rowInStripe = 0;
-  private int currentStripe = -1;
-  private long rowBaseInStripe = 0;
-  private long rowCountInStripe = 0;
-  private final Map<StreamName, InStream> streams =
-      new HashMap<StreamName, InStream>();
-  DiskRangeList bufferChunks = null;
-  private final TreeReaderFactory.TreeReader reader;
-  private final OrcProto.RowIndex[] indexes;
-  private final OrcProto.BloomFilterIndex[] bloomFilterIndices;
-  private final SargApplier sargApp;
-  // an array about which row groups aren't skipped
-  private boolean[] includedRowGroups = null;
-  private final DataReader dataReader;
-
-  /**
-   * Given a list of column names, find the given column and return the index.
-   *
-   * @param evolution the mapping from reader to file schema
-   * @param columnName  the column name to look for
-   * @return the file column id or -1 if the column wasn't found
-   */
-  static int findColumns(SchemaEvolution evolution,
-                         String columnName) {
-    TypeDescription readerSchema = evolution.getReaderBaseSchema();
-    List<String> fieldNames = readerSchema.getFieldNames();
-    List<TypeDescription> children = readerSchema.getChildren();
-    for (int i = 0; i < fieldNames.size(); ++i) {
-      if (columnName.equals(fieldNames.get(i))) {
-        TypeDescription result = evolution.getFileType(children.get(i).getId());
-        return result == null ? -1 : result.getId();
-      }
-    }
-    return -1;
-  }
-
-  /**
-   * Find the mapping from predicate leaves to columns.
-   * @param sargLeaves the search argument that we need to map
-   * @param evolution the mapping from reader to file schema
-   * @return an array mapping the sarg leaves to file column ids
-   */
-  public static int[] mapSargColumnsToOrcInternalColIdx(List<PredicateLeaf> sargLeaves,
-                             SchemaEvolution evolution) {
-    int[] result = new int[sargLeaves.size()];
-    Arrays.fill(result, -1);
-    for(int i=0; i < result.length; ++i) {
-      String colName = sargLeaves.get(i).getColumnName();
-      result[i] = findColumns(evolution, colName);
-    }
-    return result;
-  }
-
-  protected RecordReaderImpl(ReaderImpl fileReader,
-                             Reader.Options options) throws IOException {
-    boolean[] readerIncluded = options.getInclude();
-    if (options.getSchema() == null) {
-      if (LOG.isInfoEnabled()) {
-        LOG.info("Reader schema not provided -- using file schema " +
-            fileReader.getSchema());
-      }
-      evolution = new SchemaEvolution(fileReader.getSchema(), readerIncluded);
-    } else {
-
-      // Now that we are creating a record reader for a file, validate that the schema to read
-      // is compatible with the file schema.
-      //
-      evolution = new SchemaEvolution(fileReader.getSchema(),
-          options.getSchema(), readerIncluded);
-      if (LOG.isDebugEnabled() && evolution.hasConversion()) {
-        LOG.debug("ORC file " + fileReader.path.toString() +
-            " has data type conversion --\n" +
-            "reader schema: " + options.getSchema().toString() + "\n" +
-            "file schema:   " + fileReader.getSchema());
-      }
-    }
-    this.schema = evolution.getReaderSchema();
-    this.path = fileReader.path;
-    this.codec = fileReader.codec;
-    this.types = fileReader.types;
-    this.bufferSize = fileReader.bufferSize;
-    this.rowIndexStride = fileReader.rowIndexStride;
-    SearchArgument sarg = options.getSearchArgument();
-    if (sarg != null && rowIndexStride != 0) {
-      sargApp = new SargApplier(sarg, options.getColumnNames(), rowIndexStride,
-          evolution);
-    } else {
-      sargApp = null;
-    }
-    long rows = 0;
-    long skippedRows = 0;
-    long offset = options.getOffset();
-    long maxOffset = options.getMaxOffset();
-    for(StripeInformation stripe: fileReader.getStripes()) {
-      long stripeStart = stripe.getOffset();
-      if (offset > stripeStart) {
-        skippedRows += stripe.getNumberOfRows();
-      } else if (stripeStart < maxOffset) {
-        this.stripes.add(stripe);
-        rows += stripe.getNumberOfRows();
-      }
-    }
-
-    Boolean zeroCopy = options.getUseZeroCopy();
-    if (zeroCopy == null) {
-      zeroCopy = OrcConf.USE_ZEROCOPY.getBoolean(fileReader.conf);
-    }
-    if (options.getDataReader() != null) {
-      this.dataReader = options.getDataReader();
-    } else {
-      this.dataReader = RecordReaderUtils.createDefaultDataReader(
-          DataReaderProperties.builder()
-              .withBufferSize(bufferSize)
-              .withCompression(fileReader.compressionKind)
-              .withFileSystem(fileReader.fileSystem)
-              .withPath(fileReader.path)
-              .withTypeCount(types.size())
-              .withZeroCopy(zeroCopy)
-              .build());
-    }
-    this.dataReader.open();
-
-    firstRow = skippedRows;
-    totalRowCount = rows;
-    Boolean skipCorrupt = options.getSkipCorruptRecords();
-    if (skipCorrupt == null) {
-      skipCorrupt = OrcConf.SKIP_CORRUPT_DATA.getBoolean(fileReader.conf);
-    }
-
-    reader = TreeReaderFactory.createTreeReader(evolution.getReaderSchema(),
-        evolution, readerIncluded, skipCorrupt);
-    indexes = new OrcProto.RowIndex[types.size()];
-    bloomFilterIndices = new OrcProto.BloomFilterIndex[types.size()];
-    this.included = evolution.getFileIncluded();
-    advanceToNextRow(reader, 0L, true);
-  }
-
-  public static final class PositionProviderImpl implements PositionProvider {
-    private final OrcProto.RowIndexEntry entry;
-    private int index;
-
-    public PositionProviderImpl(OrcProto.RowIndexEntry entry) {
-      this(entry, 0);
-    }
-
-    public PositionProviderImpl(OrcProto.RowIndexEntry entry, int startPos) {
-      this.entry = entry;
-      this.index = startPos;
-    }
-
-    @Override
-    public long getNext() {
-      return entry.getPositions(index++);
-    }
-
-    @Override
-    public String toString() {
-      return "{" + entry.getPositionsList() + "; " + index + "}";
-    }
-  }
-
-  public OrcProto.StripeFooter readStripeFooter(StripeInformation stripe
-                                                ) throws IOException {
-    return dataReader.readStripeFooter(stripe);
-  }
-
-  enum Location {
-    BEFORE, MIN, MIDDLE, MAX, AFTER
-  }
-
-  /**
-   * Given a point and min and max, determine if the point is before, at the
-   * min, in the middle, at the max, or after the range.
-   * @param point the point to test
-   * @param min the minimum point
-   * @param max the maximum point
-   * @param <T> the type of the comparision
-   * @return the location of the point
-   */
-  static <T> Location compareToRange(Comparable<T> point, T min, T max) {
-    int minCompare = point.compareTo(min);
-    if (minCompare < 0) {
-      return Location.BEFORE;
-    } else if (minCompare == 0) {
-      return Location.MIN;
-    }
-    int maxCompare = point.compareTo(max);
-    if (maxCompare > 0) {
-      return Location.AFTER;
-    } else if (maxCompare == 0) {
-      return Location.MAX;
-    }
-    return Location.MIDDLE;
-  }
-
-  /**
-   * Get the maximum value out of an index entry.
-   * @param index
-   *          the index entry
-   * @return the object for the maximum value or null if there isn't one
-   */
-  static Object getMax(ColumnStatistics index) {
-    if (index instanceof IntegerColumnStatistics) {
-      return ((IntegerColumnStatistics) index).getMaximum();
-    } else if (index instanceof DoubleColumnStatistics) {
-      return ((DoubleColumnStatistics) index).getMaximum();
-    } else if (index instanceof StringColumnStatistics) {
-      return ((StringColumnStatistics) index).getMaximum();
-    } else if (index instanceof DateColumnStatistics) {
-      return ((DateColumnStatistics) index).getMaximum();
-    } else if (index instanceof DecimalColumnStatistics) {
-      return ((DecimalColumnStatistics) index).getMaximum();
-    } else if (index instanceof TimestampColumnStatistics) {
-      return ((TimestampColumnStatistics) index).getMaximum();
-    } else if (index instanceof BooleanColumnStatistics) {
-      if (((BooleanColumnStatistics)index).getTrueCount()!=0) {
-        return Boolean.TRUE;
-      } else {
-        return Boolean.FALSE;
-      }
-    } else {
-      return null;
-    }
-  }
-
-  /**
-   * Get the minimum value out of an index entry.
-   * @param index
-   *          the index entry
-   * @return the object for the minimum value or null if there isn't one
-   */
-  static Object getMin(ColumnStatistics index) {
-    if (index instanceof IntegerColumnStatistics) {
-      return ((IntegerColumnStatistics) index).getMinimum();
-    } else if (index instanceof DoubleColumnStatistics) {
-      return ((DoubleColumnStatistics) index).getMinimum();
-    } else if (index instanceof StringColumnStatistics) {
-      return ((StringColumnStatistics) index).getMinimum();
-    } else if (index instanceof DateColumnStatistics) {
-      return ((DateColumnStatistics) index).getMinimum();
-    } else if (index instanceof DecimalColumnStatistics) {
-      return ((DecimalColumnStatistics) index).getMinimum();
-    } else if (index instanceof TimestampColumnStatistics) {
-      return ((TimestampColumnStatistics) index).getMinimum();
-    } else if (index instanceof BooleanColumnStatistics) {
-      if (((BooleanColumnStatistics)index).getFalseCount()!=0) {
-        return Boolean.FALSE;
-      } else {
-        return Boolean.TRUE;
-      }
-    } else {
-      return UNKNOWN_VALUE; // null is not safe here
-    }
-  }
-
-  /**
-   * Evaluate a predicate with respect to the statistics from the column
-   * that is referenced in the predicate.
-   * @param statsProto the statistics for the column mentioned in the predicate
-   * @param predicate the leaf predicate we need to evaluation
-   * @param bloomFilter
-   * @return the set of truth values that may be returned for the given
-   *   predicate.
-   */
-  static TruthValue evaluatePredicateProto(OrcProto.ColumnStatistics statsProto,
-      PredicateLeaf predicate, OrcProto.BloomFilter bloomFilter) {
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(statsProto);
-    Object minValue = getMin(cs);
-    Object maxValue = getMax(cs);
-    BloomFilterIO bf = null;
-    if (bloomFilter != null) {
-      bf = new BloomFilterIO(bloomFilter);
-    }
-    return evaluatePredicateRange(predicate, minValue, maxValue, cs.hasNull(), bf);
-  }
-
-  /**
-   * Evaluate a predicate with respect to the statistics from the column
-   * that is referenced in the predicate.
-   * @param stats the statistics for the column mentioned in the predicate
-   * @param predicate the leaf predicate we need to evaluation
-   * @return the set of truth values that may be returned for the given
-   *   predicate.
-   */
-  public static TruthValue evaluatePredicate(ColumnStatistics stats,
-                                             PredicateLeaf predicate,
-                                             BloomFilterIO bloomFilter) {
-    Object minValue = getMin(stats);
-    Object maxValue = getMax(stats);
-    return evaluatePredicateRange(predicate, minValue, maxValue, stats.hasNull(), bloomFilter);
-  }
-
-  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,
-      Object max, boolean hasNull, BloomFilterIO bloomFilter) {
-    // if we didn't have any values, everything must have been null
-    if (min == null) {
-      if (predicate.getOperator() == PredicateLeaf.Operator.IS_NULL) {
-        return TruthValue.YES;
-      } else {
-        return TruthValue.NULL;
-      }
-    } else if (min == UNKNOWN_VALUE) {
-      return TruthValue.YES_NO_NULL;
-    }
-
-    TruthValue result;
-    Object baseObj = predicate.getLiteral();
-    try {
-      // Predicate object and stats objects are converted to the type of the predicate object.
-      Object minValue = getBaseObjectForComparison(predicate.getType(), min);
-      Object maxValue = getBaseObjectForComparison(predicate.getType(), max);
-      Object predObj = getBaseObjectForComparison(predicate.getType(), baseObj);
-
-      result = evaluatePredicateMinMax(predicate, predObj, minValue, maxValue, hasNull);
-      if (shouldEvaluateBloomFilter(predicate, result, bloomFilter)) {
-        result = evaluatePredicateBloomFilter(predicate, predObj, bloomFilter, hasNull);
-      }
-      // in case failed conversion, return the default YES_NO_NULL truth value
-    } catch (Exception e) {
-      if (LOG.isWarnEnabled()) {
-        final String statsType = min == null ?
-            (max == null ? "null" : max.getClass().getSimpleName()) :
-            min.getClass().getSimpleName();
-        final String predicateType = baseObj == null ? "null" : baseObj.getClass().getSimpleName();
-        final String reason = e.getClass().getSimpleName() + " when evaluating predicate." +
-            " Skipping ORC PPD." +
-            " Exception: " + e.getMessage() +
-            " StatsType: " + statsType +
-            " PredicateType: " + predicateType;
-        LOG.warn(reason);
-        LOG.debug(reason, e);
-      }
-      if (predicate.getOperator().equals(PredicateLeaf.Operator.NULL_SAFE_EQUALS) || !hasNull) {
-        result = TruthValue.YES_NO;
-      } else {
-        result = TruthValue.YES_NO_NULL;
-      }
-    }
-    return result;
-  }
-
-  private static boolean shouldEvaluateBloomFilter(PredicateLeaf predicate,
-      TruthValue result, BloomFilterIO bloomFilter) {
-    // evaluate bloom filter only when
-    // 1) Bloom filter is available
-    // 2) Min/Max evaluation yield YES or MAYBE
-    // 3) Predicate is EQUALS or IN list
-    if (bloomFilter != null
-        && result != TruthValue.NO_NULL && result != TruthValue.NO
-        && (predicate.getOperator().equals(PredicateLeaf.Operator.EQUALS)
-            || predicate.getOperator().equals(PredicateLeaf.Operator.NULL_SAFE_EQUALS)
-            || predicate.getOperator().equals(PredicateLeaf.Operator.IN))) {
-      return true;
-    }
-    return false;
-  }
-
-  private static TruthValue evaluatePredicateMinMax(PredicateLeaf predicate, Object predObj,
-      Object minValue,
-      Object maxValue,
-      boolean hasNull) {
-    Location loc;
-
-    switch (predicate.getOperator()) {
-      case NULL_SAFE_EQUALS:
-        loc = compareToRange((Comparable) predObj, minValue, maxValue);
-        if (loc == Location.BEFORE || loc == Location.AFTER) {
-          return TruthValue.NO;
-        } else {
-          return TruthValue.YES_NO;
-        }
-      case EQUALS:
-        loc = compareToRange((Comparable) predObj, minValue, maxValue);
-        if (minValue.equals(maxValue) && loc == Location.MIN) {
-          return hasNull ? TruthValue.YES_NULL : TruthValue.YES;
-        } else if (loc == Location.BEFORE || loc == Location.AFTER) {
-          return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-        } else {
-          return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-        }
-      case LESS_THAN:
-        loc = compareToRange((Comparable) predObj, minValue, maxValue);
-        if (loc == Location.AFTER) {
-          return hasNull ? TruthValue.YES_NULL : TruthValue.YES;
-        } else if (loc == Location.BEFORE || loc == Location.MIN) {
-          return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-        } else {
-          return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-        }
-      case LESS_THAN_EQUALS:
-        loc = compareToRange((Comparable) predObj, minValue, maxValue);
-        if (loc == Location.AFTER || loc == Location.MAX) {
-          return hasNull ? TruthValue.YES_NULL : TruthValue.YES;
-        } else if (loc == Location.BEFORE) {
-          return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-        } else {
-          return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-        }
-      case IN:
-        if (minValue.equals(maxValue)) {
-          // for a single value, look through to see if that value is in the
-          // set
-          for (Object arg : predicate.getLiteralList()) {
-            predObj = getBaseObjectForComparison(predicate.getType(), arg);
-            loc = compareToRange((Comparable) predObj, minValue, maxValue);
-            if (loc == Location.MIN) {
-              return hasNull ? TruthValue.YES_NULL : TruthValue.YES;
-            }
-          }
-          return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-        } else {
-          // are all of the values outside of the range?
-          for (Object arg : predicate.getLiteralList()) {
-            predObj = getBaseObjectForComparison(predicate.getType(), arg);
-            loc = compareToRange((Comparable) predObj, minValue, maxValue);
-            if (loc == Location.MIN || loc == Location.MIDDLE ||
-                loc == Location.MAX) {
-              return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-            }
-          }
-          return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-        }
-      case BETWEEN:
-        List<Object> args = predicate.getLiteralList();
-        Object predObj1 = getBaseObjectForComparison(predicate.getType(), args.get(0));
-
-        loc = compareToRange((Comparable) predObj1, minValue, maxValue);
-        if (loc == Location.BEFORE || loc == Location.MIN) {
-          Object predObj2 = getBaseObjectForComparison(predicate.getType(), args.get(1));
-
-          Location loc2 = compareToRange((Comparable) predObj2, minValue, maxValue);
-          if (loc2 == Location.AFTER || loc2 == Location.MAX) {
-            return hasNull ? TruthValue.YES_NULL : TruthValue.YES;
-          } else if (loc2 == Location.BEFORE) {
-            return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-          } else {
-            return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-          }
-        } else if (loc == Location.AFTER) {
-          return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-        } else {
-          return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-        }
-      case IS_NULL:
-        // min = null condition above handles the all-nulls YES case
-        return hasNull ? TruthValue.YES_NO : TruthValue.NO;
-      default:
-        return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-    }
-  }
-
-  private static TruthValue evaluatePredicateBloomFilter(PredicateLeaf predicate,
-      final Object predObj, BloomFilterIO bloomFilter, boolean hasNull) {
-    switch (predicate.getOperator()) {
-      case NULL_SAFE_EQUALS:
-        // null safe equals does not return *_NULL variant. So set hasNull to false
-        return checkInBloomFilter(bloomFilter, predObj, false);
-      case EQUALS:
-        return checkInBloomFilter(bloomFilter, predObj, hasNull);
-      case IN:
-        for (Object arg : predicate.getLiteralList()) {
-          // if atleast one value in IN list exist in bloom filter, qualify the row group/stripe
-          Object predObjItem = getBaseObjectForComparison(predicate.getType(), arg);
-          TruthValue result = checkInBloomFilter(bloomFilter, predObjItem, hasNull);
-          if (result == TruthValue.YES_NO_NULL || result == TruthValue.YES_NO) {
-            return result;
-          }
-        }
-        return hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-      default:
-        return hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;
-    }
-  }
-
-  private static TruthValue checkInBloomFilter(BloomFilterIO bf, Object predObj, boolean hasNull) {
-    TruthValue result = hasNull ? TruthValue.NO_NULL : TruthValue.NO;
-
-    if (predObj instanceof Long) {
-      if (bf.testLong(((Long) predObj).longValue())) {
-        result = TruthValue.YES_NO_NULL;
-      }
-    } else if (predObj instanceof Double) {
-      if (bf.testDouble(((Double) predObj).doubleValue())) {
-        result = TruthValue.YES_NO_NULL;
-      }
-    } else if (predObj instanceof String || predObj instanceof Text ||
-        predObj instanceof HiveDecimalWritable ||
-        predObj instanceof BigDecimal) {
-      if (bf.testString(predObj.toString())) {
-        result = TruthValue.YES_NO_NULL;
-      }
-    } else if (predObj instanceof Timestamp) {
-      if (bf.testLong(((Timestamp) predObj).getTime())) {
-        result = TruthValue.YES_NO_NULL;
-      }
-    } else if (predObj instanceof Date) {
-      if (bf.testLong(DateWritable.dateToDays((Date) predObj))) {
-        result = TruthValue.YES_NO_NULL;
-      }
-    } else {
-        // if the predicate object is null and if hasNull says there are no nulls then return NO
-        if (predObj == null && !hasNull) {
-          result = TruthValue.NO;
-        } else {
-          result = TruthValue.YES_NO_NULL;
-        }
-      }
-
-    if (result == TruthValue.YES_NO_NULL && !hasNull) {
-      result = TruthValue.YES_NO;
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Bloom filter evaluation: " + result.toString());
-    }
-
-    return result;
-  }
-
-  private static Object getBaseObjectForComparison(PredicateLeaf.Type type, Object obj) {
-    if (obj == null) {
-      return null;
-    }
-    switch (type) {
-      case BOOLEAN:
-        if (obj instanceof Boolean) {
-          return obj;
-        } else {
-          // will only be true if the string conversion yields "true", all other values are
-          // considered false
-          return Boolean.valueOf(obj.toString());
-        }
-      case DATE:
-        if (obj instanceof Date) {
-          return obj;
-        } else if (obj instanceof String) {
-          return Date.valueOf((String) obj);
-        } else if (obj instanceof Timestamp) {
-          return DateWritable.timeToDate(((Timestamp) obj).getTime() / 1000L);
-        }
-        // always string, but prevent the comparison to numbers (are they days/seconds/milliseconds?)
-        break;
-      case DECIMAL:
-        if (obj instanceof Boolean) {
-          return new HiveDecimalWritable(((Boolean) obj).booleanValue() ?
-              HiveDecimal.ONE : HiveDecimal.ZERO);
-        } else if (obj instanceof Integer) {
-          return new HiveDecimalWritable(((Integer) obj).intValue());
-        } else if (obj instanceof Long) {
-          return new HiveDecimalWritable(((Long) obj));
-        } else if (obj instanceof Float || obj instanceof Double ||
-            obj instanceof String) {
-          return new HiveDecimalWritable(obj.toString());
-        } else if (obj instanceof BigDecimal) {
-          return new HiveDecimalWritable(HiveDecimal.create((BigDecimal) obj));
-        } else if (obj instanceof HiveDecimal) {
-          return new HiveDecimalWritable((HiveDecimal) obj);
-        } else if (obj instanceof HiveDecimalWritable) {
-          return obj;
-        } else if (obj instanceof Timestamp) {
-          return new HiveDecimalWritable(Double.toString(
-              TimestampUtils.getDouble((Timestamp) obj)));
-        }
-        break;
-      case FLOAT:
-        if (obj instanceof Number) {
-          // widening conversion
-          return ((Number) obj).doubleValue();
-        } else if (obj instanceof HiveDecimal) {
-          return ((HiveDecimal) obj).doubleValue();
-        } else if (obj instanceof String) {
-          return Double.valueOf(obj.toString());
-        } else if (obj instanceof Timestamp) {
-          return TimestampUtils.getDouble((Timestamp) obj);
-        } else if (obj instanceof HiveDecimal) {
-          return ((HiveDecimal) obj).doubleValue();
-        } else if (obj instanceof BigDecimal) {
-          return ((BigDecimal) obj).doubleValue();
-        }
-        break;
-      case LONG:
-        if (obj instanceof Number) {
-          // widening conversion
-          return ((Number) obj).longValue();
-        } else if (obj instanceof HiveDecimal) {
-          return ((HiveDecimal) obj).longValue(); // TODO: lossy conversion!
-        } else if (obj instanceof String) {
-          return Long.valueOf(obj.toString());
-        }
-        break;
-      case STRING:
-        if (obj != null) {
-          return (obj.toString());
-        }
-        break;
-      case TIMESTAMP:
-        if (obj instanceof Timestamp) {
-          return obj;
-        } else if (obj instanceof Integer) {
-          return new Timestamp(((Number) obj).longValue());
-        } else if (obj instanceof Float) {
-          return TimestampUtils.doubleToTimestamp(((Float) obj).doubleValue());
-        } else if (obj instanceof Double) {
-          return TimestampUtils.doubleToTimestamp(((Double) obj).doubleValue());
-        } else if (obj instanceof HiveDecimal) {
-          return TimestampUtils.decimalToTimestamp((HiveDecimal) obj);
-        } else if (obj instanceof HiveDecimalWritable) {
-          return TimestampUtils.decimalToTimestamp(((HiveDecimalWritable) obj).getHiveDecimal());
-        } else if (obj instanceof Date) {
-          return new Timestamp(((Date) obj).getTime());
-        }
-        // float/double conversion to timestamp is interpreted as seconds whereas integer conversion
-        // to timestamp is interpreted as milliseconds by default. The integer to timestamp casting
-        // is also config driven. The filter operator changes its promotion based on config:
-        // "int.timestamp.conversion.in.seconds". Disable PPD for integer cases.
-        break;
-      default:
-        break;
-    }
-
-    throw new IllegalArgumentException(String.format(
-        "ORC SARGS could not convert from %s to %s", obj == null ? "(null)" : obj.getClass()
-            .getSimpleName(), type));
-  }
-
-  public static class SargApplier {
-    public final static boolean[] READ_ALL_RGS = null;
-    public final static boolean[] READ_NO_RGS = new boolean[0];
-
-    private final SearchArgument sarg;
-    private final List<PredicateLeaf> sargLeaves;
-    private final int[] filterColumns;
-    private final long rowIndexStride;
-    // same as the above array, but indices are set to true
-    private final boolean[] sargColumns;
-    private SchemaEvolution evolution;
-
-    public SargApplier(SearchArgument sarg, String[] columnNames,
-                       long rowIndexStride,
-                       SchemaEvolution evolution) {
-      this.sarg = sarg;
-      sargLeaves = sarg.getLeaves();
-      filterColumns = mapSargColumnsToOrcInternalColIdx(sargLeaves, evolution);
-      this.rowIndexStride = rowIndexStride;
-      // included will not be null, row options will fill the array with trues if null
-      sargColumns = new boolean[evolution.getFileIncluded().length];
-      for (int i : filterColumns) {
-        // filter columns may have -1 as index which could be partition column in SARG.
-        if (i > 0) {
-          sargColumns[i] = true;
-        }
-      }
-      this.evolution = evolution;
-    }
-
-    /**
-     * Pick the row groups that we need to load from the current stripe.
-     *
-     * @return an array with a boolean for each row group or null if all of the
-     * row groups must be read.
-     * @throws IOException
-     */
-    public boolean[] pickRowGroups(StripeInformation stripe, OrcProto.RowIndex[] indexes,
-        OrcProto.BloomFilterIndex[] bloomFilterIndices, boolean returnNone) throws IOException {
-      long rowsInStripe = stripe.getNumberOfRows();
-      int groupsInStripe = (int) ((rowsInStripe + rowIndexStride - 1) / rowIndexStride);
-      boolean[] result = new boolean[groupsInStripe]; // TODO: avoid alloc?
-      TruthValue[] leafValues = new TruthValue[sargLeaves.size()];
-      boolean hasSelected = false, hasSkipped = false;
-      for (int rowGroup = 0; rowGroup < result.length; ++rowGroup) {
-        for (int pred = 0; pred < leafValues.length; ++pred) {
-          int columnIx = filterColumns[pred];
-          if (columnIx != -1) {
-            if (indexes[columnIx] == null) {
-              throw new AssertionError("Index is not populated for " + columnIx);
-            }
-            OrcProto.RowIndexEntry entry = indexes[columnIx].getEntry(rowGroup);
-            if (entry == null) {
-              throw new AssertionError("RG is not populated for " + columnIx + " rg " + rowGroup);
-            }
-            OrcProto.ColumnStatistics stats = entry.getStatistics();
-            OrcProto.BloomFilter bf = null;
-            if (bloomFilterIndices != null && bloomFilterIndices[columnIx] != null) {
-              bf = bloomFilterIndices[columnIx].getBloomFilter(rowGroup);
-            }
-            if (evolution != null && evolution.isPPDSafeConversion(columnIx)) {
-              leafValues[pred] = evaluatePredicateProto(stats, sargLeaves.get(pred), bf);
-            } else {
-              leafValues[pred] = TruthValue.YES_NO_NULL;
-            }
-            if (LOG.isTraceEnabled()) {
-              LOG.trace("Stats = " + stats);
-              LOG.trace("Setting " + sargLeaves.get(pred) + " to " + leafValues[pred]);
-            }
-          } else {
-            // the column is a virtual column
-            leafValues[pred] = TruthValue.YES_NO_NULL;
-          }
-        }
-        result[rowGroup] = sarg.evaluate(leafValues).isNeeded();
-        hasSelected = hasSelected || result[rowGroup];
-        hasSkipped = hasSkipped || (!result[rowGroup]);
-        if (LOG.isDebugEnabled()) {
-          LOG.debug("Row group " + (rowIndexStride * rowGroup) + " to " +
-              (rowIndexStride * (rowGroup + 1) - 1) + " is " +
-              (result[rowGroup] ? "" : "not ") + "included.");
-        }
-      }
-
-      return hasSkipped ? ((hasSelected || !returnNone) ? result : READ_NO_RGS) : READ_ALL_RGS;
-    }
-  }
-
-  /**
-   * Pick the row groups that we need to load from the current stripe.
-   *
-   * @return an array with a boolean for each row group or null if all of the
-   * row groups must be read.
-   * @throws IOException
-   */
-  protected boolean[] pickRowGroups() throws IOException {
-    // if we don't have a sarg or indexes, we read everything
-    if (sargApp == null) {
-      return null;
-    }
-    readRowIndex(currentStripe, included, sargApp.sargColumns);
-    return sargApp.pickRowGroups(stripes.get(currentStripe), indexes, bloomFilterIndices, false);
-  }
-
-  private void clearStreams() {
-    // explicit close of all streams to de-ref ByteBuffers
-    for (InStream is : streams.values()) {
-      is.close();
-    }
-    if (bufferChunks != null) {
-      if (dataReader.isTrackingDiskRanges()) {
-        for (DiskRangeList range = bufferChunks; range != null; range = range.next) {
-          if (!(range instanceof BufferChunk)) {
-            continue;
-          }
-          dataReader.releaseBuffer(((BufferChunk) range).getChunk());
-        }
-      }
-    }
-    bufferChunks = null;
-    streams.clear();
-  }
-
-  /**
-   * Read the current stripe into memory.
-   *
-   * @throws IOException
-   */
-  private void readStripe() throws IOException {
-    StripeInformation stripe = beginReadStripe();
-    includedRowGroups = pickRowGroups();
-
-    // move forward to the first unskipped row
-    if (includedRowGroups != null) {
-      while (rowInStripe < rowCountInStripe &&
-          !includedRowGroups[(int) (rowInStripe / rowIndexStride)]) {
-        rowInStripe = Math.min(rowCountInStripe, rowInStripe + rowIndexStride);
-      }
-    }
-
-    // if we haven't skipped the whole stripe, read the data
-    if (rowInStripe < rowCountInStripe) {
-      // if we aren't projecting columns or filtering rows, just read it all
-      if (included == null && includedRowGroups == null) {
-        readAllDataStreams(stripe);
-      } else {
-        readPartialDataStreams(stripe);
-      }
-      reader.startStripe(streams, stripeFooter);
-      // if we skipped the first row group, move the pointers forward
-      if (rowInStripe != 0) {
-        seekToRowEntry(reader, (int) (rowInStripe / rowIndexStride));
-      }
-    }
-  }
-
-  private StripeInformation beginReadStripe() throws IOException {
-    StripeInformation stripe = stripes.get(currentStripe);
-    stripeFooter = readStripeFooter(stripe);
-    clearStreams();
-    // setup the position in the stripe
-    rowCountInStripe = stripe.getNumberOfRows();
-    rowInStripe = 0;
-    rowBaseInStripe = 0;
-    for (int i = 0; i < currentStripe; ++i) {
-      rowBaseInStripe += stripes.get(i).getNumberOfRows();
-    }
-    // reset all of the indexes
-    for (int i = 0; i < indexes.length; ++i) {
-      indexes[i] = null;
-    }
-    return stripe;
-  }
-
-  private void readAllDataStreams(StripeInformation stripe) throws IOException {
-    long start = stripe.getIndexLength();
-    long end = start + stripe.getDataLength();
-    // explicitly trigger 1 big read
-    DiskRangeList toRead = new DiskRangeList(start, end);
-    bufferChunks = dataReader.readFileData(toRead, stripe.getOffset(), false);
-    List<OrcProto.Stream> streamDescriptions = stripeFooter.getStreamsList();
-    createStreams(streamDescriptions, bufferChunks, null, codec, bufferSize, streams);
-  }
-
-  /**
-   * Plan the ranges of the file that we need to read given the list of
-   * columns and row groups.
-   *
-   * @param streamList        the list of streams available
-   * @param indexes           the indexes that have been loaded
-   * @param includedColumns   which columns are needed
-   * @param includedRowGroups which row groups are needed
-   * @param isCompressed      does the file have generic compression
-   * @param encodings         the encodings for each column
-   * @param types             the types of the columns
-   * @param compressionSize   the compression block size
-   * @return the list of disk ranges that will be loaded
-   */
-  static DiskRangeList planReadPartialDataStreams
-  (List<OrcProto.Stream> streamList,
-      OrcProto.RowIndex[] indexes,
-      boolean[] includedColumns,
-      boolean[] includedRowGroups,
-      boolean isCompressed,
-      List<OrcProto.ColumnEncoding> encodings,
-      List<OrcProto.Type> types,
-      int compressionSize,
-      boolean doMergeBuffers) {
-    long offset = 0;
-    // figure out which columns have a present stream
-    boolean[] hasNull = RecordReaderUtils.findPresentStreamsByColumn(streamList, types);
-    CreateHelper list = new CreateHelper();
-    for (OrcProto.Stream stream : streamList) {
-      long length = stream.getLength();
-      int column = stream.getColumn();
-      OrcProto.Stream.Kind streamKind = stream.getKind();
-      // since stream kind is optional, first check if it exists
-      if (stream.hasKind() &&
-          (StreamName.getArea(streamKind) == StreamName.Area.DATA) &&
-          (column < includedColumns.length && includedColumns[column])) {
-        // if we aren't filtering or it is a dictionary, load it.
-        if (includedRowGroups == null
-            || RecordReaderUtils.isDictionary(streamKind, encodings.get(column))) {
-          RecordReaderUtils.addEntireStreamToRanges(offset, length, list, doMergeBuffers);
-        } else {
-          RecordReaderUtils.addRgFilteredStreamToRanges(stream, includedRowGroups,
-              isCompressed, indexes[column], encodings.get(column), types.get(column),
-              compressionSize, hasNull[column], offset, length, list, doMergeBuffers);
-        }
-      }
-      offset += length;
-    }
-    return list.extract();
-  }
-
-  void createStreams(List<OrcProto.Stream> streamDescriptions,
-      DiskRangeList ranges,
-      boolean[] includeColumn,
-      CompressionCodec codec,
-      int bufferSize,
-      Map<StreamName, InStream> streams) throws IOException {
-    long streamOffset = 0;
-    for (OrcProto.Stream streamDesc : streamDescriptions) {
-      int column = streamDesc.getColumn();
-      if ((includeColumn != null &&
-          (column < included.length && !includeColumn[column])) ||
-          streamDesc.hasKind() &&
-              (StreamName.getArea(streamDesc.getKind()) != StreamName.Area.DATA)) {
-        streamOffset += streamDesc.getLength();
-        continue;
-      }
-      List<DiskRange> buffers = RecordReaderUtils.getStreamBuffers(
-          ranges, streamOffset, streamDesc.getLength());
-      StreamName name = new StreamName(column, streamDesc.getKind());
-      streams.put(name, InStream.create(name.toString(), buffers,
-          streamDesc.getLength(), codec, bufferSize));
-      streamOffset += streamDesc.getLength();
-    }
-  }
-
-  private void readPartialDataStreams(StripeInformation stripe) throws IOException {
-    List<OrcProto.Stream> streamList = stripeFooter.getStreamsList();
-    DiskRangeList toRead = planReadPartialDataStreams(streamList,
-        indexes, included, includedRowGroups, codec != null,
-        stripeFooter.getColumnsList(), types, bufferSize, true);
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("chunks = " + RecordReaderUtils.stringifyDiskRanges(toRead));
-    }
-    bufferChunks = dataReader.readFileData(toRead, stripe.getOffset(), false);
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("merge = " + RecordReaderUtils.stringifyDiskRanges(bufferChunks));
-    }
-
-    createStreams(streamList, bufferChunks, included, codec, bufferSize, streams);
-  }
-
-  /**
-   * Read the next stripe until we find a row that we don't skip.
-   *
-   * @throws IOException
-   */
-  private void advanceStripe() throws IOException {
-    rowInStripe = rowCountInStripe;
-    while (rowInStripe >= rowCountInStripe &&
-        currentStripe < stripes.size() - 1) {
-      currentStripe += 1;
-      readStripe();
-    }
-  }
-
-  /**
-   * Skip over rows that we aren't selecting, so that the next row is
-   * one that we will read.
-   *
-   * @param nextRow the row we want to go to
-   * @throws IOException
-   */
-  private boolean advanceToNextRow(
-      TreeReaderFactory.TreeReader reader, long nextRow, boolean canAdvanceStripe)
-      throws IOException {
-    long nextRowInStripe = nextRow - rowBaseInStripe;
-    // check for row skipping
-    if (rowIndexStride != 0 &&
-        includedRowGroups != null &&
-        nextRowInStripe < rowCountInStripe) {
-      int rowGroup = (int) (nextRowInStripe / rowIndexStride);
-      if (!includedRowGroups[rowGroup]) {
-        while (rowGroup < includedRowGroups.length && !includedRowGroups[rowGroup]) {
-          rowGroup += 1;
-        }
-        if (rowGroup >= includedRowGroups.length) {
-          if (canAdvanceStripe) {
-            advanceStripe();
-          }
-          return canAdvanceStripe;
-        }
-        nextRowInStripe = Math.min(rowCountInStripe, rowGroup * rowIndexStride);
-      }
-    }
-    if (nextRowInStripe >= rowCountInStripe) {
-      if (canAdvanceStripe) {
-        advanceStripe();
-      }
-      return canAdvanceStripe;
-    }
-    if (nextRowInStripe != rowInStripe) {
-      if (rowIndexStride != 0) {
-        int rowGroup = (int) (nextRowInStripe / rowIndexStride);
-        seekToRowEntry(reader, rowGroup);
-        reader.skipRows(nextRowInStripe - rowGroup * rowIndexStride);
-      } else {
-        reader.skipRows(nextRowInStripe - rowInStripe);
-      }
-      rowInStripe = nextRowInStripe;
-    }
-    return true;
-  }
-
-  @Override
-  public boolean nextBatch(VectorizedRowBatch batch) throws IOException {
-    try {
-      if (rowInStripe >= rowCountInStripe) {
-        currentStripe += 1;
-        if (currentStripe >= stripes.size()) {
-          batch.size = 0;
-          return false;
-        }
-        readStripe();
-      }
-
-      int batchSize = computeBatchSize(batch.getMaxSize());
-
-      rowInStripe += batchSize;
-      reader.setVectorColumnCount(batch.getDataColumnCount());
-      reader.nextBatch(batch, batchSize);
-      batch.selectedInUse = false;
-      batch.size = batchSize;
-      advanceToNextRow(reader, rowInStripe + rowBaseInStripe, true);
-      return batch.size  != 0;
-    } catch (IOException e) {
-      // Rethrow exception with file name in log message
-      throw new IOException("Error reading file: " + path, e);
-    }
-  }
-
-  private int computeBatchSize(long targetBatchSize) {
-    final int batchSize;
-    // In case of PPD, batch size should be aware of row group boundaries. If only a subset of row
-    // groups are selected then marker position is set to the end of range (subset of row groups
-    // within strip). Batch size computed out of marker position makes sure that batch size is
-    // aware of row group boundary and will not cause overflow when reading rows
-    // illustration of this case is here https://issues.apache.org/jira/browse/HIVE-6287
-    if (rowIndexStride != 0 && includedRowGroups != null && rowInStripe < rowCountInStripe) {
-      int startRowGroup = (int) (rowInStripe / rowIndexStride);
-      if (!includedRowGroups[startRowGroup]) {
-        while (startRowGroup < includedRowGroups.length && !includedRowGroups[startRowGroup]) {
-          startRowGroup += 1;
-        }
-      }
-
-      int endRowGroup = startRowGroup;
-      while (endRowGroup < includedRowGroups.length && includedRowGroups[endRowGroup]) {
-        endRowGroup += 1;
-      }
-
-      final long markerPosition =
-          (endRowGroup * rowIndexStride) < rowCountInStripe ? (endRowGroup * rowIndexStride)
-              : rowCountInStripe;
-      batchSize = (int) Math.min(targetBatchSize, (markerPosition - rowInStripe));
-
-      if (isLogDebugEnabled && batchSize < targetBatchSize) {
-        LOG.debug("markerPosition: " + markerPosition + " batchSize: " + batchSize);
-      }
-    } else {
-      batchSize = (int) Math.min(targetBatchSize, (rowCountInStripe - rowInStripe));
-    }
-    return batchSize;
-  }
-
-  @Override
-  public void close() throws IOException {
-    clearStreams();
-    dataReader.close();
-  }
-
-  @Override
-  public long getRowNumber() {
-    return rowInStripe + rowBaseInStripe + firstRow;
-  }
-
-  /**
-   * Return the fraction of rows that have been read from the selected.
-   * section of the file
-   *
-   * @return fraction between 0.0 and 1.0 of rows consumed
-   */
-  @Override
-  public float getProgress() {
-    return ((float) rowBaseInStripe + rowInStripe) / totalRowCount;
-  }
-
-  private int findStripe(long rowNumber) {
-    for (int i = 0; i < stripes.size(); i++) {
-      StripeInformation stripe = stripes.get(i);
-      if (stripe.getNumberOfRows() > rowNumber) {
-        return i;
-      }
-      rowNumber -= stripe.getNumberOfRows();
-    }
-    throw new IllegalArgumentException("Seek after the end of reader range");
-  }
-
-  public OrcIndex readRowIndex(int stripeIndex, boolean[] included,
-                               boolean[] sargColumns) throws IOException {
-    return readRowIndex(stripeIndex, included, null, null, sargColumns);
-  }
-
-  public OrcIndex readRowIndex(int stripeIndex, boolean[] included,
-                               OrcProto.RowIndex[] indexes,
-                               OrcProto.BloomFilterIndex[] bloomFilterIndex,
-                               boolean[] sargColumns) throws IOException {
-    StripeInformation stripe = stripes.get(stripeIndex);
-    OrcProto.StripeFooter stripeFooter = null;
-    // if this is the current stripe, use the cached objects.
-    if (stripeIndex == currentStripe) {
-      stripeFooter = this.stripeFooter;
-      indexes = indexes == null ? this.indexes : indexes;
-      bloomFilterIndex = bloomFilterIndex == null ? this.bloomFilterIndices : bloomFilterIndex;
-      sargColumns = sargColumns == null ?
-          (sargApp == null ? null : sargApp.sargColumns) : sargColumns;
-    }
-    return dataReader.readRowIndex(stripe, stripeFooter, included, indexes, sargColumns,
-        bloomFilterIndex);
-  }
-
-  private void seekToRowEntry(TreeReaderFactory.TreeReader reader, int rowEntry)
-      throws IOException {
-    PositionProvider[] index = new PositionProvider[indexes.length];
-    for (int i = 0; i < indexes.length; ++i) {
-      if (indexes[i] != null) {
-        index[i] = new PositionProviderImpl(indexes[i].getEntry(rowEntry));
-      }
-    }
-    reader.seek(index);
-  }
-
-  @Override
-  public void seekToRow(long rowNumber) throws IOException {
-    if (rowNumber < 0) {
-      throw new IllegalArgumentException("Seek to a negative row number " +
-          rowNumber);
-    } else if (rowNumber < firstRow) {
-      throw new IllegalArgumentException("Seek before reader range " +
-          rowNumber);
-    }
-    // convert to our internal form (rows from the beginning of slice)
-    rowNumber -= firstRow;
-
-    // move to the right stripe
-    int rightStripe = findStripe(rowNumber);
-    if (rightStripe != currentStripe) {
-      currentStripe = rightStripe;
-      readStripe();
-    }
-    readRowIndex(currentStripe, included, sargApp == null ? null : sargApp.sargColumns);
-
-    // if we aren't to the right row yet, advance in the stripe.
-    advanceToNextRow(reader, rowNumber, true);
-  }
-
-  private static final String TRANSLATED_SARG_SEPARATOR = "_";
-  public static String encodeTranslatedSargColumn(int rootColumn, Integer indexInSourceTable) {
-    return rootColumn + TRANSLATED_SARG_SEPARATOR
-        + ((indexInSourceTable == null) ? -1 : indexInSourceTable);
-  }
-
-  public static int[] mapTranslatedSargColumns(
-      List<OrcProto.Type> types, List<PredicateLeaf> sargLeaves) {
-    int[] result = new int[sargLeaves.size()];
-    OrcProto.Type lastRoot = null; // Root will be the same for everyone as of now.
-    String lastRootStr = null;
-    for (int i = 0; i < result.length; ++i) {
-      String[] rootAndIndex = sargLeaves.get(i).getColumnName().split(TRANSLATED_SARG_SEPARATOR);
-      assert rootAndIndex.length == 2;
-      String rootStr = rootAndIndex[0], indexStr = rootAndIndex[1];
-      int index = Integer.parseInt(indexStr);
-      // First, check if the column even maps to anything.
-      if (index == -1) {
-        result[i] = -1;
-        continue;
-      }
-      assert index >= 0;
-      // Then, find the root type if needed.
-      if (!rootStr.equals(lastRootStr)) {
-        lastRoot = types.get(Integer.parseInt(rootStr));
-        lastRootStr = rootStr;
-      }
-      // Subtypes of the root types correspond, in order, to the columns in the table schema
-      // (disregarding schema evolution that doesn't presently work). Get the index for the
-      // corresponding subtype.
-      result[i] = lastRoot.getSubtypes(index);
-    }
-    return result;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RecordReaderUtils.java b/orc/src/java/org/apache/orc/impl/RecordReaderUtils.java
deleted file mode 100644
index 6100d503d7..0000000000
--- a/orc/src/java/org/apache/orc/impl/RecordReaderUtils.java
+++ /dev/null
@@ -1,578 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.TreeMap;
-
-import com.google.common.collect.Lists;
-import org.apache.commons.lang.builder.HashCodeBuilder;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.io.DiskRange;
-import org.apache.hadoop.hive.common.io.DiskRangeList;
-import org.apache.hadoop.hive.common.io.DiskRangeList.CreateHelper;
-import org.apache.hadoop.hive.common.io.DiskRangeList.MutateHelper;
-import org.apache.orc.CompressionCodec;
-import org.apache.orc.DataReader;
-import org.apache.orc.OrcProto;
-
-import com.google.common.collect.ComparisonChain;
-import org.apache.orc.StripeInformation;
-
-/**
- * Stateless methods shared between RecordReaderImpl and EncodedReaderImpl.
- */
-public class RecordReaderUtils {
-  private static final HadoopShims SHIMS = HadoopShims.Factory.get();
-
-  private static class DefaultDataReader implements DataReader {
-    private FSDataInputStream file = null;
-    private final ByteBufferAllocatorPool pool;
-    private HadoopShims.ZeroCopyReaderShim zcr = null;
-    private final FileSystem fs;
-    private final Path path;
-    private final boolean useZeroCopy;
-    private final CompressionCodec codec;
-    private final int bufferSize;
-    private final int typeCount;
-
-    private DefaultDataReader(DefaultDataReader other) {
-      this.pool = other.pool;
-      this.bufferSize = other.bufferSize;
-      this.typeCount = other.typeCount;
-      this.fs = other.fs;
-      this.path = other.path;
-      this.useZeroCopy = other.useZeroCopy;
-      this.codec = other.codec;
-    }
-
-    private DefaultDataReader(DataReaderProperties properties) {
-      this.fs = properties.getFileSystem();
-      this.path = properties.getPath();
-      this.useZeroCopy = properties.getZeroCopy();
-      this.codec = PhysicalFsWriter.createCodec(properties.getCompression());
-      this.bufferSize = properties.getBufferSize();
-      this.typeCount = properties.getTypeCount();
-      if (useZeroCopy) {
-        this.pool = new ByteBufferAllocatorPool();
-      } else {
-        this.pool = null;
-      }
-    }
-
-    @Override
-    public void open() throws IOException {
-      this.file = fs.open(path);
-      if (useZeroCopy) {
-        zcr = RecordReaderUtils.createZeroCopyShim(file, codec, pool);
-      } else {
-        zcr = null;
-      }
-    }
-
-    @Override
-    public OrcIndex readRowIndex(StripeInformation stripe,
-                                 OrcProto.StripeFooter footer,
-                                 boolean[] included,
-                                 OrcProto.RowIndex[] indexes,
-                                 boolean[] sargColumns,
-                                 OrcProto.BloomFilterIndex[] bloomFilterIndices
-                                 ) throws IOException {
-      if (file == null) {
-        open();
-      }
-      if (footer == null) {
-        footer = readStripeFooter(stripe);
-      }
-      if (indexes == null) {
-        indexes = new OrcProto.RowIndex[typeCount];
-      }
-      if (bloomFilterIndices == null) {
-        bloomFilterIndices = new OrcProto.BloomFilterIndex[typeCount];
-      }
-      long offset = stripe.getOffset();
-      List<OrcProto.Stream> streams = footer.getStreamsList();
-      for (int i = 0; i < streams.size(); i++) {
-        OrcProto.Stream stream = streams.get(i);
-        OrcProto.Stream nextStream = null;
-        if (i < streams.size() - 1) {
-          nextStream = streams.get(i+1);
-        }
-        int col = stream.getColumn();
-        int len = (int) stream.getLength();
-        // row index stream and bloom filter are interlaced, check if the sarg column contains bloom
-        // filter and combine the io to read row index and bloom filters for that column together
-        if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.ROW_INDEX)) {
-          boolean readBloomFilter = false;
-          if (sargColumns != null && sargColumns[col] &&
-              nextStream.getKind() == OrcProto.Stream.Kind.BLOOM_FILTER) {
-            len += nextStream.getLength();
-            i += 1;
-            readBloomFilter = true;
-          }
-          if ((included == null || included[col]) && indexes[col] == null) {
-            byte[] buffer = new byte[len];
-            file.readFully(offset, buffer, 0, buffer.length);
-            ByteBuffer bb = ByteBuffer.wrap(buffer);
-            indexes[col] = OrcProto.RowIndex.parseFrom(InStream.create("index",
-                Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)), stream.getLength(),
-                codec, bufferSize));
-            if (readBloomFilter) {
-              bb.position((int) stream.getLength());
-              bloomFilterIndices[col] = OrcProto.BloomFilterIndex.parseFrom(InStream.create(
-                  "bloom_filter", Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)),
-                  nextStream.getLength(), codec, bufferSize));
-            }
-          }
-        }
-        offset += len;
-      }
-
-      OrcIndex index = new OrcIndex(indexes, bloomFilterIndices);
-      return index;
-    }
-
-    @Override
-    public OrcProto.StripeFooter readStripeFooter(StripeInformation stripe) throws IOException {
-      if (file == null) {
-        open();
-      }
-      long offset = stripe.getOffset() + stripe.getIndexLength() + stripe.getDataLength();
-      int tailLength = (int) stripe.getFooterLength();
-
-      // read the footer
-      ByteBuffer tailBuf = ByteBuffer.allocate(tailLength);
-      file.readFully(offset, tailBuf.array(), tailBuf.arrayOffset(), tailLength);
-      return OrcProto.StripeFooter.parseFrom(InStream.createCodedInputStream("footer",
-          Lists.<DiskRange>newArrayList(new BufferChunk(tailBuf, 0)),
-          tailLength, codec, bufferSize));
-    }
-
-    @Override
-    public DiskRangeList readFileData(
-        DiskRangeList range, long baseOffset, boolean doForceDirect) throws IOException {
-      return RecordReaderUtils.readDiskRanges(file, zcr, baseOffset, range, doForceDirect);
-    }
-
-    @Override
-    public void close() throws IOException {
-      if (pool != null) {
-        pool.clear();
-      }
-      // close both zcr and file
-      try (HadoopShims.ZeroCopyReaderShim myZcr = zcr) {
-        if (file != null) {
-          file.close();
-        }
-      }
-    }
-
-    @Override
-    public boolean isTrackingDiskRanges() {
-      return zcr != null;
-    }
-
-    @Override
-    public void releaseBuffer(ByteBuffer buffer) {
-      zcr.releaseBuffer(buffer);
-    }
-
-    @Override
-    public DataReader clone() {
-      return new DefaultDataReader(this);
-    }
-
-  }
-
-  public static DataReader createDefaultDataReader(DataReaderProperties properties) {
-    return new DefaultDataReader(properties);
-  }
-
-  public static boolean[] findPresentStreamsByColumn(
-      List<OrcProto.Stream> streamList, List<OrcProto.Type> types) {
-    boolean[] hasNull = new boolean[types.size()];
-    for(OrcProto.Stream stream: streamList) {
-      if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.PRESENT)) {
-        hasNull[stream.getColumn()] = true;
-      }
-    }
-    return hasNull;
-  }
-
-  /**
-   * Does region A overlap region B? The end points are inclusive on both sides.
-   * @param leftA A's left point
-   * @param rightA A's right point
-   * @param leftB B's left point
-   * @param rightB B's right point
-   * @return Does region A overlap region B?
-   */
-  static boolean overlap(long leftA, long rightA, long leftB, long rightB) {
-    if (leftA <= leftB) {
-      return rightA >= leftB;
-    }
-    return rightB >= leftA;
-  }
-
-  public static void addEntireStreamToRanges(
-      long offset, long length, CreateHelper list, boolean doMergeBuffers) {
-    list.addOrMerge(offset, offset + length, doMergeBuffers, false);
-  }
-
-  public static void addRgFilteredStreamToRanges(OrcProto.Stream stream,
-      boolean[] includedRowGroups, boolean isCompressed, OrcProto.RowIndex index,
-      OrcProto.ColumnEncoding encoding, OrcProto.Type type, int compressionSize, boolean hasNull,
-      long offset, long length, CreateHelper list, boolean doMergeBuffers) {
-    for (int group = 0; group < includedRowGroups.length; ++group) {
-      if (!includedRowGroups[group]) continue;
-      int posn = getIndexPosition(
-          encoding.getKind(), type.getKind(), stream.getKind(), isCompressed, hasNull);
-      long start = index.getEntry(group).getPositions(posn);
-      final long nextGroupOffset;
-      boolean isLast = group == (includedRowGroups.length - 1);
-      nextGroupOffset = isLast ? length : index.getEntry(group + 1).getPositions(posn);
-
-      start += offset;
-      long end = offset + estimateRgEndOffset(
-          isCompressed, isLast, nextGroupOffset, length, compressionSize);
-      list.addOrMerge(start, end, doMergeBuffers, true);
-    }
-  }
-
-  public static long estimateRgEndOffset(boolean isCompressed, boolean isLast,
-      long nextGroupOffset, long streamLength, int bufferSize) {
-    // figure out the worst case last location
-    // if adjacent groups have the same compressed block offset then stretch the slop
-    // by factor of 2 to safely accommodate the next compression block.
-    // One for the current compression block and another for the next compression block.
-    long slop = isCompressed ? 2 * (OutStream.HEADER_SIZE + bufferSize) : WORST_UNCOMPRESSED_SLOP;
-    return isLast ? streamLength : Math.min(streamLength, nextGroupOffset + slop);
-  }
-
-  private static final int BYTE_STREAM_POSITIONS = 1;
-  private static final int RUN_LENGTH_BYTE_POSITIONS = BYTE_STREAM_POSITIONS + 1;
-  private static final int BITFIELD_POSITIONS = RUN_LENGTH_BYTE_POSITIONS + 1;
-  private static final int RUN_LENGTH_INT_POSITIONS = BYTE_STREAM_POSITIONS + 1;
-
-  /**
-   * Get the offset in the index positions for the column that the given
-   * stream starts.
-   * @param columnEncoding the encoding of the column
-   * @param columnType the type of the column
-   * @param streamType the kind of the stream
-   * @param isCompressed is the file compressed
-   * @param hasNulls does the column have a PRESENT stream?
-   * @return the number of positions that will be used for that stream
-   */
-  public static int getIndexPosition(OrcProto.ColumnEncoding.Kind columnEncoding,
-                              OrcProto.Type.Kind columnType,
-                              OrcProto.Stream.Kind streamType,
-                              boolean isCompressed,
-                              boolean hasNulls) {
-    if (streamType == OrcProto.Stream.Kind.PRESENT) {
-      return 0;
-    }
-    int compressionValue = isCompressed ? 1 : 0;
-    int base = hasNulls ? (BITFIELD_POSITIONS + compressionValue) : 0;
-    switch (columnType) {
-      case BOOLEAN:
-      case BYTE:
-      case SHORT:
-      case INT:
-      case LONG:
-      case FLOAT:
-      case DOUBLE:
-      case DATE:
-      case STRUCT:
-      case MAP:
-      case LIST:
-      case UNION:
-        return base;
-      case CHAR:
-      case VARCHAR:
-      case STRING:
-        if (columnEncoding == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
-            columnEncoding == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
-          return base;
-        } else {
-          if (streamType == OrcProto.Stream.Kind.DATA) {
-            return base;
-          } else {
-            return base + BYTE_STREAM_POSITIONS + compressionValue;
-          }
-        }
-      case BINARY:
-        if (streamType == OrcProto.Stream.Kind.DATA) {
-          return base;
-        }
-        return base + BYTE_STREAM_POSITIONS + compressionValue;
-      case DECIMAL:
-        if (streamType == OrcProto.Stream.Kind.DATA) {
-          return base;
-        }
-        return base + BYTE_STREAM_POSITIONS + compressionValue;
-      case TIMESTAMP:
-        if (streamType == OrcProto.Stream.Kind.DATA) {
-          return base;
-        }
-        return base + RUN_LENGTH_INT_POSITIONS + compressionValue;
-      default:
-        throw new IllegalArgumentException("Unknown type " + columnType);
-    }
-  }
-
-  // for uncompressed streams, what is the most overlap with the following set
-  // of rows (long vint literal group).
-  static final int WORST_UNCOMPRESSED_SLOP = 2 + 8 * 512;
-
-  /**
-   * Is this stream part of a dictionary?
-   * @return is this part of a dictionary?
-   */
-  public static boolean isDictionary(OrcProto.Stream.Kind kind,
-                              OrcProto.ColumnEncoding encoding) {
-    assert kind != OrcProto.Stream.Kind.DICTIONARY_COUNT;
-    OrcProto.ColumnEncoding.Kind encodingKind = encoding.getKind();
-    return kind == OrcProto.Stream.Kind.DICTIONARY_DATA ||
-      (kind == OrcProto.Stream.Kind.LENGTH &&
-       (encodingKind == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
-        encodingKind == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2));
-  }
-
-  /**
-   * Build a string representation of a list of disk ranges.
-   * @param range ranges to stringify
-   * @return the resulting string
-   */
-  public static String stringifyDiskRanges(DiskRangeList range) {
-    StringBuilder buffer = new StringBuilder();
-    buffer.append("[");
-    boolean isFirst = true;
-    while (range != null) {
-      if (!isFirst) {
-        buffer.append(", {");
-      } else {
-        buffer.append("{");
-      }
-      isFirst = false;
-      buffer.append(range.toString());
-      buffer.append("}");
-      range = range.next;
-    }
-    buffer.append("]");
-    return buffer.toString();
-  }
-
-  /**
-   * Read the list of ranges from the file.
-   * @param file the file to read
-   * @param base the base of the stripe
-   * @param range the disk ranges within the stripe to read
-   * @return the bytes read for each disk range, which is the same length as
-   *    ranges
-   * @throws IOException
-   */
-  static DiskRangeList readDiskRanges(FSDataInputStream file,
-                                      HadoopShims.ZeroCopyReaderShim zcr,
-                                 long base,
-                                 DiskRangeList range,
-                                 boolean doForceDirect) throws IOException {
-    if (range == null) return null;
-    DiskRangeList prev = range.prev;
-    if (prev == null) {
-      prev = new MutateHelper(range);
-    }
-    while (range != null) {
-      if (range.hasData()) {
-        range = range.next;
-        continue;
-      }
-      int len = (int) (range.getEnd() - range.getOffset());
-      long off = range.getOffset();
-      if (zcr != null) {
-        file.seek(base + off);
-        boolean hasReplaced = false;
-        while (len > 0) {
-          ByteBuffer partial = zcr.readBuffer(len, false);
-          BufferChunk bc = new BufferChunk(partial, off);
-          if (!hasReplaced) {
-            range.replaceSelfWith(bc);
-            hasReplaced = true;
-          } else {
-            range.insertAfter(bc);
-          }
-          range = bc;
-          int read = partial.remaining();
-          len -= read;
-          off += read;
-        }
-      } else {
-        // Don't use HDFS ByteBuffer API because it has no readFully, and is buggy and pointless.
-        byte[] buffer = new byte[len];
-        file.readFully((base + off), buffer, 0, buffer.length);
-        ByteBuffer bb = null;
-        if (doForceDirect) {
-          bb = ByteBuffer.allocateDirect(len);
-          bb.put(buffer);
-          bb.position(0);
-          bb.limit(len);
-        } else {
-          bb = ByteBuffer.wrap(buffer);
-        }
-        range = range.replaceSelfWith(new BufferChunk(bb, range.getOffset()));
-      }
-      range = range.next;
-    }
-    return prev.next;
-  }
-
-
-  static List<DiskRange> getStreamBuffers(DiskRangeList range, long offset, long length) {
-    // This assumes sorted ranges (as do many other parts of ORC code.
-    ArrayList<DiskRange> buffers = new ArrayList<DiskRange>();
-    if (length == 0) return buffers;
-    long streamEnd = offset + length;
-    boolean inRange = false;
-    while (range != null) {
-      if (!inRange) {
-        if (range.getEnd() <= offset) {
-          range = range.next;
-          continue; // Skip until we are in range.
-        }
-        inRange = true;
-        if (range.getOffset() < offset) {
-          // Partial first buffer, add a slice of it.
-          buffers.add(range.sliceAndShift(offset, Math.min(streamEnd, range.getEnd()), -offset));
-          if (range.getEnd() >= streamEnd) break; // Partial first buffer is also partial last buffer.
-          range = range.next;
-          continue;
-        }
-      } else if (range.getOffset() >= streamEnd) {
-        break;
-      }
-      if (range.getEnd() > streamEnd) {
-        // Partial last buffer (may also be the first buffer), add a slice of it.
-        buffers.add(range.sliceAndShift(range.getOffset(), streamEnd, -offset));
-        break;
-      }
-      // Buffer that belongs entirely to one stream.
-      // TODO: ideally we would want to reuse the object and remove it from the list, but we cannot
-      //       because bufferChunks is also used by clearStreams for zcr. Create a useless dup.
-      buffers.add(range.sliceAndShift(range.getOffset(), range.getEnd(), -offset));
-      if (range.getEnd() == streamEnd) break;
-      range = range.next;
-    }
-    return buffers;
-  }
-
-  static HadoopShims.ZeroCopyReaderShim createZeroCopyShim(FSDataInputStream file,
-      CompressionCodec codec, ByteBufferAllocatorPool pool) throws IOException {
-    if ((codec == null || ((codec instanceof DirectDecompressionCodec)
-            && ((DirectDecompressionCodec) codec).isAvailable()))) {
-      /* codec is null or is available */
-      return SHIMS.getZeroCopyReader(file, pool);
-    }
-    return null;
-  }
-
-  // this is an implementation copied from ElasticByteBufferPool in hadoop-2,
-  // which lacks a clear()/clean() operation
-  public final static class ByteBufferAllocatorPool implements HadoopShims.ByteBufferPoolShim {
-    private static final class Key implements Comparable<Key> {
-      private final int capacity;
-      private final long insertionGeneration;
-
-      Key(int capacity, long insertionGeneration) {
-        this.capacity = capacity;
-        this.insertionGeneration = insertionGeneration;
-      }
-
-      @Override
-      public int compareTo(Key other) {
-        return ComparisonChain.start().compare(capacity, other.capacity)
-            .compare(insertionGeneration, other.insertionGeneration).result();
-      }
-
-      @Override
-      public boolean equals(Object rhs) {
-        if (rhs == null) {
-          return false;
-        }
-        try {
-          Key o = (Key) rhs;
-          return (compareTo(o) == 0);
-        } catch (ClassCastException e) {
-          return false;
-        }
-      }
-
-      @Override
-      public int hashCode() {
-        return new HashCodeBuilder().append(capacity).append(insertionGeneration)
-            .toHashCode();
-      }
-    }
-
-    private final TreeMap<Key, ByteBuffer> buffers = new TreeMap<Key, ByteBuffer>();
-
-    private final TreeMap<Key, ByteBuffer> directBuffers = new TreeMap<Key, ByteBuffer>();
-
-    private long currentGeneration = 0;
-
-    private final TreeMap<Key, ByteBuffer> getBufferTree(boolean direct) {
-      return direct ? directBuffers : buffers;
-    }
-
-    public void clear() {
-      buffers.clear();
-      directBuffers.clear();
-    }
-
-    @Override
-    public ByteBuffer getBuffer(boolean direct, int length) {
-      TreeMap<Key, ByteBuffer> tree = getBufferTree(direct);
-      Map.Entry<Key, ByteBuffer> entry = tree.ceilingEntry(new Key(length, 0));
-      if (entry == null) {
-        return direct ? ByteBuffer.allocateDirect(length) : ByteBuffer
-            .allocate(length);
-      }
-      tree.remove(entry.getKey());
-      return entry.getValue();
-    }
-
-    @Override
-    public void putBuffer(ByteBuffer buffer) {
-      TreeMap<Key, ByteBuffer> tree = getBufferTree(buffer.isDirect());
-      while (true) {
-        Key key = new Key(buffer.capacity(), currentGeneration++);
-        if (!tree.containsKey(key)) {
-          tree.put(key, buffer);
-          return;
-        }
-        // Buffers are indexed by (capacity, generation).
-        // If our key is not unique on the first try, we try again
-      }
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RedBlackTree.java b/orc/src/java/org/apache/orc/impl/RedBlackTree.java
deleted file mode 100644
index 41aa4b9dea..0000000000
--- a/orc/src/java/org/apache/orc/impl/RedBlackTree.java
+++ /dev/null
@@ -1,311 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.impl.DynamicIntArray;
-
-/**
- * A memory efficient red-black tree that does not allocate any objects per
- * an element. This class is abstract and assumes that the child class
- * handles the key and comparisons with the key.
- */
-abstract class RedBlackTree {
-  public static final int NULL = -1;
-
-  // Various values controlling the offset of the data within the array.
-  private static final int LEFT_OFFSET = 0;
-  private static final int RIGHT_OFFSET = 1;
-  private static final int ELEMENT_SIZE = 2;
-
-  protected int size = 0;
-  private final DynamicIntArray data;
-  protected int root = NULL;
-  protected int lastAdd = 0;
-  private boolean wasAdd = false;
-
-  /**
-   * Create a set with the given initial capacity.
-   */
-  public RedBlackTree(int initialCapacity) {
-    data = new DynamicIntArray(initialCapacity * ELEMENT_SIZE);
-  }
-
-  /**
-   * Insert a new node into the data array, growing the array as necessary.
-   *
-   * @return Returns the position of the new node.
-   */
-  private int insert(int left, int right, boolean isRed) {
-    int position = size;
-    size += 1;
-    setLeft(position, left, isRed);
-    setRight(position, right);
-    return position;
-  }
-
-  /**
-   * Compare the value at the given position to the new value.
-   * @return 0 if the values are the same, -1 if the new value is smaller and
-   *         1 if the new value is larger.
-   */
-  protected abstract int compareValue(int position);
-
-  /**
-   * Is the given node red as opposed to black? To prevent having an extra word
-   * in the data array, we just the low bit on the left child index.
-   */
-  protected boolean isRed(int position) {
-    return position != NULL &&
-        (data.get(position * ELEMENT_SIZE + LEFT_OFFSET) & 1) == 1;
-  }
-
-  /**
-   * Set the red bit true or false.
-   */
-  private void setRed(int position, boolean isRed) {
-    int offset = position * ELEMENT_SIZE + LEFT_OFFSET;
-    if (isRed) {
-      data.set(offset, data.get(offset) | 1);
-    } else {
-      data.set(offset, data.get(offset) & ~1);
-    }
-  }
-
-  /**
-   * Get the left field of the given position.
-   */
-  protected int getLeft(int position) {
-    return data.get(position * ELEMENT_SIZE + LEFT_OFFSET) >> 1;
-  }
-
-  /**
-   * Get the right field of the given position.
-   */
-  protected int getRight(int position) {
-    return data.get(position * ELEMENT_SIZE + RIGHT_OFFSET);
-  }
-
-  /**
-   * Set the left field of the given position.
-   * Note that we are storing the node color in the low bit of the left pointer.
-   */
-  private void setLeft(int position, int left) {
-    int offset = position * ELEMENT_SIZE + LEFT_OFFSET;
-    data.set(offset, (left << 1) | (data.get(offset) & 1));
-  }
-
-  /**
-   * Set the left field of the given position.
-   * Note that we are storing the node color in the low bit of the left pointer.
-   */
-  private void setLeft(int position, int left, boolean isRed) {
-    int offset = position * ELEMENT_SIZE + LEFT_OFFSET;
-    data.set(offset, (left << 1) | (isRed ? 1 : 0));
-  }
-
-  /**
-   * Set the right field of the given position.
-   */
-  private void setRight(int position, int right) {
-    data.set(position * ELEMENT_SIZE + RIGHT_OFFSET, right);
-  }
-
-  /**
-   * Insert or find a given key in the tree and rebalance the tree correctly.
-   * Rebalancing restores the red-black aspect of the tree to maintain the
-   * invariants:
-   *   1. If a node is red, both of its children are black.
-   *   2. Each child of a node has the same black height (the number of black
-   *      nodes between it and the leaves of the tree).
-   *
-   * Inserted nodes are at the leaves and are red, therefore there is at most a
-   * violation of rule 1 at the node we just put in. Instead of always keeping
-   * the parents, this routine passing down the context.
-   *
-   * The fix is broken down into 6 cases (1.{1,2,3} and 2.{1,2,3} that are
-   * left-right mirror images of each other). See Algorighms by Cormen,
-   * Leiserson, and Rivest for the explaination of the subcases.
-   *
-   * @param node The node that we are fixing right now.
-   * @param fromLeft Did we come down from the left?
-   * @param parent Nodes' parent
-   * @param grandparent Parent's parent
-   * @param greatGrandparent Grandparent's parent
-   * @return Does parent also need to be checked and/or fixed?
-   */
-  private boolean add(int node, boolean fromLeft, int parent,
-                      int grandparent, int greatGrandparent) {
-    if (node == NULL) {
-      if (root == NULL) {
-        lastAdd = insert(NULL, NULL, false);
-        root = lastAdd;
-        wasAdd = true;
-        return false;
-      } else {
-        lastAdd = insert(NULL, NULL, true);
-        node = lastAdd;
-        wasAdd = true;
-        // connect the new node into the tree
-        if (fromLeft) {
-          setLeft(parent, node);
-        } else {
-          setRight(parent, node);
-        }
-      }
-    } else {
-      int compare = compareValue(node);
-      boolean keepGoing;
-
-      // Recurse down to find where the node needs to be added
-      if (compare < 0) {
-        keepGoing = add(getLeft(node), true, node, parent, grandparent);
-      } else if (compare > 0) {
-        keepGoing = add(getRight(node), false, node, parent, grandparent);
-      } else {
-        lastAdd = node;
-        wasAdd = false;
-        return false;
-      }
-
-      // we don't need to fix the root (because it is always set to black)
-      if (node == root || !keepGoing) {
-        return false;
-      }
-    }
-
-
-    // Do we need to fix this node? Only if there are two reds right under each
-    // other.
-    if (isRed(node) && isRed(parent)) {
-      if (parent == getLeft(grandparent)) {
-        int uncle = getRight(grandparent);
-        if (isRed(uncle)) {
-          // case 1.1
-          setRed(parent, false);
-          setRed(uncle, false);
-          setRed(grandparent, true);
-          return true;
-        } else {
-          if (node == getRight(parent)) {
-            // case 1.2
-            // swap node and parent
-            int tmp = node;
-            node = parent;
-            parent = tmp;
-            // left-rotate on node
-            setLeft(grandparent, parent);
-            setRight(node, getLeft(parent));
-            setLeft(parent, node);
-          }
-
-          // case 1.2 and 1.3
-          setRed(parent, false);
-          setRed(grandparent, true);
-
-          // right-rotate on grandparent
-          if (greatGrandparent == NULL) {
-            root = parent;
-          } else if (getLeft(greatGrandparent) == grandparent) {
-            setLeft(greatGrandparent, parent);
-          } else {
-            setRight(greatGrandparent, parent);
-          }
-          setLeft(grandparent, getRight(parent));
-          setRight(parent, grandparent);
-          return false;
-        }
-      } else {
-        int uncle = getLeft(grandparent);
-        if (isRed(uncle)) {
-          // case 2.1
-          setRed(parent, false);
-          setRed(uncle, false);
-          setRed(grandparent, true);
-          return true;
-        } else {
-          if (node == getLeft(parent)) {
-            // case 2.2
-            // swap node and parent
-            int tmp = node;
-            node = parent;
-            parent = tmp;
-            // right-rotate on node
-            setRight(grandparent, parent);
-            setLeft(node, getRight(parent));
-            setRight(parent, node);
-          }
-          // case 2.2 and 2.3
-          setRed(parent, false);
-          setRed(grandparent, true);
-          // left-rotate on grandparent
-          if (greatGrandparent == NULL) {
-            root = parent;
-          } else if (getRight(greatGrandparent) == grandparent) {
-            setRight(greatGrandparent, parent);
-          } else {
-            setLeft(greatGrandparent, parent);
-          }
-          setRight(grandparent, getLeft(parent));
-          setLeft(parent, grandparent);
-          return false;
-        }
-      }
-    } else {
-      return true;
-    }
-  }
-
-  /**
-   * Add the new key to the tree.
-   * @return true if the element is a new one.
-   */
-  protected boolean add() {
-    add(root, false, NULL, NULL, NULL);
-    if (wasAdd) {
-      setRed(root, false);
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  /**
-   * Get the number of elements in the set.
-   */
-  public int size() {
-    return size;
-  }
-
-  /**
-   * Reset the table to empty.
-   */
-  public void clear() {
-    root = NULL;
-    size = 0;
-    data.clear();
-  }
-
-  /**
-   * Get the buffer size in bytes.
-   */
-  public long getSizeInBytes() {
-    return data.getSizeInBytes();
-  }
-}
-
diff --git a/orc/src/java/org/apache/orc/impl/RunLengthByteReader.java b/orc/src/java/org/apache/orc/impl/RunLengthByteReader.java
deleted file mode 100644
index 24bd05174e..0000000000
--- a/orc/src/java/org/apache/orc/impl/RunLengthByteReader.java
+++ /dev/null
@@ -1,174 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-
-/**
- * A reader that reads a sequence of bytes. A control byte is read before
- * each run with positive values 0 to 127 meaning 3 to 130 repetitions. If the
- * byte is -1 to -128, 1 to 128 literal byte values follow.
- */
-public class RunLengthByteReader {
-  private InStream input;
-  private final byte[] literals =
-    new byte[RunLengthByteWriter.MAX_LITERAL_SIZE];
-  private int numLiterals = 0;
-  private int used = 0;
-  private boolean repeat = false;
-
-  public RunLengthByteReader(InStream input) throws IOException {
-    this.input = input;
-  }
-
-  public void setInStream(InStream input) {
-    this.input = input;
-  }
-
-  private void readValues(boolean ignoreEof) throws IOException {
-    int control = input.read();
-    used = 0;
-    if (control == -1) {
-      if (!ignoreEof) {
-        throw new EOFException("Read past end of buffer RLE byte from " + input);
-      }
-      used = numLiterals = 0;
-      return;
-    } else if (control < 0x80) {
-      repeat = true;
-      numLiterals = control + RunLengthByteWriter.MIN_REPEAT_SIZE;
-      int val = input.read();
-      if (val == -1) {
-        throw new EOFException("Reading RLE byte got EOF");
-      }
-      literals[0] = (byte) val;
-    } else {
-      repeat = false;
-      numLiterals = 0x100 - control;
-      int bytes = 0;
-      while (bytes < numLiterals) {
-        int result = input.read(literals, bytes, numLiterals - bytes);
-        if (result == -1) {
-          throw new EOFException("Reading RLE byte literal got EOF in " + this);
-        }
-        bytes += result;
-      }
-    }
-  }
-
-  public boolean hasNext() throws IOException {
-    return used != numLiterals || input.available() > 0;
-  }
-
-  public byte next() throws IOException {
-    byte result;
-    if (used == numLiterals) {
-      readValues(false);
-    }
-    if (repeat) {
-      result = literals[0];
-    } else {
-      result = literals[used];
-    }
-    ++used;
-    return result;
-  }
-
-  public void nextVector(ColumnVector previous, long[] data, long size)
-      throws IOException {
-    previous.isRepeating = true;
-    for (int i = 0; i < size; i++) {
-      if (!previous.isNull[i]) {
-        data[i] = next();
-      } else {
-        // The default value of null for int types in vectorized
-        // processing is 1, so set that if the value is null
-        data[i] = 1;
-      }
-
-      // The default value for nulls in Vectorization for int types is 1
-      // and given that non null value can also be 1, we need to check for isNull also
-      // when determining the isRepeating flag.
-      if (previous.isRepeating
-          && i > 0
-          && ((data[0] != data[i]) ||
-              (previous.isNull[0] != previous.isNull[i]))) {
-        previous.isRepeating = false;
-      }
-    }
-  }
-
-  /**
-   * Read the next size bytes into the data array, skipping over any slots
-   * where isNull is true.
-   * @param isNull if non-null, skip any rows where isNull[r] is true
-   * @param data the array to read into
-   * @param size the number of elements to read
-   * @throws IOException
-   */
-  public void nextVector(boolean[] isNull, int[] data,
-                         long size) throws IOException {
-    if (isNull == null) {
-      for(int i=0; i < size; ++i) {
-        data[i] = next();
-      }
-    } else {
-      for(int i=0; i < size; ++i) {
-        if (!isNull[i]) {
-          data[i] = next();
-        }
-      }
-    }
-  }
-
-  public void seek(PositionProvider index) throws IOException {
-    input.seek(index);
-    int consumed = (int) index.getNext();
-    if (consumed != 0) {
-      // a loop is required for cases where we break the run into two parts
-      while (consumed > 0) {
-        readValues(false);
-        used = consumed;
-        consumed -= numLiterals;
-      }
-    } else {
-      used = 0;
-      numLiterals = 0;
-    }
-  }
-
-  public void skip(long items) throws IOException {
-    while (items > 0) {
-      if (used == numLiterals) {
-        readValues(false);
-      }
-      long consume = Math.min(items, numLiterals - used);
-      used += consume;
-      items -= consume;
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "byte rle " + (repeat ? "repeat" : "literal") + " used: " +
-        used + "/" + numLiterals + " from " + input;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RunLengthByteWriter.java b/orc/src/java/org/apache/orc/impl/RunLengthByteWriter.java
deleted file mode 100644
index 09108b2633..0000000000
--- a/orc/src/java/org/apache/orc/impl/RunLengthByteWriter.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-
-/**
- * A streamFactory that writes a sequence of bytes. A control byte is written before
- * each run with positive values 0 to 127 meaning 2 to 129 repetitions. If the
- * bytes is -1 to -128, 1 to 128 literal byte values follow.
- */
-public class RunLengthByteWriter {
-  static final int MIN_REPEAT_SIZE = 3;
-  static final int MAX_LITERAL_SIZE = 128;
-  static final int MAX_REPEAT_SIZE= 127 + MIN_REPEAT_SIZE;
-  private final PositionedOutputStream output;
-  private final byte[] literals = new byte[MAX_LITERAL_SIZE];
-  private int numLiterals = 0;
-  private boolean repeat = false;
-  private int tailRunLength = 0;
-
-  public RunLengthByteWriter(PositionedOutputStream output) {
-    this.output = output;
-  }
-
-  private void writeValues() throws IOException {
-    if (numLiterals != 0) {
-      if (repeat) {
-        output.write(numLiterals - MIN_REPEAT_SIZE);
-        output.write(literals, 0, 1);
-     } else {
-        output.write(-numLiterals);
-        output.write(literals, 0, numLiterals);
-      }
-      repeat = false;
-      tailRunLength = 0;
-      numLiterals = 0;
-    }
-  }
-
-  public void flush() throws IOException {
-    writeValues();
-    output.flush();
-  }
-
-  public void write(byte value) throws IOException {
-    if (numLiterals == 0) {
-      literals[numLiterals++] = value;
-      tailRunLength = 1;
-    } else if (repeat) {
-      if (value == literals[0]) {
-        numLiterals += 1;
-        if (numLiterals == MAX_REPEAT_SIZE) {
-          writeValues();
-        }
-      } else {
-        writeValues();
-        literals[numLiterals++] = value;
-        tailRunLength = 1;
-      }
-    } else {
-      if (value == literals[numLiterals - 1]) {
-        tailRunLength += 1;
-      } else {
-        tailRunLength = 1;
-      }
-      if (tailRunLength == MIN_REPEAT_SIZE) {
-        if (numLiterals + 1 == MIN_REPEAT_SIZE) {
-          repeat = true;
-          numLiterals += 1;
-        } else {
-          numLiterals -= MIN_REPEAT_SIZE - 1;
-          writeValues();
-          literals[0] = value;
-          repeat = true;
-          numLiterals = MIN_REPEAT_SIZE;
-        }
-      } else {
-        literals[numLiterals++] = value;
-        if (numLiterals == MAX_LITERAL_SIZE) {
-          writeValues();
-        }
-      }
-    }
-  }
-
-  public void getPosition(PositionRecorder recorder) throws IOException {
-    output.getPosition(recorder);
-    recorder.addPosition(numLiterals);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RunLengthIntegerReader.java b/orc/src/java/org/apache/orc/impl/RunLengthIntegerReader.java
deleted file mode 100644
index b91a263cbe..0000000000
--- a/orc/src/java/org/apache/orc/impl/RunLengthIntegerReader.java
+++ /dev/null
@@ -1,173 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-
-/**
- * A reader that reads a sequence of integers.
- * */
-public class RunLengthIntegerReader implements IntegerReader {
-  private InStream input;
-  private final boolean signed;
-  private final long[] literals =
-    new long[RunLengthIntegerWriter.MAX_LITERAL_SIZE];
-  private int numLiterals = 0;
-  private int delta = 0;
-  private int used = 0;
-  private boolean repeat = false;
-  private SerializationUtils utils;
-
-  public RunLengthIntegerReader(InStream input, boolean signed) throws IOException {
-    this.input = input;
-    this.signed = signed;
-    this.utils = new SerializationUtils();
-  }
-
-  private void readValues(boolean ignoreEof) throws IOException {
-    int control = input.read();
-    if (control == -1) {
-      if (!ignoreEof) {
-        throw new EOFException("Read past end of RLE integer from " + input);
-      }
-      used = numLiterals = 0;
-      return;
-    } else if (control < 0x80) {
-      numLiterals = control + RunLengthIntegerWriter.MIN_REPEAT_SIZE;
-      used = 0;
-      repeat = true;
-      delta = input.read();
-      if (delta == -1) {
-        throw new EOFException("End of stream in RLE Integer from " + input);
-      }
-      // convert from 0 to 255 to -128 to 127 by converting to a signed byte
-      delta = (byte) (0 + delta);
-      if (signed) {
-        literals[0] = utils.readVslong(input);
-      } else {
-        literals[0] = utils.readVulong(input);
-      }
-    } else {
-      repeat = false;
-      numLiterals = 0x100 - control;
-      used = 0;
-      for(int i=0; i < numLiterals; ++i) {
-        if (signed) {
-          literals[i] = utils.readVslong(input);
-        } else {
-          literals[i] = utils.readVulong(input);
-        }
-      }
-    }
-  }
-
-  @Override
-  public boolean hasNext() throws IOException {
-    return used != numLiterals || input.available() > 0;
-  }
-
-  @Override
-  public long next() throws IOException {
-    long result;
-    if (used == numLiterals) {
-      readValues(false);
-    }
-    if (repeat) {
-      result = literals[0] + (used++) * delta;
-    } else {
-      result = literals[used++];
-    }
-    return result;
-  }
-
-  @Override
-  public void nextVector(ColumnVector previous,
-                         long[] data,
-                         int previousLen) throws IOException {
-    previous.isRepeating = true;
-    for (int i = 0; i < previousLen; i++) {
-      if (!previous.isNull[i]) {
-        data[i] = next();
-      } else {
-        // The default value of null for int type in vectorized
-        // processing is 1, so set that if the value is null
-        data[i] = 1;
-      }
-
-      // The default value for nulls in Vectorization for int types is 1
-      // and given that non null value can also be 1, we need to check for isNull also
-      // when determining the isRepeating flag.
-      if (previous.isRepeating
-          && i > 0
-          && (data[0] != data[i] || previous.isNull[0] != previous.isNull[i])) {
-        previous.isRepeating = false;
-      }
-    }
-  }
-
-  @Override
-  public void nextVector(ColumnVector vector,
-                         int[] data,
-                         int size) throws IOException {
-    if (vector.noNulls) {
-      for(int r=0; r < data.length && r < size; ++r) {
-        data[r] = (int) next();
-      }
-    } else if (!(vector.isRepeating && vector.isNull[0])) {
-      for(int r=0; r < data.length && r < size; ++r) {
-        if (!vector.isNull[r]) {
-          data[r] = (int) next();
-        } else {
-          data[r] = 1;
-        }
-      }
-    }
-  }
-
-  @Override
-  public void seek(PositionProvider index) throws IOException {
-    input.seek(index);
-    int consumed = (int) index.getNext();
-    if (consumed != 0) {
-      // a loop is required for cases where we break the run into two parts
-      while (consumed > 0) {
-        readValues(false);
-        used = consumed;
-        consumed -= numLiterals;
-      }
-    } else {
-      used = 0;
-      numLiterals = 0;
-    }
-  }
-
-  @Override
-  public void skip(long numValues) throws IOException {
-    while (numValues > 0) {
-      if (used == numLiterals) {
-        readValues(false);
-      }
-      long consume = Math.min(numValues, numLiterals - used);
-      used += consume;
-      numValues -= consume;
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RunLengthIntegerReaderV2.java b/orc/src/java/org/apache/orc/impl/RunLengthIntegerReaderV2.java
deleted file mode 100644
index 610d9b5a6e..0000000000
--- a/orc/src/java/org/apache/orc/impl/RunLengthIntegerReaderV2.java
+++ /dev/null
@@ -1,406 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * A reader that reads a sequence of light weight compressed integers. Refer
- * {@link RunLengthIntegerWriterV2} for description of various lightweight
- * compression techniques.
- */
-public class RunLengthIntegerReaderV2 implements IntegerReader {
-  public static final Logger LOG = LoggerFactory.getLogger(RunLengthIntegerReaderV2.class);
-
-  private InStream input;
-  private final boolean signed;
-  private final long[] literals = new long[RunLengthIntegerWriterV2.MAX_SCOPE];
-  private boolean isRepeating = false;
-  private int numLiterals = 0;
-  private int used = 0;
-  private final boolean skipCorrupt;
-  private final SerializationUtils utils;
-  private RunLengthIntegerWriterV2.EncodingType currentEncoding;
-
-  public RunLengthIntegerReaderV2(InStream input, boolean signed,
-      boolean skipCorrupt) throws IOException {
-    this.input = input;
-    this.signed = signed;
-    this.skipCorrupt = skipCorrupt;
-    this.utils = new SerializationUtils();
-  }
-
-  private final static RunLengthIntegerWriterV2.EncodingType[] encodings = RunLengthIntegerWriterV2.EncodingType.values();
-  private void readValues(boolean ignoreEof) throws IOException {
-    // read the first 2 bits and determine the encoding type
-    isRepeating = false;
-    int firstByte = input.read();
-    if (firstByte < 0) {
-      if (!ignoreEof) {
-        throw new EOFException("Read past end of RLE integer from " + input);
-      }
-      used = numLiterals = 0;
-      return;
-    }
-    currentEncoding = encodings[(firstByte >>> 6) & 0x03];
-    switch (currentEncoding) {
-    case SHORT_REPEAT: readShortRepeatValues(firstByte); break;
-    case DIRECT: readDirectValues(firstByte); break;
-    case PATCHED_BASE: readPatchedBaseValues(firstByte); break;
-    case DELTA: readDeltaValues(firstByte); break;
-    default: throw new IOException("Unknown encoding " + currentEncoding);
-    }
-  }
-
-  private void readDeltaValues(int firstByte) throws IOException {
-
-    // extract the number of fixed bits
-    int fb = (firstByte >>> 1) & 0x1f;
-    if (fb != 0) {
-      fb = utils.decodeBitWidth(fb);
-    }
-
-    // extract the blob run length
-    int len = (firstByte & 0x01) << 8;
-    len |= input.read();
-
-    // read the first value stored as vint
-    long firstVal = 0;
-    if (signed) {
-      firstVal = utils.readVslong(input);
-    } else {
-      firstVal = utils.readVulong(input);
-    }
-
-    // store first value to result buffer
-    long prevVal = firstVal;
-    literals[numLiterals++] = firstVal;
-
-    // if fixed bits is 0 then all values have fixed delta
-    if (fb == 0) {
-      // read the fixed delta value stored as vint (deltas can be negative even
-      // if all number are positive)
-      long fd = utils.readVslong(input);
-      if (fd == 0) {
-        isRepeating = true;
-        assert numLiterals == 1;
-        Arrays.fill(literals, numLiterals, numLiterals + len, literals[0]);
-        numLiterals += len;
-      } else {
-        // add fixed deltas to adjacent values
-        for(int i = 0; i < len; i++) {
-          literals[numLiterals++] = literals[numLiterals - 2] + fd;
-        }
-      }
-    } else {
-      long deltaBase = utils.readVslong(input);
-      // add delta base and first value
-      literals[numLiterals++] = firstVal + deltaBase;
-      prevVal = literals[numLiterals - 1];
-      len -= 1;
-
-      // write the unpacked values, add it to previous value and store final
-      // value to result buffer. if the delta base value is negative then it
-      // is a decreasing sequence else an increasing sequence
-      utils.readInts(literals, numLiterals, len, fb, input);
-      while (len > 0) {
-        if (deltaBase < 0) {
-          literals[numLiterals] = prevVal - literals[numLiterals];
-        } else {
-          literals[numLiterals] = prevVal + literals[numLiterals];
-        }
-        prevVal = literals[numLiterals];
-        len--;
-        numLiterals++;
-      }
-    }
-  }
-
-  private void readPatchedBaseValues(int firstByte) throws IOException {
-
-    // extract the number of fixed bits
-    int fbo = (firstByte >>> 1) & 0x1f;
-    int fb = utils.decodeBitWidth(fbo);
-
-    // extract the run length of data blob
-    int len = (firstByte & 0x01) << 8;
-    len |= input.read();
-    // runs are always one off
-    len += 1;
-
-    // extract the number of bytes occupied by base
-    int thirdByte = input.read();
-    int bw = (thirdByte >>> 5) & 0x07;
-    // base width is one off
-    bw += 1;
-
-    // extract patch width
-    int pwo = thirdByte & 0x1f;
-    int pw = utils.decodeBitWidth(pwo);
-
-    // read fourth byte and extract patch gap width
-    int fourthByte = input.read();
-    int pgw = (fourthByte >>> 5) & 0x07;
-    // patch gap width is one off
-    pgw += 1;
-
-    // extract the length of the patch list
-    int pl = fourthByte & 0x1f;
-
-    // read the next base width number of bytes to extract base value
-    long base = utils.bytesToLongBE(input, bw);
-    long mask = (1L << ((bw * 8) - 1));
-    // if MSB of base value is 1 then base is negative value else positive
-    if ((base & mask) != 0) {
-      base = base & ~mask;
-      base = -base;
-    }
-
-    // unpack the data blob
-    long[] unpacked = new long[len];
-    utils.readInts(unpacked, 0, len, fb, input);
-
-    // unpack the patch blob
-    long[] unpackedPatch = new long[pl];
-
-    if ((pw + pgw) > 64 && !skipCorrupt) {
-      throw new IOException("Corruption in ORC data encountered. To skip" +
-          " reading corrupted data, set hive.exec.orc.skip.corrupt.data to" +
-          " true");
-    }
-    int bitSize = utils.getClosestFixedBits(pw + pgw);
-    utils.readInts(unpackedPatch, 0, pl, bitSize, input);
-
-    // apply the patch directly when decoding the packed data
-    int patchIdx = 0;
-    long currGap = 0;
-    long currPatch = 0;
-    long patchMask = ((1L << pw) - 1);
-    currGap = unpackedPatch[patchIdx] >>> pw;
-    currPatch = unpackedPatch[patchIdx] & patchMask;
-    long actualGap = 0;
-
-    // special case: gap is >255 then patch value will be 0.
-    // if gap is <=255 then patch value cannot be 0
-    while (currGap == 255 && currPatch == 0) {
-      actualGap += 255;
-      patchIdx++;
-      currGap = unpackedPatch[patchIdx] >>> pw;
-      currPatch = unpackedPatch[patchIdx] & patchMask;
-    }
-    // add the left over gap
-    actualGap += currGap;
-
-    // unpack data blob, patch it (if required), add base to get final result
-    for(int i = 0; i < unpacked.length; i++) {
-      if (i == actualGap) {
-        // extract the patch value
-        long patchedVal = unpacked[i] | (currPatch << fb);
-
-        // add base to patched value
-        literals[numLiterals++] = base + patchedVal;
-
-        // increment the patch to point to next entry in patch list
-        patchIdx++;
-
-        if (patchIdx < pl) {
-          // read the next gap and patch
-          currGap = unpackedPatch[patchIdx] >>> pw;
-          currPatch = unpackedPatch[patchIdx] & patchMask;
-          actualGap = 0;
-
-          // special case: gap is >255 then patch will be 0. if gap is
-          // <=255 then patch cannot be 0
-          while (currGap == 255 && currPatch == 0) {
-            actualGap += 255;
-            patchIdx++;
-            currGap = unpackedPatch[patchIdx] >>> pw;
-            currPatch = unpackedPatch[patchIdx] & patchMask;
-          }
-          // add the left over gap
-          actualGap += currGap;
-
-          // next gap is relative to the current gap
-          actualGap += i;
-        }
-      } else {
-        // no patching required. add base to unpacked value to get final value
-        literals[numLiterals++] = base + unpacked[i];
-      }
-    }
-
-  }
-
-  private void readDirectValues(int firstByte) throws IOException {
-
-    // extract the number of fixed bits
-    int fbo = (firstByte >>> 1) & 0x1f;
-    int fb = utils.decodeBitWidth(fbo);
-
-    // extract the run length
-    int len = (firstByte & 0x01) << 8;
-    len |= input.read();
-    // runs are one off
-    len += 1;
-
-    // write the unpacked values and zigzag decode to result buffer
-    utils.readInts(literals, numLiterals, len, fb, input);
-    if (signed) {
-      for(int i = 0; i < len; i++) {
-        literals[numLiterals] = utils.zigzagDecode(literals[numLiterals]);
-        numLiterals++;
-      }
-    } else {
-      numLiterals += len;
-    }
-  }
-
-  private void readShortRepeatValues(int firstByte) throws IOException {
-
-    // read the number of bytes occupied by the value
-    int size = (firstByte >>> 3) & 0x07;
-    // #bytes are one off
-    size += 1;
-
-    // read the run length
-    int len = firstByte & 0x07;
-    // run lengths values are stored only after MIN_REPEAT value is met
-    len += RunLengthIntegerWriterV2.MIN_REPEAT;
-
-    // read the repeated value which is store using fixed bytes
-    long val = utils.bytesToLongBE(input, size);
-
-    if (signed) {
-      val = utils.zigzagDecode(val);
-    }
-
-    if (numLiterals != 0) {
-      // Currently this always holds, which makes peekNextAvailLength simpler.
-      // If this changes, peekNextAvailLength should be adjusted accordingly.
-      throw new AssertionError("readValues called with existing values present");
-    }
-    // repeat the value for length times
-    isRepeating = true;
-    // TODO: this is not so useful and V1 reader doesn't do that. Fix? Same if delta == 0
-    for(int i = 0; i < len; i++) {
-      literals[i] = val;
-    }
-    numLiterals = len;
-  }
-
-  @Override
-  public boolean hasNext() throws IOException {
-    return used != numLiterals || input.available() > 0;
-  }
-
-  @Override
-  public long next() throws IOException {
-    long result;
-    if (used == numLiterals) {
-      numLiterals = 0;
-      used = 0;
-      readValues(false);
-    }
-    result = literals[used++];
-    return result;
-  }
-
-  @Override
-  public void seek(PositionProvider index) throws IOException {
-    input.seek(index);
-    int consumed = (int) index.getNext();
-    if (consumed != 0) {
-      // a loop is required for cases where we break the run into two
-      // parts
-      while (consumed > 0) {
-        numLiterals = 0;
-        readValues(false);
-        used = consumed;
-        consumed -= numLiterals;
-      }
-    } else {
-      used = 0;
-      numLiterals = 0;
-    }
-  }
-
-  @Override
-  public void skip(long numValues) throws IOException {
-    while (numValues > 0) {
-      if (used == numLiterals) {
-        numLiterals = 0;
-        used = 0;
-        readValues(false);
-      }
-      long consume = Math.min(numValues, numLiterals - used);
-      used += consume;
-      numValues -= consume;
-    }
-  }
-
-  @Override
-  public void nextVector(ColumnVector previous,
-                         long[] data,
-                         int previousLen) throws IOException {
-    previous.isRepeating = true;
-    for (int i = 0; i < previousLen; i++) {
-      if (!previous.isNull[i]) {
-        data[i] = next();
-      } else {
-        // The default value of null for int type in vectorized
-        // processing is 1, so set that if the value is null
-        data[i] = 1;
-      }
-
-      // The default value for nulls in Vectorization for int types is 1
-      // and given that non null value can also be 1, we need to check for isNull also
-      // when determining the isRepeating flag.
-      if (previous.isRepeating
-          && i > 0
-          && (data[0] != data[i] ||
-          previous.isNull[0] != previous.isNull[i])) {
-        previous.isRepeating = false;
-      }
-    }
-  }
-
-  @Override
-  public void nextVector(ColumnVector vector,
-                         int[] data,
-                         int size) throws IOException {
-    if (vector.noNulls) {
-      for(int r=0; r < data.length && r < size; ++r) {
-        data[r] = (int) next();
-      }
-    } else if (!(vector.isRepeating && vector.isNull[0])) {
-      for(int r=0; r < data.length && r < size; ++r) {
-        if (!vector.isNull[r]) {
-          data[r] = (int) next();
-        } else {
-          data[r] = 1;
-        }
-      }
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/RunLengthIntegerWriter.java b/orc/src/java/org/apache/orc/impl/RunLengthIntegerWriter.java
deleted file mode 100644
index 3e5f2e20d6..0000000000
--- a/orc/src/java/org/apache/orc/impl/RunLengthIntegerWriter.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-
-/**
- * A streamFactory that writes a sequence of integers. A control byte is written before
- * each run with positive values 0 to 127 meaning 3 to 130 repetitions, each
- * repetition is offset by a delta. If the control byte is -1 to -128, 1 to 128
- * literal vint values follow.
- */
-public class RunLengthIntegerWriter implements IntegerWriter {
-  static final int MIN_REPEAT_SIZE = 3;
-  static final int MAX_DELTA = 127;
-  static final int MIN_DELTA = -128;
-  static final int MAX_LITERAL_SIZE = 128;
-  private static final int MAX_REPEAT_SIZE = 127 + MIN_REPEAT_SIZE;
-  private final PositionedOutputStream output;
-  private final boolean signed;
-  private final long[] literals = new long[MAX_LITERAL_SIZE];
-  private int numLiterals = 0;
-  private long delta = 0;
-  private boolean repeat = false;
-  private int tailRunLength = 0;
-  private SerializationUtils utils;
-
-  public RunLengthIntegerWriter(PositionedOutputStream output,
-                         boolean signed) {
-    this.output = output;
-    this.signed = signed;
-    this.utils = new SerializationUtils();
-  }
-
-  private void writeValues() throws IOException {
-    if (numLiterals != 0) {
-      if (repeat) {
-        output.write(numLiterals - MIN_REPEAT_SIZE);
-        output.write((byte) delta);
-        if (signed) {
-          utils.writeVslong(output, literals[0]);
-        } else {
-          utils.writeVulong(output, literals[0]);
-        }
-      } else {
-        output.write(-numLiterals);
-        for(int i=0; i < numLiterals; ++i) {
-          if (signed) {
-            utils.writeVslong(output, literals[i]);
-          } else {
-            utils.writeVulong(output, literals[i]);
-          }
-        }
-      }
-      repeat = false;
-      numLiterals = 0;
-      tailRunLength = 0;
-    }
-  }
-
-  @Override
-  public void flush() throws IOException {
-    writeValues();
-    output.flush();
-  }
-
-  @Override
-  public void write(long value) throws IOException {
-    if (numLiterals == 0) {
-      literals[numLiterals++] = value;
-      tailRunLength = 1;
-    } else if (repeat) {
-      if (value == literals[0] + delta * numLiterals) {
-        numLiterals += 1;
-        if (numLiterals == MAX_REPEAT_SIZE) {
-          writeValues();
-        }
-      } else {
-        writeValues();
-        literals[numLiterals++] = value;
-        tailRunLength = 1;
-      }
-    } else {
-      if (tailRunLength == 1) {
-        delta = value - literals[numLiterals - 1];
-        if (delta < MIN_DELTA || delta > MAX_DELTA) {
-          tailRunLength = 1;
-        } else {
-          tailRunLength = 2;
-        }
-      } else if (value == literals[numLiterals - 1] + delta) {
-        tailRunLength += 1;
-      } else {
-        delta = value - literals[numLiterals - 1];
-        if (delta < MIN_DELTA || delta > MAX_DELTA) {
-          tailRunLength = 1;
-        } else {
-          tailRunLength = 2;
-        }
-      }
-      if (tailRunLength == MIN_REPEAT_SIZE) {
-        if (numLiterals + 1 == MIN_REPEAT_SIZE) {
-          repeat = true;
-          numLiterals += 1;
-        } else {
-          numLiterals -= MIN_REPEAT_SIZE - 1;
-          long base = literals[numLiterals];
-          writeValues();
-          literals[0] = base;
-          repeat = true;
-          numLiterals = MIN_REPEAT_SIZE;
-        }
-      } else {
-        literals[numLiterals++] = value;
-        if (numLiterals == MAX_LITERAL_SIZE) {
-          writeValues();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void getPosition(PositionRecorder recorder) throws IOException {
-    output.getPosition(recorder);
-    recorder.addPosition(numLiterals);
-  }
-
-}
diff --git a/orc/src/java/org/apache/orc/impl/RunLengthIntegerWriterV2.java b/orc/src/java/org/apache/orc/impl/RunLengthIntegerWriterV2.java
deleted file mode 100644
index fab2801de2..0000000000
--- a/orc/src/java/org/apache/orc/impl/RunLengthIntegerWriterV2.java
+++ /dev/null
@@ -1,831 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-
-/**
- * A writer that performs light weight compression over sequence of integers.
- * <p>
- * There are four types of lightweight integer compression
- * <ul>
- * <li>SHORT_REPEAT</li>
- * <li>DIRECT</li>
- * <li>PATCHED_BASE</li>
- * <li>DELTA</li>
- * </ul>
- * </p>
- * The description and format for these types are as below:
- * <p>
- * <b>SHORT_REPEAT:</b> Used for short repeated integer sequences.
- * <ul>
- * <li>1 byte header
- * <ul>
- * <li>2 bits for encoding type</li>
- * <li>3 bits for bytes required for repeating value</li>
- * <li>3 bits for repeat count (MIN_REPEAT + run length)</li>
- * </ul>
- * </li>
- * <li>Blob - repeat value (fixed bytes)</li>
- * </ul>
- * </p>
- * <p>
- * <b>DIRECT:</b> Used for random integer sequences whose number of bit
- * requirement doesn't vary a lot.
- * <ul>
- * <li>2 bytes header
- * <ul>
- * 1st byte
- * <li>2 bits for encoding type</li>
- * <li>5 bits for fixed bit width of values in blob</li>
- * <li>1 bit for storing MSB of run length</li>
- * </ul>
- * <ul>
- * 2nd byte
- * <li>8 bits for lower run length bits</li>
- * </ul>
- * </li>
- * <li>Blob - stores the direct values using fixed bit width. The length of the
- * data blob is (fixed width * run length) bits long</li>
- * </ul>
- * </p>
- * <p>
- * <b>PATCHED_BASE:</b> Used for random integer sequences whose number of bit
- * requirement varies beyond a threshold.
- * <ul>
- * <li>4 bytes header
- * <ul>
- * 1st byte
- * <li>2 bits for encoding type</li>
- * <li>5 bits for fixed bit width of values in blob</li>
- * <li>1 bit for storing MSB of run length</li>
- * </ul>
- * <ul>
- * 2nd byte
- * <li>8 bits for lower run length bits</li>
- * </ul>
- * <ul>
- * 3rd byte
- * <li>3 bits for bytes required to encode base value</li>
- * <li>5 bits for patch width</li>
- * </ul>
- * <ul>
- * 4th byte
- * <li>3 bits for patch gap width</li>
- * <li>5 bits for patch length</li>
- * </ul>
- * </li>
- * <li>Base value - Stored using fixed number of bytes. If MSB is set, base
- * value is negative else positive. Length of base value is (base width * 8)
- * bits.</li>
- * <li>Data blob - Base reduced values as stored using fixed bit width. Length
- * of data blob is (fixed width * run length) bits.</li>
- * <li>Patch blob - Patch blob is a list of gap and patch value. Each entry in
- * the patch list is (patch width + patch gap width) bits long. Gap between the
- * subsequent elements to be patched are stored in upper part of entry whereas
- * patch values are stored in lower part of entry. Length of patch blob is
- * ((patch width + patch gap width) * patch length) bits.</li>
- * </ul>
- * </p>
- * <p>
- * <b>DELTA</b> Used for monotonically increasing or decreasing sequences,
- * sequences with fixed delta values or long repeated sequences.
- * <ul>
- * <li>2 bytes header
- * <ul>
- * 1st byte
- * <li>2 bits for encoding type</li>
- * <li>5 bits for fixed bit width of values in blob</li>
- * <li>1 bit for storing MSB of run length</li>
- * </ul>
- * <ul>
- * 2nd byte
- * <li>8 bits for lower run length bits</li>
- * </ul>
- * </li>
- * <li>Base value - zigzag encoded value written as varint</li>
- * <li>Delta base - zigzag encoded value written as varint</li>
- * <li>Delta blob - only positive values. monotonicity and orderness are decided
- * based on the sign of the base value and delta base</li>
- * </ul>
- * </p>
- */
-public class RunLengthIntegerWriterV2 implements IntegerWriter {
-
-  public enum EncodingType {
-    SHORT_REPEAT, DIRECT, PATCHED_BASE, DELTA
-  }
-
-  static final int MAX_SCOPE = 512;
-  static final int MIN_REPEAT = 3;
-  private static final int MAX_SHORT_REPEAT_LENGTH = 10;
-  private long prevDelta = 0;
-  private int fixedRunLength = 0;
-  private int variableRunLength = 0;
-  private final long[] literals = new long[MAX_SCOPE];
-  private final PositionedOutputStream output;
-  private final boolean signed;
-  private EncodingType encoding;
-  private int numLiterals;
-  private final long[] zigzagLiterals = new long[MAX_SCOPE];
-  private final long[] baseRedLiterals = new long[MAX_SCOPE];
-  private final long[] adjDeltas = new long[MAX_SCOPE];
-  private long fixedDelta;
-  private int zzBits90p;
-  private int zzBits100p;
-  private int brBits95p;
-  private int brBits100p;
-  private int bitsDeltaMax;
-  private int patchWidth;
-  private int patchGapWidth;
-  private int patchLength;
-  private long[] gapVsPatchList;
-  private long min;
-  private boolean isFixedDelta;
-  private SerializationUtils utils;
-  private boolean alignedBitpacking;
-
-  RunLengthIntegerWriterV2(PositionedOutputStream output, boolean signed) {
-    this(output, signed, true);
-  }
-
-  public RunLengthIntegerWriterV2(PositionedOutputStream output, boolean signed,
-      boolean alignedBitpacking) {
-    this.output = output;
-    this.signed = signed;
-    this.alignedBitpacking = alignedBitpacking;
-    this.utils = new SerializationUtils();
-    clear();
-  }
-
-  private void writeValues() throws IOException {
-    if (numLiterals != 0) {
-
-      if (encoding.equals(EncodingType.SHORT_REPEAT)) {
-        writeShortRepeatValues();
-      } else if (encoding.equals(EncodingType.DIRECT)) {
-        writeDirectValues();
-      } else if (encoding.equals(EncodingType.PATCHED_BASE)) {
-        writePatchedBaseValues();
-      } else {
-        writeDeltaValues();
-      }
-
-      // clear all the variables
-      clear();
-    }
-  }
-
-  private void writeDeltaValues() throws IOException {
-    int len = 0;
-    int fb = bitsDeltaMax;
-    int efb = 0;
-
-    if (alignedBitpacking) {
-      fb = utils.getClosestAlignedFixedBits(fb);
-    }
-
-    if (isFixedDelta) {
-      // if fixed run length is greater than threshold then it will be fixed
-      // delta sequence with delta value 0 else fixed delta sequence with
-      // non-zero delta value
-      if (fixedRunLength > MIN_REPEAT) {
-        // ex. sequence: 2 2 2 2 2 2 2 2
-        len = fixedRunLength - 1;
-        fixedRunLength = 0;
-      } else {
-        // ex. sequence: 4 6 8 10 12 14 16
-        len = variableRunLength - 1;
-        variableRunLength = 0;
-      }
-    } else {
-      // fixed width 0 is used for long repeating values.
-      // sequences that require only 1 bit to encode will have an additional bit
-      if (fb == 1) {
-        fb = 2;
-      }
-      efb = utils.encodeBitWidth(fb);
-      efb = efb << 1;
-      len = variableRunLength - 1;
-      variableRunLength = 0;
-    }
-
-    // extract the 9th bit of run length
-    final int tailBits = (len & 0x100) >>> 8;
-
-    // create first byte of the header
-    final int headerFirstByte = getOpcode() | efb | tailBits;
-
-    // second byte of the header stores the remaining 8 bits of runlength
-    final int headerSecondByte = len & 0xff;
-
-    // write header
-    output.write(headerFirstByte);
-    output.write(headerSecondByte);
-
-    // store the first value from zigzag literal array
-    if (signed) {
-      utils.writeVslong(output, literals[0]);
-    } else {
-      utils.writeVulong(output, literals[0]);
-    }
-
-    if (isFixedDelta) {
-      // if delta is fixed then we don't need to store delta blob
-      utils.writeVslong(output, fixedDelta);
-    } else {
-      // store the first value as delta value using zigzag encoding
-      utils.writeVslong(output, adjDeltas[0]);
-
-      // adjacent delta values are bit packed. The length of adjDeltas array is
-      // always one less than the number of literals (delta difference for n
-      // elements is n-1). We have already written one element, write the
-      // remaining numLiterals - 2 elements here
-      utils.writeInts(adjDeltas, 1, numLiterals - 2, fb, output);
-    }
-  }
-
-  private void writePatchedBaseValues() throws IOException {
-
-    // NOTE: Aligned bit packing cannot be applied for PATCHED_BASE encoding
-    // because patch is applied to MSB bits. For example: If fixed bit width of
-    // base value is 7 bits and if patch is 3 bits, the actual value is
-    // constructed by shifting the patch to left by 7 positions.
-    // actual_value = patch << 7 | base_value
-    // So, if we align base_value then actual_value can not be reconstructed.
-
-    // write the number of fixed bits required in next 5 bits
-    final int fb = brBits95p;
-    final int efb = utils.encodeBitWidth(fb) << 1;
-
-    // adjust variable run length, they are one off
-    variableRunLength -= 1;
-
-    // extract the 9th bit of run length
-    final int tailBits = (variableRunLength & 0x100) >>> 8;
-
-    // create first byte of the header
-    final int headerFirstByte = getOpcode() | efb | tailBits;
-
-    // second byte of the header stores the remaining 8 bits of runlength
-    final int headerSecondByte = variableRunLength & 0xff;
-
-    // if the min value is negative toggle the sign
-    final boolean isNegative = min < 0 ? true : false;
-    if (isNegative) {
-      min = -min;
-    }
-
-    // find the number of bytes required for base and shift it by 5 bits
-    // to accommodate patch width. The additional bit is used to store the sign
-    // of the base value.
-    final int baseWidth = utils.findClosestNumBits(min) + 1;
-    final int baseBytes = baseWidth % 8 == 0 ? baseWidth / 8 : (baseWidth / 8) + 1;
-    final int bb = (baseBytes - 1) << 5;
-
-    // if the base value is negative then set MSB to 1
-    if (isNegative) {
-      min |= (1L << ((baseBytes * 8) - 1));
-    }
-
-    // third byte contains 3 bits for number of bytes occupied by base
-    // and 5 bits for patchWidth
-    final int headerThirdByte = bb | utils.encodeBitWidth(patchWidth);
-
-    // fourth byte contains 3 bits for page gap width and 5 bits for
-    // patch length
-    final int headerFourthByte = (patchGapWidth - 1) << 5 | patchLength;
-
-    // write header
-    output.write(headerFirstByte);
-    output.write(headerSecondByte);
-    output.write(headerThirdByte);
-    output.write(headerFourthByte);
-
-    // write the base value using fixed bytes in big endian order
-    for(int i = baseBytes - 1; i >= 0; i--) {
-      byte b = (byte) ((min >>> (i * 8)) & 0xff);
-      output.write(b);
-    }
-
-    // base reduced literals are bit packed
-    int closestFixedBits = utils.getClosestFixedBits(fb);
-
-    utils.writeInts(baseRedLiterals, 0, numLiterals, closestFixedBits,
-        output);
-
-    // write patch list
-    closestFixedBits = utils.getClosestFixedBits(patchGapWidth + patchWidth);
-
-    utils.writeInts(gapVsPatchList, 0, gapVsPatchList.length, closestFixedBits,
-        output);
-
-    // reset run length
-    variableRunLength = 0;
-  }
-
-  /**
-   * Store the opcode in 2 MSB bits
-   * @return opcode
-   */
-  private int getOpcode() {
-    return encoding.ordinal() << 6;
-  }
-
-  private void writeDirectValues() throws IOException {
-
-    // write the number of fixed bits required in next 5 bits
-    int fb = zzBits100p;
-
-    if (alignedBitpacking) {
-      fb = utils.getClosestAlignedFixedBits(fb);
-    }
-
-    final int efb = utils.encodeBitWidth(fb) << 1;
-
-    // adjust variable run length
-    variableRunLength -= 1;
-
-    // extract the 9th bit of run length
-    final int tailBits = (variableRunLength & 0x100) >>> 8;
-
-    // create first byte of the header
-    final int headerFirstByte = getOpcode() | efb | tailBits;
-
-    // second byte of the header stores the remaining 8 bits of runlength
-    final int headerSecondByte = variableRunLength & 0xff;
-
-    // write header
-    output.write(headerFirstByte);
-    output.write(headerSecondByte);
-
-    // bit packing the zigzag encoded literals
-    utils.writeInts(zigzagLiterals, 0, numLiterals, fb, output);
-
-    // reset run length
-    variableRunLength = 0;
-  }
-
-  private void writeShortRepeatValues() throws IOException {
-    // get the value that is repeating, compute the bits and bytes required
-    long repeatVal = 0;
-    if (signed) {
-      repeatVal = utils.zigzagEncode(literals[0]);
-    } else {
-      repeatVal = literals[0];
-    }
-
-    final int numBitsRepeatVal = utils.findClosestNumBits(repeatVal);
-    final int numBytesRepeatVal = numBitsRepeatVal % 8 == 0 ? numBitsRepeatVal >>> 3
-        : (numBitsRepeatVal >>> 3) + 1;
-
-    // write encoding type in top 2 bits
-    int header = getOpcode();
-
-    // write the number of bytes required for the value
-    header |= ((numBytesRepeatVal - 1) << 3);
-
-    // write the run length
-    fixedRunLength -= MIN_REPEAT;
-    header |= fixedRunLength;
-
-    // write the header
-    output.write(header);
-
-    // write the repeating value in big endian byte order
-    for(int i = numBytesRepeatVal - 1; i >= 0; i--) {
-      int b = (int) ((repeatVal >>> (i * 8)) & 0xff);
-      output.write(b);
-    }
-
-    fixedRunLength = 0;
-  }
-
-  private void determineEncoding() {
-
-    // we need to compute zigzag values for DIRECT encoding if we decide to
-    // break early for delta overflows or for shorter runs
-    computeZigZagLiterals();
-
-    zzBits100p = utils.percentileBits(zigzagLiterals, 0, numLiterals, 1.0);
-
-    // not a big win for shorter runs to determine encoding
-    if (numLiterals <= MIN_REPEAT) {
-      encoding = EncodingType.DIRECT;
-      return;
-    }
-
-    // DELTA encoding check
-
-    // for identifying monotonic sequences
-    boolean isIncreasing = true;
-    boolean isDecreasing = true;
-    this.isFixedDelta = true;
-
-    this.min = literals[0];
-    long max = literals[0];
-    final long initialDelta = literals[1] - literals[0];
-    long currDelta = initialDelta;
-    long deltaMax = initialDelta;
-    this.adjDeltas[0] = initialDelta;
-
-    for (int i = 1; i < numLiterals; i++) {
-      final long l1 = literals[i];
-      final long l0 = literals[i - 1];
-      currDelta = l1 - l0;
-      min = Math.min(min, l1);
-      max = Math.max(max, l1);
-
-      isIncreasing &= (l0 <= l1);
-      isDecreasing &= (l0 >= l1);
-
-      isFixedDelta &= (currDelta == initialDelta);
-      if (i > 1) {
-        adjDeltas[i - 1] = Math.abs(currDelta);
-        deltaMax = Math.max(deltaMax, adjDeltas[i - 1]);
-      }
-    }
-
-    // its faster to exit under delta overflow condition without checking for
-    // PATCHED_BASE condition as encoding using DIRECT is faster and has less
-    // overhead than PATCHED_BASE
-    if (!utils.isSafeSubtract(max, min)) {
-      encoding = EncodingType.DIRECT;
-      return;
-    }
-
-    // invariant - subtracting any number from any other in the literals after
-    // this point won't overflow
-
-    // if min is equal to max then the delta is 0, this condition happens for
-    // fixed values run >10 which cannot be encoded with SHORT_REPEAT
-    if (min == max) {
-      assert isFixedDelta : min + "==" + max +
-          ", isFixedDelta cannot be false";
-      assert currDelta == 0 : min + "==" + max + ", currDelta should be zero";
-      fixedDelta = 0;
-      encoding = EncodingType.DELTA;
-      return;
-    }
-
-    if (isFixedDelta) {
-      assert currDelta == initialDelta
-          : "currDelta should be equal to initialDelta for fixed delta encoding";
-      encoding = EncodingType.DELTA;
-      fixedDelta = currDelta;
-      return;
-    }
-
-    // if initialDelta is 0 then we cannot delta encode as we cannot identify
-    // the sign of deltas (increasing or decreasing)
-    if (initialDelta != 0) {
-      // stores the number of bits required for packing delta blob in
-      // delta encoding
-      bitsDeltaMax = utils.findClosestNumBits(deltaMax);
-
-      // monotonic condition
-      if (isIncreasing || isDecreasing) {
-        encoding = EncodingType.DELTA;
-        return;
-      }
-    }
-
-    // PATCHED_BASE encoding check
-
-    // percentile values are computed for the zigzag encoded values. if the
-    // number of bit requirement between 90th and 100th percentile varies
-    // beyond a threshold then we need to patch the values. if the variation
-    // is not significant then we can use direct encoding
-
-    zzBits90p = utils.percentileBits(zigzagLiterals, 0, numLiterals, 0.9);
-    int diffBitsLH = zzBits100p - zzBits90p;
-
-    // if the difference between 90th percentile and 100th percentile fixed
-    // bits is > 1 then we need patch the values
-    if (diffBitsLH > 1) {
-
-      // patching is done only on base reduced values.
-      // remove base from literals
-      for (int i = 0; i < numLiterals; i++) {
-        baseRedLiterals[i] = literals[i] - min;
-      }
-
-      // 95th percentile width is used to determine max allowed value
-      // after which patching will be done
-      brBits95p = utils.percentileBits(baseRedLiterals, 0, numLiterals, 0.95);
-
-      // 100th percentile is used to compute the max patch width
-      brBits100p = utils.percentileBits(baseRedLiterals, 0, numLiterals, 1.0);
-
-      // after base reducing the values, if the difference in bits between
-      // 95th percentile and 100th percentile value is zero then there
-      // is no point in patching the values, in which case we will
-      // fallback to DIRECT encoding.
-      // The decision to use patched base was based on zigzag values, but the
-      // actual patching is done on base reduced literals.
-      if ((brBits100p - brBits95p) != 0) {
-        encoding = EncodingType.PATCHED_BASE;
-        preparePatchedBlob();
-        return;
-      } else {
-        encoding = EncodingType.DIRECT;
-        return;
-      }
-    } else {
-      // if difference in bits between 95th percentile and 100th percentile is
-      // 0, then patch length will become 0. Hence we will fallback to direct
-      encoding = EncodingType.DIRECT;
-      return;
-    }
-  }
-
-  private void computeZigZagLiterals() {
-    // populate zigzag encoded literals
-    long zzEncVal = 0;
-    for (int i = 0; i < numLiterals; i++) {
-      if (signed) {
-        zzEncVal = utils.zigzagEncode(literals[i]);
-      } else {
-        zzEncVal = literals[i];
-      }
-      zigzagLiterals[i] = zzEncVal;
-    }
-  }
-
-  private void preparePatchedBlob() {
-    // mask will be max value beyond which patch will be generated
-    long mask = (1L << brBits95p) - 1;
-
-    // since we are considering only 95 percentile, the size of gap and
-    // patch array can contain only be 5% values
-    patchLength = (int) Math.ceil((numLiterals * 0.05));
-
-    int[] gapList = new int[patchLength];
-    long[] patchList = new long[patchLength];
-
-    // #bit for patch
-    patchWidth = brBits100p - brBits95p;
-    patchWidth = utils.getClosestFixedBits(patchWidth);
-
-    // if patch bit requirement is 64 then it will not possible to pack
-    // gap and patch together in a long. To make sure gap and patch can be
-    // packed together adjust the patch width
-    if (patchWidth == 64) {
-      patchWidth = 56;
-      brBits95p = 8;
-      mask = (1L << brBits95p) - 1;
-    }
-
-    int gapIdx = 0;
-    int patchIdx = 0;
-    int prev = 0;
-    int gap = 0;
-    int maxGap = 0;
-
-    for(int i = 0; i < numLiterals; i++) {
-      // if value is above mask then create the patch and record the gap
-      if (baseRedLiterals[i] > mask) {
-        gap = i - prev;
-        if (gap > maxGap) {
-          maxGap = gap;
-        }
-
-        // gaps are relative, so store the previous patched value index
-        prev = i;
-        gapList[gapIdx++] = gap;
-
-        // extract the most significant bits that are over mask bits
-        long patch = baseRedLiterals[i] >>> brBits95p;
-        patchList[patchIdx++] = patch;
-
-        // strip off the MSB to enable safe bit packing
-        baseRedLiterals[i] &= mask;
-      }
-    }
-
-    // adjust the patch length to number of entries in gap list
-    patchLength = gapIdx;
-
-    // if the element to be patched is the first and only element then
-    // max gap will be 0, but to store the gap as 0 we need atleast 1 bit
-    if (maxGap == 0 && patchLength != 0) {
-      patchGapWidth = 1;
-    } else {
-      patchGapWidth = utils.findClosestNumBits(maxGap);
-    }
-
-    // special case: if the patch gap width is greater than 256, then
-    // we need 9 bits to encode the gap width. But we only have 3 bits in
-    // header to record the gap width. To deal with this case, we will save
-    // two entries in patch list in the following way
-    // 256 gap width => 0 for patch value
-    // actual gap - 256 => actual patch value
-    // We will do the same for gap width = 511. If the element to be patched is
-    // the last element in the scope then gap width will be 511. In this case we
-    // will have 3 entries in the patch list in the following way
-    // 255 gap width => 0 for patch value
-    // 255 gap width => 0 for patch value
-    // 1 gap width => actual patch value
-    if (patchGapWidth > 8) {
-      patchGapWidth = 8;
-      // for gap = 511, we need two additional entries in patch list
-      if (maxGap == 511) {
-        patchLength += 2;
-      } else {
-        patchLength += 1;
-      }
-    }
-
-    // create gap vs patch list
-    gapIdx = 0;
-    patchIdx = 0;
-    gapVsPatchList = new long[patchLength];
-    for(int i = 0; i < patchLength; i++) {
-      long g = gapList[gapIdx++];
-      long p = patchList[patchIdx++];
-      while (g > 255) {
-        gapVsPatchList[i++] = (255L << patchWidth);
-        g -= 255;
-      }
-
-      // store patch value in LSBs and gap in MSBs
-      gapVsPatchList[i] = (g << patchWidth) | p;
-    }
-  }
-
-  /**
-   * clears all the variables
-   */
-  private void clear() {
-    numLiterals = 0;
-    encoding = null;
-    prevDelta = 0;
-    fixedDelta = 0;
-    zzBits90p = 0;
-    zzBits100p = 0;
-    brBits95p = 0;
-    brBits100p = 0;
-    bitsDeltaMax = 0;
-    patchGapWidth = 0;
-    patchLength = 0;
-    patchWidth = 0;
-    gapVsPatchList = null;
-    min = 0;
-    isFixedDelta = true;
-  }
-
-  @Override
-  public void flush() throws IOException {
-    if (numLiterals != 0) {
-      if (variableRunLength != 0) {
-        determineEncoding();
-        writeValues();
-      } else if (fixedRunLength != 0) {
-        if (fixedRunLength < MIN_REPEAT) {
-          variableRunLength = fixedRunLength;
-          fixedRunLength = 0;
-          determineEncoding();
-          writeValues();
-        } else if (fixedRunLength >= MIN_REPEAT
-            && fixedRunLength <= MAX_SHORT_REPEAT_LENGTH) {
-          encoding = EncodingType.SHORT_REPEAT;
-          writeValues();
-        } else {
-          encoding = EncodingType.DELTA;
-          isFixedDelta = true;
-          writeValues();
-        }
-      }
-    }
-    output.flush();
-  }
-
-  @Override
-  public void write(long val) throws IOException {
-    if (numLiterals == 0) {
-      initializeLiterals(val);
-    } else {
-      if (numLiterals == 1) {
-        prevDelta = val - literals[0];
-        literals[numLiterals++] = val;
-        // if both values are same count as fixed run else variable run
-        if (val == literals[0]) {
-          fixedRunLength = 2;
-          variableRunLength = 0;
-        } else {
-          fixedRunLength = 0;
-          variableRunLength = 2;
-        }
-      } else {
-        long currentDelta = val - literals[numLiterals - 1];
-        if (prevDelta == 0 && currentDelta == 0) {
-          // fixed delta run
-
-          literals[numLiterals++] = val;
-
-          // if variable run is non-zero then we are seeing repeating
-          // values at the end of variable run in which case keep
-          // updating variable and fixed runs
-          if (variableRunLength > 0) {
-            fixedRunLength = 2;
-          }
-          fixedRunLength += 1;
-
-          // if fixed run met the minimum condition and if variable
-          // run is non-zero then flush the variable run and shift the
-          // tail fixed runs to start of the buffer
-          if (fixedRunLength >= MIN_REPEAT && variableRunLength > 0) {
-            numLiterals -= MIN_REPEAT;
-            variableRunLength -= MIN_REPEAT - 1;
-            // copy the tail fixed runs
-            long[] tailVals = new long[MIN_REPEAT];
-            System.arraycopy(literals, numLiterals, tailVals, 0, MIN_REPEAT);
-
-            // determine variable encoding and flush values
-            determineEncoding();
-            writeValues();
-
-            // shift tail fixed runs to beginning of the buffer
-            for(long l : tailVals) {
-              literals[numLiterals++] = l;
-            }
-          }
-
-          // if fixed runs reached max repeat length then write values
-          if (fixedRunLength == MAX_SCOPE) {
-            determineEncoding();
-            writeValues();
-          }
-        } else {
-          // variable delta run
-
-          // if fixed run length is non-zero and if it satisfies the
-          // short repeat conditions then write the values as short repeats
-          // else use delta encoding
-          if (fixedRunLength >= MIN_REPEAT) {
-            if (fixedRunLength <= MAX_SHORT_REPEAT_LENGTH) {
-              encoding = EncodingType.SHORT_REPEAT;
-              writeValues();
-            } else {
-              encoding = EncodingType.DELTA;
-              isFixedDelta = true;
-              writeValues();
-            }
-          }
-
-          // if fixed run length is <MIN_REPEAT and current value is
-          // different from previous then treat it as variable run
-          if (fixedRunLength > 0 && fixedRunLength < MIN_REPEAT) {
-            if (val != literals[numLiterals - 1]) {
-              variableRunLength = fixedRunLength;
-              fixedRunLength = 0;
-            }
-          }
-
-          // after writing values re-initialize the variables
-          if (numLiterals == 0) {
-            initializeLiterals(val);
-          } else {
-            // keep updating variable run lengths
-            prevDelta = val - literals[numLiterals - 1];
-            literals[numLiterals++] = val;
-            variableRunLength += 1;
-
-            // if variable run length reach the max scope, write it
-            if (variableRunLength == MAX_SCOPE) {
-              determineEncoding();
-              writeValues();
-            }
-          }
-        }
-      }
-    }
-  }
-
-  private void initializeLiterals(long val) {
-    literals[numLiterals++] = val;
-    fixedRunLength = 1;
-    variableRunLength = 1;
-  }
-
-  @Override
-  public void getPosition(PositionRecorder recorder) throws IOException {
-    output.getPosition(recorder);
-    recorder.addPosition(numLiterals);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/SchemaEvolution.java b/orc/src/java/org/apache/orc/impl/SchemaEvolution.java
deleted file mode 100644
index bb5bcf7a4d..0000000000
--- a/orc/src/java/org/apache/orc/impl/SchemaEvolution.java
+++ /dev/null
@@ -1,399 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.orc.TypeDescription;
-
-/**
- * Take the file types and the (optional) configuration column names/types and see if there
- * has been schema evolution.
- */
-public class SchemaEvolution {
-  // indexed by reader column id
-  private final TypeDescription[] readerFileTypes;
-  // indexed by reader column id
-  private final boolean[] readerIncluded;
-  // the offset to the first column id ignoring any ACID columns
-  private final int readerColumnOffset;
-  // indexed by file column id
-  private final boolean[] fileIncluded;
-  private final TypeDescription fileSchema;
-  private final TypeDescription readerSchema;
-  private boolean hasConversion;
-  // indexed by reader column id
-  private final boolean[] ppdSafeConversion;
-
-  public SchemaEvolution(TypeDescription fileSchema, boolean[] includedCols) {
-    this(fileSchema, null, includedCols);
-  }
-
-  public SchemaEvolution(TypeDescription fileSchema,
-                         TypeDescription readerSchema,
-                         boolean[] includeCols) {
-    this.readerIncluded = includeCols == null ? null : Arrays.copyOf(includeCols, includeCols.length);
-    this.hasConversion = false;
-    this.fileSchema = fileSchema;
-    boolean isAcid = checkAcidSchema(fileSchema);
-    this.readerColumnOffset = isAcid ? acidEventFieldNames.size() : 0;
-    if (readerSchema != null) {
-      if (isAcid) {
-        this.readerSchema = createEventSchema(readerSchema);
-      } else {
-        this.readerSchema = readerSchema;
-      }
-      if (readerIncluded != null &&
-          readerIncluded.length + readerColumnOffset != this.readerSchema.getMaximumId() + 1) {
-        throw new IllegalArgumentException("Include vector the wrong length: " +
-            this.readerSchema.toJson() + " with include length " +
-            readerIncluded.length);
-      }
-      this.readerFileTypes = new TypeDescription[this.readerSchema.getMaximumId() + 1];
-      this.fileIncluded = new boolean[fileSchema.getMaximumId() + 1];
-      buildConversionFileTypesArray(fileSchema, this.readerSchema);
-    } else {
-      this.readerSchema = fileSchema;
-      this.readerFileTypes = new TypeDescription[this.readerSchema.getMaximumId() + 1];
-      this.fileIncluded = readerIncluded;
-      if (readerIncluded != null &&
-          readerIncluded.length + readerColumnOffset != this.readerSchema.getMaximumId() + 1) {
-        throw new IllegalArgumentException("Include vector the wrong length: " +
-            this.readerSchema.toJson() + " with include length " +
-            readerIncluded.length);
-      }
-      buildSameSchemaFileTypesArray();
-    }
-    this.ppdSafeConversion = populatePpdSafeConversion();
-  }
-
-  public TypeDescription getReaderSchema() {
-    return readerSchema;
-  }
-
-  /**
-   * Returns the non-ACID (aka base) reader type description.
-   *
-   * @return the reader type ignoring the ACID rowid columns, if any
-   */
-  public TypeDescription getReaderBaseSchema() {
-    return readerSchema.findSubtype(readerColumnOffset);
-  }
-
-  /**
-   * Is there Schema Evolution data type conversion?
-   * @return
-   */
-  public boolean hasConversion() {
-    return hasConversion;
-  }
-
-  public TypeDescription getFileType(TypeDescription readerType) {
-    return getFileType(readerType.getId());
-  }
-
-  /**
-   * Get whether each column is included from the reader's point of view.
-   * @return a boolean array indexed by reader column id
-   */
-  public boolean[] getReaderIncluded() {
-    return readerIncluded;
-  }
-
-  /**
-   * Get whether each column is included from the file's point of view.
-   * @return a boolean array indexed by file column id
-   */
-  public boolean[] getFileIncluded() {
-    return fileIncluded;
-  }
-
-  /**
-   * Get the file type by reader type id.
-   * @param id reader column id
-   * @return
-   */
-  public TypeDescription getFileType(int id) {
-    return readerFileTypes[id];
-  }
-
-  /**
-   * Check if column is safe for ppd evaluation
-   * @param colId reader column id
-   * @return true if the specified column is safe for ppd evaluation else false
-   */
-  public boolean isPPDSafeConversion(final int colId) {
-    if (hasConversion()) {
-      if (colId < 0 || colId >= ppdSafeConversion.length) {
-        return false;
-      }
-      return ppdSafeConversion[colId];
-    }
-
-    // when there is no schema evolution PPD is safe
-    return true;
-  }
-
-  private boolean[] populatePpdSafeConversion() {
-    if (fileSchema == null || readerSchema == null || readerFileTypes == null) {
-      return null;
-    }
-
-    boolean[] result = new boolean[readerSchema.getMaximumId() + 1];
-    boolean safePpd = validatePPDConversion(fileSchema, readerSchema);
-    result[readerSchema.getId()] = safePpd;
-    List<TypeDescription> children = readerSchema.getChildren();
-    if (children != null) {
-      for (TypeDescription child : children) {
-        TypeDescription fileType = getFileType(child.getId());
-        safePpd = validatePPDConversion(fileType, child);
-        result[child.getId()] = safePpd;
-      }
-    }
-    return result;
-  }
-
-  private boolean validatePPDConversion(final TypeDescription fileType,
-      final TypeDescription readerType) {
-    if (fileType == null) {
-      return false;
-    }
-    if (fileType.getCategory().isPrimitive()) {
-      if (fileType.getCategory().equals(readerType.getCategory())) {
-        // for decimals alone do equality check to not mess up with precision change
-        if (fileType.getCategory().equals(TypeDescription.Category.DECIMAL) &&
-            !fileType.equals(readerType)) {
-          return false;
-        }
-        return true;
-      }
-
-      // only integer and string evolutions are safe
-      // byte -> short -> int -> long
-      // string <-> char <-> varchar
-      // NOTE: Float to double evolution is not safe as floats are stored as doubles in ORC's
-      // internal index, but when doing predicate evaluation for queries like "select * from
-      // orc_float where f = 74.72" the constant on the filter is converted from string -> double
-      // so the precisions will be different and the comparison will fail.
-      // Soon, we should convert all sargs that compare equality between floats or
-      // doubles to range predicates.
-
-      // Similarly string -> char and varchar -> char and vice versa is not possible, as ORC stores
-      // char with padded spaces in its internal index.
-      switch (fileType.getCategory()) {
-        case BYTE:
-          if (readerType.getCategory().equals(TypeDescription.Category.SHORT) ||
-              readerType.getCategory().equals(TypeDescription.Category.INT) ||
-              readerType.getCategory().equals(TypeDescription.Category.LONG)) {
-            return true;
-          }
-          break;
-        case SHORT:
-          if (readerType.getCategory().equals(TypeDescription.Category.INT) ||
-              readerType.getCategory().equals(TypeDescription.Category.LONG)) {
-            return true;
-          }
-          break;
-        case INT:
-          if (readerType.getCategory().equals(TypeDescription.Category.LONG)) {
-            return true;
-          }
-          break;
-        case STRING:
-          if (readerType.getCategory().equals(TypeDescription.Category.VARCHAR)) {
-            return true;
-          }
-          break;
-        case VARCHAR:
-          if (readerType.getCategory().equals(TypeDescription.Category.STRING)) {
-            return true;
-          }
-          break;
-        default:
-          break;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Should we read the given reader column?
-   * @param readerId the id of column in the extended reader schema
-   * @return true if the column should be read
-   */
-  public boolean includeReaderColumn(int readerId) {
-    return readerIncluded == null ||
-        readerId <= readerColumnOffset ||
-        readerIncluded[readerId - readerColumnOffset];
-  }
-
-  void buildConversionFileTypesArray(TypeDescription fileType,
-                                     TypeDescription readerType) {
-    // if the column isn't included, don't map it
-    int readerId = readerType.getId();
-    if (!includeReaderColumn(readerId)) {
-      return;
-    }
-    boolean isOk = true;
-    // check the easy case first
-    if (fileType.getCategory() == readerType.getCategory()) {
-      switch (readerType.getCategory()) {
-        case BOOLEAN:
-        case BYTE:
-        case SHORT:
-        case INT:
-        case LONG:
-        case DOUBLE:
-        case FLOAT:
-        case STRING:
-        case TIMESTAMP:
-        case BINARY:
-        case DATE:
-          // these are always a match
-          break;
-        case CHAR:
-        case VARCHAR:
-          // We do conversion when same CHAR/VARCHAR type but different maxLength.
-          if (fileType.getMaxLength() != readerType.getMaxLength()) {
-            hasConversion = true;
-          }
-          break;
-        case DECIMAL:
-          // We do conversion when same DECIMAL type but different precision/scale.
-          if (fileType.getPrecision() != readerType.getPrecision() ||
-              fileType.getScale() != readerType.getScale()) {
-            hasConversion = true;
-          }
-          break;
-        case UNION:
-        case MAP:
-        case LIST: {
-          // these must be an exact match
-          List<TypeDescription> fileChildren = fileType.getChildren();
-          List<TypeDescription> readerChildren = readerType.getChildren();
-          if (fileChildren.size() == readerChildren.size()) {
-            for(int i=0; i < fileChildren.size(); ++i) {
-              buildConversionFileTypesArray(fileChildren.get(i), readerChildren.get(i));
-            }
-          } else {
-            isOk = false;
-          }
-          break;
-        }
-        case STRUCT: {
-          // allow either side to have fewer fields than the other
-          List<TypeDescription> fileChildren = fileType.getChildren();
-          List<TypeDescription> readerChildren = readerType.getChildren();
-          if (fileChildren.size() != readerChildren.size()) {
-            hasConversion = true;
-          }
-          int jointSize = Math.min(fileChildren.size(), readerChildren.size());
-          for(int i=0; i < jointSize; ++i) {
-            buildConversionFileTypesArray(fileChildren.get(i), readerChildren.get(i));
-          }
-          break;
-        }
-        default:
-          throw new IllegalArgumentException("Unknown type " + readerType);
-      }
-    } else {
-      /*
-       * Check for the few cases where will not convert....
-       */
-
-      isOk = ConvertTreeReaderFactory.canConvert(fileType, readerType);
-      hasConversion = true;
-    }
-    if (isOk) {
-      if (readerFileTypes[readerId] != null) {
-        throw new RuntimeException("reader to file type entry already assigned");
-      }
-      readerFileTypes[readerId] = fileType;
-      fileIncluded[fileType.getId()] = true;
-    } else {
-      throw new IllegalArgumentException(
-          String.format(
-              "ORC does not support type conversion from file type %s (%d) to reader type %s (%d)",
-              fileType.toString(), fileType.getId(),
-              readerType.toString(), readerId));
-    }
-  }
-
-  /**
-   * Use to make a reader to file type array when the schema is the same.
-   * @return
-   */
-  private void buildSameSchemaFileTypesArray() {
-    buildSameSchemaFileTypesArrayRecurse(readerSchema);
-  }
-
-  void buildSameSchemaFileTypesArrayRecurse(TypeDescription readerType) {
-    int id = readerType.getId();
-    if (!includeReaderColumn(id)) {
-      return;
-    }
-    if (readerFileTypes[id] != null) {
-      throw new RuntimeException("reader to file type entry already assigned");
-    }
-    readerFileTypes[id] = readerType;
-    List<TypeDescription> children = readerType.getChildren();
-    if (children != null) {
-      for (TypeDescription child : children) {
-        buildSameSchemaFileTypesArrayRecurse(child);
-      }
-    }
-  }
-
-  private static boolean checkAcidSchema(TypeDescription type) {
-    if (type.getCategory().equals(TypeDescription.Category.STRUCT)) {
-      List<String> rootFields = type.getFieldNames();
-      if (acidEventFieldNames.equals(rootFields)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * @param typeDescr
-   * @return ORC types for the ACID event based on the row's type description
-   */
-  public static TypeDescription createEventSchema(TypeDescription typeDescr) {
-    TypeDescription result = TypeDescription.createStruct()
-        .addField("operation", TypeDescription.createInt())
-        .addField("originalTransaction", TypeDescription.createLong())
-        .addField("bucket", TypeDescription.createInt())
-        .addField("rowId", TypeDescription.createLong())
-        .addField("currentTransaction", TypeDescription.createLong())
-        .addField("row", typeDescr.clone());
-    return result;
-  }
-
-  public static final List<String> acidEventFieldNames= new ArrayList<String>();
-  static {
-    acidEventFieldNames.add("operation");
-    acidEventFieldNames.add("originalTransaction");
-    acidEventFieldNames.add("bucket");
-    acidEventFieldNames.add("rowId");
-    acidEventFieldNames.add("currentTransaction");
-    acidEventFieldNames.add("row");
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/SerializationUtils.java b/orc/src/java/org/apache/orc/impl/SerializationUtils.java
deleted file mode 100644
index 2e5a59bb81..0000000000
--- a/orc/src/java/org/apache/orc/impl/SerializationUtils.java
+++ /dev/null
@@ -1,1311 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.math.BigInteger;
-
-public final class SerializationUtils {
-
-  private final static int BUFFER_SIZE = 64;
-  private final byte[] readBuffer;
-  private final byte[] writeBuffer;
-
-  public SerializationUtils() {
-    this.readBuffer = new byte[BUFFER_SIZE];
-    this.writeBuffer = new byte[BUFFER_SIZE];
-  }
-
-  public void writeVulong(OutputStream output,
-                          long value) throws IOException {
-    while (true) {
-      if ((value & ~0x7f) == 0) {
-        output.write((byte) value);
-        return;
-      } else {
-        output.write((byte) (0x80 | (value & 0x7f)));
-        value >>>= 7;
-      }
-    }
-  }
-
-  public void writeVslong(OutputStream output,
-                          long value) throws IOException {
-    writeVulong(output, (value << 1) ^ (value >> 63));
-  }
-
-
-  public long readVulong(InputStream in) throws IOException {
-    long result = 0;
-    long b;
-    int offset = 0;
-    do {
-      b = in.read();
-      if (b == -1) {
-        throw new EOFException("Reading Vulong past EOF");
-      }
-      result |= (0x7f & b) << offset;
-      offset += 7;
-    } while (b >= 0x80);
-    return result;
-  }
-
-  public long readVslong(InputStream in) throws IOException {
-    long result = readVulong(in);
-    return (result >>> 1) ^ -(result & 1);
-  }
-
-  public float readFloat(InputStream in) throws IOException {
-    readFully(in, readBuffer, 0, 4);
-    int val = (((readBuffer[0] & 0xff) << 0)
-        + ((readBuffer[1] & 0xff) << 8)
-        + ((readBuffer[2] & 0xff) << 16)
-        + ((readBuffer[3] & 0xff) << 24));
-    return Float.intBitsToFloat(val);
-  }
-
-  public void writeFloat(OutputStream output,
-                         float value) throws IOException {
-    int ser = Float.floatToIntBits(value);
-    writeBuffer[0] = (byte) ((ser >> 0)  & 0xff);
-    writeBuffer[1] = (byte) ((ser >> 8)  & 0xff);
-    writeBuffer[2] = (byte) ((ser >> 16) & 0xff);
-    writeBuffer[3] = (byte) ((ser >> 24) & 0xff);
-    output.write(writeBuffer, 0, 4);
-  }
-
-  public double readDouble(InputStream in) throws IOException {
-    return Double.longBitsToDouble(readLongLE(in));
-  }
-
-  public long readLongLE(InputStream in) throws IOException {
-    readFully(in, readBuffer, 0, 8);
-    return (((readBuffer[0] & 0xff) << 0)
-        + ((readBuffer[1] & 0xff) << 8)
-        + ((readBuffer[2] & 0xff) << 16)
-        + ((long) (readBuffer[3] & 0xff) << 24)
-        + ((long) (readBuffer[4] & 0xff) << 32)
-        + ((long) (readBuffer[5] & 0xff) << 40)
-        + ((long) (readBuffer[6] & 0xff) << 48)
-        + ((long) (readBuffer[7] & 0xff) << 56));
-  }
-
-  private void readFully(final InputStream in, final byte[] buffer, final int off, final int len)
-      throws IOException {
-    int n = 0;
-    while (n < len) {
-      int count = in.read(buffer, off + n, len - n);
-      if (count < 0) {
-        throw new EOFException("Read past EOF for " + in);
-      }
-      n += count;
-    }
-  }
-
-  public void writeDouble(OutputStream output,
-                          double value) throws IOException {
-    writeLongLE(output, Double.doubleToLongBits(value));
-  }
-
-  private void writeLongLE(OutputStream output, long value) throws IOException {
-    writeBuffer[0] = (byte) ((value >> 0)  & 0xff);
-    writeBuffer[1] = (byte) ((value >> 8)  & 0xff);
-    writeBuffer[2] = (byte) ((value >> 16) & 0xff);
-    writeBuffer[3] = (byte) ((value >> 24) & 0xff);
-    writeBuffer[4] = (byte) ((value >> 32) & 0xff);
-    writeBuffer[5] = (byte) ((value >> 40) & 0xff);
-    writeBuffer[6] = (byte) ((value >> 48) & 0xff);
-    writeBuffer[7] = (byte) ((value >> 56) & 0xff);
-    output.write(writeBuffer, 0, 8);
-  }
-
-  /**
-   * Write the arbitrarily sized signed BigInteger in vint format.
-   *
-   * Signed integers are encoded using the low bit as the sign bit using zigzag
-   * encoding.
-   *
-   * Each byte uses the low 7 bits for data and the high bit for stop/continue.
-   *
-   * Bytes are stored LSB first.
-   * @param output the stream to write to
-   * @param value the value to output
-   * @throws IOException
-   */
-  public static void writeBigInteger(OutputStream output,
-                                     BigInteger value) throws IOException {
-    // encode the signed number as a positive integer
-    value = value.shiftLeft(1);
-    int sign = value.signum();
-    if (sign < 0) {
-      value = value.negate();
-      value = value.subtract(BigInteger.ONE);
-    }
-    int length = value.bitLength();
-    while (true) {
-      long lowBits = value.longValue() & 0x7fffffffffffffffL;
-      length -= 63;
-      // write out the next 63 bits worth of data
-      for(int i=0; i < 9; ++i) {
-        // if this is the last byte, leave the high bit off
-        if (length <= 0 && (lowBits & ~0x7f) == 0) {
-          output.write((byte) lowBits);
-          return;
-        } else {
-          output.write((byte) (0x80 | (lowBits & 0x7f)));
-          lowBits >>>= 7;
-        }
-      }
-      value = value.shiftRight(63);
-    }
-  }
-
-  /**
-   * Read the signed arbitrary sized BigInteger BigInteger in vint format
-   * @param input the stream to read from
-   * @return the read BigInteger
-   * @throws IOException
-   */
-  public static BigInteger readBigInteger(InputStream input) throws IOException {
-    BigInteger result = BigInteger.ZERO;
-    long work = 0;
-    int offset = 0;
-    long b;
-    do {
-      b = input.read();
-      if (b == -1) {
-        throw new EOFException("Reading BigInteger past EOF from " + input);
-      }
-      work |= (0x7f & b) << (offset % 63);
-      offset += 7;
-      // if we've read 63 bits, roll them into the result
-      if (offset == 63) {
-        result = BigInteger.valueOf(work);
-        work = 0;
-      } else if (offset % 63 == 0) {
-        result = result.or(BigInteger.valueOf(work).shiftLeft(offset-63));
-        work = 0;
-      }
-    } while (b >= 0x80);
-    if (work != 0) {
-      result = result.or(BigInteger.valueOf(work).shiftLeft((offset/63)*63));
-    }
-    // convert back to a signed number
-    boolean isNegative = result.testBit(0);
-    if (isNegative) {
-      result = result.add(BigInteger.ONE);
-      result = result.negate();
-    }
-    result = result.shiftRight(1);
-    return result;
-  }
-
-  public enum FixedBitSizes {
-    ONE, TWO, THREE, FOUR, FIVE, SIX, SEVEN, EIGHT, NINE, TEN, ELEVEN, TWELVE,
-    THIRTEEN, FOURTEEN, FIFTEEN, SIXTEEN, SEVENTEEN, EIGHTEEN, NINETEEN,
-    TWENTY, TWENTYONE, TWENTYTWO, TWENTYTHREE, TWENTYFOUR, TWENTYSIX,
-    TWENTYEIGHT, THIRTY, THIRTYTWO, FORTY, FORTYEIGHT, FIFTYSIX, SIXTYFOUR;
-  }
-
-  /**
-   * Count the number of bits required to encode the given value
-   * @param value
-   * @return bits required to store value
-   */
-  public int findClosestNumBits(long value) {
-    int count = 0;
-    while (value != 0) {
-      count++;
-      value = value >>> 1;
-    }
-    return getClosestFixedBits(count);
-  }
-
-  /**
-   * zigzag encode the given value
-   * @param val
-   * @return zigzag encoded value
-   */
-  public long zigzagEncode(long val) {
-    return (val << 1) ^ (val >> 63);
-  }
-
-  /**
-   * zigzag decode the given value
-   * @param val
-   * @return zizag decoded value
-   */
-  public long zigzagDecode(long val) {
-    return (val >>> 1) ^ -(val & 1);
-  }
-
-  /**
-   * Compute the bits required to represent pth percentile value
-   * @param data - array
-   * @param p - percentile value (>=0.0 to <=1.0)
-   * @return pth percentile bits
-   */
-  public int percentileBits(long[] data, int offset, int length,
-                            double p) {
-    if ((p > 1.0) || (p <= 0.0)) {
-      return -1;
-    }
-
-    // histogram that store the encoded bit requirement for each values.
-    // maximum number of bits that can encoded is 32 (refer FixedBitSizes)
-    int[] hist = new int[32];
-
-    // compute the histogram
-    for(int i = offset; i < (offset + length); i++) {
-      int idx = encodeBitWidth(findClosestNumBits(data[i]));
-      hist[idx] += 1;
-    }
-
-    int perLen = (int) (length * (1.0 - p));
-
-    // return the bits required by pth percentile length
-    for(int i = hist.length - 1; i >= 0; i--) {
-      perLen -= hist[i];
-      if (perLen < 0) {
-        return decodeBitWidth(i);
-      }
-    }
-
-    return 0;
-  }
-
-  /**
-   * Read n bytes in big endian order and convert to long
-   * @return long value
-   */
-  public long bytesToLongBE(InStream input, int n) throws IOException {
-    long out = 0;
-    long val = 0;
-    while (n > 0) {
-      n--;
-      // store it in a long and then shift else integer overflow will occur
-      val = input.read();
-      out |= (val << (n * 8));
-    }
-    return out;
-  }
-
-  /**
-   * Calculate the number of bytes required
-   * @param n - number of values
-   * @param numBits - bit width
-   * @return number of bytes required
-   */
-  int getTotalBytesRequired(int n, int numBits) {
-    return (n * numBits + 7) / 8;
-  }
-
-  /**
-   * For a given fixed bit this function will return the closest available fixed
-   * bit
-   * @param n
-   * @return closest valid fixed bit
-   */
-  public int getClosestFixedBits(int n) {
-    if (n == 0) {
-      return 1;
-    }
-
-    if (n >= 1 && n <= 24) {
-      return n;
-    } else if (n > 24 && n <= 26) {
-      return 26;
-    } else if (n > 26 && n <= 28) {
-      return 28;
-    } else if (n > 28 && n <= 30) {
-      return 30;
-    } else if (n > 30 && n <= 32) {
-      return 32;
-    } else if (n > 32 && n <= 40) {
-      return 40;
-    } else if (n > 40 && n <= 48) {
-      return 48;
-    } else if (n > 48 && n <= 56) {
-      return 56;
-    } else {
-      return 64;
-    }
-  }
-
-  public int getClosestAlignedFixedBits(int n) {
-    if (n == 0 ||  n == 1) {
-      return 1;
-    } else if (n > 1 && n <= 2) {
-      return 2;
-    } else if (n > 2 && n <= 4) {
-      return 4;
-    } else if (n > 4 && n <= 8) {
-      return 8;
-    } else if (n > 8 && n <= 16) {
-      return 16;
-    } else if (n > 16 && n <= 24) {
-      return 24;
-    } else if (n > 24 && n <= 32) {
-      return 32;
-    } else if (n > 32 && n <= 40) {
-      return 40;
-    } else if (n > 40 && n <= 48) {
-      return 48;
-    } else if (n > 48 && n <= 56) {
-      return 56;
-    } else {
-      return 64;
-    }
-  }
-
-  /**
-   * Finds the closest available fixed bit width match and returns its encoded
-   * value (ordinal)
-   * @param n - fixed bit width to encode
-   * @return encoded fixed bit width
-   */
-  public int encodeBitWidth(int n) {
-    n = getClosestFixedBits(n);
-
-    if (n >= 1 && n <= 24) {
-      return n - 1;
-    } else if (n > 24 && n <= 26) {
-      return FixedBitSizes.TWENTYSIX.ordinal();
-    } else if (n > 26 && n <= 28) {
-      return FixedBitSizes.TWENTYEIGHT.ordinal();
-    } else if (n > 28 && n <= 30) {
-      return FixedBitSizes.THIRTY.ordinal();
-    } else if (n > 30 && n <= 32) {
-      return FixedBitSizes.THIRTYTWO.ordinal();
-    } else if (n > 32 && n <= 40) {
-      return FixedBitSizes.FORTY.ordinal();
-    } else if (n > 40 && n <= 48) {
-      return FixedBitSizes.FORTYEIGHT.ordinal();
-    } else if (n > 48 && n <= 56) {
-      return FixedBitSizes.FIFTYSIX.ordinal();
-    } else {
-      return FixedBitSizes.SIXTYFOUR.ordinal();
-    }
-  }
-
-  /**
-   * Decodes the ordinal fixed bit value to actual fixed bit width value
-   * @param n - encoded fixed bit width
-   * @return decoded fixed bit width
-   */
-  public int decodeBitWidth(int n) {
-    if (n >= FixedBitSizes.ONE.ordinal()
-        && n <= FixedBitSizes.TWENTYFOUR.ordinal()) {
-      return n + 1;
-    } else if (n == FixedBitSizes.TWENTYSIX.ordinal()) {
-      return 26;
-    } else if (n == FixedBitSizes.TWENTYEIGHT.ordinal()) {
-      return 28;
-    } else if (n == FixedBitSizes.THIRTY.ordinal()) {
-      return 30;
-    } else if (n == FixedBitSizes.THIRTYTWO.ordinal()) {
-      return 32;
-    } else if (n == FixedBitSizes.FORTY.ordinal()) {
-      return 40;
-    } else if (n == FixedBitSizes.FORTYEIGHT.ordinal()) {
-      return 48;
-    } else if (n == FixedBitSizes.FIFTYSIX.ordinal()) {
-      return 56;
-    } else {
-      return 64;
-    }
-  }
-
-  /**
-   * Bitpack and write the input values to underlying output stream
-   * @param input - values to write
-   * @param offset - offset
-   * @param len - length
-   * @param bitSize - bit width
-   * @param output - output stream
-   * @throws IOException
-   */
-  public void writeInts(long[] input, int offset, int len, int bitSize,
-                        OutputStream output) throws IOException {
-    if (input == null || input.length < 1 || offset < 0 || len < 1
-        || bitSize < 1) {
-      return;
-    }
-
-    switch (bitSize) {
-    case 1:
-      unrolledBitPack1(input, offset, len, output);
-      return;
-    case 2:
-      unrolledBitPack2(input, offset, len, output);
-      return;
-    case 4:
-      unrolledBitPack4(input, offset, len, output);
-      return;
-    case 8:
-      unrolledBitPack8(input, offset, len, output);
-      return;
-    case 16:
-      unrolledBitPack16(input, offset, len, output);
-      return;
-    case 24:
-      unrolledBitPack24(input, offset, len, output);
-      return;
-    case 32:
-      unrolledBitPack32(input, offset, len, output);
-      return;
-    case 40:
-      unrolledBitPack40(input, offset, len, output);
-      return;
-    case 48:
-      unrolledBitPack48(input, offset, len, output);
-      return;
-    case 56:
-      unrolledBitPack56(input, offset, len, output);
-      return;
-    case 64:
-      unrolledBitPack64(input, offset, len, output);
-      return;
-    default:
-      break;
-    }
-
-    int bitsLeft = 8;
-    byte current = 0;
-    for(int i = offset; i < (offset + len); i++) {
-      long value = input[i];
-      int bitsToWrite = bitSize;
-      while (bitsToWrite > bitsLeft) {
-        // add the bits to the bottom of the current word
-        current |= value >>> (bitsToWrite - bitsLeft);
-        // subtract out the bits we just added
-        bitsToWrite -= bitsLeft;
-        // zero out the bits above bitsToWrite
-        value &= (1L << bitsToWrite) - 1;
-        output.write(current);
-        current = 0;
-        bitsLeft = 8;
-      }
-      bitsLeft -= bitsToWrite;
-      current |= value << bitsLeft;
-      if (bitsLeft == 0) {
-        output.write(current);
-        current = 0;
-        bitsLeft = 8;
-      }
-    }
-
-    // flush
-    if (bitsLeft != 8) {
-      output.write(current);
-      current = 0;
-      bitsLeft = 8;
-    }
-  }
-
-  private void unrolledBitPack1(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    final int numHops = 8;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int val = 0;
-    for (int i = offset; i < endUnroll; i = i + numHops) {
-      val = (int) (val | ((input[i] & 1) << 7)
-          | ((input[i + 1] & 1) << 6)
-          | ((input[i + 2] & 1) << 5)
-          | ((input[i + 3] & 1) << 4)
-          | ((input[i + 4] & 1) << 3)
-          | ((input[i + 5] & 1) << 2)
-          | ((input[i + 6] & 1) << 1)
-          | (input[i + 7]) & 1);
-      output.write(val);
-      val = 0;
-    }
-
-    if (remainder > 0) {
-      int startShift = 7;
-      for (int i = endUnroll; i < endOffset; i++) {
-        val = (int) (val | (input[i] & 1) << startShift);
-        startShift -= 1;
-      }
-      output.write(val);
-    }
-  }
-
-  private void unrolledBitPack2(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    final int numHops = 4;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int val = 0;
-    for (int i = offset; i < endUnroll; i = i + numHops) {
-      val = (int) (val | ((input[i] & 3) << 6)
-          | ((input[i + 1] & 3) << 4)
-          | ((input[i + 2] & 3) << 2)
-          | (input[i + 3]) & 3);
-      output.write(val);
-      val = 0;
-    }
-
-    if (remainder > 0) {
-      int startShift = 6;
-      for (int i = endUnroll; i < endOffset; i++) {
-        val = (int) (val | (input[i] & 3) << startShift);
-        startShift -= 2;
-      }
-      output.write(val);
-    }
-  }
-
-  private void unrolledBitPack4(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    final int numHops = 2;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int val = 0;
-    for (int i = offset; i < endUnroll; i = i + numHops) {
-      val = (int) (val | ((input[i] & 15) << 4) | (input[i + 1]) & 15);
-      output.write(val);
-      val = 0;
-    }
-
-    if (remainder > 0) {
-      int startShift = 4;
-      for (int i = endUnroll; i < endOffset; i++) {
-        val = (int) (val | (input[i] & 15) << startShift);
-        startShift -= 4;
-      }
-      output.write(val);
-    }
-  }
-
-  private void unrolledBitPack8(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 1);
-  }
-
-  private void unrolledBitPack16(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 2);
-  }
-
-  private void unrolledBitPack24(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 3);
-  }
-
-  private void unrolledBitPack32(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 4);
-  }
-
-  private void unrolledBitPack40(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 5);
-  }
-
-  private void unrolledBitPack48(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 6);
-  }
-
-  private void unrolledBitPack56(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 7);
-  }
-
-  private void unrolledBitPack64(long[] input, int offset, int len,
-      OutputStream output) throws IOException {
-    unrolledBitPackBytes(input, offset, len, output, 8);
-  }
-
-  private void unrolledBitPackBytes(long[] input, int offset, int len, OutputStream output, int numBytes) throws IOException {
-    final int numHops = 8;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int i = offset;
-    for (; i < endUnroll; i = i + numHops) {
-      writeLongBE(output, input, i, numHops, numBytes);
-    }
-
-    if (remainder > 0) {
-      writeRemainingLongs(output, i, input, remainder, numBytes);
-    }
-  }
-
-  private void writeRemainingLongs(OutputStream output, int offset, long[] input, int remainder,
-      int numBytes) throws IOException {
-    final int numHops = remainder;
-
-    int idx = 0;
-    switch (numBytes) {
-    case 1:
-      while (remainder > 0) {
-        writeBuffer[idx] = (byte) (input[offset + idx] & 255);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 2:
-      while (remainder > 0) {
-        writeLongBE2(output, input[offset + idx], idx * 2);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 3:
-      while (remainder > 0) {
-        writeLongBE3(output, input[offset + idx], idx * 3);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 4:
-      while (remainder > 0) {
-        writeLongBE4(output, input[offset + idx], idx * 4);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 5:
-      while (remainder > 0) {
-        writeLongBE5(output, input[offset + idx], idx * 5);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 6:
-      while (remainder > 0) {
-        writeLongBE6(output, input[offset + idx], idx * 6);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 7:
-      while (remainder > 0) {
-        writeLongBE7(output, input[offset + idx], idx * 7);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 8:
-      while (remainder > 0) {
-        writeLongBE8(output, input[offset + idx], idx * 8);
-        remainder--;
-        idx++;
-      }
-      break;
-    default:
-      break;
-    }
-
-    final int toWrite = numHops * numBytes;
-    output.write(writeBuffer, 0, toWrite);
-  }
-
-  private void writeLongBE(OutputStream output, long[] input, int offset, int numHops, int numBytes) throws IOException {
-
-    switch (numBytes) {
-    case 1:
-      writeBuffer[0] = (byte) (input[offset + 0] & 255);
-      writeBuffer[1] = (byte) (input[offset + 1] & 255);
-      writeBuffer[2] = (byte) (input[offset + 2] & 255);
-      writeBuffer[3] = (byte) (input[offset + 3] & 255);
-      writeBuffer[4] = (byte) (input[offset + 4] & 255);
-      writeBuffer[5] = (byte) (input[offset + 5] & 255);
-      writeBuffer[6] = (byte) (input[offset + 6] & 255);
-      writeBuffer[7] = (byte) (input[offset + 7] & 255);
-      break;
-    case 2:
-      writeLongBE2(output, input[offset + 0], 0);
-      writeLongBE2(output, input[offset + 1], 2);
-      writeLongBE2(output, input[offset + 2], 4);
-      writeLongBE2(output, input[offset + 3], 6);
-      writeLongBE2(output, input[offset + 4], 8);
-      writeLongBE2(output, input[offset + 5], 10);
-      writeLongBE2(output, input[offset + 6], 12);
-      writeLongBE2(output, input[offset + 7], 14);
-      break;
-    case 3:
-      writeLongBE3(output, input[offset + 0], 0);
-      writeLongBE3(output, input[offset + 1], 3);
-      writeLongBE3(output, input[offset + 2], 6);
-      writeLongBE3(output, input[offset + 3], 9);
-      writeLongBE3(output, input[offset + 4], 12);
-      writeLongBE3(output, input[offset + 5], 15);
-      writeLongBE3(output, input[offset + 6], 18);
-      writeLongBE3(output, input[offset + 7], 21);
-      break;
-    case 4:
-      writeLongBE4(output, input[offset + 0], 0);
-      writeLongBE4(output, input[offset + 1], 4);
-      writeLongBE4(output, input[offset + 2], 8);
-      writeLongBE4(output, input[offset + 3], 12);
-      writeLongBE4(output, input[offset + 4], 16);
-      writeLongBE4(output, input[offset + 5], 20);
-      writeLongBE4(output, input[offset + 6], 24);
-      writeLongBE4(output, input[offset + 7], 28);
-      break;
-    case 5:
-      writeLongBE5(output, input[offset + 0], 0);
-      writeLongBE5(output, input[offset + 1], 5);
-      writeLongBE5(output, input[offset + 2], 10);
-      writeLongBE5(output, input[offset + 3], 15);
-      writeLongBE5(output, input[offset + 4], 20);
-      writeLongBE5(output, input[offset + 5], 25);
-      writeLongBE5(output, input[offset + 6], 30);
-      writeLongBE5(output, input[offset + 7], 35);
-      break;
-    case 6:
-      writeLongBE6(output, input[offset + 0], 0);
-      writeLongBE6(output, input[offset + 1], 6);
-      writeLongBE6(output, input[offset + 2], 12);
-      writeLongBE6(output, input[offset + 3], 18);
-      writeLongBE6(output, input[offset + 4], 24);
-      writeLongBE6(output, input[offset + 5], 30);
-      writeLongBE6(output, input[offset + 6], 36);
-      writeLongBE6(output, input[offset + 7], 42);
-      break;
-    case 7:
-      writeLongBE7(output, input[offset + 0], 0);
-      writeLongBE7(output, input[offset + 1], 7);
-      writeLongBE7(output, input[offset + 2], 14);
-      writeLongBE7(output, input[offset + 3], 21);
-      writeLongBE7(output, input[offset + 4], 28);
-      writeLongBE7(output, input[offset + 5], 35);
-      writeLongBE7(output, input[offset + 6], 42);
-      writeLongBE7(output, input[offset + 7], 49);
-      break;
-    case 8:
-      writeLongBE8(output, input[offset + 0], 0);
-      writeLongBE8(output, input[offset + 1], 8);
-      writeLongBE8(output, input[offset + 2], 16);
-      writeLongBE8(output, input[offset + 3], 24);
-      writeLongBE8(output, input[offset + 4], 32);
-      writeLongBE8(output, input[offset + 5], 40);
-      writeLongBE8(output, input[offset + 6], 48);
-      writeLongBE8(output, input[offset + 7], 56);
-      break;
-      default:
-        break;
-    }
-
-    final int toWrite = numHops * numBytes;
-    output.write(writeBuffer, 0, toWrite);
-  }
-
-  private void writeLongBE2(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 0);
-  }
-
-  private void writeLongBE3(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 16);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 2] =  (byte) (val >>> 0);
-  }
-
-  private void writeLongBE4(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 24);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 16);
-    writeBuffer[wbOffset + 2] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 3] =  (byte) (val >>> 0);
-  }
-
-  private void writeLongBE5(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 32);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 24);
-    writeBuffer[wbOffset + 2] =  (byte) (val >>> 16);
-    writeBuffer[wbOffset + 3] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 4] =  (byte) (val >>> 0);
-  }
-
-  private void writeLongBE6(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 40);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 32);
-    writeBuffer[wbOffset + 2] =  (byte) (val >>> 24);
-    writeBuffer[wbOffset + 3] =  (byte) (val >>> 16);
-    writeBuffer[wbOffset + 4] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 5] =  (byte) (val >>> 0);
-  }
-
-  private void writeLongBE7(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 48);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 40);
-    writeBuffer[wbOffset + 2] =  (byte) (val >>> 32);
-    writeBuffer[wbOffset + 3] =  (byte) (val >>> 24);
-    writeBuffer[wbOffset + 4] =  (byte) (val >>> 16);
-    writeBuffer[wbOffset + 5] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 6] =  (byte) (val >>> 0);
-  }
-
-  private void writeLongBE8(OutputStream output, long val, int wbOffset) {
-    writeBuffer[wbOffset + 0] =  (byte) (val >>> 56);
-    writeBuffer[wbOffset + 1] =  (byte) (val >>> 48);
-    writeBuffer[wbOffset + 2] =  (byte) (val >>> 40);
-    writeBuffer[wbOffset + 3] =  (byte) (val >>> 32);
-    writeBuffer[wbOffset + 4] =  (byte) (val >>> 24);
-    writeBuffer[wbOffset + 5] =  (byte) (val >>> 16);
-    writeBuffer[wbOffset + 6] =  (byte) (val >>> 8);
-    writeBuffer[wbOffset + 7] =  (byte) (val >>> 0);
-  }
-
-  /**
-   * Read bitpacked integers from input stream
-   * @param buffer - input buffer
-   * @param offset - offset
-   * @param len - length
-   * @param bitSize - bit width
-   * @param input - input stream
-   * @throws IOException
-   */
-  public void readInts(long[] buffer, int offset, int len, int bitSize,
-                       InStream input) throws IOException {
-    int bitsLeft = 0;
-    int current = 0;
-
-    switch (bitSize) {
-    case 1:
-      unrolledUnPack1(buffer, offset, len, input);
-      return;
-    case 2:
-      unrolledUnPack2(buffer, offset, len, input);
-      return;
-    case 4:
-      unrolledUnPack4(buffer, offset, len, input);
-      return;
-    case 8:
-      unrolledUnPack8(buffer, offset, len, input);
-      return;
-    case 16:
-      unrolledUnPack16(buffer, offset, len, input);
-      return;
-    case 24:
-      unrolledUnPack24(buffer, offset, len, input);
-      return;
-    case 32:
-      unrolledUnPack32(buffer, offset, len, input);
-      return;
-    case 40:
-      unrolledUnPack40(buffer, offset, len, input);
-      return;
-    case 48:
-      unrolledUnPack48(buffer, offset, len, input);
-      return;
-    case 56:
-      unrolledUnPack56(buffer, offset, len, input);
-      return;
-    case 64:
-      unrolledUnPack64(buffer, offset, len, input);
-      return;
-    default:
-      break;
-    }
-
-    for(int i = offset; i < (offset + len); i++) {
-      long result = 0;
-      int bitsLeftToRead = bitSize;
-      while (bitsLeftToRead > bitsLeft) {
-        result <<= bitsLeft;
-        result |= current & ((1 << bitsLeft) - 1);
-        bitsLeftToRead -= bitsLeft;
-        current = input.read();
-        bitsLeft = 8;
-      }
-
-      // handle the left over bits
-      if (bitsLeftToRead > 0) {
-        result <<= bitsLeftToRead;
-        bitsLeft -= bitsLeftToRead;
-        result |= (current >> bitsLeft) & ((1 << bitsLeftToRead) - 1);
-      }
-      buffer[i] = result;
-    }
-  }
-
-
-  private void unrolledUnPack1(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    final int numHops = 8;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int val = 0;
-    for (int i = offset; i < endUnroll; i = i + numHops) {
-      val = input.read();
-      buffer[i] = (val >>> 7) & 1;
-      buffer[i + 1] = (val >>> 6) & 1;
-      buffer[i + 2] = (val >>> 5) & 1;
-      buffer[i + 3] = (val >>> 4) & 1;
-      buffer[i + 4] = (val >>> 3) & 1;
-      buffer[i + 5] = (val >>> 2) & 1;
-      buffer[i + 6] = (val >>> 1) & 1;
-      buffer[i + 7] = val & 1;
-    }
-
-    if (remainder > 0) {
-      int startShift = 7;
-      val = input.read();
-      for (int i = endUnroll; i < endOffset; i++) {
-        buffer[i] = (val >>> startShift) & 1;
-        startShift -= 1;
-      }
-    }
-  }
-
-  private void unrolledUnPack2(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    final int numHops = 4;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int val = 0;
-    for (int i = offset; i < endUnroll; i = i + numHops) {
-      val = input.read();
-      buffer[i] = (val >>> 6) & 3;
-      buffer[i + 1] = (val >>> 4) & 3;
-      buffer[i + 2] = (val >>> 2) & 3;
-      buffer[i + 3] = val & 3;
-    }
-
-    if (remainder > 0) {
-      int startShift = 6;
-      val = input.read();
-      for (int i = endUnroll; i < endOffset; i++) {
-        buffer[i] = (val >>> startShift) & 3;
-        startShift -= 2;
-      }
-    }
-  }
-
-  private void unrolledUnPack4(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    final int numHops = 2;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int val = 0;
-    for (int i = offset; i < endUnroll; i = i + numHops) {
-      val = input.read();
-      buffer[i] = (val >>> 4) & 15;
-      buffer[i + 1] = val & 15;
-    }
-
-    if (remainder > 0) {
-      int startShift = 4;
-      val = input.read();
-      for (int i = endUnroll; i < endOffset; i++) {
-        buffer[i] = (val >>> startShift) & 15;
-        startShift -= 4;
-      }
-    }
-  }
-
-  private void unrolledUnPack8(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 1);
-  }
-
-  private void unrolledUnPack16(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 2);
-  }
-
-  private void unrolledUnPack24(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 3);
-  }
-
-  private void unrolledUnPack32(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 4);
-  }
-
-  private void unrolledUnPack40(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 5);
-  }
-
-  private void unrolledUnPack48(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 6);
-  }
-
-  private void unrolledUnPack56(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 7);
-  }
-
-  private void unrolledUnPack64(long[] buffer, int offset, int len,
-      InStream input) throws IOException {
-    unrolledUnPackBytes(buffer, offset, len, input, 8);
-  }
-
-  private void unrolledUnPackBytes(long[] buffer, int offset, int len, InStream input, int numBytes)
-      throws IOException {
-    final int numHops = 8;
-    final int remainder = len % numHops;
-    final int endOffset = offset + len;
-    final int endUnroll = endOffset - remainder;
-    int i = offset;
-    for (; i < endUnroll; i = i + numHops) {
-      readLongBE(input, buffer, i, numHops, numBytes);
-    }
-
-    if (remainder > 0) {
-      readRemainingLongs(buffer, i, input, remainder, numBytes);
-    }
-  }
-
-  private void readRemainingLongs(long[] buffer, int offset, InStream input, int remainder,
-      int numBytes) throws IOException {
-    final int toRead = remainder * numBytes;
-    // bulk read to buffer
-    int bytesRead = input.read(readBuffer, 0, toRead);
-    while (bytesRead != toRead) {
-      bytesRead += input.read(readBuffer, bytesRead, toRead - bytesRead);
-    }
-
-    int idx = 0;
-    switch (numBytes) {
-    case 1:
-      while (remainder > 0) {
-        buffer[offset++] = readBuffer[idx] & 255;
-        remainder--;
-        idx++;
-      }
-      break;
-    case 2:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE2(input, idx * 2);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 3:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE3(input, idx * 3);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 4:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE4(input, idx * 4);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 5:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE5(input, idx * 5);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 6:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE6(input, idx * 6);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 7:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE7(input, idx * 7);
-        remainder--;
-        idx++;
-      }
-      break;
-    case 8:
-      while (remainder > 0) {
-        buffer[offset++] = readLongBE8(input, idx * 8);
-        remainder--;
-        idx++;
-      }
-      break;
-    default:
-      break;
-    }
-  }
-
-  private void readLongBE(InStream in, long[] buffer, int start, int numHops, int numBytes)
-      throws IOException {
-    final int toRead = numHops * numBytes;
-    // bulk read to buffer
-    int bytesRead = in.read(readBuffer, 0, toRead);
-    while (bytesRead != toRead) {
-      bytesRead += in.read(readBuffer, bytesRead, toRead - bytesRead);
-    }
-
-    switch (numBytes) {
-    case 1:
-      buffer[start + 0] = readBuffer[0] & 255;
-      buffer[start + 1] = readBuffer[1] & 255;
-      buffer[start + 2] = readBuffer[2] & 255;
-      buffer[start + 3] = readBuffer[3] & 255;
-      buffer[start + 4] = readBuffer[4] & 255;
-      buffer[start + 5] = readBuffer[5] & 255;
-      buffer[start + 6] = readBuffer[6] & 255;
-      buffer[start + 7] = readBuffer[7] & 255;
-      break;
-    case 2:
-      buffer[start + 0] = readLongBE2(in, 0);
-      buffer[start + 1] = readLongBE2(in, 2);
-      buffer[start + 2] = readLongBE2(in, 4);
-      buffer[start + 3] = readLongBE2(in, 6);
-      buffer[start + 4] = readLongBE2(in, 8);
-      buffer[start + 5] = readLongBE2(in, 10);
-      buffer[start + 6] = readLongBE2(in, 12);
-      buffer[start + 7] = readLongBE2(in, 14);
-      break;
-    case 3:
-      buffer[start + 0] = readLongBE3(in, 0);
-      buffer[start + 1] = readLongBE3(in, 3);
-      buffer[start + 2] = readLongBE3(in, 6);
-      buffer[start + 3] = readLongBE3(in, 9);
-      buffer[start + 4] = readLongBE3(in, 12);
-      buffer[start + 5] = readLongBE3(in, 15);
-      buffer[start + 6] = readLongBE3(in, 18);
-      buffer[start + 7] = readLongBE3(in, 21);
-      break;
-    case 4:
-      buffer[start + 0] = readLongBE4(in, 0);
-      buffer[start + 1] = readLongBE4(in, 4);
-      buffer[start + 2] = readLongBE4(in, 8);
-      buffer[start + 3] = readLongBE4(in, 12);
-      buffer[start + 4] = readLongBE4(in, 16);
-      buffer[start + 5] = readLongBE4(in, 20);
-      buffer[start + 6] = readLongBE4(in, 24);
-      buffer[start + 7] = readLongBE4(in, 28);
-      break;
-    case 5:
-      buffer[start + 0] = readLongBE5(in, 0);
-      buffer[start + 1] = readLongBE5(in, 5);
-      buffer[start + 2] = readLongBE5(in, 10);
-      buffer[start + 3] = readLongBE5(in, 15);
-      buffer[start + 4] = readLongBE5(in, 20);
-      buffer[start + 5] = readLongBE5(in, 25);
-      buffer[start + 6] = readLongBE5(in, 30);
-      buffer[start + 7] = readLongBE5(in, 35);
-      break;
-    case 6:
-      buffer[start + 0] = readLongBE6(in, 0);
-      buffer[start + 1] = readLongBE6(in, 6);
-      buffer[start + 2] = readLongBE6(in, 12);
-      buffer[start + 3] = readLongBE6(in, 18);
-      buffer[start + 4] = readLongBE6(in, 24);
-      buffer[start + 5] = readLongBE6(in, 30);
-      buffer[start + 6] = readLongBE6(in, 36);
-      buffer[start + 7] = readLongBE6(in, 42);
-      break;
-    case 7:
-      buffer[start + 0] = readLongBE7(in, 0);
-      buffer[start + 1] = readLongBE7(in, 7);
-      buffer[start + 2] = readLongBE7(in, 14);
-      buffer[start + 3] = readLongBE7(in, 21);
-      buffer[start + 4] = readLongBE7(in, 28);
-      buffer[start + 5] = readLongBE7(in, 35);
-      buffer[start + 6] = readLongBE7(in, 42);
-      buffer[start + 7] = readLongBE7(in, 49);
-      break;
-    case 8:
-      buffer[start + 0] = readLongBE8(in, 0);
-      buffer[start + 1] = readLongBE8(in, 8);
-      buffer[start + 2] = readLongBE8(in, 16);
-      buffer[start + 3] = readLongBE8(in, 24);
-      buffer[start + 4] = readLongBE8(in, 32);
-      buffer[start + 5] = readLongBE8(in, 40);
-      buffer[start + 6] = readLongBE8(in, 48);
-      buffer[start + 7] = readLongBE8(in, 56);
-      break;
-    default:
-      break;
-    }
-  }
-
-  private long readLongBE2(InStream in, int rbOffset) {
-    return (((readBuffer[rbOffset] & 255) << 8)
-        + ((readBuffer[rbOffset + 1] & 255) << 0));
-  }
-
-  private long readLongBE3(InStream in, int rbOffset) {
-    return (((readBuffer[rbOffset] & 255) << 16)
-        + ((readBuffer[rbOffset + 1] & 255) << 8)
-        + ((readBuffer[rbOffset + 2] & 255) << 0));
-  }
-
-  private long readLongBE4(InStream in, int rbOffset) {
-    return (((long) (readBuffer[rbOffset] & 255) << 24)
-        + ((readBuffer[rbOffset + 1] & 255) << 16)
-        + ((readBuffer[rbOffset + 2] & 255) << 8)
-        + ((readBuffer[rbOffset + 3] & 255) << 0));
-  }
-
-  private long readLongBE5(InStream in, int rbOffset) {
-    return (((long) (readBuffer[rbOffset] & 255) << 32)
-        + ((long) (readBuffer[rbOffset + 1] & 255) << 24)
-        + ((readBuffer[rbOffset + 2] & 255) << 16)
-        + ((readBuffer[rbOffset + 3] & 255) << 8)
-        + ((readBuffer[rbOffset + 4] & 255) << 0));
-  }
-
-  private long readLongBE6(InStream in, int rbOffset) {
-    return (((long) (readBuffer[rbOffset] & 255) << 40)
-        + ((long) (readBuffer[rbOffset + 1] & 255) << 32)
-        + ((long) (readBuffer[rbOffset + 2] & 255) << 24)
-        + ((readBuffer[rbOffset + 3] & 255) << 16)
-        + ((readBuffer[rbOffset + 4] & 255) << 8)
-        + ((readBuffer[rbOffset + 5] & 255) << 0));
-  }
-
-  private long readLongBE7(InStream in, int rbOffset) {
-    return (((long) (readBuffer[rbOffset] & 255) << 48)
-        + ((long) (readBuffer[rbOffset + 1] & 255) << 40)
-        + ((long) (readBuffer[rbOffset + 2] & 255) << 32)
-        + ((long) (readBuffer[rbOffset + 3] & 255) << 24)
-        + ((readBuffer[rbOffset + 4] & 255) << 16)
-        + ((readBuffer[rbOffset + 5] & 255) << 8)
-        + ((readBuffer[rbOffset + 6] & 255) << 0));
-  }
-
-  private long readLongBE8(InStream in, int rbOffset) {
-    return (((long) (readBuffer[rbOffset] & 255) << 56)
-        + ((long) (readBuffer[rbOffset + 1] & 255) << 48)
-        + ((long) (readBuffer[rbOffset + 2] & 255) << 40)
-        + ((long) (readBuffer[rbOffset + 3] & 255) << 32)
-        + ((long) (readBuffer[rbOffset + 4] & 255) << 24)
-        + ((readBuffer[rbOffset + 5] & 255) << 16)
-        + ((readBuffer[rbOffset + 6] & 255) << 8)
-        + ((readBuffer[rbOffset + 7] & 255) << 0));
-  }
-
-  // Do not want to use Guava LongMath.checkedSubtract() here as it will throw
-  // ArithmeticException in case of overflow
-  public boolean isSafeSubtract(long left, long right) {
-    return (left ^ right) >= 0 | (left ^ (left - right)) >= 0;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/SettableUncompressedStream.java b/orc/src/java/org/apache/orc/impl/SettableUncompressedStream.java
deleted file mode 100644
index f9e29eb9fc..0000000000
--- a/orc/src/java/org/apache/orc/impl/SettableUncompressedStream.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.util.List;
-
-import org.apache.hadoop.hive.common.DiskRangeInfo;
-import org.apache.hadoop.hive.common.io.DiskRange;
-import org.apache.orc.impl.InStream;
-
-/**
- * An uncompressed stream whose underlying byte buffer can be set.
- */
-public class SettableUncompressedStream extends InStream.UncompressedStream {
-
-  public SettableUncompressedStream(String name, List<DiskRange> input, long length) {
-    super(name, input, length);
-    setOffset(input);
-  }
-
-  public void setBuffers(DiskRangeInfo diskRangeInfo) {
-    reset(diskRangeInfo.getDiskRanges(), diskRangeInfo.getTotalLength());
-    setOffset(diskRangeInfo.getDiskRanges());
-  }
-
-  private void setOffset(List<DiskRange> list) {
-    currentOffset = list.isEmpty() ? 0 : list.get(0).getOffset();
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/SnappyCodec.java b/orc/src/java/org/apache/orc/impl/SnappyCodec.java
deleted file mode 100644
index dd4f30c18f..0000000000
--- a/orc/src/java/org/apache/orc/impl/SnappyCodec.java
+++ /dev/null
@@ -1,108 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.CompressionCodec;
-import org.iq80.snappy.Snappy;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.EnumSet;
-
-public class SnappyCodec implements CompressionCodec, DirectDecompressionCodec {
-  private static final HadoopShims SHIMS = HadoopShims.Factory.get();
-
-  Boolean direct = null;
-
-  @Override
-  public boolean compress(ByteBuffer in, ByteBuffer out,
-                          ByteBuffer overflow) throws IOException {
-    int inBytes = in.remaining();
-    // I should work on a patch for Snappy to support an overflow buffer
-    // to prevent the extra buffer copy.
-    byte[] compressed = new byte[Snappy.maxCompressedLength(inBytes)];
-    int outBytes =
-        Snappy.compress(in.array(), in.arrayOffset() + in.position(), inBytes,
-            compressed, 0);
-    if (outBytes < inBytes) {
-      int remaining = out.remaining();
-      if (remaining >= outBytes) {
-        System.arraycopy(compressed, 0, out.array(), out.arrayOffset() +
-            out.position(), outBytes);
-        out.position(out.position() + outBytes);
-      } else {
-        System.arraycopy(compressed, 0, out.array(), out.arrayOffset() +
-            out.position(), remaining);
-        out.position(out.limit());
-        System.arraycopy(compressed, remaining, overflow.array(),
-            overflow.arrayOffset(), outBytes - remaining);
-        overflow.position(outBytes - remaining);
-      }
-      return true;
-    } else {
-      return false;
-    }
-  }
-
-  @Override
-  public void decompress(ByteBuffer in, ByteBuffer out) throws IOException {
-    if(in.isDirect() && out.isDirect()) {
-      directDecompress(in, out);
-      return;
-    }
-    int inOffset = in.position();
-    int uncompressLen =
-        Snappy.uncompress(in.array(), in.arrayOffset() + inOffset,
-        in.limit() - inOffset, out.array(), out.arrayOffset() + out.position());
-    out.position(uncompressLen + out.position());
-    out.flip();
-  }
-
-  @Override
-  public boolean isAvailable() {
-    if (direct == null) {
-      try {
-        if (SHIMS.getDirectDecompressor(
-            HadoopShims.DirectCompressionType.SNAPPY) != null) {
-          direct = Boolean.valueOf(true);
-        } else {
-          direct = Boolean.valueOf(false);
-        }
-      } catch (UnsatisfiedLinkError ule) {
-        direct = Boolean.valueOf(false);
-      }
-    }
-    return direct.booleanValue();
-  }
-
-  @Override
-  public void directDecompress(ByteBuffer in, ByteBuffer out)
-      throws IOException {
-    HadoopShims.DirectDecompressor decompressShim =
-        SHIMS.getDirectDecompressor(HadoopShims.DirectCompressionType.SNAPPY);
-    decompressShim.decompress(in, out);
-    out.flip(); // flip for read
-  }
-
-  @Override
-  public CompressionCodec modify(EnumSet<Modifier> modifiers) {
-    // snappy allows no modifications
-    return this;
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/StreamName.java b/orc/src/java/org/apache/orc/impl/StreamName.java
deleted file mode 100644
index b3fd1450e9..0000000000
--- a/orc/src/java/org/apache/orc/impl/StreamName.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.OrcProto;
-
-/**
- * The name of a stream within a stripe.
- */
-public class StreamName implements Comparable<StreamName> {
-  private final int column;
-  private final OrcProto.Stream.Kind kind;
-
-  public static enum Area {
-    DATA, INDEX
-  }
-
-  public StreamName(int column, OrcProto.Stream.Kind kind) {
-    this.column = column;
-    this.kind = kind;
-  }
-
-  public boolean equals(Object obj) {
-    if (obj != null && obj instanceof  StreamName) {
-      StreamName other = (StreamName) obj;
-      return other.column == column && other.kind == kind;
-    } else {
-      return false;
-    }
-  }
-
-  @Override
-  public int compareTo(StreamName streamName) {
-    if (streamName == null) {
-      return -1;
-    }
-    Area area = getArea(kind);
-    Area otherArea = streamName.getArea(streamName.kind);
-    if (area != otherArea) {
-      return -area.compareTo(otherArea);
-    }
-    if (column != streamName.column) {
-      return column < streamName.column ? -1 : 1;
-    }
-    return kind.compareTo(streamName.kind);
-  }
-
-  public int getColumn() {
-    return column;
-  }
-
-  public OrcProto.Stream.Kind getKind() {
-    return kind;
-  }
-
-  public Area getArea() {
-    return getArea(kind);
-  }
-
-  public static Area getArea(OrcProto.Stream.Kind kind) {
-    switch (kind) {
-      case ROW_INDEX:
-      case DICTIONARY_COUNT:
-      case BLOOM_FILTER:
-        return Area.INDEX;
-      default:
-        return Area.DATA;
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "Stream for column " + column + " kind " + kind;
-  }
-
-  @Override
-  public int hashCode() {
-    return column * 101 + kind.getNumber();
-  }
-}
-
diff --git a/orc/src/java/org/apache/orc/impl/StringRedBlackTree.java b/orc/src/java/org/apache/orc/impl/StringRedBlackTree.java
deleted file mode 100644
index c353ab00fb..0000000000
--- a/orc/src/java/org/apache/orc/impl/StringRedBlackTree.java
+++ /dev/null
@@ -1,210 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.io.OutputStream;
-
-import org.apache.hadoop.io.Text;
-import org.apache.orc.impl.DynamicByteArray;
-import org.apache.orc.impl.DynamicIntArray;
-import org.apache.orc.impl.RedBlackTree;
-
-/**
- * A red-black tree that stores strings. The strings are stored as UTF-8 bytes
- * and an offset for each entry.
- */
-public class StringRedBlackTree extends RedBlackTree {
-  private final DynamicByteArray byteArray = new DynamicByteArray();
-  private final DynamicIntArray keyOffsets;
-  private final Text newKey = new Text();
-
-  public StringRedBlackTree(int initialCapacity) {
-    super(initialCapacity);
-    keyOffsets = new DynamicIntArray(initialCapacity);
-  }
-
-  public int add(String value) {
-    newKey.set(value);
-    return addNewKey();
-  }
-
-  private int addNewKey() {
-    // if the newKey is actually new, add it to our byteArray and store the offset & length
-    if (add()) {
-      int len = newKey.getLength();
-      keyOffsets.add(byteArray.add(newKey.getBytes(), 0, len));
-    }
-    return lastAdd;
-  }
-
-  public int add(Text value) {
-    newKey.set(value);
-    return addNewKey();
-  }
-
-  public int add(byte[] bytes, int offset, int length) {
-    newKey.set(bytes, offset, length);
-    return addNewKey();
-  }
-
-  @Override
-  protected int compareValue(int position) {
-    int start = keyOffsets.get(position);
-    int end;
-    if (position + 1 == keyOffsets.size()) {
-      end = byteArray.size();
-    } else {
-      end = keyOffsets.get(position+1);
-    }
-    return byteArray.compare(newKey.getBytes(), 0, newKey.getLength(),
-                             start, end - start);
-  }
-
-  /**
-   * The information about each node.
-   */
-  public interface VisitorContext {
-    /**
-     * Get the position where the key was originally added.
-     * @return the number returned by add.
-     */
-    int getOriginalPosition();
-
-    /**
-     * Write the bytes for the string to the given output stream.
-     * @param out the stream to write to.
-     * @throws IOException
-     */
-    void writeBytes(OutputStream out) throws IOException;
-
-    /**
-     * Get the original string.
-     * @return the string
-     */
-    Text getText();
-
-    /**
-     * Get the number of bytes.
-     * @return the string's length in bytes
-     */
-    int getLength();
-  }
-
-  /**
-   * The interface for visitors.
-   */
-  public interface Visitor {
-    /**
-     * Called once for each node of the tree in sort order.
-     * @param context the information about each node
-     * @throws IOException
-     */
-    void visit(VisitorContext context) throws IOException;
-  }
-
-  private class VisitorContextImpl implements VisitorContext {
-    private int originalPosition;
-    private int start;
-    private int end;
-    private final Text text = new Text();
-
-    public int getOriginalPosition() {
-      return originalPosition;
-    }
-
-    public Text getText() {
-      byteArray.setText(text, start, end - start);
-      return text;
-    }
-
-    public void writeBytes(OutputStream out) throws IOException {
-      byteArray.write(out, start, end - start);
-    }
-
-    public int getLength() {
-      return end - start;
-    }
-
-    void setPosition(int position) {
-      originalPosition = position;
-      start = keyOffsets.get(originalPosition);
-      if (position + 1 == keyOffsets.size()) {
-        end = byteArray.size();
-      } else {
-        end = keyOffsets.get(originalPosition + 1);
-      }
-    }
-  }
-
-  private void recurse(int node, Visitor visitor, VisitorContextImpl context
-                      ) throws IOException {
-    if (node != NULL) {
-      recurse(getLeft(node), visitor, context);
-      context.setPosition(node);
-      visitor.visit(context);
-      recurse(getRight(node), visitor, context);
-    }
-  }
-
-  /**
-   * Visit all of the nodes in the tree in sorted order.
-   * @param visitor the action to be applied to each node
-   * @throws IOException
-   */
-  public void visit(Visitor visitor) throws IOException {
-    recurse(root, visitor, new VisitorContextImpl());
-  }
-
-  /**
-   * Reset the table to empty.
-   */
-  public void clear() {
-    super.clear();
-    byteArray.clear();
-    keyOffsets.clear();
-  }
-
-  public void getText(Text result, int originalPosition) {
-    int offset = keyOffsets.get(originalPosition);
-    int length;
-    if (originalPosition + 1 == keyOffsets.size()) {
-      length = byteArray.size() - offset;
-    } else {
-      length = keyOffsets.get(originalPosition + 1) - offset;
-    }
-    byteArray.setText(result, offset, length);
-  }
-
-  /**
-   * Get the size of the character data in the table.
-   * @return the bytes used by the table
-   */
-  public int getCharacterSize() {
-    return byteArray.size();
-  }
-
-  /**
-   * Calculate the approximate size in memory.
-   * @return the number of bytes used in storing the tree.
-   */
-  public long getSizeInBytes() {
-    return byteArray.getSizeInBytes() + keyOffsets.getSizeInBytes() +
-      super.getSizeInBytes();
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java b/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
deleted file mode 100644
index 8b097ba965..0000000000
--- a/orc/src/java/org/apache/orc/impl/TreeReaderFactory.java
+++ /dev/null
@@ -1,2162 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.EOFException;
-import java.io.IOException;
-import java.math.BigInteger;
-import java.text.ParseException;
-import java.text.SimpleDateFormat;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.TimeZone;
-
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.OrcProto;
-
-/**
- * Factory for creating ORC tree readers.
- */
-public class TreeReaderFactory {
-
-  public abstract static class TreeReader {
-    protected final int columnId;
-    protected BitFieldReader present = null;
-    protected boolean valuePresent = false;
-    protected int vectorColumnCount;
-
-    TreeReader(int columnId) throws IOException {
-      this(columnId, null);
-    }
-
-    protected TreeReader(int columnId, InStream in) throws IOException {
-      this.columnId = columnId;
-      if (in == null) {
-        present = null;
-        valuePresent = true;
-      } else {
-        present = new BitFieldReader(in, 1);
-      }
-      vectorColumnCount = -1;
-    }
-
-    void setVectorColumnCount(int vectorColumnCount) {
-      this.vectorColumnCount = vectorColumnCount;
-    }
-
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    protected static IntegerReader createIntegerReader(OrcProto.ColumnEncoding.Kind kind,
-        InStream in,
-        boolean signed, boolean skipCorrupt) throws IOException {
-      switch (kind) {
-        case DIRECT_V2:
-        case DICTIONARY_V2:
-          return new RunLengthIntegerReaderV2(in, signed, skipCorrupt);
-        case DIRECT:
-        case DICTIONARY:
-          return new RunLengthIntegerReader(in, signed);
-        default:
-          throw new IllegalArgumentException("Unknown encoding " + kind);
-      }
-    }
-
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      checkEncoding(stripeFooter.getColumnsList().get(columnId));
-      InStream in = streams.get(new StreamName(columnId,
-          OrcProto.Stream.Kind.PRESENT));
-      if (in == null) {
-        present = null;
-        valuePresent = true;
-      } else {
-        present = new BitFieldReader(in, 1);
-      }
-    }
-
-    /**
-     * Seek to the given position.
-     *
-     * @param index the indexes loaded from the file
-     * @throws IOException
-     */
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    public void seek(PositionProvider index) throws IOException {
-      if (present != null) {
-        present.seek(index);
-      }
-    }
-
-    protected long countNonNulls(long rows) throws IOException {
-      if (present != null) {
-        long result = 0;
-        for (long c = 0; c < rows; ++c) {
-          if (present.next() == 1) {
-            result += 1;
-          }
-        }
-        return result;
-      } else {
-        return rows;
-      }
-    }
-
-    abstract void skipRows(long rows) throws IOException;
-
-    /**
-     * Called at the top level to read into the given batch.
-     * @param batch the batch to read into
-     * @param batchSize the number of rows to read
-     * @throws IOException
-     */
-    public void nextBatch(VectorizedRowBatch batch,
-                          int batchSize) throws IOException {
-      batch.cols[0].reset();
-      batch.cols[0].ensureSize(batchSize, false);
-      nextVector(batch.cols[0], null, batchSize);
-    }
-
-    /**
-     * Populates the isNull vector array in the previousVector object based on
-     * the present stream values. This function is called from all the child
-     * readers, and they all set the values based on isNull field value.
-     *
-     * @param previous The columnVector object whose isNull value is populated
-     * @param isNull Whether the each value was null at a higher level. If
-     *               isNull is null, all values are non-null.
-     * @param batchSize      Size of the column vector
-     * @throws IOException
-     */
-    public void nextVector(ColumnVector previous,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      if (present != null || isNull != null) {
-        // Set noNulls and isNull vector of the ColumnVector based on
-        // present stream
-        previous.noNulls = true;
-        boolean allNull = true;
-        for (int i = 0; i < batchSize; i++) {
-          if (isNull == null || !isNull[i]) {
-            if (present != null && present.next() != 1) {
-              previous.noNulls = false;
-              previous.isNull[i] = true;
-            } else {
-              previous.isNull[i] = false;
-              allNull = false;
-            }
-          } else {
-            previous.noNulls = false;
-            previous.isNull[i] = true;
-          }
-        }
-        previous.isRepeating = !previous.noNulls && allNull;
-      } else {
-        // There is no present stream, this means that all the values are
-        // present.
-        previous.noNulls = true;
-        for (int i = 0; i < batchSize; i++) {
-          previous.isNull[i] = false;
-        }
-      }
-    }
-
-    public BitFieldReader getPresent() {
-      return present;
-    }
-
-    public int getColumnId() {
-      return columnId;
-    }
-  }
-
-  public static class NullTreeReader extends TreeReader {
-
-    public NullTreeReader(int columnId) throws IOException {
-      super(columnId);
-    }
-
-    @Override
-    public void startStripe(Map<StreamName, InStream> streams,
-                            OrcProto.StripeFooter footer) {
-      // PASS
-    }
-
-    @Override
-    void skipRows(long rows) {
-      // PASS
-    }
-
-    @Override
-    public void seek(PositionProvider position) {
-      // PASS
-    }
-
-    @Override
-    public void seek(PositionProvider[] position) {
-      // PASS
-    }
-
-    @Override
-    public void nextVector(ColumnVector vector, boolean[] isNull, int size) {
-      vector.noNulls = false;
-      vector.isNull[0] = true;
-      vector.isRepeating = true;
-    }
-  }
-
-  public static class BooleanTreeReader extends TreeReader {
-    protected BitFieldReader reader = null;
-
-    BooleanTreeReader(int columnId) throws IOException {
-      this(columnId, null, null);
-    }
-
-    protected BooleanTreeReader(int columnId, InStream present, InStream data) throws IOException {
-      super(columnId, present);
-      if (data != null) {
-        reader = new BitFieldReader(data, 1);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      reader = new BitFieldReader(streams.get(new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA)), 1);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      LongColumnVector result = (LongColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      // Read value entries based on isNull entries
-      reader.nextVector(result, batchSize);
-    }
-  }
-
-  public static class ByteTreeReader extends TreeReader {
-    protected RunLengthByteReader reader = null;
-
-    ByteTreeReader(int columnId) throws IOException {
-      this(columnId, null, null);
-    }
-
-    protected ByteTreeReader(int columnId, InStream present, InStream data) throws IOException {
-      super(columnId, present);
-      this.reader = new RunLengthByteReader(data);
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      reader = new RunLengthByteReader(streams.get(new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA)));
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final LongColumnVector result = (LongColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      // Read value entries based on isNull entries
-      reader.nextVector(result, result.vector, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-  }
-
-  public static class ShortTreeReader extends TreeReader {
-    protected IntegerReader reader = null;
-
-    ShortTreeReader(int columnId) throws IOException {
-      this(columnId, null, null, null);
-    }
-
-    protected ShortTreeReader(int columnId, InStream present, InStream data,
-        OrcProto.ColumnEncoding encoding)
-        throws IOException {
-      super(columnId, present);
-      if (data != null && encoding != null) {
-        checkEncoding(encoding);
-        this.reader = createIntegerReader(encoding.getKind(), data, true, false);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(name), true, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final LongColumnVector result = (LongColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      // Read value entries based on isNull entries
-      reader.nextVector(result, result.vector, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-  }
-
-  public static class IntTreeReader extends TreeReader {
-    protected IntegerReader reader = null;
-
-    IntTreeReader(int columnId) throws IOException {
-      this(columnId, null, null, null);
-    }
-
-    protected IntTreeReader(int columnId, InStream present, InStream data,
-        OrcProto.ColumnEncoding encoding)
-        throws IOException {
-      super(columnId, present);
-      if (data != null && encoding != null) {
-        checkEncoding(encoding);
-        this.reader = createIntegerReader(encoding.getKind(), data, true, false);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(name), true, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final LongColumnVector result = (LongColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      // Read value entries based on isNull entries
-      reader.nextVector(result, result.vector, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-  }
-
-  public static class LongTreeReader extends TreeReader {
-    protected IntegerReader reader = null;
-
-    LongTreeReader(int columnId, boolean skipCorrupt) throws IOException {
-      this(columnId, null, null, null, skipCorrupt);
-    }
-
-    protected LongTreeReader(int columnId, InStream present, InStream data,
-        OrcProto.ColumnEncoding encoding,
-        boolean skipCorrupt)
-        throws IOException {
-      super(columnId, present);
-      if (data != null && encoding != null) {
-        checkEncoding(encoding);
-        this.reader = createIntegerReader(encoding.getKind(), data, true, skipCorrupt);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(name), true, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final LongColumnVector result = (LongColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      // Read value entries based on isNull entries
-      reader.nextVector(result, result.vector, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-  }
-
-  public static class FloatTreeReader extends TreeReader {
-    protected InStream stream;
-    private final SerializationUtils utils;
-
-    FloatTreeReader(int columnId) throws IOException {
-      this(columnId, null, null);
-    }
-
-    protected FloatTreeReader(int columnId, InStream present, InStream data) throws IOException {
-      super(columnId, present);
-      this.utils = new SerializationUtils();
-      this.stream = data;
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      stream = streams.get(name);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      stream.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final DoubleColumnVector result = (DoubleColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      final boolean hasNulls = !result.noNulls;
-      boolean allNulls = hasNulls;
-
-      if (hasNulls) {
-        // conditions to ensure bounds checks skips
-        for (int i = 0; batchSize <= result.isNull.length && i < batchSize; i++) {
-          allNulls = allNulls & result.isNull[i];
-        }
-        if (allNulls) {
-          result.vector[0] = Double.NaN;
-          result.isRepeating = true;
-        } else {
-          // some nulls
-          result.isRepeating = false;
-          // conditions to ensure bounds checks skips
-          for (int i = 0; batchSize <= result.isNull.length
-              && batchSize <= result.vector.length && i < batchSize; i++) {
-            if (!result.isNull[i]) {
-              result.vector[i] = utils.readFloat(stream);
-            } else {
-              // If the value is not present then set NaN
-              result.vector[i] = Double.NaN;
-            }
-          }
-        }
-      } else {
-        // no nulls & > 1 row (check repeating)
-        boolean repeating = (batchSize > 1);
-        final float f1 = utils.readFloat(stream);
-        result.vector[0] = f1;
-        // conditions to ensure bounds checks skips
-        for (int i = 1; i < batchSize && batchSize <= result.vector.length; i++) {
-          final float f2 = utils.readFloat(stream);
-          repeating = repeating && (f1 == f2);
-          result.vector[i] = f2;
-        }
-        result.isRepeating = repeating;
-      }
-    }
-
-    @Override
-    protected void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      for (int i = 0; i < items; ++i) {
-        utils.readFloat(stream);
-      }
-    }
-  }
-
-  public static class DoubleTreeReader extends TreeReader {
-    protected InStream stream;
-    private final SerializationUtils utils;
-
-    DoubleTreeReader(int columnId) throws IOException {
-      this(columnId, null, null);
-    }
-
-    protected DoubleTreeReader(int columnId, InStream present, InStream data) throws IOException {
-      super(columnId, present);
-      this.utils = new SerializationUtils();
-      this.stream = data;
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name =
-          new StreamName(columnId,
-              OrcProto.Stream.Kind.DATA);
-      stream = streams.get(name);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      stream.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final DoubleColumnVector result = (DoubleColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      final boolean hasNulls = !result.noNulls;
-      boolean allNulls = hasNulls;
-
-      if (hasNulls) {
-        // conditions to ensure bounds checks skips
-        for (int i = 0; i < batchSize && batchSize <= result.isNull.length; i++) {
-          allNulls = allNulls & result.isNull[i];
-        }
-        if (allNulls) {
-          result.vector[0] = Double.NaN;
-          result.isRepeating = true;
-        } else {
-          // some nulls
-          result.isRepeating = false;
-          // conditions to ensure bounds checks skips
-          for (int i = 0; batchSize <= result.isNull.length
-              && batchSize <= result.vector.length && i < batchSize; i++) {
-            if (!result.isNull[i]) {
-              result.vector[i] = utils.readDouble(stream);
-            } else {
-              // If the value is not present then set NaN
-              result.vector[i] = Double.NaN;
-            }
-          }
-        }
-      } else {
-        // no nulls
-        boolean repeating = (batchSize > 1);
-        final double d1 = utils.readDouble(stream);
-        result.vector[0] = d1;
-        // conditions to ensure bounds checks skips
-        for (int i = 1; i < batchSize && batchSize <= result.vector.length; i++) {
-          final double d2 = utils.readDouble(stream);
-          repeating = repeating && (d1 == d2);
-          result.vector[i] = d2;
-        }
-        result.isRepeating = repeating;
-      }
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      long len = items * 8;
-      while (len > 0) {
-        len -= stream.skip(len);
-      }
-    }
-  }
-
-  public static class BinaryTreeReader extends TreeReader {
-    protected InStream stream;
-    protected IntegerReader lengths = null;
-    protected final LongColumnVector scratchlcv;
-
-    BinaryTreeReader(int columnId) throws IOException {
-      this(columnId, null, null, null, null);
-    }
-
-    protected BinaryTreeReader(int columnId, InStream present, InStream data, InStream length,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present);
-      scratchlcv = new LongColumnVector();
-      this.stream = data;
-      if (length != null && encoding != null) {
-        checkEncoding(encoding);
-        this.lengths = createIntegerReader(encoding.getKind(), length, false, false);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      stream = streams.get(name);
-      lengths = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId, OrcProto.Stream.Kind.LENGTH)), false, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      stream.seek(index);
-      lengths.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final BytesColumnVector result = (BytesColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      BytesColumnVectorUtil.readOrcByteArrays(stream, lengths, scratchlcv, result, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      long lengthToSkip = 0;
-      for (int i = 0; i < items; ++i) {
-        lengthToSkip += lengths.next();
-      }
-      while (lengthToSkip > 0) {
-        lengthToSkip -= stream.skip(lengthToSkip);
-      }
-    }
-  }
-
-  public static class TimestampTreeReader extends TreeReader {
-    protected IntegerReader data = null;
-    protected IntegerReader nanos = null;
-    private final boolean skipCorrupt;
-    private Map<String, Long> baseTimestampMap;
-    protected long base_timestamp;
-    private final TimeZone readerTimeZone;
-    private TimeZone writerTimeZone;
-    private boolean hasSameTZRules;
-
-    TimestampTreeReader(int columnId, boolean skipCorrupt) throws IOException {
-      this(columnId, null, null, null, null, skipCorrupt, null);
-    }
-
-    protected TimestampTreeReader(int columnId, InStream presentStream, InStream dataStream,
-        InStream nanosStream, OrcProto.ColumnEncoding encoding, boolean skipCorrupt, String writerTimezone)
-        throws IOException {
-      super(columnId, presentStream);
-      this.skipCorrupt = skipCorrupt;
-      this.baseTimestampMap = new HashMap<>();
-      this.readerTimeZone = TimeZone.getDefault();
-      this.writerTimeZone = readerTimeZone;
-      this.hasSameTZRules = writerTimeZone.hasSameRules(readerTimeZone);
-      this.base_timestamp = getBaseTimestamp(readerTimeZone.getID());
-      if (encoding != null) {
-        checkEncoding(encoding);
-
-        if (dataStream != null) {
-          this.data = createIntegerReader(encoding.getKind(), dataStream, true, skipCorrupt);
-        }
-
-        if (nanosStream != null) {
-          this.nanos = createIntegerReader(encoding.getKind(), nanosStream, false, skipCorrupt);
-        }
-        base_timestamp = getBaseTimestamp(writerTimezone);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      data = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.DATA)), true, skipCorrupt);
-      nanos = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.SECONDARY)), false, skipCorrupt);
-      base_timestamp = getBaseTimestamp(stripeFooter.getWriterTimezone());
-    }
-
-    protected long getBaseTimestamp(String timeZoneId) throws IOException {
-      // to make sure new readers read old files in the same way
-      if (timeZoneId == null || timeZoneId.isEmpty()) {
-        timeZoneId = readerTimeZone.getID();
-      }
-
-      if (!baseTimestampMap.containsKey(timeZoneId)) {
-        writerTimeZone = TimeZone.getTimeZone(timeZoneId);
-        hasSameTZRules = writerTimeZone.hasSameRules(readerTimeZone);
-        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
-        sdf.setTimeZone(writerTimeZone);
-        try {
-          long epoch =
-              sdf.parse(WriterImpl.BASE_TIMESTAMP_STRING).getTime() / WriterImpl.MILLIS_PER_SECOND;
-          baseTimestampMap.put(timeZoneId, epoch);
-          return epoch;
-        } catch (ParseException e) {
-          throw new IOException("Unable to create base timestamp", e);
-        } finally {
-          sdf.setTimeZone(readerTimeZone);
-        }
-      }
-
-      return baseTimestampMap.get(timeZoneId);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      data.seek(index);
-      nanos.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      TimestampColumnVector result = (TimestampColumnVector) previousVector;
-      super.nextVector(previousVector, isNull, batchSize);
-
-      for (int i = 0; i < batchSize; i++) {
-        if (result.noNulls || !result.isNull[i]) {
-          long millis = data.next() + base_timestamp;
-          int newNanos = parseNanos(nanos.next());
-          if (millis < 0 && newNanos != 0) {
-            millis -= 1;
-          }
-          millis *= WriterImpl.MILLIS_PER_SECOND;
-          long offset = 0;
-          // If reader and writer time zones have different rules, adjust the timezone difference
-          // between reader and writer taking day light savings into account.
-          if (!hasSameTZRules) {
-            offset = writerTimeZone.getOffset(millis) - readerTimeZone.getOffset(millis);
-          }
-          long adjustedMillis = millis + offset;
-          // Sometimes the reader timezone might have changed after adding the adjustedMillis.
-          // To account for that change, check for any difference in reader timezone after
-          // adding adjustedMillis. If so use the new offset (offset at adjustedMillis point of time).
-          if (!hasSameTZRules &&
-              (readerTimeZone.getOffset(millis) != readerTimeZone.getOffset(adjustedMillis))) {
-            long newOffset =
-                writerTimeZone.getOffset(millis) - readerTimeZone.getOffset(adjustedMillis);
-            adjustedMillis = millis + newOffset;
-          }
-          result.time[i] = adjustedMillis;
-          result.nanos[i] = newNanos;
-          if (result.isRepeating && i != 0 &&
-              (result.time[0] != result.time[i] ||
-                  result.nanos[0] != result.nanos[i])) {
-            result.isRepeating = false;
-          }
-        }
-      }
-    }
-
-    private static int parseNanos(long serialized) {
-      int zeros = 7 & (int) serialized;
-      int result = (int) (serialized >>> 3);
-      if (zeros != 0) {
-        for (int i = 0; i <= zeros; ++i) {
-          result *= 10;
-        }
-      }
-      return result;
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      data.skip(items);
-      nanos.skip(items);
-    }
-  }
-
-  public static class DateTreeReader extends TreeReader {
-    protected IntegerReader reader = null;
-
-    DateTreeReader(int columnId) throws IOException {
-      this(columnId, null, null, null);
-    }
-
-    protected DateTreeReader(int columnId, InStream present, InStream data,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present);
-      if (data != null && encoding != null) {
-        checkEncoding(encoding);
-        reader = createIntegerReader(encoding.getKind(), data, true, false);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(name), true, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final LongColumnVector result = (LongColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      // Read value entries based on isNull entries
-      reader.nextVector(result, result.vector, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-  }
-
-  public static class DecimalTreeReader extends TreeReader {
-    protected InStream valueStream;
-    protected IntegerReader scaleReader = null;
-    private int[] scratchScaleVector;
-    private byte[] scratchBytes;
-
-    private final int precision;
-    private final int scale;
-
-    DecimalTreeReader(int columnId, int precision, int scale) throws IOException {
-      this(columnId, precision, scale, null, null, null, null);
-    }
-
-    protected DecimalTreeReader(int columnId, int precision, int scale, InStream present,
-        InStream valueStream, InStream scaleStream, OrcProto.ColumnEncoding encoding)
-        throws IOException {
-      super(columnId, present);
-      this.precision = precision;
-      this.scale = scale;
-      this.scratchScaleVector = new int[VectorizedRowBatch.DEFAULT_SIZE];
-      this.valueStream = valueStream;
-      this.scratchBytes = new byte[HiveDecimal.SCRATCH_BUFFER_LEN_SERIALIZATION_UTILS_READ];
-      if (scaleStream != null && encoding != null) {
-        checkEncoding(encoding);
-        this.scaleReader = createIntegerReader(encoding.getKind(), scaleStream, true, false);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      valueStream = streams.get(new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA));
-      scaleReader = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId, OrcProto.Stream.Kind.SECONDARY)), true, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      valueStream.seek(index);
-      scaleReader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final DecimalColumnVector result = (DecimalColumnVector) previousVector;
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      if (batchSize > scratchScaleVector.length) {
-        scratchScaleVector = new int[(int) batchSize];
-      }
-      // read the scales
-      scaleReader.nextVector(result, scratchScaleVector, batchSize);
-      // Read value entries based on isNull entries
-      // Use the fast ORC deserialization method that emulates SerializationUtils.readBigInteger
-      // provided by HiveDecimalWritable.
-      HiveDecimalWritable[] vector = result.vector;
-      HiveDecimalWritable decWritable;
-      if (result.noNulls) {
-        for (int r=0; r < batchSize; ++r) {
-          decWritable = vector[r];
-          if (!decWritable.serializationUtilsRead(
-              valueStream, scratchScaleVector[r],
-              scratchBytes)) {
-            result.isNull[r] = true;
-            result.noNulls = false;
-          }
-        }
-      } else if (!result.isRepeating || !result.isNull[0]) {
-        for (int r=0; r < batchSize; ++r) {
-          if (!result.isNull[r]) {
-            decWritable = vector[r];
-            if (!decWritable.serializationUtilsRead(
-                valueStream, scratchScaleVector[r],
-                scratchBytes)) {
-              result.isNull[r] = true;
-              result.noNulls = false;
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      HiveDecimalWritable scratchDecWritable = new HiveDecimalWritable();
-      for (int i = 0; i < items; i++) {
-        scratchDecWritable.serializationUtilsRead(valueStream, 0, scratchBytes);
-      }
-      scaleReader.skip(items);
-    }
-  }
-
-  /**
-   * A tree reader that will read string columns. At the start of the
-   * stripe, it creates an internal reader based on whether a direct or
-   * dictionary encoding was used.
-   */
-  public static class StringTreeReader extends TreeReader {
-    protected TreeReader reader;
-
-    StringTreeReader(int columnId) throws IOException {
-      super(columnId);
-    }
-
-    protected StringTreeReader(int columnId, InStream present, InStream data, InStream length,
-        InStream dictionary, OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present);
-      if (encoding != null) {
-        switch (encoding.getKind()) {
-          case DIRECT:
-          case DIRECT_V2:
-            reader = new StringDirectTreeReader(columnId, present, data, length,
-                encoding.getKind());
-            break;
-          case DICTIONARY:
-          case DICTIONARY_V2:
-            reader = new StringDictionaryTreeReader(columnId, present, data, length, dictionary,
-                encoding);
-            break;
-          default:
-            throw new IllegalArgumentException("Unsupported encoding " +
-                encoding.getKind());
-        }
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      reader.checkEncoding(encoding);
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      // For each stripe, checks the encoding and initializes the appropriate
-      // reader
-      switch (stripeFooter.getColumnsList().get(columnId).getKind()) {
-        case DIRECT:
-        case DIRECT_V2:
-          reader = new StringDirectTreeReader(columnId);
-          break;
-        case DICTIONARY:
-        case DICTIONARY_V2:
-          reader = new StringDictionaryTreeReader(columnId);
-          break;
-        default:
-          throw new IllegalArgumentException("Unsupported encoding " +
-              stripeFooter.getColumnsList().get(columnId).getKind());
-      }
-      reader.startStripe(streams, stripeFooter);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      reader.seek(index);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      reader.nextVector(previousVector, isNull, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skipRows(items);
-    }
-  }
-
-  private static org.slf4j.Logger LOG = org.slf4j.LoggerFactory.getLogger(TreeReaderFactory.class);
-  // This class collects together very similar methods for reading an ORC vector of byte arrays and
-  // creating the BytesColumnVector.
-  //
-  public static class BytesColumnVectorUtil {
-
-    private static byte[] commonReadByteArrays(InStream stream, IntegerReader lengths,
-        LongColumnVector scratchlcv,
-        BytesColumnVector result, final int batchSize) throws IOException {
-      // Read lengths
-      scratchlcv.isNull = result.isNull;  // Notice we are replacing the isNull vector here...
-      scratchlcv.ensureSize(batchSize, false);
-      lengths.nextVector(scratchlcv, scratchlcv.vector, batchSize);
-      int totalLength = 0;
-      if (!scratchlcv.isRepeating) {
-        for (int i = 0; i < batchSize; i++) {
-          if (!scratchlcv.isNull[i]) {
-            totalLength += (int) scratchlcv.vector[i];
-          }
-        }
-      } else {
-        if (!scratchlcv.isNull[0]) {
-          totalLength = (int) (batchSize * scratchlcv.vector[0]);
-        }
-      }
-
-      // Read all the strings for this batch
-      byte[] allBytes = new byte[totalLength];
-      int offset = 0;
-      int len = totalLength;
-      while (len > 0) {
-        int bytesRead = stream.read(allBytes, offset, len);
-        if (bytesRead < 0) {
-          throw new EOFException("Can't finish byte read from " + stream);
-        }
-        len -= bytesRead;
-        offset += bytesRead;
-      }
-
-      return allBytes;
-    }
-
-    // This method has the common code for reading in bytes into a BytesColumnVector.
-    public static void readOrcByteArrays(InStream stream,
-                                         IntegerReader lengths,
-                                         LongColumnVector scratchlcv,
-                                         BytesColumnVector result,
-                                         final int batchSize) throws IOException {
-      if (result.noNulls || !(result.isRepeating && result.isNull[0])) {
-        byte[] allBytes = commonReadByteArrays(stream, lengths, scratchlcv,
-            result, (int) batchSize);
-
-        // Too expensive to figure out 'repeating' by comparisons.
-        result.isRepeating = false;
-        int offset = 0;
-        if (!scratchlcv.isRepeating) {
-          for (int i = 0; i < batchSize; i++) {
-            if (!scratchlcv.isNull[i]) {
-              result.setRef(i, allBytes, offset, (int) scratchlcv.vector[i]);
-              offset += scratchlcv.vector[i];
-            } else {
-              result.setRef(i, allBytes, 0, 0);
-            }
-          }
-        } else {
-          for (int i = 0; i < batchSize; i++) {
-            if (!scratchlcv.isNull[i]) {
-              result.setRef(i, allBytes, offset, (int) scratchlcv.vector[0]);
-              offset += scratchlcv.vector[0];
-            } else {
-              result.setRef(i, allBytes, 0, 0);
-            }
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * A reader for string columns that are direct encoded in the current
-   * stripe.
-   */
-  public static class StringDirectTreeReader extends TreeReader {
-    private static final HadoopShims SHIMS = HadoopShims.Factory.get();
-    protected InStream stream;
-    protected HadoopShims.TextReaderShim data;
-    protected IntegerReader lengths;
-    private final LongColumnVector scratchlcv;
-
-    StringDirectTreeReader(int columnId) throws IOException {
-      this(columnId, null, null, null, null);
-    }
-
-    protected StringDirectTreeReader(int columnId, InStream present, InStream data,
-        InStream length, OrcProto.ColumnEncoding.Kind encoding) throws IOException {
-      super(columnId, present);
-      this.scratchlcv = new LongColumnVector();
-      this.stream = data;
-      if (length != null && encoding != null) {
-        this.lengths = createIntegerReader(encoding, length, false, false);
-        this.data = SHIMS.getTextReaderShim(this.stream);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT &&
-          encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA);
-      stream = streams.get(name);
-      data = SHIMS.getTextReaderShim(this.stream);
-      lengths = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId, OrcProto.Stream.Kind.LENGTH)),
-          false, false);
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      stream.seek(index);
-      // don't seek data stream
-      lengths.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final BytesColumnVector result = (BytesColumnVector) previousVector;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      BytesColumnVectorUtil.readOrcByteArrays(stream, lengths, scratchlcv,
-          result, batchSize);
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      long lengthToSkip = 0;
-      for (int i = 0; i < items; ++i) {
-        lengthToSkip += lengths.next();
-      }
-
-      while (lengthToSkip > 0) {
-        lengthToSkip -= stream.skip(lengthToSkip);
-      }
-    }
-
-    public IntegerReader getLengths() {
-      return lengths;
-    }
-
-    public InStream getStream() {
-      return stream;
-    }
-  }
-
-  /**
-   * A reader for string columns that are dictionary encoded in the current
-   * stripe.
-   */
-  public static class StringDictionaryTreeReader extends TreeReader {
-    private static final byte[] EMPTY_BYTE_ARRAY = new byte[0];
-    private DynamicByteArray dictionaryBuffer;
-    private int[] dictionaryOffsets;
-    protected IntegerReader reader;
-
-    private byte[] dictionaryBufferInBytesCache = null;
-    private final LongColumnVector scratchlcv;
-
-    StringDictionaryTreeReader(int columnId) throws IOException {
-      this(columnId, null, null, null, null, null);
-    }
-
-    protected StringDictionaryTreeReader(int columnId, InStream present, InStream data,
-        InStream length, InStream dictionary, OrcProto.ColumnEncoding encoding)
-        throws IOException {
-      super(columnId, present);
-      scratchlcv = new LongColumnVector();
-      if (data != null && encoding != null) {
-        this.reader = createIntegerReader(encoding.getKind(), data, false, false);
-      }
-
-      if (dictionary != null && encoding != null) {
-        readDictionaryStream(dictionary);
-      }
-
-      if (length != null && encoding != null) {
-        readDictionaryLengthStream(length, encoding);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DICTIONARY &&
-          encoding.getKind() != OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-
-      // read the dictionary blob
-      StreamName name = new StreamName(columnId,
-          OrcProto.Stream.Kind.DICTIONARY_DATA);
-      InStream in = streams.get(name);
-      readDictionaryStream(in);
-
-      // read the lengths
-      name = new StreamName(columnId, OrcProto.Stream.Kind.LENGTH);
-      in = streams.get(name);
-      readDictionaryLengthStream(in, stripeFooter.getColumnsList().get(columnId));
-
-      // set up the row reader
-      name = new StreamName(columnId, OrcProto.Stream.Kind.DATA);
-      reader = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(name), false, false);
-    }
-
-    private void readDictionaryLengthStream(InStream in, OrcProto.ColumnEncoding encoding)
-        throws IOException {
-      int dictionarySize = encoding.getDictionarySize();
-      if (in != null) { // Guard against empty LENGTH stream.
-        IntegerReader lenReader = createIntegerReader(encoding.getKind(), in, false, false);
-        int offset = 0;
-        if (dictionaryOffsets == null ||
-            dictionaryOffsets.length < dictionarySize + 1) {
-          dictionaryOffsets = new int[dictionarySize + 1];
-        }
-        for (int i = 0; i < dictionarySize; ++i) {
-          dictionaryOffsets[i] = offset;
-          offset += (int) lenReader.next();
-        }
-        dictionaryOffsets[dictionarySize] = offset;
-        in.close();
-      }
-
-    }
-
-    private void readDictionaryStream(InStream in) throws IOException {
-      if (in != null) { // Guard against empty dictionary stream.
-        if (in.available() > 0) {
-          dictionaryBuffer = new DynamicByteArray(64, in.available());
-          dictionaryBuffer.readAll(in);
-          // Since its start of strip invalidate the cache.
-          dictionaryBufferInBytesCache = null;
-        }
-        in.close();
-      } else {
-        dictionaryBuffer = null;
-      }
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      seek(index[columnId]);
-    }
-
-    @Override
-    public void seek(PositionProvider index) throws IOException {
-      super.seek(index);
-      reader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      final BytesColumnVector result = (BytesColumnVector) previousVector;
-      int offset;
-      int length;
-
-      // Read present/isNull stream
-      super.nextVector(result, isNull, batchSize);
-
-      if (dictionaryBuffer != null) {
-
-        // Load dictionaryBuffer into cache.
-        if (dictionaryBufferInBytesCache == null) {
-          dictionaryBufferInBytesCache = dictionaryBuffer.get();
-        }
-
-        // Read string offsets
-        scratchlcv.isNull = result.isNull;
-        scratchlcv.ensureSize((int) batchSize, false);
-        reader.nextVector(scratchlcv, scratchlcv.vector, batchSize);
-        if (!scratchlcv.isRepeating) {
-
-          // The vector has non-repeating strings. Iterate thru the batch
-          // and set strings one by one
-          for (int i = 0; i < batchSize; i++) {
-            if (!scratchlcv.isNull[i]) {
-              offset = dictionaryOffsets[(int) scratchlcv.vector[i]];
-              length = getDictionaryEntryLength((int) scratchlcv.vector[i], offset);
-              result.setRef(i, dictionaryBufferInBytesCache, offset, length);
-            } else {
-              // If the value is null then set offset and length to zero (null string)
-              result.setRef(i, dictionaryBufferInBytesCache, 0, 0);
-            }
-          }
-        } else {
-          // If the value is repeating then just set the first value in the
-          // vector and set the isRepeating flag to true. No need to iterate thru and
-          // set all the elements to the same value
-          offset = dictionaryOffsets[(int) scratchlcv.vector[0]];
-          length = getDictionaryEntryLength((int) scratchlcv.vector[0], offset);
-          result.setRef(0, dictionaryBufferInBytesCache, offset, length);
-        }
-        result.isRepeating = scratchlcv.isRepeating;
-      } else {
-        if (dictionaryOffsets == null) {
-          // Entire stripe contains null strings.
-          result.isRepeating = true;
-          result.noNulls = false;
-          result.isNull[0] = true;
-          result.setRef(0, EMPTY_BYTE_ARRAY, 0, 0);
-        } else {
-          // stripe contains nulls and empty strings
-          for (int i = 0; i < batchSize; i++) {
-            if (!result.isNull[i]) {
-              result.setRef(i, EMPTY_BYTE_ARRAY, 0, 0);
-            }
-          }
-        }
-      }
-    }
-
-    int getDictionaryEntryLength(int entry, int offset) {
-      final int length;
-      // if it isn't the last entry, subtract the offsets otherwise use
-      // the buffer length.
-      if (entry < dictionaryOffsets.length - 1) {
-        length = dictionaryOffsets[entry + 1] - offset;
-      } else {
-        length = dictionaryBuffer.size() - offset;
-      }
-      return length;
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      reader.skip(countNonNulls(items));
-    }
-
-    public IntegerReader getReader() {
-      return reader;
-    }
-  }
-
-  public static class CharTreeReader extends StringTreeReader {
-    int maxLength;
-
-    CharTreeReader(int columnId, int maxLength) throws IOException {
-      this(columnId, maxLength, null, null, null, null, null);
-    }
-
-    protected CharTreeReader(int columnId, int maxLength, InStream present, InStream data,
-        InStream length, InStream dictionary, OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, length, dictionary, encoding);
-      this.maxLength = maxLength;
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      // Get the vector of strings from StringTreeReader, then make a 2nd pass to
-      // adjust down the length (right trim and truncate) if necessary.
-      super.nextVector(previousVector, isNull, batchSize);
-      BytesColumnVector result = (BytesColumnVector) previousVector;
-      int adjustedDownLen;
-      if (result.isRepeating) {
-        if (result.noNulls || !result.isNull[0]) {
-          adjustedDownLen = StringExpr
-              .rightTrimAndTruncate(result.vector[0], result.start[0], result.length[0], maxLength);
-          if (adjustedDownLen < result.length[0]) {
-            result.setRef(0, result.vector[0], result.start[0], adjustedDownLen);
-          }
-        }
-      } else {
-        if (result.noNulls) {
-          for (int i = 0; i < batchSize; i++) {
-            adjustedDownLen = StringExpr
-                .rightTrimAndTruncate(result.vector[i], result.start[i], result.length[i],
-                    maxLength);
-            if (adjustedDownLen < result.length[i]) {
-              result.setRef(i, result.vector[i], result.start[i], adjustedDownLen);
-            }
-          }
-        } else {
-          for (int i = 0; i < batchSize; i++) {
-            if (!result.isNull[i]) {
-              adjustedDownLen = StringExpr
-                  .rightTrimAndTruncate(result.vector[i], result.start[i], result.length[i],
-                      maxLength);
-              if (adjustedDownLen < result.length[i]) {
-                result.setRef(i, result.vector[i], result.start[i], adjustedDownLen);
-              }
-            }
-          }
-        }
-      }
-    }
-  }
-
-  public static class VarcharTreeReader extends StringTreeReader {
-    int maxLength;
-
-    VarcharTreeReader(int columnId, int maxLength) throws IOException {
-      this(columnId, maxLength, null, null, null, null, null);
-    }
-
-    protected VarcharTreeReader(int columnId, int maxLength, InStream present, InStream data,
-        InStream length, InStream dictionary, OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, length, dictionary, encoding);
-      this.maxLength = maxLength;
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      // Get the vector of strings from StringTreeReader, then make a 2nd pass to
-      // adjust down the length (truncate) if necessary.
-      super.nextVector(previousVector, isNull, batchSize);
-      BytesColumnVector result = (BytesColumnVector) previousVector;
-
-      int adjustedDownLen;
-      if (result.isRepeating) {
-        if (result.noNulls || !result.isNull[0]) {
-          adjustedDownLen = StringExpr
-              .truncate(result.vector[0], result.start[0], result.length[0], maxLength);
-          if (adjustedDownLen < result.length[0]) {
-            result.setRef(0, result.vector[0], result.start[0], adjustedDownLen);
-          }
-        }
-      } else {
-        if (result.noNulls) {
-          for (int i = 0; i < batchSize; i++) {
-            adjustedDownLen = StringExpr
-                .truncate(result.vector[i], result.start[i], result.length[i], maxLength);
-            if (adjustedDownLen < result.length[i]) {
-              result.setRef(i, result.vector[i], result.start[i], adjustedDownLen);
-            }
-          }
-        } else {
-          for (int i = 0; i < batchSize; i++) {
-            if (!result.isNull[i]) {
-              adjustedDownLen = StringExpr
-                  .truncate(result.vector[i], result.start[i], result.length[i], maxLength);
-              if (adjustedDownLen < result.length[i]) {
-                result.setRef(i, result.vector[i], result.start[i], adjustedDownLen);
-              }
-            }
-          }
-        }
-      }
-    }
-  }
-
-  public static class StructTreeReader extends TreeReader {
-    protected final TreeReader[] fields;
-
-    protected StructTreeReader(int columnId,
-                               TypeDescription readerSchema,
-                               SchemaEvolution evolution,
-                               boolean[] included,
-                               boolean skipCorrupt) throws IOException {
-      super(columnId);
-
-      List<TypeDescription> childrenTypes = readerSchema.getChildren();
-      this.fields = new TreeReader[childrenTypes.size()];
-      for (int i = 0; i < fields.length; ++i) {
-        TypeDescription subtype = childrenTypes.get(i);
-        this.fields[i] = createTreeReader(subtype, evolution, included, skipCorrupt);
-      }
-    }
-
-    public TreeReader[] getChildReaders() {
-      return fields;
-    }
-
-    protected StructTreeReader(int columnId, InStream present,
-        OrcProto.ColumnEncoding encoding, TreeReader[] childReaders) throws IOException {
-      super(columnId, present);
-      if (encoding != null) {
-        checkEncoding(encoding);
-      }
-      this.fields = childReaders;
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      super.seek(index);
-      for (TreeReader kid : fields) {
-        if (kid != null) {
-          kid.seek(index);
-        }
-      }
-    }
-
-    @Override
-    public void nextBatch(VectorizedRowBatch batch,
-                          int batchSize) throws IOException {
-      for(int i=0; i < fields.length &&
-          (vectorColumnCount == -1 || i < vectorColumnCount); ++i) {
-        ColumnVector colVector = batch.cols[i];
-        if (colVector != null) {
-          colVector.reset();
-          colVector.ensureSize((int) batchSize, false);
-          fields[i].nextVector(colVector, null, batchSize);
-        }
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      super.nextVector(previousVector, isNull, batchSize);
-      StructColumnVector result = (StructColumnVector) previousVector;
-      if (result.noNulls || !(result.isRepeating && result.isNull[0])) {
-        result.isRepeating = false;
-
-        // Read all the members of struct as column vectors
-        boolean[] mask = result.noNulls ? null : result.isNull;
-        for (int f = 0; f < fields.length; f++) {
-          if (fields[f] != null) {
-            fields[f].nextVector(result.fields[f], mask, batchSize);
-          }
-        }
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      for (TreeReader field : fields) {
-        if (field != null) {
-          field.startStripe(streams, stripeFooter);
-        }
-      }
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      for (TreeReader field : fields) {
-        if (field != null) {
-          field.skipRows(items);
-        }
-      }
-    }
-  }
-
-  public static class UnionTreeReader extends TreeReader {
-    protected final TreeReader[] fields;
-    protected RunLengthByteReader tags;
-
-    protected UnionTreeReader(int fileColumn,
-                              TypeDescription readerSchema,
-                              SchemaEvolution evolution,
-                              boolean[] included,
-                              boolean skipCorrupt) throws IOException {
-      super(fileColumn);
-      List<TypeDescription> childrenTypes = readerSchema.getChildren();
-      int fieldCount = childrenTypes.size();
-      this.fields = new TreeReader[fieldCount];
-      for (int i = 0; i < fieldCount; ++i) {
-        TypeDescription subtype = childrenTypes.get(i);
-        this.fields[i] = createTreeReader(subtype, evolution, included, skipCorrupt);
-      }
-    }
-
-    protected UnionTreeReader(int columnId, InStream present,
-        OrcProto.ColumnEncoding encoding, TreeReader[] childReaders) throws IOException {
-      super(columnId, present);
-      if (encoding != null) {
-        checkEncoding(encoding);
-      }
-      this.fields = childReaders;
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      super.seek(index);
-      tags.seek(index[columnId]);
-      for (TreeReader kid : fields) {
-        kid.seek(index);
-      }
-    }
-
-    @Override
-    public void nextVector(ColumnVector previousVector,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      UnionColumnVector result = (UnionColumnVector) previousVector;
-      super.nextVector(result, isNull, batchSize);
-      if (result.noNulls || !(result.isRepeating && result.isNull[0])) {
-        result.isRepeating = false;
-        tags.nextVector(result.noNulls ? null : result.isNull, result.tags,
-            batchSize);
-        boolean[] ignore = new boolean[(int) batchSize];
-        for (int f = 0; f < result.fields.length; ++f) {
-          // build the ignore list for this tag
-          for (int r = 0; r < batchSize; ++r) {
-            ignore[r] = (!result.noNulls && result.isNull[r]) ||
-                result.tags[r] != f;
-          }
-          fields[f].nextVector(result.fields[f], ignore, batchSize);
-        }
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      tags = new RunLengthByteReader(streams.get(new StreamName(columnId,
-          OrcProto.Stream.Kind.DATA)));
-      for (TreeReader field : fields) {
-        if (field != null) {
-          field.startStripe(streams, stripeFooter);
-        }
-      }
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      long[] counts = new long[fields.length];
-      for (int i = 0; i < items; ++i) {
-        counts[tags.next()] += 1;
-      }
-      for (int i = 0; i < counts.length; ++i) {
-        fields[i].skipRows(counts[i]);
-      }
-    }
-  }
-
-  public static class ListTreeReader extends TreeReader {
-    protected final TreeReader elementReader;
-    protected IntegerReader lengths = null;
-
-    protected ListTreeReader(int fileColumn,
-                             TypeDescription readerSchema,
-                             SchemaEvolution evolution,
-                             boolean[] included,
-                             boolean skipCorrupt) throws IOException {
-      super(fileColumn);
-      TypeDescription elementType = readerSchema.getChildren().get(0);
-      elementReader = createTreeReader(elementType, evolution, included,
-          skipCorrupt);
-    }
-
-    protected ListTreeReader(int columnId, InStream present, InStream data,
-        OrcProto.ColumnEncoding encoding, TreeReader elementReader) throws IOException {
-      super(columnId, present);
-      if (data != null && encoding != null) {
-        checkEncoding(encoding);
-        this.lengths = createIntegerReader(encoding.getKind(), data, false, false);
-      }
-      this.elementReader = elementReader;
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      super.seek(index);
-      lengths.seek(index[columnId]);
-      elementReader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previous,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      ListColumnVector result = (ListColumnVector) previous;
-      super.nextVector(result, isNull, batchSize);
-      // if we have some none-null values, then read them
-      if (result.noNulls || !(result.isRepeating && result.isNull[0])) {
-        lengths.nextVector(result, result.lengths, batchSize);
-        // even with repeating lengths, the list doesn't repeat
-        result.isRepeating = false;
-        // build the offsets vector and figure out how many children to read
-        result.childCount = 0;
-        for (int r = 0; r < batchSize; ++r) {
-          if (result.noNulls || !result.isNull[r]) {
-            result.offsets[r] = result.childCount;
-            result.childCount += result.lengths[r];
-          }
-        }
-        result.child.ensureSize(result.childCount, false);
-        elementReader.nextVector(result.child, null, result.childCount);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      lengths = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.LENGTH)), false, false);
-      if (elementReader != null) {
-        elementReader.startStripe(streams, stripeFooter);
-      }
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      long childSkip = 0;
-      for (long i = 0; i < items; ++i) {
-        childSkip += lengths.next();
-      }
-      elementReader.skipRows(childSkip);
-    }
-  }
-
-  public static class MapTreeReader extends TreeReader {
-    protected final TreeReader keyReader;
-    protected final TreeReader valueReader;
-    protected IntegerReader lengths = null;
-
-    protected MapTreeReader(int fileColumn,
-                            TypeDescription readerSchema,
-                            SchemaEvolution evolution,
-                            boolean[] included,
-                            boolean skipCorrupt) throws IOException {
-      super(fileColumn);
-      TypeDescription keyType = readerSchema.getChildren().get(0);
-      TypeDescription valueType = readerSchema.getChildren().get(1);
-      keyReader = createTreeReader(keyType, evolution, included, skipCorrupt);
-      valueReader = createTreeReader(valueType, evolution, included, skipCorrupt);
-    }
-
-    protected MapTreeReader(int columnId, InStream present, InStream data,
-        OrcProto.ColumnEncoding encoding, TreeReader keyReader, TreeReader valueReader)
-        throws IOException {
-      super(columnId, present);
-      if (data != null && encoding != null) {
-        checkEncoding(encoding);
-        this.lengths = createIntegerReader(encoding.getKind(), data, false, false);
-      }
-      this.keyReader = keyReader;
-      this.valueReader = valueReader;
-    }
-
-    @Override
-    public void seek(PositionProvider[] index) throws IOException {
-      super.seek(index);
-      lengths.seek(index[columnId]);
-      keyReader.seek(index);
-      valueReader.seek(index);
-    }
-
-    @Override
-    public void nextVector(ColumnVector previous,
-                           boolean[] isNull,
-                           final int batchSize) throws IOException {
-      MapColumnVector result = (MapColumnVector) previous;
-      super.nextVector(result, isNull, batchSize);
-      if (result.noNulls || !(result.isRepeating && result.isNull[0])) {
-        lengths.nextVector(result, result.lengths, batchSize);
-        // even with repeating lengths, the map doesn't repeat
-        result.isRepeating = false;
-        // build the offsets vector and figure out how many children to read
-        result.childCount = 0;
-        for (int r = 0; r < batchSize; ++r) {
-          if (result.noNulls || !result.isNull[r]) {
-            result.offsets[r] = result.childCount;
-            result.childCount += result.lengths[r];
-          }
-        }
-        result.keys.ensureSize(result.childCount, false);
-        result.values.ensureSize(result.childCount, false);
-        keyReader.nextVector(result.keys, null, result.childCount);
-        valueReader.nextVector(result.values, null, result.childCount);
-      }
-    }
-
-    @Override
-    void checkEncoding(OrcProto.ColumnEncoding encoding) throws IOException {
-      if ((encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT) &&
-          (encoding.getKind() != OrcProto.ColumnEncoding.Kind.DIRECT_V2)) {
-        throw new IOException("Unknown encoding " + encoding + " in column " +
-            columnId);
-      }
-    }
-
-    @Override
-    void startStripe(Map<StreamName, InStream> streams,
-        OrcProto.StripeFooter stripeFooter
-    ) throws IOException {
-      super.startStripe(streams, stripeFooter);
-      lengths = createIntegerReader(stripeFooter.getColumnsList().get(columnId).getKind(),
-          streams.get(new StreamName(columnId,
-              OrcProto.Stream.Kind.LENGTH)), false, false);
-      if (keyReader != null) {
-        keyReader.startStripe(streams, stripeFooter);
-      }
-      if (valueReader != null) {
-        valueReader.startStripe(streams, stripeFooter);
-      }
-    }
-
-    @Override
-    void skipRows(long items) throws IOException {
-      items = countNonNulls(items);
-      long childSkip = 0;
-      for (long i = 0; i < items; ++i) {
-        childSkip += lengths.next();
-      }
-      keyReader.skipRows(childSkip);
-      valueReader.skipRows(childSkip);
-    }
-  }
-
-  public static TreeReader createTreeReader(TypeDescription readerType,
-                                            SchemaEvolution evolution,
-                                            boolean[] included,
-                                            boolean skipCorrupt
-                                            ) throws IOException {
-    TypeDescription fileType = evolution.getFileType(readerType);
-    if (fileType == null || !evolution.includeReaderColumn(readerType.getId())){
-      return new NullTreeReader(0);
-    }
-    TypeDescription.Category readerTypeCategory = readerType.getCategory();
-    if (!fileType.equals(readerType) &&
-        (readerTypeCategory != TypeDescription.Category.STRUCT &&
-         readerTypeCategory != TypeDescription.Category.MAP &&
-         readerTypeCategory != TypeDescription.Category.LIST &&
-         readerTypeCategory != TypeDescription.Category.UNION)) {
-      // We only convert complex children.
-      return ConvertTreeReaderFactory.createConvertTreeReader(readerType, evolution,
-          included, skipCorrupt);
-    }
-    switch (readerTypeCategory) {
-      case BOOLEAN:
-        return new BooleanTreeReader(fileType.getId());
-      case BYTE:
-        return new ByteTreeReader(fileType.getId());
-      case DOUBLE:
-        return new DoubleTreeReader(fileType.getId());
-      case FLOAT:
-        return new FloatTreeReader(fileType.getId());
-      case SHORT:
-        return new ShortTreeReader(fileType.getId());
-      case INT:
-        return new IntTreeReader(fileType.getId());
-      case LONG:
-        return new LongTreeReader(fileType.getId(), skipCorrupt);
-      case STRING:
-        return new StringTreeReader(fileType.getId());
-      case CHAR:
-        return new CharTreeReader(fileType.getId(), readerType.getMaxLength());
-      case VARCHAR:
-        return new VarcharTreeReader(fileType.getId(), readerType.getMaxLength());
-      case BINARY:
-        return new BinaryTreeReader(fileType.getId());
-      case TIMESTAMP:
-        return new TimestampTreeReader(fileType.getId(), skipCorrupt);
-      case DATE:
-        return new DateTreeReader(fileType.getId());
-      case DECIMAL:
-        return new DecimalTreeReader(fileType.getId(), readerType.getPrecision(),
-            readerType.getScale());
-      case STRUCT:
-        return new StructTreeReader(fileType.getId(), readerType,
-            evolution, included, skipCorrupt);
-      case LIST:
-        return new ListTreeReader(fileType.getId(), readerType,
-            evolution, included, skipCorrupt);
-      case MAP:
-        return new MapTreeReader(fileType.getId(), readerType, evolution,
-            included, skipCorrupt);
-      case UNION:
-        return new UnionTreeReader(fileType.getId(), readerType,
-            evolution, included, skipCorrupt);
-      default:
-        throw new IllegalArgumentException("Unsupported type " +
-            readerTypeCategory);
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/WriterImpl.java b/orc/src/java/org/apache/orc/impl/WriterImpl.java
deleted file mode 100644
index b1f3cfb1ca..0000000000
--- a/orc/src/java/org/apache/orc/impl/WriterImpl.java
+++ /dev/null
@@ -1,2444 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import static com.google.common.base.Preconditions.checkArgument;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.sql.Timestamp;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.TimeZone;
-import java.util.TreeMap;
-
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.hive.ql.util.JavaDataModel;
-import org.apache.orc.BinaryColumnStatistics;
-import org.apache.orc.BloomFilterIO;
-import org.apache.orc.OrcConf;
-import org.apache.orc.OrcFile;
-import org.apache.orc.OrcProto;
-import org.apache.orc.OrcProto.BloomFilterIndex;
-import org.apache.orc.OrcProto.RowIndex;
-import org.apache.orc.OrcProto.Stream;
-import org.apache.orc.OrcUtils;
-import org.apache.orc.StringColumnStatistics;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.io.Text;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Lists;
-import com.google.common.primitives.Longs;
-import com.google.protobuf.ByteString;
-
-/**
- * An ORC file writer. The file is divided into stripes, which is the natural
- * unit of work when reading. Each stripe is buffered in memory until the
- * memory reaches the stripe size and then it is written out broken down by
- * columns. Each column is written by a TreeWriter that is specific to that
- * type of column. TreeWriters may have children TreeWriters that handle the
- * sub-types. Each of the TreeWriters writes the column's data as a set of
- * streams.
- *
- * This class is unsynchronized like most Stream objects, so from the creation
- * of an OrcFile and all access to a single instance has to be from a single
- * thread.
- *
- * There are no known cases where these happen between different threads today.
- *
- * Caveat: the MemoryManager is created during WriterOptions create, that has
- * to be confined to a single thread as well.
- *
- */
-public class WriterImpl implements Writer, MemoryManager.Callback {
-
-  private static final Logger LOG = LoggerFactory.getLogger(WriterImpl.class);
-
-  private static final int MIN_ROW_INDEX_STRIDE = 1000;
-
-  private final Path path;
-  private final int rowIndexStride;
-  private final TypeDescription schema;
-
-  @VisibleForTesting
-  protected final PhysicalWriter physWriter;
-  private int columnCount;
-  private long rowCount = 0;
-  private long rowsInStripe = 0;
-  private long rawDataSize = 0;
-  private int rowsInIndex = 0;
-  private int stripesAtLastFlush = -1;
-  private final List<OrcProto.StripeInformation> stripes =
-    new ArrayList<OrcProto.StripeInformation>();
-  private final Map<String, ByteString> userMetadata =
-    new TreeMap<String, ByteString>();
-  private final StreamFactory streamFactory = new StreamFactory();
-  private final TreeWriter treeWriter;
-  private final boolean buildIndex;
-  private final MemoryManager memoryManager;
-  private final OrcFile.Version version;
-  private final Configuration conf;
-  private final OrcFile.WriterCallback callback;
-  private final OrcFile.WriterContext callbackContext;
-  private final OrcFile.EncodingStrategy encodingStrategy;
-  private final boolean[] bloomFilterColumns;
-  private final double bloomFilterFpp;
-  private boolean writeTimeZone;
-
-  public WriterImpl(FileSystem fs,
-                    Path path,
-                    OrcFile.WriterOptions opts) throws IOException {
-    this(new PhysicalFsWriter(fs, path, opts.getSchema().getMaximumId() + 1, opts), path, opts);
-  }
-
-  public WriterImpl(PhysicalWriter writer,
-                    Path pathForMem,
-                    OrcFile.WriterOptions opts) throws IOException {
-    this.physWriter = writer;
-    this.path = pathForMem;
-    this.conf = opts.getConfiguration();
-    this.schema = opts.getSchema();
-    this.callback = opts.getCallback();
-    if (callback != null) {
-      callbackContext = new OrcFile.WriterContext(){
-
-        @Override
-        public Writer getWriter() {
-          return WriterImpl.this;
-        }
-      };
-    } else {
-      callbackContext = null;
-    }
-    this.version = opts.getVersion();
-    this.encodingStrategy = opts.getEncodingStrategy();
-    this.rowIndexStride = opts.getRowIndexStride();
-    this.memoryManager = opts.getMemoryManager();
-    buildIndex = rowIndexStride > 0;
-    if (version == OrcFile.Version.V_0_11) {
-      /* do not write bloom filters for ORC v11 */
-      this.bloomFilterColumns = new boolean[schema.getMaximumId() + 1];
-    } else {
-      this.bloomFilterColumns =
-          OrcUtils.includeColumns(opts.getBloomFilterColumns(), schema);
-    }
-    this.bloomFilterFpp = opts.getBloomFilterFpp();
-    treeWriter = createTreeWriter(schema, streamFactory, false);
-    if (buildIndex && rowIndexStride < MIN_ROW_INDEX_STRIDE) {
-      throw new IllegalArgumentException("Row stride must be at least " +
-          MIN_ROW_INDEX_STRIDE);
-    }
-
-    // ensure that we are able to handle callbacks before we register ourselves
-    if (path != null) {
-      memoryManager.addWriter(path, opts.getStripeSize(), this);
-    }
-  }
-
-  @Override
-  public boolean checkMemory(double newScale) throws IOException {
-    long limit = (long) Math.round(physWriter.getPhysicalStripeSize() * newScale);
-    long size = estimateStripeSize();
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("ORC writer " + path + " size = " + size + " limit = " +
-                limit);
-    }
-    if (size > limit) {
-      flushStripe();
-      return true;
-    }
-    return false;
-  }
-
-  private static class RowIndexPositionRecorder implements PositionRecorder {
-    private final OrcProto.RowIndexEntry.Builder builder;
-
-    RowIndexPositionRecorder(OrcProto.RowIndexEntry.Builder builder) {
-      this.builder = builder;
-    }
-
-    @Override
-    public void addPosition(long position) {
-      builder.addPositions(position);
-    }
-  }
-
-  /**
-   * Interface from the Writer to the TreeWriters. This limits the visibility
-   * that the TreeWriters have into the Writer.
-   */
-  private class StreamFactory {
-    /**
-     * Create a stream to store part of a column.
-     * @param column the column id for the stream
-     * @param kind the kind of stream
-     * @return The output outStream that the section needs to be written to.
-     * @throws IOException
-     */
-    public OutStream createStream(int column,
-                                  OrcProto.Stream.Kind kind
-                                  ) throws IOException {
-      final StreamName name = new StreamName(column, kind);
-      return physWriter.getOrCreatePhysicalStream(name);
-    }
-
-    public void writeIndex(int column, RowIndex.Builder rowIndex) throws IOException {
-      physWriter.writeIndexStream(new StreamName(column, Stream.Kind.ROW_INDEX), rowIndex);
-    }
-
-    public void writeBloomFilter(
-        int column, BloomFilterIndex.Builder bloomFilterIndex) throws IOException {
-      physWriter.writeBloomFilterStream(
-          new StreamName(column, Stream.Kind.BLOOM_FILTER), bloomFilterIndex);
-    }
-    /**
-     * Get the next column id.
-     * @return a number from 0 to the number of columns - 1
-     */
-    public int getNextColumnId() {
-      return columnCount++;
-    }
-
-    /**
-     * Get the stride rate of the row index.
-     */
-    public int getRowIndexStride() {
-      return rowIndexStride;
-    }
-
-    /**
-     * Should be building the row index.
-     * @return true if we are building the index
-     */
-    public boolean buildIndex() {
-      return buildIndex;
-    }
-
-    /**
-     * Is the ORC file compressed?
-     * @return are the streams compressed
-     */
-    public boolean isCompressed() {
-      return physWriter.isCompressed();
-    }
-
-    /**
-     * Get the encoding strategy to use.
-     * @return encoding strategy
-     */
-    public OrcFile.EncodingStrategy getEncodingStrategy() {
-      return encodingStrategy;
-    }
-
-    /**
-     * Get the bloom filter columns
-     * @return bloom filter columns
-     */
-    public boolean[] getBloomFilterColumns() {
-      return bloomFilterColumns;
-    }
-
-    /**
-     * Get bloom filter false positive percentage.
-     * @return fpp
-     */
-    public double getBloomFilterFPP() {
-      return bloomFilterFpp;
-    }
-
-    /**
-     * Get the writer's configuration.
-     * @return configuration
-     */
-    public Configuration getConfiguration() {
-      return conf;
-    }
-
-    /**
-     * Get the version of the file to write.
-     */
-    public OrcFile.Version getVersion() {
-      return version;
-    }
-
-    public void useWriterTimeZone(boolean val) {
-      writeTimeZone = val;
-    }
-
-    public boolean hasWriterTimeZone() {
-      return writeTimeZone;
-    }
-  }
-
-  /**
-   * The parent class of all of the writers for each column. Each column
-   * is written by an instance of this class. The compound types (struct,
-   * list, map, and union) have children tree writers that write the children
-   * types.
-   */
-  private abstract static class TreeWriter {
-    protected final int id;
-    protected final BitFieldWriter isPresent;
-    private final boolean isCompressed;
-    protected final ColumnStatisticsImpl indexStatistics;
-    protected final ColumnStatisticsImpl stripeColStatistics;
-    private final ColumnStatisticsImpl fileStatistics;
-    protected TreeWriter[] childrenWriters;
-    protected final RowIndexPositionRecorder rowIndexPosition;
-    private final OrcProto.RowIndex.Builder rowIndex;
-    private final OrcProto.RowIndexEntry.Builder rowIndexEntry;
-    protected final BloomFilterIO bloomFilter;
-    protected final boolean createBloomFilter;
-    private final OrcProto.BloomFilterIndex.Builder bloomFilterIndex;
-    private final OrcProto.BloomFilter.Builder bloomFilterEntry;
-    private boolean foundNulls;
-    private OutStream isPresentOutStream;
-    private final List<OrcProto.StripeStatistics.Builder> stripeStatsBuilders;
-    private final StreamFactory streamFactory;
-
-    /**
-     * Create a tree writer.
-     * @param columnId the column id of the column to write
-     * @param schema the row schema
-     * @param streamFactory limited access to the Writer's data.
-     * @param nullable can the value be null?
-     * @throws IOException
-     */
-    TreeWriter(int columnId,
-               TypeDescription schema,
-               StreamFactory streamFactory,
-               boolean nullable) throws IOException {
-      this.streamFactory = streamFactory;
-      this.isCompressed = streamFactory.isCompressed();
-      this.id = columnId;
-      if (nullable) {
-        isPresentOutStream = streamFactory.createStream(id,
-            OrcProto.Stream.Kind.PRESENT);
-        isPresent = new BitFieldWriter(isPresentOutStream, 1);
-      } else {
-        isPresent = null;
-      }
-      this.foundNulls = false;
-      createBloomFilter = streamFactory.getBloomFilterColumns()[columnId];
-      indexStatistics = ColumnStatisticsImpl.create(schema);
-      stripeColStatistics = ColumnStatisticsImpl.create(schema);
-      fileStatistics = ColumnStatisticsImpl.create(schema);
-      childrenWriters = new TreeWriter[0];
-      rowIndex = OrcProto.RowIndex.newBuilder();
-      rowIndexEntry = OrcProto.RowIndexEntry.newBuilder();
-      rowIndexPosition = new RowIndexPositionRecorder(rowIndexEntry);
-      stripeStatsBuilders = Lists.newArrayList();
-      if (createBloomFilter) {
-        bloomFilterEntry = OrcProto.BloomFilter.newBuilder();
-        bloomFilterIndex = OrcProto.BloomFilterIndex.newBuilder();
-        bloomFilter = new BloomFilterIO(streamFactory.getRowIndexStride(),
-            streamFactory.getBloomFilterFPP());
-      } else {
-        bloomFilterEntry = null;
-        bloomFilterIndex = null;
-        bloomFilter = null;
-      }
-    }
-
-    protected OrcProto.RowIndex.Builder getRowIndex() {
-      return rowIndex;
-    }
-
-    protected ColumnStatisticsImpl getStripeStatistics() {
-      return stripeColStatistics;
-    }
-
-    protected OrcProto.RowIndexEntry.Builder getRowIndexEntry() {
-      return rowIndexEntry;
-    }
-
-    IntegerWriter createIntegerWriter(PositionedOutputStream output,
-                                      boolean signed, boolean isDirectV2,
-                                      StreamFactory writer) {
-      if (isDirectV2) {
-        boolean alignedBitpacking = false;
-        if (writer.getEncodingStrategy().equals(OrcFile.EncodingStrategy.SPEED)) {
-          alignedBitpacking = true;
-        }
-        return new RunLengthIntegerWriterV2(output, signed, alignedBitpacking);
-      } else {
-        return new RunLengthIntegerWriter(output, signed);
-      }
-    }
-
-    boolean isNewWriteFormat(StreamFactory writer) {
-      return writer.getVersion() != OrcFile.Version.V_0_11;
-    }
-
-    /**
-     * Handle the top level object write.
-     *
-     * This default method is used for all types except structs, which are the
-     * typical case. VectorizedRowBatch assumes the top level object is a
-     * struct, so we use the first column for all other types.
-     * @param batch the batch to write from
-     * @param offset the row to start on
-     * @param length the number of rows to write
-     * @throws IOException
-     */
-    void writeRootBatch(VectorizedRowBatch batch, int offset,
-                        int length) throws IOException {
-      writeBatch(batch.cols[0], offset, length);
-    }
-
-    /**
-     * Write the values from the given vector from offset for length elements.
-     * @param vector the vector to write from
-     * @param offset the first value from the vector to write
-     * @param length the number of values from the vector to write
-     * @throws IOException
-     */
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      if (vector.noNulls) {
-        indexStatistics.increment(length);
-        if (isPresent != null) {
-          for (int i = 0; i < length; ++i) {
-            isPresent.write(1);
-          }
-        }
-      } else {
-        if (vector.isRepeating) {
-          boolean isNull = vector.isNull[0];
-          if (isPresent != null) {
-            for (int i = 0; i < length; ++i) {
-              isPresent.write(isNull ? 0 : 1);
-            }
-          }
-          if (isNull) {
-            foundNulls = true;
-            indexStatistics.setNull();
-          } else {
-            indexStatistics.increment(length);
-          }
-        } else {
-          // count the number of non-null values
-          int nonNullCount = 0;
-          for(int i = 0; i < length; ++i) {
-            boolean isNull = vector.isNull[i + offset];
-            if (!isNull) {
-              nonNullCount += 1;
-            }
-            if (isPresent != null) {
-              isPresent.write(isNull ? 0 : 1);
-            }
-          }
-          indexStatistics.increment(nonNullCount);
-          if (nonNullCount != length) {
-            foundNulls = true;
-            indexStatistics.setNull();
-          }
-        }
-      }
-    }
-
-    private void removeIsPresentPositions() {
-      for(int i=0; i < rowIndex.getEntryCount(); ++i) {
-        OrcProto.RowIndexEntry.Builder entry = rowIndex.getEntryBuilder(i);
-        List<Long> positions = entry.getPositionsList();
-        // bit streams use 3 positions if uncompressed, 4 if compressed
-        positions = positions.subList(isCompressed ? 4 : 3, positions.size());
-        entry.clearPositions();
-        entry.addAllPositions(positions);
-      }
-    }
-
-    /**
-     * Write the stripe out to the file.
-     * @param builder the stripe footer that contains the information about the
-     *                layout of the stripe. The TreeWriter is required to update
-     *                the footer with its information.
-     * @param requiredIndexEntries the number of index entries that are
-     *                             required. this is to check to make sure the
-     *                             row index is well formed.
-     * @throws IOException
-     */
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      if (isPresent != null) {
-        isPresent.flush();
-
-        // if no nulls are found in a stream, then suppress the stream
-        if (!foundNulls) {
-          isPresentOutStream.suppress();
-          // since isPresent bitstream is suppressed, update the index to
-          // remove the positions of the isPresent stream
-          if (streamFactory.buildIndex()) {
-            removeIsPresentPositions();
-          }
-        }
-      }
-
-      // merge stripe-level column statistics to file statistics and write it to
-      // stripe statistics
-      OrcProto.StripeStatistics.Builder stripeStatsBuilder = OrcProto.StripeStatistics.newBuilder();
-      writeStripeStatistics(stripeStatsBuilder, this);
-      stripeStatsBuilders.add(stripeStatsBuilder);
-
-      // reset the flag for next stripe
-      foundNulls = false;
-
-      builder.addColumns(getEncoding());
-      if (streamFactory.hasWriterTimeZone()) {
-        builder.setWriterTimezone(TimeZone.getDefault().getID());
-      }
-      if (streamFactory.buildIndex()) {
-        if (rowIndex.getEntryCount() != requiredIndexEntries) {
-          throw new IllegalArgumentException("Column has wrong number of " +
-               "index entries found: " + rowIndex.getEntryCount() + " expected: " +
-               requiredIndexEntries);
-        }
-        streamFactory.writeIndex(id, rowIndex);
-      }
-
-      rowIndex.clear();
-      rowIndexEntry.clear();
-
-      // write the bloom filter to out stream
-      if (createBloomFilter) {
-        streamFactory.writeBloomFilter(id, bloomFilterIndex);
-        bloomFilterIndex.clear();
-        bloomFilterEntry.clear();
-      }
-    }
-
-    private void writeStripeStatistics(OrcProto.StripeStatistics.Builder builder,
-        TreeWriter treeWriter) {
-      treeWriter.fileStatistics.merge(treeWriter.stripeColStatistics);
-      builder.addColStats(treeWriter.stripeColStatistics.serialize().build());
-      treeWriter.stripeColStatistics.reset();
-      for (TreeWriter child : treeWriter.getChildrenWriters()) {
-        writeStripeStatistics(builder, child);
-      }
-    }
-
-    TreeWriter[] getChildrenWriters() {
-      return childrenWriters;
-    }
-
-    /**
-     * Get the encoding for this column.
-     * @return the information about the encoding of this column
-     */
-    OrcProto.ColumnEncoding getEncoding() {
-      return OrcProto.ColumnEncoding.newBuilder().setKind(
-          OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    /**
-     * Create a row index entry with the previous location and the current
-     * index statistics. Also merges the index statistics into the file
-     * statistics before they are cleared. Finally, it records the start of the
-     * next index and ensures all of the children columns also create an entry.
-     * @throws IOException
-     */
-    void createRowIndexEntry() throws IOException {
-      stripeColStatistics.merge(indexStatistics);
-      rowIndexEntry.setStatistics(indexStatistics.serialize());
-      indexStatistics.reset();
-      rowIndex.addEntry(rowIndexEntry);
-      rowIndexEntry.clear();
-      addBloomFilterEntry();
-      recordPosition(rowIndexPosition);
-      for(TreeWriter child: childrenWriters) {
-        child.createRowIndexEntry();
-      }
-    }
-
-    void addBloomFilterEntry() {
-      if (createBloomFilter) {
-        bloomFilterEntry.setNumHashFunctions(bloomFilter.getNumHashFunctions());
-        bloomFilterEntry.addAllBitset(Longs.asList(bloomFilter.getBitSet()));
-        bloomFilterIndex.addBloomFilter(bloomFilterEntry.build());
-        bloomFilter.reset();
-        bloomFilterEntry.clear();
-      }
-    }
-
-    /**
-     * Record the current position in each of this column's streams.
-     * @param recorder where should the locations be recorded
-     * @throws IOException
-     */
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      if (isPresent != null) {
-        isPresent.getPosition(recorder);
-      }
-    }
-
-    /**
-     * Estimate how much memory the writer is consuming excluding the streams.
-     * @return the number of bytes.
-     */
-    long estimateMemory() {
-      long result = 0;
-      for (TreeWriter child: childrenWriters) {
-        result += child.estimateMemory();
-      }
-      return result;
-    }
-  }
-
-  private static class BooleanTreeWriter extends TreeWriter {
-    private final BitFieldWriter writer;
-
-    BooleanTreeWriter(int columnId,
-                      TypeDescription schema,
-                      StreamFactory writer,
-                      boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      PositionedOutputStream out = writer.createStream(id,
-          OrcProto.Stream.Kind.DATA);
-      this.writer = new BitFieldWriter(out, 1);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      LongColumnVector vec = (LongColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          int value = vec.vector[0] == 0 ? 0 : 1;
-          indexStatistics.updateBoolean(value != 0, length);
-          for(int i=0; i < length; ++i) {
-            writer.write(value);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            int value = vec.vector[i + offset] == 0 ? 0 : 1;
-            writer.write(value);
-            indexStatistics.updateBoolean(value != 0, 1);
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      writer.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      writer.getPosition(recorder);
-    }
-  }
-
-  private static class ByteTreeWriter extends TreeWriter {
-    private final RunLengthByteWriter writer;
-
-    ByteTreeWriter(int columnId,
-                      TypeDescription schema,
-                      StreamFactory writer,
-                      boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.writer = new RunLengthByteWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.DATA));
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      LongColumnVector vec = (LongColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          byte value = (byte) vec.vector[0];
-          indexStatistics.updateInteger(value, length);
-          if (createBloomFilter) {
-            bloomFilter.addLong(value);
-          }
-          for(int i=0; i < length; ++i) {
-            writer.write(value);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            byte value = (byte) vec.vector[i + offset];
-            writer.write(value);
-            indexStatistics.updateInteger(value, 1);
-            if (createBloomFilter) {
-              bloomFilter.addLong(value);
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      writer.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      writer.getPosition(recorder);
-    }
-  }
-
-  private static class IntegerTreeWriter extends TreeWriter {
-    private final IntegerWriter writer;
-    private boolean isDirectV2 = true;
-
-    IntegerTreeWriter(int columnId,
-                      TypeDescription schema,
-                      StreamFactory writer,
-                      boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      OutStream out = writer.createStream(id,
-          OrcProto.Stream.Kind.DATA);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      this.writer = createIntegerWriter(out, true, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      LongColumnVector vec = (LongColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          long value = vec.vector[0];
-          indexStatistics.updateInteger(value, length);
-          if (createBloomFilter) {
-            bloomFilter.addLong(value);
-          }
-          for(int i=0; i < length; ++i) {
-            writer.write(value);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            long value = vec.vector[i + offset];
-            writer.write(value);
-            indexStatistics.updateInteger(value, 1);
-            if (createBloomFilter) {
-              bloomFilter.addLong(value);
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      writer.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      writer.getPosition(recorder);
-    }
-  }
-
-  private static class FloatTreeWriter extends TreeWriter {
-    private final PositionedOutputStream stream;
-    private final SerializationUtils utils;
-
-    FloatTreeWriter(int columnId,
-                      TypeDescription schema,
-                      StreamFactory writer,
-                      boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.stream = writer.createStream(id,
-          OrcProto.Stream.Kind.DATA);
-      this.utils = new SerializationUtils();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      DoubleColumnVector vec = (DoubleColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          float value = (float) vec.vector[0];
-          indexStatistics.updateDouble(value);
-          if (createBloomFilter) {
-            bloomFilter.addDouble(value);
-          }
-          for(int i=0; i < length; ++i) {
-            utils.writeFloat(stream, value);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            float value = (float) vec.vector[i + offset];
-            utils.writeFloat(stream, value);
-            indexStatistics.updateDouble(value);
-            if (createBloomFilter) {
-              bloomFilter.addDouble(value);
-            }
-          }
-        }
-      }
-    }
-
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      stream.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      stream.getPosition(recorder);
-    }
-  }
-
-  private static class DoubleTreeWriter extends TreeWriter {
-    private final PositionedOutputStream stream;
-    private final SerializationUtils utils;
-
-    DoubleTreeWriter(int columnId,
-                    TypeDescription schema,
-                    StreamFactory writer,
-                    boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.stream = writer.createStream(id,
-          OrcProto.Stream.Kind.DATA);
-      this.utils = new SerializationUtils();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      DoubleColumnVector vec = (DoubleColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          double value = vec.vector[0];
-          indexStatistics.updateDouble(value);
-          if (createBloomFilter) {
-            bloomFilter.addDouble(value);
-          }
-          for(int i=0; i < length; ++i) {
-            utils.writeDouble(stream, value);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            double value = vec.vector[i + offset];
-            utils.writeDouble(stream, value);
-            indexStatistics.updateDouble(value);
-            if (createBloomFilter) {
-              bloomFilter.addDouble(value);
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      stream.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      stream.getPosition(recorder);
-    }
-  }
-
-  private static abstract class StringBaseTreeWriter extends TreeWriter {
-    private static final int INITIAL_DICTIONARY_SIZE = 4096;
-    private final OutStream stringOutput;
-    private final IntegerWriter lengthOutput;
-    private final IntegerWriter rowOutput;
-    protected final StringRedBlackTree dictionary =
-        new StringRedBlackTree(INITIAL_DICTIONARY_SIZE);
-    protected final DynamicIntArray rows = new DynamicIntArray();
-    protected final PositionedOutputStream directStreamOutput;
-    protected final IntegerWriter directLengthOutput;
-    private final List<OrcProto.RowIndexEntry> savedRowIndex =
-        new ArrayList<OrcProto.RowIndexEntry>();
-    private final boolean buildIndex;
-    private final List<Long> rowIndexValueCount = new ArrayList<Long>();
-    // If the number of keys in a dictionary is greater than this fraction of
-    //the total number of non-null rows, turn off dictionary encoding
-    private final double dictionaryKeySizeThreshold;
-    protected boolean useDictionaryEncoding;
-    private boolean isDirectV2 = true;
-    private boolean doneDictionaryCheck;
-    private final boolean strideDictionaryCheck;
-
-    StringBaseTreeWriter(int columnId,
-                     TypeDescription schema,
-                     StreamFactory writer,
-                     boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      stringOutput = writer.createStream(id,
-          OrcProto.Stream.Kind.DICTIONARY_DATA);
-      lengthOutput = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.LENGTH), false, isDirectV2, writer);
-      rowOutput = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.DATA), false, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-      rowIndexValueCount.add(0L);
-      buildIndex = writer.buildIndex();
-      directStreamOutput = writer.createStream(id, OrcProto.Stream.Kind.DATA);
-      directLengthOutput = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.LENGTH), false, isDirectV2, writer);
-      Configuration conf = writer.getConfiguration();
-      dictionaryKeySizeThreshold =
-          OrcConf.DICTIONARY_KEY_SIZE_THRESHOLD.getDouble(conf);
-      strideDictionaryCheck =
-          OrcConf.ROW_INDEX_STRIDE_DICTIONARY_CHECK.getBoolean(conf);
-      useDictionaryEncoding =  dictionaryKeySizeThreshold >= 0.000001; // Epsilon.
-      doneDictionaryCheck = !useDictionaryEncoding;
-    }
-
-    private boolean checkDictionaryEncoding() {
-      if (!doneDictionaryCheck) {
-        // Set the flag indicating whether or not to use dictionary encoding
-        // based on whether or not the fraction of distinct keys over number of
-        // non-null rows is less than the configured threshold
-        float ratio = rows.size() > 0 ? (float) (dictionary.size()) / rows.size() : 0.0f;
-        useDictionaryEncoding = !isDirectV2 || ratio <= dictionaryKeySizeThreshold;
-        doneDictionaryCheck = true;
-      }
-      return useDictionaryEncoding;
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      // if rows in stripe is less than dictionaryCheckAfterRows, dictionary
-      // checking would not have happened. So do it again here.
-      checkDictionaryEncoding();
-
-      if (useDictionaryEncoding) {
-        flushDictionary();
-      } else {
-        // flushout any left over entries from dictionary
-        if (rows.size() > 0) {
-          flushDictionary();
-        }
-
-        // suppress the stream for every stripe if dictionary is disabled
-        stringOutput.suppress();
-      }
-
-      // we need to build the rowindex before calling super, since it
-      // writes it out.
-      super.writeStripe(builder, requiredIndexEntries);
-      stringOutput.flush();
-      lengthOutput.flush();
-      rowOutput.flush();
-      directStreamOutput.flush();
-      directLengthOutput.flush();
-      // reset all of the fields to be ready for the next stripe.
-      dictionary.clear();
-      savedRowIndex.clear();
-      rowIndexValueCount.clear();
-      recordPosition(rowIndexPosition);
-      rowIndexValueCount.add(0L);
-
-      if (!useDictionaryEncoding) {
-        // record the start positions of first index stride of next stripe i.e
-        // beginning of the direct streams when dictionary is disabled
-        recordDirectStreamPosition();
-      }
-    }
-
-    private void flushDictionary() throws IOException {
-      final int[] dumpOrder = new int[dictionary.size()];
-
-      if (useDictionaryEncoding) {
-        // Write the dictionary by traversing the red-black tree writing out
-        // the bytes and lengths; and creating the map from the original order
-        // to the final sorted order.
-
-        dictionary.visit(new StringRedBlackTree.Visitor() {
-          private int currentId = 0;
-          @Override
-          public void visit(StringRedBlackTree.VisitorContext context
-                           ) throws IOException {
-            context.writeBytes(stringOutput);
-            lengthOutput.write(context.getLength());
-            dumpOrder[context.getOriginalPosition()] = currentId++;
-          }
-        });
-      } else {
-        // for direct encoding, we don't want the dictionary data stream
-        stringOutput.suppress();
-      }
-      int length = rows.size();
-      int rowIndexEntry = 0;
-      OrcProto.RowIndex.Builder rowIndex = getRowIndex();
-      Text text = new Text();
-      // write the values translated into the dump order.
-      for(int i = 0; i <= length; ++i) {
-        // now that we are writing out the row values, we can finalize the
-        // row index
-        if (buildIndex) {
-          while (i == rowIndexValueCount.get(rowIndexEntry) &&
-              rowIndexEntry < savedRowIndex.size()) {
-            OrcProto.RowIndexEntry.Builder base =
-                savedRowIndex.get(rowIndexEntry++).toBuilder();
-            if (useDictionaryEncoding) {
-              rowOutput.getPosition(new RowIndexPositionRecorder(base));
-            } else {
-              PositionRecorder posn = new RowIndexPositionRecorder(base);
-              directStreamOutput.getPosition(posn);
-              directLengthOutput.getPosition(posn);
-            }
-            rowIndex.addEntry(base.build());
-          }
-        }
-        if (i != length) {
-          if (useDictionaryEncoding) {
-            rowOutput.write(dumpOrder[rows.get(i)]);
-          } else {
-            dictionary.getText(text, rows.get(i));
-            directStreamOutput.write(text.getBytes(), 0, text.getLength());
-            directLengthOutput.write(text.getLength());
-          }
-        }
-      }
-      rows.clear();
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      // Returns the encoding used for the last call to writeStripe
-      if (useDictionaryEncoding) {
-        if(isDirectV2) {
-          return OrcProto.ColumnEncoding.newBuilder().setKind(
-              OrcProto.ColumnEncoding.Kind.DICTIONARY_V2).
-              setDictionarySize(dictionary.size()).build();
-        }
-        return OrcProto.ColumnEncoding.newBuilder().setKind(
-            OrcProto.ColumnEncoding.Kind.DICTIONARY).
-            setDictionarySize(dictionary.size()).build();
-      } else {
-        if(isDirectV2) {
-          return OrcProto.ColumnEncoding.newBuilder().setKind(
-              OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-        }
-        return OrcProto.ColumnEncoding.newBuilder().setKind(
-            OrcProto.ColumnEncoding.Kind.DIRECT).build();
-      }
-    }
-
-    /**
-     * This method doesn't call the super method, because unlike most of the
-     * other TreeWriters, this one can't record the position in the streams
-     * until the stripe is being flushed. Therefore it saves all of the entries
-     * and augments them with the final information as the stripe is written.
-     * @throws IOException
-     */
-    @Override
-    void createRowIndexEntry() throws IOException {
-      getStripeStatistics().merge(indexStatistics);
-      OrcProto.RowIndexEntry.Builder rowIndexEntry = getRowIndexEntry();
-      rowIndexEntry.setStatistics(indexStatistics.serialize());
-      indexStatistics.reset();
-      OrcProto.RowIndexEntry base = rowIndexEntry.build();
-      savedRowIndex.add(base);
-      rowIndexEntry.clear();
-      addBloomFilterEntry();
-      recordPosition(rowIndexPosition);
-      rowIndexValueCount.add(Long.valueOf(rows.size()));
-      if (strideDictionaryCheck) {
-        checkDictionaryEncoding();
-      }
-      if (!useDictionaryEncoding) {
-        if (rows.size() > 0) {
-          flushDictionary();
-          // just record the start positions of next index stride
-          recordDirectStreamPosition();
-        } else {
-          // record the start positions of next index stride
-          recordDirectStreamPosition();
-          getRowIndex().addEntry(base);
-        }
-      }
-    }
-
-    private void recordDirectStreamPosition() throws IOException {
-      directStreamOutput.getPosition(rowIndexPosition);
-      directLengthOutput.getPosition(rowIndexPosition);
-    }
-
-    @Override
-    long estimateMemory() {
-      return rows.getSizeInBytes() + dictionary.getSizeInBytes();
-    }
-  }
-
-  private static class StringTreeWriter extends StringBaseTreeWriter {
-    StringTreeWriter(int columnId,
-                   TypeDescription schema,
-                   StreamFactory writer,
-                   boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      BytesColumnVector vec = (BytesColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          if (useDictionaryEncoding) {
-            int id = dictionary.add(vec.vector[0], vec.start[0], vec.length[0]);
-            for(int i=0; i < length; ++i) {
-              rows.add(id);
-            }
-          } else {
-            for(int i=0; i < length; ++i) {
-              directStreamOutput.write(vec.vector[0], vec.start[0],
-                  vec.length[0]);
-              directLengthOutput.write(vec.length[0]);
-            }
-          }
-          indexStatistics.updateString(vec.vector[0], vec.start[0],
-              vec.length[0], length);
-          if (createBloomFilter) {
-            bloomFilter.addBytes(vec.vector[0], vec.start[0], vec.length[0]);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            if (useDictionaryEncoding) {
-              rows.add(dictionary.add(vec.vector[offset + i],
-                  vec.start[offset + i], vec.length[offset + i]));
-            } else {
-              directStreamOutput.write(vec.vector[offset + i],
-                  vec.start[offset + i], vec.length[offset + i]);
-              directLengthOutput.write(vec.length[offset + i]);
-            }
-            indexStatistics.updateString(vec.vector[offset + i],
-                vec.start[offset + i], vec.length[offset + i], 1);
-            if (createBloomFilter) {
-              bloomFilter.addBytes(vec.vector[offset + i],
-                  vec.start[offset + i], vec.length[offset + i]);
-            }
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Under the covers, char is written to ORC the same way as string.
-   */
-  private static class CharTreeWriter extends StringBaseTreeWriter {
-    private final int itemLength;
-    private final byte[] padding;
-
-    CharTreeWriter(int columnId,
-        TypeDescription schema,
-        StreamFactory writer,
-        boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      itemLength = schema.getMaxLength();
-      padding = new byte[itemLength];
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      BytesColumnVector vec = (BytesColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          byte[] ptr;
-          int ptrOffset;
-          if (vec.length[0] >= itemLength) {
-            ptr = vec.vector[0];
-            ptrOffset = vec.start[0];
-          } else {
-            ptr = padding;
-            ptrOffset = 0;
-            System.arraycopy(vec.vector[0], vec.start[0], ptr, 0,
-                vec.length[0]);
-            Arrays.fill(ptr, vec.length[0], itemLength, (byte) ' ');
-          }
-          if (useDictionaryEncoding) {
-            int id = dictionary.add(ptr, ptrOffset, itemLength);
-            for(int i=0; i < length; ++i) {
-              rows.add(id);
-            }
-          } else {
-            for(int i=0; i < length; ++i) {
-              directStreamOutput.write(ptr, ptrOffset, itemLength);
-              directLengthOutput.write(itemLength);
-            }
-          }
-          indexStatistics.updateString(ptr, ptrOffset, itemLength, length);
-          if (createBloomFilter) {
-            bloomFilter.addBytes(ptr, ptrOffset, itemLength);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            byte[] ptr;
-            int ptrOffset;
-            if (vec.length[offset + i] >= itemLength) {
-              ptr = vec.vector[offset + i];
-              ptrOffset = vec.start[offset + i];
-            } else {
-              // it is the wrong length, so copy it
-              ptr = padding;
-              ptrOffset = 0;
-              System.arraycopy(vec.vector[offset + i], vec.start[offset + i],
-                  ptr, 0, vec.length[offset + i]);
-              Arrays.fill(ptr, vec.length[offset + i], itemLength, (byte) ' ');
-            }
-            if (useDictionaryEncoding) {
-              rows.add(dictionary.add(ptr, ptrOffset, itemLength));
-            } else {
-              directStreamOutput.write(ptr, ptrOffset, itemLength);
-              directLengthOutput.write(itemLength);
-            }
-            indexStatistics.updateString(ptr, ptrOffset, itemLength, 1);
-            if (createBloomFilter) {
-              bloomFilter.addBytes(ptr, ptrOffset, itemLength);
-            }
-          }
-        }
-      }
-    }
-  }
-
-  /**
-   * Under the covers, varchar is written to ORC the same way as string.
-   */
-  private static class VarcharTreeWriter extends StringBaseTreeWriter {
-    private final int maxLength;
-
-    VarcharTreeWriter(int columnId,
-        TypeDescription schema,
-        StreamFactory writer,
-        boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      maxLength = schema.getMaxLength();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      BytesColumnVector vec = (BytesColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          int itemLength = Math.min(vec.length[0], maxLength);
-          if (useDictionaryEncoding) {
-            int id = dictionary.add(vec.vector[0], vec.start[0], itemLength);
-            for(int i=0; i < length; ++i) {
-              rows.add(id);
-            }
-          } else {
-            for(int i=0; i < length; ++i) {
-              directStreamOutput.write(vec.vector[0], vec.start[0],
-                  itemLength);
-              directLengthOutput.write(itemLength);
-            }
-          }
-          indexStatistics.updateString(vec.vector[0], vec.start[0],
-              itemLength, length);
-          if (createBloomFilter) {
-            bloomFilter.addBytes(vec.vector[0], vec.start[0], itemLength);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            int itemLength = Math.min(vec.length[offset + i], maxLength);
-            if (useDictionaryEncoding) {
-              rows.add(dictionary.add(vec.vector[offset + i],
-                  vec.start[offset + i], itemLength));
-            } else {
-              directStreamOutput.write(vec.vector[offset + i],
-                  vec.start[offset + i], itemLength);
-              directLengthOutput.write(itemLength);
-            }
-            indexStatistics.updateString(vec.vector[offset + i],
-                vec.start[offset + i], itemLength, 1);
-            if (createBloomFilter) {
-              bloomFilter.addBytes(vec.vector[offset + i],
-                  vec.start[offset + i], itemLength);
-            }
-          }
-        }
-      }
-    }
-  }
-
-  private static class BinaryTreeWriter extends TreeWriter {
-    private final PositionedOutputStream stream;
-    private final IntegerWriter length;
-    private boolean isDirectV2 = true;
-
-    BinaryTreeWriter(int columnId,
-                     TypeDescription schema,
-                     StreamFactory writer,
-                     boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.stream = writer.createStream(id,
-          OrcProto.Stream.Kind.DATA);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      this.length = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.LENGTH), false, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      BytesColumnVector vec = (BytesColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          for(int i=0; i < length; ++i) {
-            stream.write(vec.vector[0], vec.start[0],
-                  vec.length[0]);
-            this.length.write(vec.length[0]);
-          }
-          indexStatistics.updateBinary(vec.vector[0], vec.start[0],
-              vec.length[0], length);
-          if (createBloomFilter) {
-            bloomFilter.addBytes(vec.vector[0], vec.start[0], vec.length[0]);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            stream.write(vec.vector[offset + i],
-                vec.start[offset + i], vec.length[offset + i]);
-            this.length.write(vec.length[offset + i]);
-            indexStatistics.updateBinary(vec.vector[offset + i],
-                vec.start[offset + i], vec.length[offset + i], 1);
-            if (createBloomFilter) {
-              bloomFilter.addBytes(vec.vector[offset + i],
-                  vec.start[offset + i], vec.length[offset + i]);
-            }
-          }
-        }
-      }
-    }
-
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      stream.flush();
-      length.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      stream.getPosition(recorder);
-      length.getPosition(recorder);
-    }
-  }
-
-  public static long MILLIS_PER_DAY = 24 * 60 * 60 * 1000;
-  public static long NANOS_PER_MILLI = 1000000;
-  public static final int MILLIS_PER_SECOND = 1000;
-  static final int NANOS_PER_SECOND = 1000000000;
-  public static final String BASE_TIMESTAMP_STRING = "2015-01-01 00:00:00";
-
-  private static class TimestampTreeWriter extends TreeWriter {
-    private final IntegerWriter seconds;
-    private final IntegerWriter nanos;
-    private final boolean isDirectV2;
-    private final long base_timestamp;
-
-    TimestampTreeWriter(int columnId,
-                     TypeDescription schema,
-                     StreamFactory writer,
-                     boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      this.seconds = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.DATA), true, isDirectV2, writer);
-      this.nanos = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.SECONDARY), false, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-      // for unit tests to set different time zones
-      this.base_timestamp = Timestamp.valueOf(BASE_TIMESTAMP_STRING).getTime() / MILLIS_PER_SECOND;
-      writer.useWriterTimeZone(true);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      TimestampColumnVector vec = (TimestampColumnVector) vector;
-      Timestamp val;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          val = vec.asScratchTimestamp(0);
-          long millis = val.getTime();
-          indexStatistics.updateTimestamp(millis);
-          if (createBloomFilter) {
-            bloomFilter.addLong(millis);
-          }
-          final long secs = millis / MILLIS_PER_SECOND - base_timestamp;
-          final long nano = formatNanos(val.getNanos());
-          for(int i=0; i < length; ++i) {
-            seconds.write(secs);
-            nanos.write(nano);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            val = vec.asScratchTimestamp(i + offset);
-            long millis = val.getTime();
-            long secs = millis / MILLIS_PER_SECOND - base_timestamp;
-            seconds.write(secs);
-            nanos.write(formatNanos(val.getNanos()));
-            indexStatistics.updateTimestamp(millis);
-            if (createBloomFilter) {
-              bloomFilter.addLong(millis);
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      seconds.flush();
-      nanos.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    private static long formatNanos(int nanos) {
-      if (nanos == 0) {
-        return 0;
-      } else if (nanos % 100 != 0) {
-        return ((long) nanos) << 3;
-      } else {
-        nanos /= 100;
-        int trailingZeros = 1;
-        while (nanos % 10 == 0 && trailingZeros < 7) {
-          nanos /= 10;
-          trailingZeros += 1;
-        }
-        return ((long) nanos) << 3 | trailingZeros;
-      }
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      seconds.getPosition(recorder);
-      nanos.getPosition(recorder);
-    }
-  }
-
-  private static class DateTreeWriter extends TreeWriter {
-    private final IntegerWriter writer;
-    private final boolean isDirectV2;
-
-    DateTreeWriter(int columnId,
-                   TypeDescription schema,
-                   StreamFactory writer,
-                   boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      OutStream out = writer.createStream(id,
-          OrcProto.Stream.Kind.DATA);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      this.writer = createIntegerWriter(out, true, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      LongColumnVector vec = (LongColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          int value = (int) vec.vector[0];
-          indexStatistics.updateDate(value);
-          if (createBloomFilter) {
-            bloomFilter.addLong(value);
-          }
-          for(int i=0; i < length; ++i) {
-            writer.write(value);
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            int value = (int) vec.vector[i + offset];
-            writer.write(value);
-            indexStatistics.updateDate(value);
-            if (createBloomFilter) {
-              bloomFilter.addLong(value);
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      writer.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      writer.getPosition(recorder);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-  }
-
-  private static class DecimalTreeWriter extends TreeWriter {
-    private final PositionedOutputStream valueStream;
-
-    // These scratch buffers allow us to serialize decimals much faster.
-    private final long[] scratchLongs;
-    private final byte[] scratchBuffer;
-
-    private final IntegerWriter scaleStream;
-    private final boolean isDirectV2;
-
-    DecimalTreeWriter(int columnId,
-                        TypeDescription schema,
-                        StreamFactory writer,
-                        boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      valueStream = writer.createStream(id, OrcProto.Stream.Kind.DATA);
-      scratchLongs = new long[HiveDecimal.SCRATCH_LONGS_LEN];
-      scratchBuffer = new byte[HiveDecimal.SCRATCH_BUFFER_LEN_TO_BYTES];
-      this.scaleStream = createIntegerWriter(writer.createStream(id,
-          OrcProto.Stream.Kind.SECONDARY), true, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      DecimalColumnVector vec = (DecimalColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          HiveDecimalWritable value = vec.vector[0];
-          indexStatistics.updateDecimal(value);
-          if (createBloomFilter) {
-
-            // The HiveDecimalWritable toString() method with a scratch buffer for good performance
-            // when creating the String.  We need to use a String hash code and not UTF-8 byte[]
-            // hash code in order to get the right hash code.
-            bloomFilter.addString(value.toString(scratchBuffer));
-          }
-          for(int i=0; i < length; ++i) {
-
-            // Use the fast ORC serialization method that emulates SerializationUtils.writeBigInteger
-            // provided by HiveDecimalWritable.
-            value.serializationUtilsWrite(
-                valueStream,
-                scratchLongs);
-            scaleStream.write(value.scale());
-          }
-        }
-      } else {
-        for(int i=0; i < length; ++i) {
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            HiveDecimalWritable value = vec.vector[i + offset];
-            value.serializationUtilsWrite(
-                valueStream,
-                scratchLongs);
-            scaleStream.write(value.scale());
-            indexStatistics.updateDecimal(value);
-            if (createBloomFilter) {
-              bloomFilter.addString(value.toString(scratchBuffer));
-            }
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      valueStream.flush();
-      scaleStream.flush();
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      valueStream.getPosition(recorder);
-      scaleStream.getPosition(recorder);
-    }
-  }
-
-  private static class StructTreeWriter extends TreeWriter {
-    StructTreeWriter(int columnId,
-                     TypeDescription schema,
-                     StreamFactory writer,
-                     boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      List<TypeDescription> children = schema.getChildren();
-      childrenWriters = new TreeWriter[children.size()];
-      for(int i=0; i < childrenWriters.length; ++i) {
-        childrenWriters[i] = createTreeWriter(
-          children.get(i), writer,
-          true);
-      }
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeRootBatch(VectorizedRowBatch batch, int offset,
-                        int length) throws IOException {
-      // update the statistics for the root column
-      indexStatistics.increment(length);
-      // I'm assuming that the root column isn't nullable so that I don't need
-      // to update isPresent.
-      for(int i=0; i < childrenWriters.length; ++i) {
-        childrenWriters[i].writeBatch(batch.cols[i], offset, length);
-      }
-    }
-
-    private static void writeFields(StructColumnVector vector,
-                                    TreeWriter[] childrenWriters,
-                                    int offset, int length) throws IOException {
-      for(int field=0; field < childrenWriters.length; ++field) {
-        childrenWriters[field].writeBatch(vector.fields[field], offset, length);
-      }
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      StructColumnVector vec = (StructColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          writeFields(vec, childrenWriters, offset, length);
-        }
-      } else if (vector.noNulls) {
-        writeFields(vec, childrenWriters, offset, length);
-      } else {
-        // write the records in runs
-        int currentRun = 0;
-        boolean started = false;
-        for(int i=0; i < length; ++i) {
-          if (!vec.isNull[i + offset]) {
-            if (!started) {
-              started = true;
-              currentRun = i;
-            }
-          } else if (started) {
-            started = false;
-            writeFields(vec, childrenWriters, offset + currentRun,
-                i - currentRun);
-          }
-        }
-        if (started) {
-          writeFields(vec, childrenWriters, offset + currentRun,
-              length - currentRun);
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      for(TreeWriter child: childrenWriters) {
-        child.writeStripe(builder, requiredIndexEntries);
-      }
-      recordPosition(rowIndexPosition);
-    }
-  }
-
-  private static class ListTreeWriter extends TreeWriter {
-    private final IntegerWriter lengths;
-    private final boolean isDirectV2;
-
-    ListTreeWriter(int columnId,
-                   TypeDescription schema,
-                   StreamFactory writer,
-                   boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      childrenWriters = new TreeWriter[1];
-      childrenWriters[0] =
-        createTreeWriter(schema.getChildren().get(0), writer, true);
-      lengths = createIntegerWriter(writer.createStream(columnId,
-          OrcProto.Stream.Kind.LENGTH), false, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      ListColumnVector vec = (ListColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          int childOffset = (int) vec.offsets[0];
-          int childLength = (int) vec.lengths[0];
-          for(int i=0; i < length; ++i) {
-            lengths.write(childLength);
-            childrenWriters[0].writeBatch(vec.child, childOffset, childLength);
-          }
-          if (createBloomFilter) {
-            bloomFilter.addLong(childLength);
-          }
-        }
-      } else {
-        // write the elements in runs
-        int currentOffset = 0;
-        int currentLength = 0;
-        for(int i=0; i < length; ++i) {
-          if (!vec.isNull[i + offset]) {
-            int nextLength = (int) vec.lengths[offset + i];
-            int nextOffset = (int) vec.offsets[offset + i];
-            lengths.write(nextLength);
-            if (currentLength == 0) {
-              currentOffset = nextOffset;
-              currentLength = nextLength;
-            } else if (currentOffset + currentLength != nextOffset) {
-              childrenWriters[0].writeBatch(vec.child, currentOffset,
-                  currentLength);
-              currentOffset = nextOffset;
-              currentLength = nextLength;
-            } else {
-              currentLength += nextLength;
-            }
-          }
-        }
-        if (currentLength != 0) {
-          childrenWriters[0].writeBatch(vec.child, currentOffset,
-              currentLength);
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      lengths.flush();
-      for(TreeWriter child: childrenWriters) {
-        child.writeStripe(builder, requiredIndexEntries);
-      }
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      lengths.getPosition(recorder);
-    }
-  }
-
-  private static class MapTreeWriter extends TreeWriter {
-    private final IntegerWriter lengths;
-    private final boolean isDirectV2;
-
-    MapTreeWriter(int columnId,
-                  TypeDescription schema,
-                  StreamFactory writer,
-                  boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      this.isDirectV2 = isNewWriteFormat(writer);
-      childrenWriters = new TreeWriter[2];
-      List<TypeDescription> children = schema.getChildren();
-      childrenWriters[0] =
-        createTreeWriter(children.get(0), writer, true);
-      childrenWriters[1] =
-        createTreeWriter(children.get(1), writer, true);
-      lengths = createIntegerWriter(writer.createStream(columnId,
-          OrcProto.Stream.Kind.LENGTH), false, isDirectV2, writer);
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    OrcProto.ColumnEncoding getEncoding() {
-      if (isDirectV2) {
-        return OrcProto.ColumnEncoding.newBuilder()
-            .setKind(OrcProto.ColumnEncoding.Kind.DIRECT_V2).build();
-      }
-      return OrcProto.ColumnEncoding.newBuilder()
-          .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build();
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      MapColumnVector vec = (MapColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          int childOffset = (int) vec.offsets[0];
-          int childLength = (int) vec.lengths[0];
-          for(int i=0; i < length; ++i) {
-            lengths.write(childLength);
-            childrenWriters[0].writeBatch(vec.keys, childOffset, childLength);
-            childrenWriters[1].writeBatch(vec.values, childOffset, childLength);
-          }
-          if (createBloomFilter) {
-            bloomFilter.addLong(childLength);
-          }
-        }
-      } else {
-        // write the elements in runs
-        int currentOffset = 0;
-        int currentLength = 0;
-        for(int i=0; i < length; ++i) {
-          if (!vec.isNull[i + offset]) {
-            int nextLength = (int) vec.lengths[offset + i];
-            int nextOffset = (int) vec.offsets[offset + i];
-            lengths.write(nextLength);
-            if (currentLength == 0) {
-              currentOffset = nextOffset;
-              currentLength = nextLength;
-            } else if (currentOffset + currentLength != nextOffset) {
-              childrenWriters[0].writeBatch(vec.keys, currentOffset,
-                  currentLength);
-              childrenWriters[1].writeBatch(vec.values, currentOffset,
-                  currentLength);
-              currentOffset = nextOffset;
-              currentLength = nextLength;
-            } else {
-              currentLength += nextLength;
-            }
-          }
-        }
-        if (currentLength != 0) {
-          childrenWriters[0].writeBatch(vec.keys, currentOffset,
-              currentLength);
-          childrenWriters[1].writeBatch(vec.values, currentOffset,
-              currentLength);
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      lengths.flush();
-      for(TreeWriter child: childrenWriters) {
-        child.writeStripe(builder, requiredIndexEntries);
-      }
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      lengths.getPosition(recorder);
-    }
-  }
-
-  private static class UnionTreeWriter extends TreeWriter {
-    private final RunLengthByteWriter tags;
-
-    UnionTreeWriter(int columnId,
-                  TypeDescription schema,
-                  StreamFactory writer,
-                  boolean nullable) throws IOException {
-      super(columnId, schema, writer, nullable);
-      List<TypeDescription> children = schema.getChildren();
-      childrenWriters = new TreeWriter[children.size()];
-      for(int i=0; i < childrenWriters.length; ++i) {
-        childrenWriters[i] =
-            createTreeWriter(children.get(i), writer, true);
-      }
-      tags =
-        new RunLengthByteWriter(writer.createStream(columnId,
-            OrcProto.Stream.Kind.DATA));
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void writeBatch(ColumnVector vector, int offset,
-                    int length) throws IOException {
-      super.writeBatch(vector, offset, length);
-      UnionColumnVector vec = (UnionColumnVector) vector;
-      if (vector.isRepeating) {
-        if (vector.noNulls || !vector.isNull[0]) {
-          byte tag = (byte) vec.tags[0];
-          for(int i=0; i < length; ++i) {
-            tags.write(tag);
-          }
-          if (createBloomFilter) {
-            bloomFilter.addLong(tag);
-          }
-          childrenWriters[tag].writeBatch(vec.fields[tag], offset, length);
-        }
-      } else {
-        // write the records in runs of the same tag
-        int[] currentStart = new int[vec.fields.length];
-        int[] currentLength = new int[vec.fields.length];
-        for(int i=0; i < length; ++i) {
-          // only need to deal with the non-nulls, since the nulls were dealt
-          // with in the super method.
-          if (vec.noNulls || !vec.isNull[i + offset]) {
-            byte tag = (byte) vec.tags[offset + i];
-            tags.write(tag);
-            if (currentLength[tag] == 0) {
-              // start a new sequence
-              currentStart[tag] = i + offset;
-              currentLength[tag] = 1;
-            } else if (currentStart[tag] + currentLength[tag] == i + offset) {
-              // ok, we are extending the current run for that tag.
-              currentLength[tag] += 1;
-            } else {
-              // otherwise, we need to close off the old run and start a new one
-              childrenWriters[tag].writeBatch(vec.fields[tag],
-                  currentStart[tag], currentLength[tag]);
-              currentStart[tag] = i + offset;
-              currentLength[tag] = 1;
-            }
-          }
-        }
-        // write out any left over sequences
-        for(int tag=0; tag < currentStart.length; ++tag) {
-          if (currentLength[tag] != 0) {
-            childrenWriters[tag].writeBatch(vec.fields[tag], currentStart[tag],
-                currentLength[tag]);
-          }
-        }
-      }
-    }
-
-    @Override
-    void writeStripe(OrcProto.StripeFooter.Builder builder,
-                     int requiredIndexEntries) throws IOException {
-      super.writeStripe(builder, requiredIndexEntries);
-      tags.flush();
-      for(TreeWriter child: childrenWriters) {
-        child.writeStripe(builder, requiredIndexEntries);
-      }
-      recordPosition(rowIndexPosition);
-    }
-
-    @Override
-    void recordPosition(PositionRecorder recorder) throws IOException {
-      super.recordPosition(recorder);
-      tags.getPosition(recorder);
-    }
-  }
-
-  private static TreeWriter createTreeWriter(TypeDescription schema,
-                                             StreamFactory streamFactory,
-                                             boolean nullable) throws IOException {
-    switch (schema.getCategory()) {
-      case BOOLEAN:
-        return new BooleanTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case BYTE:
-        return new ByteTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case SHORT:
-      case INT:
-      case LONG:
-        return new IntegerTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case FLOAT:
-        return new FloatTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case DOUBLE:
-        return new DoubleTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case STRING:
-        return new StringTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case CHAR:
-        return new CharTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case VARCHAR:
-        return new VarcharTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case BINARY:
-        return new BinaryTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case TIMESTAMP:
-        return new TimestampTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case DATE:
-        return new DateTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case DECIMAL:
-        return new DecimalTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory,  nullable);
-      case STRUCT:
-        return new StructTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case MAP:
-        return new MapTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case LIST:
-        return new ListTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      case UNION:
-        return new UnionTreeWriter(streamFactory.getNextColumnId(),
-            schema, streamFactory, nullable);
-      default:
-        throw new IllegalArgumentException("Bad category: " +
-            schema.getCategory());
-    }
-  }
-
-  private static void writeTypes(OrcProto.Footer.Builder builder,
-                                 TypeDescription schema) {
-    OrcProto.Type.Builder type = OrcProto.Type.newBuilder();
-    List<TypeDescription> children = OrcUtils.setTypeBuilderFromSchema(type, schema);
-    builder.addTypes(type);
-    if (children != null) {
-      for(TypeDescription child: children) {
-        writeTypes(builder, child);
-      }
-    }
-  }
-
-  @VisibleForTesting
-  public void ensureStream() throws IOException {
-    physWriter.initialize();
-  }
-
-  private void createRowIndexEntry() throws IOException {
-    treeWriter.createRowIndexEntry();
-    rowsInIndex = 0;
-  }
-
-  private void flushStripe() throws IOException {
-    ensureStream();
-    if (buildIndex && rowsInIndex != 0) {
-      createRowIndexEntry();
-    }
-    if (rowsInStripe != 0) {
-      if (callback != null) {
-        callback.preStripeWrite(callbackContext);
-      }
-      // finalize the data for the stripe
-      int requiredIndexEntries = rowIndexStride == 0 ? 0 :
-          (int) ((rowsInStripe + rowIndexStride - 1) / rowIndexStride);
-      OrcProto.StripeFooter.Builder builder = OrcProto.StripeFooter.newBuilder();
-      OrcProto.StripeInformation.Builder dirEntry = OrcProto.StripeInformation
-          .newBuilder().setNumberOfRows(rowsInStripe);
-      treeWriter.writeStripe(builder, requiredIndexEntries);
-      physWriter.finalizeStripe(builder, dirEntry);
-      stripes.add(dirEntry.build());
-      rowCount += rowsInStripe;
-      rowsInStripe = 0;
-    }
-  }
-
-  private long computeRawDataSize() {
-    return getRawDataSize(treeWriter, schema);
-  }
-
-  private long getRawDataSize(TreeWriter child,
-                              TypeDescription schema) {
-    long total = 0;
-    long numVals = child.fileStatistics.getNumberOfValues();
-    switch (schema.getCategory()) {
-      case BOOLEAN:
-      case BYTE:
-      case SHORT:
-      case INT:
-      case FLOAT:
-        return numVals * JavaDataModel.get().primitive1();
-      case LONG:
-      case DOUBLE:
-        return numVals * JavaDataModel.get().primitive2();
-      case STRING:
-      case VARCHAR:
-      case CHAR:
-        // ORC strings are converted to java Strings. so use JavaDataModel to
-        // compute the overall size of strings
-        StringColumnStatistics scs = (StringColumnStatistics) child.fileStatistics;
-        numVals = numVals == 0 ? 1 : numVals;
-        int avgStringLen = (int) (scs.getSum() / numVals);
-        return numVals * JavaDataModel.get().lengthForStringOfLength(avgStringLen);
-      case DECIMAL:
-        return numVals * JavaDataModel.get().lengthOfDecimal();
-      case DATE:
-        return numVals * JavaDataModel.get().lengthOfDate();
-      case BINARY:
-        // get total length of binary blob
-        BinaryColumnStatistics bcs = (BinaryColumnStatistics) child.fileStatistics;
-        return bcs.getSum();
-      case TIMESTAMP:
-        return numVals * JavaDataModel.get().lengthOfTimestamp();
-      case LIST:
-      case MAP:
-      case UNION:
-      case STRUCT: {
-        TreeWriter[] childWriters = child.getChildrenWriters();
-        List<TypeDescription> childTypes = schema.getChildren();
-        for (int i=0; i < childWriters.length; ++i) {
-          total += getRawDataSize(childWriters[i], childTypes.get(i));
-        }
-        break;
-      }
-      default:
-        LOG.debug("Unknown object inspector category.");
-        break;
-    }
-    return total;
-  }
-
-  private void writeFileStatistics(OrcProto.Footer.Builder builder,
-                                   TreeWriter writer) throws IOException {
-    builder.addStatistics(writer.fileStatistics.serialize());
-    for(TreeWriter child: writer.getChildrenWriters()) {
-      writeFileStatistics(builder, child);
-    }
-  }
-
-  private void writeMetadata() throws IOException {
-    ensureStream();
-    OrcProto.Metadata.Builder builder = OrcProto.Metadata.newBuilder();
-    for(OrcProto.StripeStatistics.Builder ssb : treeWriter.stripeStatsBuilders) {
-      builder.addStripeStats(ssb.build());
-    }
-
-    physWriter.writeFileMetadata(builder);
-  }
-
-  private void writeFooter() throws IOException {
-    ensureStream();
-    OrcProto.Footer.Builder builder = OrcProto.Footer.newBuilder();
-    builder.setNumberOfRows(rowCount);
-    builder.setRowIndexStride(rowIndexStride);
-    // populate raw data size
-    rawDataSize = computeRawDataSize();
-    // serialize the types
-    writeTypes(builder, schema);
-    // add the stripe information
-    for(OrcProto.StripeInformation stripe: stripes) {
-      builder.addStripes(stripe);
-    }
-    // add the column statistics
-    writeFileStatistics(builder, treeWriter);
-    // add all of the user metadata
-    for(Map.Entry<String, ByteString> entry: userMetadata.entrySet()) {
-      builder.addMetadata(OrcProto.UserMetadataItem.newBuilder()
-        .setName(entry.getKey()).setValue(entry.getValue()));
-    }
-    physWriter.writeFileFooter(builder);
-  }
-
-  private void writePostScript() throws IOException {
-    OrcProto.PostScript.Builder builder =
-      OrcProto.PostScript.newBuilder()
-        .setMagic(OrcFile.MAGIC)
-        .addVersion(version.getMajor())
-        .addVersion(version.getMinor())
-        .setWriterVersion(OrcFile.CURRENT_WRITER.getId());
-    physWriter.writePostScript(builder);
-  }
-
-  private long estimateStripeSize() {
-    return physWriter.estimateMemory() + treeWriter.estimateMemory();
-  }
-
-  @Override
-  public TypeDescription getSchema() {
-    return schema;
-  }
-
-  @Override
-  public void addUserMetadata(String name, ByteBuffer value) {
-    userMetadata.put(name, ByteString.copyFrom(value));
-  }
-
-  @Override
-  public void addRowBatch(VectorizedRowBatch batch) throws IOException {
-    if (buildIndex) {
-      // Batch the writes up to the rowIndexStride so that we can get the
-      // right size indexes.
-      int posn = 0;
-      while (posn < batch.size) {
-        int chunkSize = Math.min(batch.size - posn,
-            rowIndexStride - rowsInIndex);
-        treeWriter.writeRootBatch(batch, posn, chunkSize);
-        posn += chunkSize;
-        rowsInIndex += chunkSize;
-        rowsInStripe += chunkSize;
-        if (rowsInIndex >= rowIndexStride) {
-          createRowIndexEntry();
-        }
-      }
-    } else {
-      rowsInStripe += batch.size;
-      treeWriter.writeRootBatch(batch, 0, batch.size);
-    }
-    memoryManager.addedRow(batch.size);
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (callback != null) {
-      callback.preFooterWrite(callbackContext);
-    }
-    // remove us from the memory manager so that we don't get any callbacks
-    if (path != null) {
-      memoryManager.removeWriter(path);
-    }
-    // actually close the file
-    flushStripe();
-    writeMetadata();
-    writeFooter();
-    writePostScript();
-    physWriter.close();
-  }
-
-  /**
-   * Raw data size will be compute when writing the file footer. Hence raw data
-   * size value will be available only after closing the writer.
-   */
-  @Override
-  public long getRawDataSize() {
-    return rawDataSize;
-  }
-
-  /**
-   * Row count gets updated when flushing the stripes. To get accurate row
-   * count call this method after writer is closed.
-   */
-  @Override
-  public long getNumberOfRows() {
-    return rowCount;
-  }
-
-  @Override
-  public long writeIntermediateFooter() throws IOException {
-    // flush any buffered rows
-    flushStripe();
-    // write a footer
-    if (stripesAtLastFlush != stripes.size()) {
-      if (callback != null) {
-        callback.preFooterWrite(callbackContext);
-      }
-      writeMetadata();
-      writeFooter();
-      writePostScript();
-      stripesAtLastFlush = stripes.size();
-      physWriter.flush();
-    }
-    return physWriter.getRawWriterPosition();
-  }
-
-  @Override
-  public void appendStripe(byte[] stripe, int offset, int length,
-      StripeInformation stripeInfo,
-      OrcProto.StripeStatistics stripeStatistics) throws IOException {
-    checkArgument(stripe != null, "Stripe must not be null");
-    checkArgument(length <= stripe.length,
-        "Specified length must not be greater specified array length");
-    checkArgument(stripeInfo != null, "Stripe information must not be null");
-    checkArgument(stripeStatistics != null,
-        "Stripe statistics must not be null");
-
-    ensureStream();
-    OrcProto.StripeInformation.Builder dirEntry = OrcProto.StripeInformation.newBuilder();
-    physWriter.appendRawStripe(stripe, offset, length, dirEntry);
-
-    rowsInStripe = stripeStatistics.getColStats(0).getNumberOfValues();
-    rowCount += rowsInStripe;
-
-    // since we have already written the stripe, just update stripe statistics
-    treeWriter.stripeStatsBuilders.add(stripeStatistics.toBuilder());
-
-    // update file level statistics
-    updateFileStatistics(stripeStatistics);
-
-    // update stripe information
-    stripes.add(dirEntry.setNumberOfRows(rowsInStripe)
-        .setIndexLength(stripeInfo.getIndexLength())
-        .setDataLength(stripeInfo.getDataLength())
-        .setFooterLength(stripeInfo.getFooterLength())
-        .build());
-
-    // reset it after writing the stripe
-    rowsInStripe = 0;
-  }
-
-  private void updateFileStatistics(OrcProto.StripeStatistics stripeStatistics) {
-    List<OrcProto.ColumnStatistics> cs = stripeStatistics.getColStatsList();
-    List<TreeWriter> allWriters = getAllColumnTreeWriters(treeWriter);
-    for (int i = 0; i < allWriters.size(); i++) {
-      allWriters.get(i).fileStatistics.merge(ColumnStatisticsImpl.deserialize(cs.get(i)));
-    }
-  }
-
-  private List<TreeWriter> getAllColumnTreeWriters(TreeWriter rootTreeWriter) {
-    List<TreeWriter> result = Lists.newArrayList();
-    getAllColumnTreeWritersImpl(rootTreeWriter, result);
-    return result;
-  }
-
-  private void getAllColumnTreeWritersImpl(TreeWriter tw,
-      List<TreeWriter> result) {
-    result.add(tw);
-    for (TreeWriter child : tw.childrenWriters) {
-      getAllColumnTreeWritersImpl(child, result);
-    }
-  }
-
-  @Override
-  public void appendUserMetadata(List<OrcProto.UserMetadataItem> userMetadata) {
-    if (userMetadata != null) {
-      for (OrcProto.UserMetadataItem item : userMetadata) {
-        this.userMetadata.put(item.getName(), item.getValue());
-      }
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/impl/ZeroCopyShims.java b/orc/src/java/org/apache/orc/impl/ZeroCopyShims.java
deleted file mode 100644
index de02c8b278..0000000000
--- a/orc/src/java/org/apache/orc/impl/ZeroCopyShims.java
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.EnumSet;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.ReadOption;
-import org.apache.hadoop.io.ByteBufferPool;
-
-class ZeroCopyShims {
-  private static final class ByteBufferPoolAdapter implements ByteBufferPool {
-    private HadoopShims.ByteBufferPoolShim pool;
-
-    public ByteBufferPoolAdapter(HadoopShims.ByteBufferPoolShim pool) {
-      this.pool = pool;
-    }
-
-    @Override
-    public final ByteBuffer getBuffer(boolean direct, int length) {
-      return this.pool.getBuffer(direct, length);
-    }
-
-    @Override
-    public final void putBuffer(ByteBuffer buffer) {
-      this.pool.putBuffer(buffer);
-    }
-  }
-
-  private static final class ZeroCopyAdapter implements HadoopShims.ZeroCopyReaderShim {
-    private final FSDataInputStream in;
-    private final ByteBufferPoolAdapter pool;
-    private final static EnumSet<ReadOption> CHECK_SUM = EnumSet
-        .noneOf(ReadOption.class);
-    private final static EnumSet<ReadOption> NO_CHECK_SUM = EnumSet
-        .of(ReadOption.SKIP_CHECKSUMS);
-
-    public ZeroCopyAdapter(FSDataInputStream in,
-                           HadoopShims.ByteBufferPoolShim poolshim) {
-      this.in = in;
-      if (poolshim != null) {
-        pool = new ByteBufferPoolAdapter(poolshim);
-      } else {
-        pool = null;
-      }
-    }
-
-    public final ByteBuffer readBuffer(int maxLength, boolean verifyChecksums)
-        throws IOException {
-      EnumSet<ReadOption> options = NO_CHECK_SUM;
-      if (verifyChecksums) {
-        options = CHECK_SUM;
-      }
-      return this.in.read(this.pool, maxLength, options);
-    }
-
-    public final void releaseBuffer(ByteBuffer buffer) {
-      this.in.releaseBuffer(buffer);
-    }
-
-    @Override
-    public final void close() throws IOException {
-      this.in.close();
-    }
-  }
-
-  public static HadoopShims.ZeroCopyReaderShim getZeroCopyReader(FSDataInputStream in,
-                                                                 HadoopShims.ByteBufferPoolShim pool) throws IOException {
-    return new ZeroCopyAdapter(in, pool);
-  }
-
-}
diff --git a/orc/src/java/org/apache/orc/impl/ZlibCodec.java b/orc/src/java/org/apache/orc/impl/ZlibCodec.java
deleted file mode 100644
index 5f648a8042..0000000000
--- a/orc/src/java/org/apache/orc/impl/ZlibCodec.java
+++ /dev/null
@@ -1,169 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.EnumSet;
-import java.util.zip.DataFormatException;
-import java.util.zip.Deflater;
-import java.util.zip.Inflater;
-
-import javax.annotation.Nullable;
-
-import org.apache.hadoop.io.compress.DirectDecompressor;
-import org.apache.orc.CompressionCodec;
-
-public class ZlibCodec implements CompressionCodec, DirectDecompressionCodec {
-  private static final HadoopShims SHIMS = HadoopShims.Factory.get();
-  private Boolean direct = null;
-
-  private final int level;
-  private final int strategy;
-
-  public ZlibCodec() {
-    level = Deflater.DEFAULT_COMPRESSION;
-    strategy = Deflater.DEFAULT_STRATEGY;
-  }
-
-  private ZlibCodec(int level, int strategy) {
-    this.level = level;
-    this.strategy = strategy;
-  }
-
-  @Override
-  public boolean compress(ByteBuffer in, ByteBuffer out,
-                          ByteBuffer overflow) throws IOException {
-    Deflater deflater = new Deflater(level, true);
-    deflater.setStrategy(strategy);
-    int length = in.remaining();
-    deflater.setInput(in.array(), in.arrayOffset() + in.position(), length);
-    deflater.finish();
-    int outSize = 0;
-    int offset = out.arrayOffset() + out.position();
-    while (!deflater.finished() && (length > outSize)) {
-      int size = deflater.deflate(out.array(), offset, out.remaining());
-      out.position(size + out.position());
-      outSize += size;
-      offset += size;
-      // if we run out of space in the out buffer, use the overflow
-      if (out.remaining() == 0) {
-        if (overflow == null) {
-          deflater.end();
-          return false;
-        }
-        out = overflow;
-        offset = out.arrayOffset() + out.position();
-      }
-    }
-    deflater.end();
-    return length > outSize;
-  }
-
-  @Override
-  public void decompress(ByteBuffer in, ByteBuffer out) throws IOException {
-
-    if(in.isDirect() && out.isDirect()) {
-      directDecompress(in, out);
-      return;
-    }
-
-    Inflater inflater = new Inflater(true);
-    inflater.setInput(in.array(), in.arrayOffset() + in.position(),
-                      in.remaining());
-    while (!(inflater.finished() || inflater.needsDictionary() ||
-             inflater.needsInput())) {
-      try {
-        int count = inflater.inflate(out.array(),
-                                     out.arrayOffset() + out.position(),
-                                     out.remaining());
-        out.position(count + out.position());
-      } catch (DataFormatException dfe) {
-        throw new IOException("Bad compression data", dfe);
-      }
-    }
-    out.flip();
-    inflater.end();
-    in.position(in.limit());
-  }
-
-  @Override
-  public boolean isAvailable() {
-    if (direct == null) {
-      // see nowrap option in new Inflater(boolean) which disables zlib headers
-      try {
-        if (SHIMS.getDirectDecompressor(
-            HadoopShims.DirectCompressionType.ZLIB_NOHEADER) != null) {
-          direct = Boolean.valueOf(true);
-        } else {
-          direct = Boolean.valueOf(false);
-        }
-      } catch (UnsatisfiedLinkError ule) {
-        direct = Boolean.valueOf(false);
-      }
-    }
-    return direct.booleanValue();
-  }
-
-  @Override
-  public void directDecompress(ByteBuffer in, ByteBuffer out)
-      throws IOException {
-    HadoopShims.DirectDecompressor decompressShim =
-        SHIMS.getDirectDecompressor(HadoopShims.DirectCompressionType.ZLIB_NOHEADER);
-    decompressShim.decompress(in, out);
-    out.flip(); // flip for read
-  }
-
-  @Override
-  public CompressionCodec modify(@Nullable EnumSet<Modifier> modifiers) {
-
-    if (modifiers == null) {
-      return this;
-    }
-
-    int l = this.level;
-    int s = this.strategy;
-
-    for (Modifier m : modifiers) {
-      switch (m) {
-      case BINARY:
-        /* filtered == less LZ77, more huffman */
-        s = Deflater.FILTERED;
-        break;
-      case TEXT:
-        s = Deflater.DEFAULT_STRATEGY;
-        break;
-      case FASTEST:
-        // deflate_fast looking for 8 byte patterns
-        l = Deflater.BEST_SPEED;
-        break;
-      case FAST:
-        // deflate_fast looking for 16 byte patterns
-        l = Deflater.BEST_SPEED + 1;
-        break;
-      case DEFAULT:
-        // deflate_slow looking for 128 byte patterns
-        l = Deflater.DEFAULT_COMPRESSION;
-        break;
-      default:
-        break;
-      }
-    }
-    return new ZlibCodec(l, s);
-  }
-}
diff --git a/orc/src/java/org/apache/orc/tools/FileDump.java b/orc/src/java/org/apache/orc/tools/FileDump.java
deleted file mode 100644
index 1a1d8ab639..0000000000
--- a/orc/src/java/org/apache/orc/tools/FileDump.java
+++ /dev/null
@@ -1,946 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.tools;
-
-import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.PrintStream;
-import java.text.DecimalFormat;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.OptionBuilder;
-import org.apache.commons.cli.Options;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.hdfs.DistributedFileSystem;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.orc.BloomFilterIO;
-import org.apache.orc.ColumnStatistics;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcFile;
-import org.apache.orc.Reader;
-import org.apache.orc.RecordReader;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.apache.orc.impl.AcidStats;
-import org.apache.orc.impl.ColumnStatisticsImpl;
-import org.apache.orc.impl.OrcAcidUtils;
-import org.apache.orc.impl.OrcIndex;
-import org.apache.orc.OrcProto;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.StripeStatistics;
-import org.apache.orc.impl.RecordReaderImpl;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONWriter;
-
-import com.google.common.base.Joiner;
-import com.google.common.base.Strings;
-import com.google.common.collect.Lists;
-
-/**
- * A tool for printing out the file structure of ORC files.
- */
-public final class FileDump {
-  public static final String UNKNOWN = "UNKNOWN";
-  public static final String SEPARATOR = Strings.repeat("_", 120) + "\n";
-  public static final int DEFAULT_BLOCK_SIZE = 256 * 1024 * 1024;
-  public static final String DEFAULT_BACKUP_PATH = System.getProperty("java.io.tmpdir");
-  public static final PathFilter HIDDEN_AND_SIDE_FILE_FILTER = new PathFilter() {
-    public boolean accept(Path p) {
-      String name = p.getName();
-      return !name.startsWith("_") && !name.startsWith(".") && !name.endsWith(
-          OrcAcidUtils.DELTA_SIDE_FILE_SUFFIX);
-    }
-  };
-
-  // not used
-  private FileDump() {
-  }
-
-  public static void main(String[] args) throws Exception {
-    Configuration conf = new Configuration();
-
-    List<Integer> rowIndexCols = new ArrayList<Integer>(0);
-    Options opts = createOptions();
-    CommandLine cli = new GnuParser().parse(opts, args);
-
-    if (cli.hasOption('h')) {
-      HelpFormatter formatter = new HelpFormatter();
-      formatter.printHelp("orcfiledump", opts);
-      return;
-    }
-
-    boolean dumpData = cli.hasOption('d');
-    boolean recover = cli.hasOption("recover");
-    boolean skipDump = cli.hasOption("skip-dump");
-    String backupPath = DEFAULT_BACKUP_PATH;
-    if (cli.hasOption("backup-path")) {
-      backupPath = cli.getOptionValue("backup-path");
-    }
-
-    if (cli.hasOption("r")) {
-      String val = cli.getOptionValue("r");
-      if (val != null && val.trim().equals("*")) {
-        rowIndexCols = null; // All the columns
-      } else {
-        String[] colStrs = cli.getOptionValue("r").split(",");
-        rowIndexCols = new ArrayList<Integer>(colStrs.length);
-        for (String colStr : colStrs) {
-          rowIndexCols.add(Integer.parseInt(colStr));
-        }
-      }
-    }
-
-    boolean printTimeZone = cli.hasOption('t');
-    boolean jsonFormat = cli.hasOption('j');
-    String[] files = cli.getArgs();
-    if (files.length == 0) {
-      System.err.println("Error : ORC files are not specified");
-      return;
-    }
-
-    // if the specified path is directory, iterate through all files and print the file dump
-    List<String> filesInPath = Lists.newArrayList();
-    for (String filename : files) {
-      Path path = new Path(filename);
-      filesInPath.addAll(getAllFilesInPath(path, conf));
-    }
-
-    if (dumpData) {
-      printData(filesInPath, conf);
-    } else if (recover && skipDump) {
-      recoverFiles(filesInPath, conf, backupPath);
-    } else {
-      if (jsonFormat) {
-        boolean prettyPrint = cli.hasOption('p');
-        JsonFileDump.printJsonMetaData(filesInPath, conf, rowIndexCols, prettyPrint, printTimeZone);
-      } else {
-        printMetaData(filesInPath, conf, rowIndexCols, printTimeZone, recover, backupPath);
-      }
-    }
-  }
-
-  /**
-   * This method returns an ORC reader object if the specified file is readable. If the specified
-   * file has side file (_flush_length) file, then max footer offset will be read from the side
-   * file and orc reader will be created from that offset. Since both data file and side file
-   * use hflush() for flushing the data, there could be some inconsistencies and both files could be
-   * out-of-sync. Following are the cases under which null will be returned
-   *
-   * 1) If the file specified by path or its side file is still open for writes
-   * 2) If *_flush_length file does not return any footer offset
-   * 3) If *_flush_length returns a valid footer offset but the data file is not readable at that
-   *    position (incomplete data file)
-   * 4) If *_flush_length file length is not a multiple of 8, then reader will be created from
-   *    previous valid footer. If there is no such footer (file length > 0 and < 8), then null will
-   *    be returned
-   *
-   * Also, if this method detects any file corruption (mismatch between data file and side file)
-   * then it will add the corresponding file to the specified input list for corrupted files.
-   *
-   * In all other cases, where the file is readable this method will return a reader object.
-   *
-   * @param path - file to get reader for
-   * @param conf - configuration object
-   * @param corruptFiles - fills this list with all possible corrupted files
-   * @return - reader for the specified file or null
-   * @throws IOException
-   */
-  static Reader getReader(final Path path, final Configuration conf,
-      final List<String> corruptFiles) throws IOException {
-    FileSystem fs = path.getFileSystem(conf);
-    long dataFileLen = fs.getFileStatus(path).getLen();
-    System.err.println("Processing data file " + path + " [length: " + dataFileLen + "]");
-    Path sideFile = OrcAcidUtils.getSideFile(path);
-    final boolean sideFileExists = fs.exists(sideFile);
-    boolean openDataFile = false;
-    boolean openSideFile = false;
-    if (fs instanceof DistributedFileSystem) {
-      DistributedFileSystem dfs = (DistributedFileSystem) fs;
-      openDataFile = !dfs.isFileClosed(path);
-      openSideFile = sideFileExists && !dfs.isFileClosed(sideFile);
-    }
-
-    if (openDataFile || openSideFile) {
-      if (openDataFile && openSideFile) {
-        System.err.println("Unable to perform file dump as " + path + " and " + sideFile +
-            " are still open for writes.");
-      } else if (openSideFile) {
-        System.err.println("Unable to perform file dump as " + sideFile +
-            " is still open for writes.");
-      } else {
-        System.err.println("Unable to perform file dump as " + path +
-            " is still open for writes.");
-      }
-
-      return null;
-    }
-
-    Reader reader = null;
-    if (sideFileExists) {
-      final long maxLen = OrcAcidUtils.getLastFlushLength(fs, path);
-      final long sideFileLen = fs.getFileStatus(sideFile).getLen();
-      System.err.println("Found flush length file " + sideFile
-          + " [length: " + sideFileLen + ", maxFooterOffset: " + maxLen + "]");
-      // no offsets read from side file
-      if (maxLen == -1) {
-
-        // if data file is larger than last flush length, then additional data could be recovered
-        if (dataFileLen > maxLen) {
-          System.err.println("Data file has more data than max footer offset:" + maxLen +
-              ". Adding data file to recovery list.");
-          if (corruptFiles != null) {
-            corruptFiles.add(path.toUri().toString());
-          }
-        }
-        return null;
-      }
-
-      try {
-        reader = OrcFile.createReader(path, OrcFile.readerOptions(conf).maxLength(maxLen));
-
-        // if data file is larger than last flush length, then additional data could be recovered
-        if (dataFileLen > maxLen) {
-          System.err.println("Data file has more data than max footer offset:" + maxLen +
-              ". Adding data file to recovery list.");
-          if (corruptFiles != null) {
-            corruptFiles.add(path.toUri().toString());
-          }
-        }
-      } catch (Exception e) {
-        if (corruptFiles != null) {
-          corruptFiles.add(path.toUri().toString());
-        }
-        System.err.println("Unable to read data from max footer offset." +
-            " Adding data file to recovery list.");
-        return null;
-      }
-    } else {
-      reader = OrcFile.createReader(path, OrcFile.readerOptions(conf));
-    }
-
-    return reader;
-  }
-
-  public static Collection<String> getAllFilesInPath(final Path path,
-      final Configuration conf) throws IOException {
-    List<String> filesInPath = Lists.newArrayList();
-    FileSystem fs = path.getFileSystem(conf);
-    FileStatus fileStatus = fs.getFileStatus(path);
-    if (fileStatus.isDir()) {
-      FileStatus[] fileStatuses = fs.listStatus(path, HIDDEN_AND_SIDE_FILE_FILTER);
-      for (FileStatus fileInPath : fileStatuses) {
-        if (fileInPath.isDir()) {
-          filesInPath.addAll(getAllFilesInPath(fileInPath.getPath(), conf));
-        } else {
-          filesInPath.add(fileInPath.getPath().toString());
-        }
-      }
-    } else {
-      filesInPath.add(path.toString());
-    }
-
-    return filesInPath;
-  }
-
-  private static void printData(List<String> files,
-      Configuration conf) throws IOException,
-      JSONException {
-    for (String file : files) {
-      try {
-        Path path = new Path(file);
-        Reader reader = getReader(path, conf, Lists.<String>newArrayList());
-        if (reader == null) {
-          continue;
-        }
-        printJsonData(reader);
-        System.out.println(SEPARATOR);
-      } catch (Exception e) {
-        System.err.println("Unable to dump data for file: " + file);
-        continue;
-      }
-    }
-  }
-
-  private static void printMetaData(List<String> files, Configuration conf,
-      List<Integer> rowIndexCols, boolean printTimeZone, final boolean recover,
-      final String backupPath)
-      throws IOException {
-    List<String> corruptFiles = Lists.newArrayList();
-    for (String filename : files) {
-      printMetaDataImpl(filename, conf, rowIndexCols, printTimeZone, corruptFiles);
-      System.out.println(SEPARATOR);
-    }
-
-    if (!corruptFiles.isEmpty()) {
-      if (recover) {
-        recoverFiles(corruptFiles, conf, backupPath);
-      } else {
-        System.err.println(corruptFiles.size() + " file(s) are corrupted." +
-            " Run the following command to recover corrupted files.\n");
-        String fileNames = Joiner.on(" ").skipNulls().join(corruptFiles);
-        System.err.println("hive --orcfiledump --recover --skip-dump " + fileNames);
-        System.out.println(SEPARATOR);
-      }
-    }
-  }
-
-  private static void printMetaDataImpl(final String filename,
-      final Configuration conf, List<Integer> rowIndexCols, final boolean printTimeZone,
-      final List<String> corruptFiles) throws IOException {
-    Path file = new Path(filename);
-    Reader reader = getReader(file, conf, corruptFiles);
-    // if we can create reader then footer is not corrupt and file will readable
-    if (reader == null) {
-      return;
-    }
-
-    System.out.println("Structure for " + filename);
-    System.out.println("File Version: " + reader.getFileVersion().getName() +
-        " with " + reader.getWriterVersion());
-    RecordReaderImpl rows = (RecordReaderImpl) reader.rows();
-    System.out.println("Rows: " + reader.getNumberOfRows());
-    System.out.println("Compression: " + reader.getCompressionKind());
-    if (reader.getCompressionKind() != CompressionKind.NONE) {
-      System.out.println("Compression size: " + reader.getCompressionSize());
-    }
-    System.out.println("Type: " + reader.getSchema().toString());
-    System.out.println("\nStripe Statistics:");
-    List<StripeStatistics> stripeStats = reader.getStripeStatistics();
-    for (int n = 0; n < stripeStats.size(); n++) {
-      System.out.println("  Stripe " + (n + 1) + ":");
-      StripeStatistics ss = stripeStats.get(n);
-      for (int i = 0; i < ss.getColumnStatistics().length; ++i) {
-        System.out.println("    Column " + i + ": " +
-            ss.getColumnStatistics()[i].toString());
-      }
-    }
-    ColumnStatistics[] stats = reader.getStatistics();
-    int colCount = stats.length;
-    if (rowIndexCols == null) {
-      rowIndexCols = new ArrayList<>(colCount);
-      for (int i = 0; i < colCount; ++i) {
-        rowIndexCols.add(i);
-      }
-    }
-    System.out.println("\nFile Statistics:");
-    for (int i = 0; i < stats.length; ++i) {
-      System.out.println("  Column " + i + ": " + stats[i].toString());
-    }
-    System.out.println("\nStripes:");
-    int stripeIx = -1;
-    for (StripeInformation stripe : reader.getStripes()) {
-      ++stripeIx;
-      long stripeStart = stripe.getOffset();
-      OrcProto.StripeFooter footer = rows.readStripeFooter(stripe);
-      if (printTimeZone) {
-        String tz = footer.getWriterTimezone();
-        if (tz == null || tz.isEmpty()) {
-          tz = UNKNOWN;
-        }
-        System.out.println("  Stripe: " + stripe.toString() + " timezone: " + tz);
-      } else {
-        System.out.println("  Stripe: " + stripe.toString());
-      }
-      long sectionStart = stripeStart;
-      for (OrcProto.Stream section : footer.getStreamsList()) {
-        String kind = section.hasKind() ? section.getKind().name() : UNKNOWN;
-        System.out.println("    Stream: column " + section.getColumn() +
-            " section " + kind + " start: " + sectionStart +
-            " length " + section.getLength());
-        sectionStart += section.getLength();
-      }
-      for (int i = 0; i < footer.getColumnsCount(); ++i) {
-        OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-        StringBuilder buf = new StringBuilder();
-        buf.append("    Encoding column ");
-        buf.append(i);
-        buf.append(": ");
-        buf.append(encoding.getKind());
-        if (encoding.getKind() == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
-            encoding.getKind() == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
-          buf.append("[");
-          buf.append(encoding.getDictionarySize());
-          buf.append("]");
-        }
-        System.out.println(buf);
-      }
-      if (rowIndexCols != null && !rowIndexCols.isEmpty()) {
-        // include the columns that are specified, only if the columns are included, bloom filter
-        // will be read
-        boolean[] sargColumns = new boolean[colCount];
-        for (int colIdx : rowIndexCols) {
-          sargColumns[colIdx] = true;
-        }
-        OrcIndex indices = rows
-            .readRowIndex(stripeIx, null, null, null, sargColumns);
-        for (int col : rowIndexCols) {
-          StringBuilder buf = new StringBuilder();
-          String rowIdxString = getFormattedRowIndices(col, indices.getRowGroupIndex());
-          buf.append(rowIdxString);
-          String bloomFilString = getFormattedBloomFilters(col, indices.getBloomFilterIndex());
-          buf.append(bloomFilString);
-          System.out.println(buf);
-        }
-      }
-    }
-
-    FileSystem fs = file.getFileSystem(conf);
-    long fileLen = fs.getFileStatus(file).getLen();
-    long paddedBytes = getTotalPaddingSize(reader);
-    // empty ORC file is ~45 bytes. Assumption here is file length always >0
-    double percentPadding = ((double) paddedBytes / (double) fileLen) * 100;
-    DecimalFormat format = new DecimalFormat("##.##");
-    System.out.println("\nFile length: " + fileLen + " bytes");
-    System.out.println("Padding length: " + paddedBytes + " bytes");
-    System.out.println("Padding ratio: " + format.format(percentPadding) + "%");
-    AcidStats acidStats = OrcAcidUtils.parseAcidStats(reader);
-    if (acidStats != null) {
-      System.out.println("ACID stats:" + acidStats);
-    }
-    rows.close();
-  }
-
-  private static void recoverFiles(final List<String> corruptFiles, final Configuration conf,
-      final String backup)
-      throws IOException {
-    for (String corruptFile : corruptFiles) {
-      System.err.println("Recovering file " + corruptFile);
-      Path corruptPath = new Path(corruptFile);
-      FileSystem fs = corruptPath.getFileSystem(conf);
-      FSDataInputStream fdis = fs.open(corruptPath);
-      try {
-        long corruptFileLen = fs.getFileStatus(corruptPath).getLen();
-        long remaining = corruptFileLen;
-        List<Long> footerOffsets = Lists.newArrayList();
-
-        // start reading the data file form top to bottom and record the valid footers
-        while (remaining > 0) {
-          int toRead = (int) Math.min(DEFAULT_BLOCK_SIZE, remaining);
-          byte[] data = new byte[toRead];
-          long startPos = corruptFileLen - remaining;
-          fdis.readFully(startPos, data, 0, toRead);
-
-          // find all MAGIC string and see if the file is readable from there
-          int index = 0;
-          long nextFooterOffset;
-
-          while (index != -1) {
-            index = indexOf(data, OrcFile.MAGIC.getBytes(), index + 1);
-            if (index != -1) {
-              nextFooterOffset = startPos + index + OrcFile.MAGIC.length() + 1;
-              if (isReadable(corruptPath, conf, nextFooterOffset)) {
-                footerOffsets.add(nextFooterOffset);
-              }
-            }
-          }
-
-          System.err.println("Scanning for valid footers - startPos: " + startPos +
-              " toRead: " + toRead + " remaining: " + remaining);
-          remaining = remaining - toRead;
-        }
-
-        System.err.println("Readable footerOffsets: " + footerOffsets);
-        recoverFile(corruptPath, fs, conf, footerOffsets, backup);
-      } catch (Exception e) {
-        Path recoveryFile = getRecoveryFile(corruptPath);
-        if (fs.exists(recoveryFile)) {
-          fs.delete(recoveryFile, false);
-        }
-        System.err.println("Unable to recover file " + corruptFile);
-        e.printStackTrace();
-        System.err.println(SEPARATOR);
-        continue;
-      } finally {
-        fdis.close();
-      }
-      System.err.println(corruptFile + " recovered successfully!");
-      System.err.println(SEPARATOR);
-    }
-  }
-
-  private static void recoverFile(final Path corruptPath, final FileSystem fs,
-      final Configuration conf, final List<Long> footerOffsets, final String backup)
-      throws IOException {
-
-    // first recover the file to .recovered file and then once successful rename it to actual file
-    Path recoveredPath = getRecoveryFile(corruptPath);
-
-    // make sure that file does not exist
-    if (fs.exists(recoveredPath)) {
-      fs.delete(recoveredPath, false);
-    }
-
-    // if there are no valid footers, the file should still be readable so create an empty orc file
-    if (footerOffsets == null || footerOffsets.isEmpty()) {
-      System.err.println("No readable footers found. Creating empty orc file.");
-      TypeDescription schema = TypeDescription.createStruct();
-      Writer writer = OrcFile.createWriter(recoveredPath,
-          OrcFile.writerOptions(conf).setSchema(schema));
-      writer.close();
-    } else {
-      FSDataInputStream fdis = fs.open(corruptPath);
-      FileStatus fileStatus = fs.getFileStatus(corruptPath);
-      // read corrupt file and copy it to recovered file until last valid footer
-      FSDataOutputStream fdos = fs.create(recoveredPath, true,
-          conf.getInt("io.file.buffer.size", 4096),
-          fileStatus.getReplication(),
-          fileStatus.getBlockSize());
-      try {
-        long fileLen = footerOffsets.get(footerOffsets.size() - 1);
-        long remaining = fileLen;
-
-        while (remaining > 0) {
-          int toRead = (int) Math.min(DEFAULT_BLOCK_SIZE, remaining);
-          byte[] data = new byte[toRead];
-          long startPos = fileLen - remaining;
-          fdis.readFully(startPos, data, 0, toRead);
-          fdos.write(data);
-          System.err.println("Copying data to recovery file - startPos: " + startPos +
-              " toRead: " + toRead + " remaining: " + remaining);
-          remaining = remaining - toRead;
-        }
-      } catch (Exception e) {
-        fs.delete(recoveredPath, false);
-        throw new IOException(e);
-      } finally {
-        fdis.close();
-        fdos.close();
-      }
-    }
-
-    // validate the recovered file once again and start moving corrupt files to backup folder
-    if (isReadable(recoveredPath, conf, Long.MAX_VALUE)) {
-      Path backupDataPath;
-      String scheme = corruptPath.toUri().getScheme();
-      String authority = corruptPath.toUri().getAuthority();
-      String filePath = corruptPath.toUri().getPath();
-
-      // use the same filesystem as corrupt file if backup-path is not explicitly specified
-      if (backup.equals(DEFAULT_BACKUP_PATH)) {
-        backupDataPath = new Path(scheme, authority, DEFAULT_BACKUP_PATH + filePath);
-      } else {
-        backupDataPath = Path.mergePaths(new Path(backup), corruptPath);
-      }
-
-      // Move data file to backup path
-      moveFiles(fs, corruptPath, backupDataPath);
-
-      // Move side file to backup path
-      Path sideFilePath = OrcAcidUtils.getSideFile(corruptPath);
-      Path backupSideFilePath = new Path(backupDataPath.getParent(), sideFilePath.getName());
-      moveFiles(fs, sideFilePath, backupSideFilePath);
-
-      // finally move recovered file to actual file
-      moveFiles(fs, recoveredPath, corruptPath);
-
-      // we are done recovering, backing up and validating
-      System.err.println("Validation of recovered file successful!");
-    }
-  }
-
-  private static void moveFiles(final FileSystem fs, final Path src, final Path dest)
-      throws IOException {
-    try {
-      // create the dest directory if not exist
-      if (!fs.exists(dest.getParent())) {
-        fs.mkdirs(dest.getParent());
-      }
-
-      // if the destination file exists for some reason delete it
-      fs.delete(dest, false);
-
-      if (fs.rename(src, dest)) {
-        System.err.println("Moved " + src + " to " + dest);
-      } else {
-        throw new IOException("Unable to move " + src + " to " + dest);
-      }
-
-    } catch (Exception e) {
-      throw new IOException("Unable to move " + src + " to " + dest, e);
-    }
-  }
-
-  private static Path getRecoveryFile(final Path corruptPath) {
-    return new Path(corruptPath.getParent(), corruptPath.getName() + ".recovered");
-  }
-
-  private static boolean isReadable(final Path corruptPath, final Configuration conf,
-      final long maxLen) {
-    try {
-      OrcFile.createReader(corruptPath, OrcFile.readerOptions(conf).maxLength(maxLen));
-      return true;
-    } catch (Exception e) {
-      // ignore this exception as maxLen is unreadable
-      return false;
-    }
-  }
-
-  // search for byte pattern in another byte array
-  private static int indexOf(final byte[] data, final byte[] pattern, final int index) {
-    if (data == null || data.length == 0 || pattern == null || pattern.length == 0 ||
-        index > data.length || index < 0) {
-      return -1;
-    }
-
-    int j = 0;
-    for (int i = index; i < data.length; i++) {
-      if (pattern[j] == data[i]) {
-        j++;
-      } else {
-        j = 0;
-      }
-
-      if (j == pattern.length) {
-        return i - pattern.length + 1;
-      }
-    }
-
-    return -1;
-  }
-
-  private static String getFormattedBloomFilters(int col,
-      OrcProto.BloomFilterIndex[] bloomFilterIndex) {
-    StringBuilder buf = new StringBuilder();
-    BloomFilterIO stripeLevelBF = null;
-    if (bloomFilterIndex != null && bloomFilterIndex[col] != null) {
-      int idx = 0;
-      buf.append("\n    Bloom filters for column ").append(col).append(":");
-      for (OrcProto.BloomFilter bf : bloomFilterIndex[col].getBloomFilterList()) {
-        BloomFilterIO toMerge = new BloomFilterIO(bf);
-        buf.append("\n      Entry ").append(idx++).append(":").append(getBloomFilterStats(toMerge));
-        if (stripeLevelBF == null) {
-          stripeLevelBF = toMerge;
-        } else {
-          stripeLevelBF.merge(toMerge);
-        }
-      }
-      String bloomFilterStats = getBloomFilterStats(stripeLevelBF);
-      buf.append("\n      Stripe level merge:").append(bloomFilterStats);
-    }
-    return buf.toString();
-  }
-
-  private static String getBloomFilterStats(BloomFilterIO bf) {
-    StringBuilder sb = new StringBuilder();
-    int bitCount = bf.getBitSize();
-    int popCount = 0;
-    for (long l : bf.getBitSet()) {
-      popCount += Long.bitCount(l);
-    }
-    int k = bf.getNumHashFunctions();
-    float loadFactor = (float) popCount / (float) bitCount;
-    float expectedFpp = (float) Math.pow(loadFactor, k);
-    DecimalFormat df = new DecimalFormat("###.####");
-    sb.append(" numHashFunctions: ").append(k);
-    sb.append(" bitCount: ").append(bitCount);
-    sb.append(" popCount: ").append(popCount);
-    sb.append(" loadFactor: ").append(df.format(loadFactor));
-    sb.append(" expectedFpp: ").append(expectedFpp);
-    return sb.toString();
-  }
-
-  private static String getFormattedRowIndices(int col,
-                                               OrcProto.RowIndex[] rowGroupIndex) {
-    StringBuilder buf = new StringBuilder();
-    OrcProto.RowIndex index;
-    buf.append("    Row group indices for column ").append(col).append(":");
-    if (rowGroupIndex == null || (col >= rowGroupIndex.length) ||
-        ((index = rowGroupIndex[col]) == null)) {
-      buf.append(" not found\n");
-      return buf.toString();
-    }
-
-    for (int entryIx = 0; entryIx < index.getEntryCount(); ++entryIx) {
-      buf.append("\n      Entry ").append(entryIx).append(": ");
-      OrcProto.RowIndexEntry entry = index.getEntry(entryIx);
-      if (entry == null) {
-        buf.append("unknown\n");
-        continue;
-      }
-      OrcProto.ColumnStatistics colStats = entry.getStatistics();
-      if (colStats == null) {
-        buf.append("no stats at ");
-      } else {
-        ColumnStatistics cs = ColumnStatisticsImpl.deserialize(colStats);
-        buf.append(cs.toString());
-      }
-      buf.append(" positions: ");
-      for (int posIx = 0; posIx < entry.getPositionsCount(); ++posIx) {
-        if (posIx != 0) {
-          buf.append(",");
-        }
-        buf.append(entry.getPositions(posIx));
-      }
-    }
-    return buf.toString();
-  }
-
-  public static long getTotalPaddingSize(Reader reader) throws IOException {
-    long paddedBytes = 0;
-    List<StripeInformation> stripes = reader.getStripes();
-    for (int i = 1; i < stripes.size(); i++) {
-      long prevStripeOffset = stripes.get(i - 1).getOffset();
-      long prevStripeLen = stripes.get(i - 1).getLength();
-      paddedBytes += stripes.get(i).getOffset() - (prevStripeOffset + prevStripeLen);
-    }
-    return paddedBytes;
-  }
-
-  @SuppressWarnings("static-access")
-  static Options createOptions() {
-    Options result = new Options();
-
-    // add -d and --data to print the rows
-    result.addOption(OptionBuilder
-        .withLongOpt("data")
-        .withDescription("Should the data be printed")
-        .create('d'));
-
-    // to avoid breaking unit tests (when run in different time zones) for file dump, printing
-    // of timezone is made optional
-    result.addOption(OptionBuilder
-        .withLongOpt("timezone")
-        .withDescription("Print writer's time zone")
-        .create('t'));
-
-    result.addOption(OptionBuilder
-        .withLongOpt("help")
-        .withDescription("print help message")
-        .create('h'));
-
-    result.addOption(OptionBuilder
-        .withLongOpt("rowindex")
-        .withArgName("comma separated list of column ids for which row index should be printed")
-        .withDescription("Dump stats for column number(s)")
-        .hasArg()
-        .create('r'));
-
-    result.addOption(OptionBuilder
-        .withLongOpt("json")
-        .withDescription("Print metadata in JSON format")
-        .create('j'));
-
-    result.addOption(OptionBuilder
-        .withLongOpt("pretty")
-        .withDescription("Pretty print json metadata output")
-        .create('p'));
-
-    result.addOption(OptionBuilder
-        .withLongOpt("recover")
-        .withDescription("recover corrupted orc files generated by streaming")
-        .create());
-
-    result.addOption(OptionBuilder
-        .withLongOpt("skip-dump")
-        .withDescription("used along with --recover to directly recover files without dumping")
-        .create());
-
-    result.addOption(OptionBuilder
-        .withLongOpt("backup-path")
-        .withDescription("specify a backup path to store the corrupted files (default: /tmp)")
-        .hasArg()
-        .create());
-    return result;
-  }
-
-  private static void printMap(JSONWriter writer,
-                               MapColumnVector vector,
-                               TypeDescription schema,
-                               int row) throws JSONException {
-    writer.array();
-    TypeDescription keyType = schema.getChildren().get(0);
-    TypeDescription valueType = schema.getChildren().get(1);
-    int offset = (int) vector.offsets[row];
-    for (int i = 0; i < vector.lengths[row]; ++i) {
-      writer.object();
-      writer.key("_key");
-      printValue(writer, vector.keys, keyType, offset + i);
-      writer.key("_value");
-      printValue(writer, vector.values, valueType, offset + i);
-      writer.endObject();
-    }
-    writer.endArray();
-  }
-
-  private static void printList(JSONWriter writer,
-                                ListColumnVector vector,
-                                TypeDescription schema,
-                                int row) throws JSONException {
-    writer.array();
-    int offset = (int) vector.offsets[row];
-    TypeDescription childType = schema.getChildren().get(0);
-    for (int i = 0; i < vector.lengths[row]; ++i) {
-      printValue(writer, vector.child, childType, offset + i);
-    }
-    writer.endArray();
-  }
-
-  private static void printUnion(JSONWriter writer,
-                                 UnionColumnVector vector,
-                                 TypeDescription schema,
-                                 int row) throws JSONException {
-    int tag = vector.tags[row];
-    printValue(writer, vector.fields[tag], schema.getChildren().get(tag), row);
-  }
-
-  static void printStruct(JSONWriter writer,
-                          StructColumnVector batch,
-                          TypeDescription schema,
-                          int row) throws JSONException {
-    writer.object();
-    List<String> fieldNames = schema.getFieldNames();
-    List<TypeDescription> fieldTypes = schema.getChildren();
-    for (int i = 0; i < fieldTypes.size(); ++i) {
-      writer.key(fieldNames.get(i));
-      printValue(writer, batch.fields[i], fieldTypes.get(i), row);
-    }
-    writer.endObject();
-  }
-
-  static void printBinary(JSONWriter writer, BytesColumnVector vector,
-                          int row) throws JSONException {
-    writer.array();
-    int offset = vector.start[row];
-    for(int i=0; i < vector.length[row]; ++i) {
-      writer.value(0xff & (int) vector.vector[row][offset + i]);
-    }
-    writer.endArray();
-  }
-  static void printValue(JSONWriter writer, ColumnVector vector,
-                         TypeDescription schema, int row) throws JSONException {
-    if (vector.isRepeating) {
-      row = 0;
-    }
-    if (vector.noNulls || !vector.isNull[row]) {
-      switch (schema.getCategory()) {
-        case BOOLEAN:
-          writer.value(((LongColumnVector) vector).vector[row] != 0);
-          break;
-        case BYTE:
-        case SHORT:
-        case INT:
-        case LONG:
-          writer.value(((LongColumnVector) vector).vector[row]);
-          break;
-        case FLOAT:
-        case DOUBLE:
-          writer.value(((DoubleColumnVector) vector).vector[row]);
-          break;
-        case STRING:
-        case CHAR:
-        case VARCHAR:
-          writer.value(((BytesColumnVector) vector).toString(row));
-          break;
-        case BINARY:
-          printBinary(writer, (BytesColumnVector) vector, row);
-          break;
-        case DECIMAL:
-          writer.value(((DecimalColumnVector) vector).vector[row].toString());
-          break;
-        case DATE:
-          writer.value(new DateWritable(
-              (int) ((LongColumnVector) vector).vector[row]).toString());
-          break;
-        case TIMESTAMP:
-          writer.value(((TimestampColumnVector) vector)
-              .asScratchTimestamp(row).toString());
-          break;
-        case LIST:
-          printList(writer, (ListColumnVector) vector, schema, row);
-          break;
-        case MAP:
-          printMap(writer, (MapColumnVector) vector, schema, row);
-          break;
-        case STRUCT:
-          printStruct(writer, (StructColumnVector) vector, schema, row);
-          break;
-        case UNION:
-          printUnion(writer, (UnionColumnVector) vector, schema, row);
-          break;
-        default:
-          throw new IllegalArgumentException("Unknown type " +
-              schema.toString());
-      }
-    } else {
-      writer.value(null);
-    }
-  }
-
-  static void printRow(JSONWriter writer,
-                       VectorizedRowBatch batch,
-                       TypeDescription schema,
-                       int row) throws JSONException {
-    if (schema.getCategory() == TypeDescription.Category.STRUCT) {
-      List<TypeDescription> fieldTypes = schema.getChildren();
-      List<String> fieldNames = schema.getFieldNames();
-      writer.object();
-      for (int c = 0; c < batch.cols.length; ++c) {
-        writer.key(fieldNames.get(c));
-        printValue(writer, batch.cols[c], fieldTypes.get(c), row);
-      }
-      writer.endObject();
-    } else {
-      printValue(writer, batch.cols[0], schema, row);
-    }
-  }
-
-  static void printJsonData(final Reader reader) throws IOException, JSONException {
-    PrintStream printStream = System.out;
-    OutputStreamWriter out = new OutputStreamWriter(printStream, "UTF-8");
-    RecordReader rows = reader.rows();
-    try {
-      TypeDescription schema = reader.getSchema();
-      VectorizedRowBatch batch = schema.createRowBatch();
-      while (rows.nextBatch(batch)) {
-        for(int r=0; r < batch.size; ++r) {
-          JSONWriter writer = new JSONWriter(out);
-          printRow(writer, batch, schema, r);
-          out.write("\n");
-          out.flush();
-          if (printStream.checkError()) {
-            throw new IOException("Error encountered when writing to stdout.");
-          }
-        }
-      }
-    } finally {
-      rows.close();
-    }
-  }
-}
diff --git a/orc/src/java/org/apache/orc/tools/JsonFileDump.java b/orc/src/java/org/apache/orc/tools/JsonFileDump.java
deleted file mode 100644
index e2048eaa67..0000000000
--- a/orc/src/java/org/apache/orc/tools/JsonFileDump.java
+++ /dev/null
@@ -1,412 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.tools;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.Reader;
-import org.apache.orc.impl.AcidStats;
-import org.apache.orc.impl.OrcAcidUtils;
-import org.apache.orc.impl.RecordReaderImpl;
-import org.codehaus.jettison.json.JSONArray;
-import org.apache.orc.BloomFilterIO;
-import org.apache.orc.BinaryColumnStatistics;
-import org.apache.orc.BooleanColumnStatistics;
-import org.apache.orc.ColumnStatistics;
-import org.apache.orc.impl.ColumnStatisticsImpl;
-import org.apache.orc.DateColumnStatistics;
-import org.apache.orc.DecimalColumnStatistics;
-import org.apache.orc.DoubleColumnStatistics;
-import org.apache.orc.IntegerColumnStatistics;
-import org.apache.orc.impl.OrcIndex;
-import org.apache.orc.OrcProto;
-import org.apache.orc.StringColumnStatistics;
-import org.apache.orc.StripeInformation;
-import org.apache.orc.StripeStatistics;
-import org.apache.orc.TimestampColumnStatistics;
-import org.codehaus.jettison.json.JSONException;
-import org.codehaus.jettison.json.JSONObject;
-import org.codehaus.jettison.json.JSONStringer;
-import org.codehaus.jettison.json.JSONWriter;
-
-/**
- * File dump tool with json formatted output.
- */
-public class JsonFileDump {
-
-  public static void printJsonMetaData(List<String> files,
-      Configuration conf,
-      List<Integer> rowIndexCols, boolean prettyPrint, boolean printTimeZone)
-      throws JSONException, IOException {
-    if (files.isEmpty()) {
-      return;
-    }
-    JSONStringer writer = new JSONStringer();
-    boolean multiFile = files.size() > 1;
-    if (multiFile) {
-      writer.array();
-    } else {
-      writer.object();
-    }
-    for (String filename : files) {
-      try {
-        if (multiFile) {
-          writer.object();
-        }
-        writer.key("fileName").value(filename);
-        Path path = new Path(filename);
-        Reader reader = FileDump.getReader(path, conf, null);
-        if (reader == null) {
-          writer.key("status").value("FAILED");
-          continue;
-        }
-        writer.key("fileVersion").value(reader.getFileVersion().getName());
-        writer.key("writerVersion").value(reader.getWriterVersion());
-        RecordReaderImpl rows = (RecordReaderImpl) reader.rows();
-        writer.key("numberOfRows").value(reader.getNumberOfRows());
-        writer.key("compression").value(reader.getCompressionKind());
-        if (reader.getCompressionKind() != CompressionKind.NONE) {
-          writer.key("compressionBufferSize").value(reader.getCompressionSize());
-        }
-        writer.key("schemaString").value(reader.getSchema().toString());
-        writer.key("schema").array();
-        writeSchema(writer, reader.getTypes());
-        writer.endArray();
-
-        writer.key("stripeStatistics").array();
-        List<StripeStatistics> stripeStatistics = reader.getStripeStatistics();
-        for (int n = 0; n < stripeStatistics.size(); n++) {
-          writer.object();
-          writer.key("stripeNumber").value(n + 1);
-          StripeStatistics ss = stripeStatistics.get(n);
-          writer.key("columnStatistics").array();
-          for (int i = 0; i < ss.getColumnStatistics().length; i++) {
-            writer.object();
-            writer.key("columnId").value(i);
-            writeColumnStatistics(writer, ss.getColumnStatistics()[i]);
-            writer.endObject();
-          }
-          writer.endArray();
-          writer.endObject();
-        }
-        writer.endArray();
-
-        ColumnStatistics[] stats = reader.getStatistics();
-        int colCount = stats.length;
-        if (rowIndexCols == null) {
-          rowIndexCols = new ArrayList<>(colCount);
-          for (int i = 0; i < colCount; ++i) {
-            rowIndexCols.add(i);
-          }
-        }
-        writer.key("fileStatistics").array();
-        for (int i = 0; i < stats.length; ++i) {
-          writer.object();
-          writer.key("columnId").value(i);
-          writeColumnStatistics(writer, stats[i]);
-          writer.endObject();
-        }
-        writer.endArray();
-
-        writer.key("stripes").array();
-        int stripeIx = -1;
-        for (StripeInformation stripe : reader.getStripes()) {
-          ++stripeIx;
-          long stripeStart = stripe.getOffset();
-          OrcProto.StripeFooter footer = rows.readStripeFooter(stripe);
-          writer.object(); // start of stripe information
-          writer.key("stripeNumber").value(stripeIx + 1);
-          writer.key("stripeInformation");
-          writeStripeInformation(writer, stripe);
-          if (printTimeZone) {
-            writer.key("writerTimezone").value(
-                footer.hasWriterTimezone() ? footer.getWriterTimezone() : FileDump.UNKNOWN);
-          }
-          long sectionStart = stripeStart;
-
-          writer.key("streams").array();
-          for (OrcProto.Stream section : footer.getStreamsList()) {
-            writer.object();
-            String kind = section.hasKind() ? section.getKind().name() : FileDump.UNKNOWN;
-            writer.key("columnId").value(section.getColumn());
-            writer.key("section").value(kind);
-            writer.key("startOffset").value(sectionStart);
-            writer.key("length").value(section.getLength());
-            sectionStart += section.getLength();
-            writer.endObject();
-          }
-          writer.endArray();
-
-          writer.key("encodings").array();
-          for (int i = 0; i < footer.getColumnsCount(); ++i) {
-            writer.object();
-            OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-            writer.key("columnId").value(i);
-            writer.key("kind").value(encoding.getKind());
-            if (encoding.getKind() == OrcProto.ColumnEncoding.Kind.DICTIONARY ||
-                encoding.getKind() == OrcProto.ColumnEncoding.Kind.DICTIONARY_V2) {
-              writer.key("dictionarySize").value(encoding.getDictionarySize());
-            }
-            writer.endObject();
-          }
-          writer.endArray();
-          if (!rowIndexCols.isEmpty()) {
-            // include the columns that are specified, only if the columns are included, bloom filter
-            // will be read
-            boolean[] sargColumns = new boolean[colCount];
-            for (int colIdx : rowIndexCols) {
-              sargColumns[colIdx] = true;
-            }
-            OrcIndex indices = rows.readRowIndex(stripeIx, null, sargColumns);
-            writer.key("indexes").array();
-            for (int col : rowIndexCols) {
-              writer.object();
-              writer.key("columnId").value(col);
-              writeRowGroupIndexes(writer, col, indices.getRowGroupIndex());
-              writeBloomFilterIndexes(writer, col, indices.getBloomFilterIndex());
-              writer.endObject();
-            }
-            writer.endArray();
-          }
-          writer.endObject(); // end of stripe information
-        }
-        writer.endArray();
-
-        FileSystem fs = path.getFileSystem(conf);
-        long fileLen = fs.getContentSummary(path).getLength();
-        long paddedBytes = FileDump.getTotalPaddingSize(reader);
-        // empty ORC file is ~45 bytes. Assumption here is file length always >0
-        double percentPadding = ((double) paddedBytes / (double) fileLen) * 100;
-        writer.key("fileLength").value(fileLen);
-        writer.key("paddingLength").value(paddedBytes);
-        writer.key("paddingRatio").value(percentPadding);
-        AcidStats acidStats = OrcAcidUtils.parseAcidStats(reader);
-        if (acidStats != null) {
-          writer.key("numInserts").value(acidStats.inserts);
-          writer.key("numDeletes").value(acidStats.deletes);
-          writer.key("numUpdates").value(acidStats.updates);
-        }
-        writer.key("status").value("OK");
-        rows.close();
-
-        writer.endObject();
-      } catch (Exception e) {
-        writer.key("status").value("FAILED");
-        throw e;
-      }
-    }
-    if (multiFile) {
-      writer.endArray();
-    }
-
-    if (prettyPrint) {
-      final String prettyJson;
-      if (multiFile) {
-        JSONArray jsonArray = new JSONArray(writer.toString());
-        prettyJson = jsonArray.toString(2);
-      } else {
-        JSONObject jsonObject = new JSONObject(writer.toString());
-        prettyJson = jsonObject.toString(2);
-      }
-      System.out.println(prettyJson);
-    } else {
-      System.out.println(writer.toString());
-    }
-  }
-
-  private static void writeSchema(JSONStringer writer, List<OrcProto.Type> types)
-      throws JSONException {
-    int i = 0;
-    for(OrcProto.Type type : types) {
-      writer.object();
-      writer.key("columnId").value(i++);
-      writer.key("columnType").value(type.getKind());
-      if (type.getFieldNamesCount() > 0) {
-        writer.key("childColumnNames").array();
-        for (String field : type.getFieldNamesList()) {
-          writer.value(field);
-        }
-        writer.endArray();
-        writer.key("childColumnIds").array();
-        for (Integer colId : type.getSubtypesList()) {
-          writer.value(colId);
-        }
-        writer.endArray();
-      }
-      if (type.hasPrecision()) {
-        writer.key("precision").value(type.getPrecision());
-      }
-
-      if (type.hasScale()) {
-        writer.key("scale").value(type.getScale());
-      }
-
-      if (type.hasMaximumLength()) {
-        writer.key("maxLength").value(type.getMaximumLength());
-      }
-      writer.endObject();
-    }
-  }
-
-  private static void writeStripeInformation(JSONWriter writer, StripeInformation stripe)
-      throws JSONException {
-    writer.object();
-    writer.key("offset").value(stripe.getOffset());
-    writer.key("indexLength").value(stripe.getIndexLength());
-    writer.key("dataLength").value(stripe.getDataLength());
-    writer.key("footerLength").value(stripe.getFooterLength());
-    writer.key("rowCount").value(stripe.getNumberOfRows());
-    writer.endObject();
-  }
-
-  private static void writeColumnStatistics(JSONWriter writer, ColumnStatistics cs)
-      throws JSONException {
-    if (cs != null) {
-      writer.key("count").value(cs.getNumberOfValues());
-      writer.key("hasNull").value(cs.hasNull());
-      if (cs instanceof BinaryColumnStatistics) {
-        writer.key("totalLength").value(((BinaryColumnStatistics) cs).getSum());
-        writer.key("type").value(OrcProto.Type.Kind.BINARY);
-      } else if (cs instanceof BooleanColumnStatistics) {
-        writer.key("trueCount").value(((BooleanColumnStatistics) cs).getTrueCount());
-        writer.key("falseCount").value(((BooleanColumnStatistics) cs).getFalseCount());
-        writer.key("type").value(OrcProto.Type.Kind.BOOLEAN);
-      } else if (cs instanceof IntegerColumnStatistics) {
-        writer.key("min").value(((IntegerColumnStatistics) cs).getMinimum());
-        writer.key("max").value(((IntegerColumnStatistics) cs).getMaximum());
-        if (((IntegerColumnStatistics) cs).isSumDefined()) {
-          writer.key("sum").value(((IntegerColumnStatistics) cs).getSum());
-        }
-        writer.key("type").value(OrcProto.Type.Kind.LONG);
-      } else if (cs instanceof DoubleColumnStatistics) {
-        writer.key("min").value(((DoubleColumnStatistics) cs).getMinimum());
-        writer.key("max").value(((DoubleColumnStatistics) cs).getMaximum());
-        writer.key("sum").value(((DoubleColumnStatistics) cs).getSum());
-        writer.key("type").value(OrcProto.Type.Kind.DOUBLE);
-      } else if (cs instanceof StringColumnStatistics) {
-        writer.key("min").value(((StringColumnStatistics) cs).getMinimum());
-        writer.key("max").value(((StringColumnStatistics) cs).getMaximum());
-        writer.key("totalLength").value(((StringColumnStatistics) cs).getSum());
-        writer.key("type").value(OrcProto.Type.Kind.STRING);
-      } else if (cs instanceof DateColumnStatistics) {
-        if (((DateColumnStatistics) cs).getMaximum() != null) {
-          writer.key("min").value(((DateColumnStatistics) cs).getMinimum());
-          writer.key("max").value(((DateColumnStatistics) cs).getMaximum());
-        }
-        writer.key("type").value(OrcProto.Type.Kind.DATE);
-      } else if (cs instanceof TimestampColumnStatistics) {
-        if (((TimestampColumnStatistics) cs).getMaximum() != null) {
-          writer.key("min").value(((TimestampColumnStatistics) cs).getMinimum());
-          writer.key("max").value(((TimestampColumnStatistics) cs).getMaximum());
-        }
-        writer.key("type").value(OrcProto.Type.Kind.TIMESTAMP);
-      } else if (cs instanceof DecimalColumnStatistics) {
-        if (((DecimalColumnStatistics) cs).getMaximum() != null) {
-          writer.key("min").value(((DecimalColumnStatistics) cs).getMinimum());
-          writer.key("max").value(((DecimalColumnStatistics) cs).getMaximum());
-          writer.key("sum").value(((DecimalColumnStatistics) cs).getSum());
-        }
-        writer.key("type").value(OrcProto.Type.Kind.DECIMAL);
-      }
-    }
-  }
-
-  private static void writeBloomFilterIndexes(JSONWriter writer, int col,
-      OrcProto.BloomFilterIndex[] bloomFilterIndex) throws JSONException {
-
-    BloomFilterIO stripeLevelBF = null;
-    if (bloomFilterIndex != null && bloomFilterIndex[col] != null) {
-      int entryIx = 0;
-      writer.key("bloomFilterIndexes").array();
-      for (OrcProto.BloomFilter bf : bloomFilterIndex[col].getBloomFilterList()) {
-        writer.object();
-        writer.key("entryId").value(entryIx++);
-        BloomFilterIO toMerge = new BloomFilterIO(bf);
-        writeBloomFilterStats(writer, toMerge);
-        if (stripeLevelBF == null) {
-          stripeLevelBF = toMerge;
-        } else {
-          stripeLevelBF.merge(toMerge);
-        }
-        writer.endObject();
-      }
-      writer.endArray();
-    }
-    if (stripeLevelBF != null) {
-      writer.key("stripeLevelBloomFilter");
-      writer.object();
-      writeBloomFilterStats(writer, stripeLevelBF);
-      writer.endObject();
-    }
-  }
-
-  private static void writeBloomFilterStats(JSONWriter writer, BloomFilterIO bf)
-      throws JSONException {
-    int bitCount = bf.getBitSize();
-    int popCount = 0;
-    for (long l : bf.getBitSet()) {
-      popCount += Long.bitCount(l);
-    }
-    int k = bf.getNumHashFunctions();
-    float loadFactor = (float) popCount / (float) bitCount;
-    float expectedFpp = (float) Math.pow(loadFactor, k);
-    writer.key("numHashFunctions").value(k);
-    writer.key("bitCount").value(bitCount);
-    writer.key("popCount").value(popCount);
-    writer.key("loadFactor").value(loadFactor);
-    writer.key("expectedFpp").value(expectedFpp);
-  }
-
-  private static void writeRowGroupIndexes(JSONWriter writer, int col,
-      OrcProto.RowIndex[] rowGroupIndex)
-      throws JSONException {
-
-    OrcProto.RowIndex index;
-    if (rowGroupIndex == null || (col >= rowGroupIndex.length) ||
-        ((index = rowGroupIndex[col]) == null)) {
-      return;
-    }
-
-    writer.key("rowGroupIndexes").array();
-    for (int entryIx = 0; entryIx < index.getEntryCount(); ++entryIx) {
-      writer.object();
-      writer.key("entryId").value(entryIx);
-      OrcProto.RowIndexEntry entry = index.getEntry(entryIx);
-      if (entry == null) {
-        continue;
-      }
-      OrcProto.ColumnStatistics colStats = entry.getStatistics();
-      writeColumnStatistics(writer, ColumnStatisticsImpl.deserialize(colStats));
-      writer.key("positions").array();
-      for (int posIx = 0; posIx < entry.getPositionsCount(); ++posIx) {
-        writer.value(entry.getPositions(posIx));
-      }
-      writer.endArray();
-      writer.endObject();
-    }
-    writer.endArray();
-  }
-
-}
diff --git a/orc/src/protobuf/orc_proto.proto b/orc/src/protobuf/orc_proto.proto
deleted file mode 100644
index ae5945ba4a..0000000000
--- a/orc/src/protobuf/orc_proto.proto
+++ /dev/null
@@ -1,230 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package orc.proto;
-
-option java_package = "org.apache.orc";
-
-message IntegerStatistics  {
-  optional sint64 minimum = 1;
-  optional sint64 maximum = 2;
-  optional sint64 sum = 3;
-}
-
-message DoubleStatistics {
-  optional double minimum = 1;
-  optional double maximum = 2;
-  optional double sum = 3;
-}
-
-message StringStatistics {
-  optional string minimum = 1;
-  optional string maximum = 2;
-  // sum will store the total length of all strings in a stripe
-  optional sint64 sum = 3;
-}
-
-message BucketStatistics {
-  repeated uint64 count = 1 [packed=true];
-}
-
-message DecimalStatistics {
-  optional string minimum = 1;
-  optional string maximum = 2;
-  optional string sum = 3;
-}
-
-message DateStatistics {
-  // min,max values saved as days since epoch
-  optional sint32 minimum = 1;
-  optional sint32 maximum = 2;
-}
-
-message TimestampStatistics {
-  // min,max values saved as milliseconds since epoch
-  optional sint64 minimum = 1;
-  optional sint64 maximum = 2;
-}
-
-message BinaryStatistics {
-  // sum will store the total binary blob length in a stripe
-  optional sint64 sum = 1;
-}
-
-message ColumnStatistics {
-  optional uint64 numberOfValues = 1;
-  optional IntegerStatistics intStatistics = 2;
-  optional DoubleStatistics doubleStatistics = 3;
-  optional StringStatistics stringStatistics = 4;
-  optional BucketStatistics bucketStatistics = 5;
-  optional DecimalStatistics decimalStatistics = 6;
-  optional DateStatistics dateStatistics = 7;
-  optional BinaryStatistics binaryStatistics = 8;
-  optional TimestampStatistics timestampStatistics = 9;
-  optional bool hasNull = 10;
-}
-
-message RowIndexEntry {
-  repeated uint64 positions = 1 [packed=true];
-  optional ColumnStatistics statistics = 2;
-}
-
-message RowIndex {
-  repeated RowIndexEntry entry = 1;
-}
-
-message BloomFilter {
-  optional uint32 numHashFunctions = 1;
-  repeated fixed64 bitset = 2;
-}
-
-message BloomFilterIndex {
-  repeated BloomFilter bloomFilter = 1;
-}
-
-message Stream {
-  // if you add new index stream kinds, you need to make sure to update
-  // StreamName to ensure it is added to the stripe in the right area
-  enum Kind {
-    PRESENT = 0;
-    DATA = 1;
-    LENGTH = 2;
-    DICTIONARY_DATA = 3;
-    DICTIONARY_COUNT = 4;
-    SECONDARY = 5;
-    ROW_INDEX = 6;
-    BLOOM_FILTER = 7;
-  }
-  optional Kind kind = 1;
-  optional uint32 column = 2;
-  optional uint64 length = 3;
-}
-
-message ColumnEncoding {
-  enum Kind {
-    DIRECT = 0;
-    DICTIONARY = 1;
-    DIRECT_V2 = 2;
-    DICTIONARY_V2 = 3;
-  }
-  optional Kind kind = 1;
-  optional uint32 dictionarySize = 2;
-}
-
-message StripeFooter {
-  repeated Stream streams = 1;
-  repeated ColumnEncoding columns = 2;
-  optional string writerTimezone = 3;
-}
-
-message Type {
-  enum Kind {
-    BOOLEAN = 0;
-    BYTE = 1;
-    SHORT = 2;
-    INT = 3;
-    LONG = 4;
-    FLOAT = 5;
-    DOUBLE = 6;
-    STRING = 7;
-    BINARY = 8;
-    TIMESTAMP = 9;
-    LIST = 10;
-    MAP = 11;
-    STRUCT = 12;
-    UNION = 13;
-    DECIMAL = 14;
-    DATE = 15;
-    VARCHAR = 16;
-    CHAR = 17;
-  }
-  optional Kind kind = 1;
-  repeated uint32 subtypes = 2 [packed=true];
-  repeated string fieldNames = 3;
-  optional uint32 maximumLength = 4;
-  optional uint32 precision = 5;
-  optional uint32 scale = 6;
-}
-
-message StripeInformation {
-  optional uint64 offset = 1;
-  optional uint64 indexLength = 2;
-  optional uint64 dataLength = 3;
-  optional uint64 footerLength = 4;
-  optional uint64 numberOfRows = 5;
-}
-
-message UserMetadataItem {
-  optional string name = 1;
-  optional bytes value = 2;
-}
-
-message StripeStatistics {
-  repeated ColumnStatistics colStats = 1;
-}
-
-message Metadata {
-  repeated StripeStatistics stripeStats = 1;
-}
-
-message Footer {
-  optional uint64 headerLength = 1;
-  optional uint64 contentLength = 2;
-  repeated StripeInformation stripes = 3;
-  repeated Type types = 4;
-  repeated UserMetadataItem metadata = 5;
-  optional uint64 numberOfRows = 6;
-  repeated ColumnStatistics statistics = 7;
-  optional uint32 rowIndexStride = 8;
-}
-
-enum CompressionKind {
-  NONE = 0;
-  ZLIB = 1;
-  SNAPPY = 2;
-  LZO = 3;
-}
-
-// Serialized length must be less that 255 bytes
-message PostScript {
-  optional uint64 footerLength = 1;
-  optional CompressionKind compression = 2;
-  optional uint64 compressionBlockSize = 3;
-  // the version of the file format
-  //   [0, 11] = Hive 0.11
-  //   [0, 12] = Hive 0.12
-  repeated uint32 version = 4 [packed = true];
-  optional uint64 metadataLength = 5;
-  // Version of the writer:
-  //   0 (or missing) = original
-  //   1 = HIVE-8732 fixed
-  //   2 = HIVE-4243 fixed
-  //   3 = HIVE-12055 fixed
-  //   4 = HIVE-13083 fixed
-  optional uint32 writerVersion = 6;
-  // Leave this last in the record
-  optional string magic = 8000;
-}
-
-// This gets serialized as part of OrcSplit, also used by footer cache.
-message FileTail {
-  optional PostScript postscript = 1;
-  optional Footer footer = 2;
-  optional uint64 fileLength = 3;
-  optional uint64 postscriptLength = 4;
-}
diff --git a/orc/src/test/org/apache/orc/TestColumnStatistics.java b/orc/src/test/org/apache/orc/TestColumnStatistics.java
deleted file mode 100644
index 93d4bdb94a..0000000000
--- a/orc/src/test/org/apache/orc/TestColumnStatistics.java
+++ /dev/null
@@ -1,365 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import static junit.framework.Assert.assertEquals;
-import static org.junit.Assume.assumeTrue;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.PrintStream;
-import java.sql.Timestamp;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.orc.impl.ColumnStatisticsImpl;
-import org.apache.orc.tools.FileDump;
-import org.apache.orc.tools.TestFileDump;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-/**
- * Test ColumnStatisticsImpl for ORC.
- */
-public class TestColumnStatistics {
-
-  @Test
-  public void testLongMerge() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-
-    ColumnStatisticsImpl stats1 = ColumnStatisticsImpl.create(schema);
-    ColumnStatisticsImpl stats2 = ColumnStatisticsImpl.create(schema);
-    stats1.updateInteger(10, 2);
-    stats2.updateInteger(1, 1);
-    stats2.updateInteger(1000, 1);
-    stats1.merge(stats2);
-    IntegerColumnStatistics typed = (IntegerColumnStatistics) stats1;
-    assertEquals(1, typed.getMinimum());
-    assertEquals(1000, typed.getMaximum());
-    stats1.reset();
-    stats1.updateInteger(-10, 1);
-    stats1.updateInteger(10000, 1);
-    stats1.merge(stats2);
-    assertEquals(-10, typed.getMinimum());
-    assertEquals(10000, typed.getMaximum());
-  }
-
-  @Test
-  public void testDoubleMerge() throws Exception {
-    TypeDescription schema = TypeDescription.createDouble();
-
-    ColumnStatisticsImpl stats1 = ColumnStatisticsImpl.create(schema);
-    ColumnStatisticsImpl stats2 = ColumnStatisticsImpl.create(schema);
-    stats1.updateDouble(10.0);
-    stats1.updateDouble(100.0);
-    stats2.updateDouble(1.0);
-    stats2.updateDouble(1000.0);
-    stats1.merge(stats2);
-    DoubleColumnStatistics typed = (DoubleColumnStatistics) stats1;
-    assertEquals(1.0, typed.getMinimum(), 0.001);
-    assertEquals(1000.0, typed.getMaximum(), 0.001);
-    stats1.reset();
-    stats1.updateDouble(-10);
-    stats1.updateDouble(10000);
-    stats1.merge(stats2);
-    assertEquals(-10, typed.getMinimum(), 0.001);
-    assertEquals(10000, typed.getMaximum(), 0.001);
-  }
-
-
-  @Test
-  public void testStringMerge() throws Exception {
-    TypeDescription schema = TypeDescription.createString();
-
-    ColumnStatisticsImpl stats1 = ColumnStatisticsImpl.create(schema);
-    ColumnStatisticsImpl stats2 = ColumnStatisticsImpl.create(schema);
-    stats1.updateString(new Text("bob"));
-    stats1.updateString(new Text("david"));
-    stats1.updateString(new Text("charles"));
-    stats2.updateString(new Text("anne"));
-    byte[] erin = new byte[]{0, 1, 2, 3, 4, 5, 101, 114, 105, 110};
-    stats2.updateString(erin, 6, 4, 5);
-    assertEquals(24, ((StringColumnStatistics)stats2).getSum());
-    stats1.merge(stats2);
-    StringColumnStatistics typed = (StringColumnStatistics) stats1;
-    assertEquals("anne", typed.getMinimum());
-    assertEquals("erin", typed.getMaximum());
-    assertEquals(39, typed.getSum());
-    stats1.reset();
-    stats1.updateString(new Text("aaa"));
-    stats1.updateString(new Text("zzz"));
-    stats1.merge(stats2);
-    assertEquals("aaa", typed.getMinimum());
-    assertEquals("zzz", typed.getMaximum());
-  }
-
-  @Test
-  public void testDateMerge() throws Exception {
-    TypeDescription schema = TypeDescription.createDate();
-
-    ColumnStatisticsImpl stats1 = ColumnStatisticsImpl.create(schema);
-    ColumnStatisticsImpl stats2 = ColumnStatisticsImpl.create(schema);
-    stats1.updateDate(new DateWritable(1000));
-    stats1.updateDate(new DateWritable(100));
-    stats2.updateDate(new DateWritable(10));
-    stats2.updateDate(new DateWritable(2000));
-    stats1.merge(stats2);
-    DateColumnStatistics typed = (DateColumnStatistics) stats1;
-    assertEquals(new DateWritable(10).get(), typed.getMinimum());
-    assertEquals(new DateWritable(2000).get(), typed.getMaximum());
-    stats1.reset();
-    stats1.updateDate(new DateWritable(-10));
-    stats1.updateDate(new DateWritable(10000));
-    stats1.merge(stats2);
-    assertEquals(new DateWritable(-10).get(), typed.getMinimum());
-    assertEquals(new DateWritable(10000).get(), typed.getMaximum());
-  }
-
-  @Test
-  public void testTimestampMerge() throws Exception {
-    TypeDescription schema = TypeDescription.createTimestamp();
-
-    ColumnStatisticsImpl stats1 = ColumnStatisticsImpl.create(schema);
-    ColumnStatisticsImpl stats2 = ColumnStatisticsImpl.create(schema);
-    stats1.updateTimestamp(new Timestamp(10));
-    stats1.updateTimestamp(new Timestamp(100));
-    stats2.updateTimestamp(new Timestamp(1));
-    stats2.updateTimestamp(new Timestamp(1000));
-    stats1.merge(stats2);
-    TimestampColumnStatistics typed = (TimestampColumnStatistics) stats1;
-    assertEquals(1, typed.getMinimum().getTime());
-    assertEquals(1000, typed.getMaximum().getTime());
-    stats1.reset();
-    stats1.updateTimestamp(new Timestamp(-10));
-    stats1.updateTimestamp(new Timestamp(10000));
-    stats1.merge(stats2);
-    assertEquals(-10, typed.getMinimum().getTime());
-    assertEquals(10000, typed.getMaximum().getTime());
-  }
-
-  @Test
-  public void testDecimalMerge() throws Exception {
-    TypeDescription schema = TypeDescription.createDecimal()
-        .withPrecision(38).withScale(16);
-
-    ColumnStatisticsImpl stats1 = ColumnStatisticsImpl.create(schema);
-    ColumnStatisticsImpl stats2 = ColumnStatisticsImpl.create(schema);
-    stats1.updateDecimal(new HiveDecimalWritable(10));
-    stats1.updateDecimal(new HiveDecimalWritable(100));
-    stats2.updateDecimal(new HiveDecimalWritable(1));
-    stats2.updateDecimal(new HiveDecimalWritable(1000));
-    stats1.merge(stats2);
-    DecimalColumnStatistics typed = (DecimalColumnStatistics) stats1;
-    assertEquals(1, typed.getMinimum().longValue());
-    assertEquals(1000, typed.getMaximum().longValue());
-    stats1.reset();
-    stats1.updateDecimal(new HiveDecimalWritable(-10));
-    stats1.updateDecimal(new HiveDecimalWritable(10000));
-    stats1.merge(stats2);
-    assertEquals(-10, typed.getMinimum().longValue());
-    assertEquals(10000, typed.getMaximum().longValue());
-  }
-
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    fs.setWorkingDirectory(workDir);
-    testFilePath = new Path("TestOrcFile." + testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  private static BytesWritable bytes(int... items) {
-    BytesWritable result = new BytesWritable();
-    result.setSize(items.length);
-    for (int i = 0; i < items.length; ++i) {
-      result.getBytes()[i] = (byte) items[i];
-    }
-    return result;
-  }
-
-  void appendRow(VectorizedRowBatch batch, BytesWritable bytes,
-                 String str) {
-    int row = batch.size++;
-    if (bytes == null) {
-      batch.cols[0].noNulls = false;
-      batch.cols[0].isNull[row] = true;
-    } else {
-      ((BytesColumnVector) batch.cols[0]).setVal(row, bytes.getBytes(),
-          0, bytes.getLength());
-    }
-    if (str == null) {
-      batch.cols[1].noNulls = false;
-      batch.cols[1].isNull[row] = true;
-    } else {
-      ((BytesColumnVector) batch.cols[1]).setVal(row, str.getBytes());
-    }
-  }
-
-  @Test
-  public void testHasNull() throws Exception {
-    TypeDescription schema =
-        TypeDescription.createStruct()
-        .addField("bytes1", TypeDescription.createBinary())
-        .addField("string1", TypeDescription.createString());
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .rowIndexStride(1000)
-            .stripeSize(10000)
-            .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch(5000);
-    // STRIPE 1
-    // RG1
-    for(int i=0; i<1000; i++) {
-      appendRow(batch, bytes(1, 2, 3), "RG1");
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // RG2
-    for(int i=0; i<1000; i++) {
-      appendRow(batch, bytes(1, 2, 3), null);
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // RG3
-    for(int i=0; i<1000; i++) {
-      appendRow(batch, bytes(1, 2, 3), "RG3");
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // RG4
-    for (int i = 0; i < 1000; i++) {
-      appendRow(batch, bytes(1,2,3), null);
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // RG5
-    for(int i=0; i<1000; i++) {
-      appendRow(batch, bytes(1, 2, 3), null);
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // STRIPE 2
-    for (int i = 0; i < 5000; i++) {
-      appendRow(batch, bytes(1,2,3), null);
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // STRIPE 3
-    for (int i = 0; i < 5000; i++) {
-      appendRow(batch, bytes(1,2,3), "STRIPE-3");
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    // STRIPE 4
-    for (int i = 0; i < 5000; i++) {
-      appendRow(batch, bytes(1,2,3), null);
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    // check the file level stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(20000, stats[0].getNumberOfValues());
-    assertEquals(20000, stats[1].getNumberOfValues());
-    assertEquals(7000, stats[2].getNumberOfValues());
-    assertEquals(false, stats[0].hasNull());
-    assertEquals(false, stats[1].hasNull());
-    assertEquals(true, stats[2].hasNull());
-
-    // check the stripe level stats
-    List<StripeStatistics> stripeStats = reader.getStripeStatistics();
-    // stripe 1 stats
-    StripeStatistics ss1 = stripeStats.get(0);
-    ColumnStatistics ss1_cs1 = ss1.getColumnStatistics()[0];
-    ColumnStatistics ss1_cs2 = ss1.getColumnStatistics()[1];
-    ColumnStatistics ss1_cs3 = ss1.getColumnStatistics()[2];
-    assertEquals(false, ss1_cs1.hasNull());
-    assertEquals(false, ss1_cs2.hasNull());
-    assertEquals(true, ss1_cs3.hasNull());
-
-    // stripe 2 stats
-    StripeStatistics ss2 = stripeStats.get(1);
-    ColumnStatistics ss2_cs1 = ss2.getColumnStatistics()[0];
-    ColumnStatistics ss2_cs2 = ss2.getColumnStatistics()[1];
-    ColumnStatistics ss2_cs3 = ss2.getColumnStatistics()[2];
-    assertEquals(false, ss2_cs1.hasNull());
-    assertEquals(false, ss2_cs2.hasNull());
-    assertEquals(true, ss2_cs3.hasNull());
-
-    // stripe 3 stats
-    StripeStatistics ss3 = stripeStats.get(2);
-    ColumnStatistics ss3_cs1 = ss3.getColumnStatistics()[0];
-    ColumnStatistics ss3_cs2 = ss3.getColumnStatistics()[1];
-    ColumnStatistics ss3_cs3 = ss3.getColumnStatistics()[2];
-    assertEquals(false, ss3_cs1.hasNull());
-    assertEquals(false, ss3_cs2.hasNull());
-    assertEquals(false, ss3_cs3.hasNull());
-
-    // stripe 4 stats
-    StripeStatistics ss4 = stripeStats.get(3);
-    ColumnStatistics ss4_cs1 = ss4.getColumnStatistics()[0];
-    ColumnStatistics ss4_cs2 = ss4.getColumnStatistics()[1];
-    ColumnStatistics ss4_cs3 = ss4.getColumnStatistics()[2];
-    assertEquals(false, ss4_cs1.hasNull());
-    assertEquals(false, ss4_cs2.hasNull());
-    assertEquals(true, ss4_cs3.hasNull());
-
-    // Test file dump
-    PrintStream origOut = System.out;
-    String outputFilename = "orc-file-has-null.out";
-    FileOutputStream myOut = new FileOutputStream(workDir + File.separator + outputFilename);
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "--rowindex=2"});
-    System.out.flush();
-    System.setOut(origOut);
-    // If called with an expression evaluating to false, the test will halt
-    // and be ignored.
-    assumeTrue(!System.getProperty("os.name").startsWith("Windows"));
-    TestFileDump.checkOutput(outputFilename, workDir + File.separator + outputFilename);
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestNewIntegerEncoding.java b/orc/src/test/org/apache/orc/TestNewIntegerEncoding.java
deleted file mode 100644
index 526dd81dfc..0000000000
--- a/orc/src/test/org/apache/orc/TestNewIntegerEncoding.java
+++ /dev/null
@@ -1,1373 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.io.File;
-import java.sql.Timestamp;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-import org.junit.runners.Parameterized.Parameters;
-
-import com.google.common.collect.Lists;
-import com.google.common.primitives.Longs;
-
-@RunWith(value = Parameterized.class)
-public class TestNewIntegerEncoding {
-
-  private OrcFile.EncodingStrategy encodingStrategy;
-
-  public TestNewIntegerEncoding( OrcFile.EncodingStrategy es) {
-    this.encodingStrategy = es;
-  }
-
-  @Parameters
-  public static Collection<Object[]> data() {
-    Object[][] data = new Object[][] { {  OrcFile.EncodingStrategy.COMPRESSION },
-        {  OrcFile.EncodingStrategy.SPEED } };
-    return Arrays.asList(data);
-  }
-
-  public static class TSRow {
-    Timestamp ts;
-
-    public TSRow(Timestamp ts) {
-      this.ts = ts;
-    }
-  }
-
-  public static TypeDescription getRowSchema() {
-    return TypeDescription.createStruct()
-        .addField("int1", TypeDescription.createInt())
-        .addField("long1", TypeDescription.createLong());
-  }
-
-  public static void appendRow(VectorizedRowBatch batch,
-                               int int1, long long1) {
-    int row = batch.size++;
-    ((LongColumnVector) batch.cols[0]).vector[row] = int1;
-    ((LongColumnVector) batch.cols[1]).vector[row] = long1;
-  }
-
-  public static void appendLong(VectorizedRowBatch batch,
-                                long long1) {
-    int row = batch.size++;
-    ((LongColumnVector) batch.cols[0]).vector[row] = long1;
-  }
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir", "target"
-      + File.separator + "test" + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile."
-        + testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @Test
-  public void testBasicRow() throws Exception {
-    TypeDescription schema= getRowSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(100000)
-                                         .compress(CompressionKind.NONE)
-                                         .bufferSize(10000)
-                                         .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    appendRow(batch, 111, 1111L);
-    appendRow(batch, 111, 1111L);
-    appendRow(batch, 111, 1111L);
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(111, ((LongColumnVector) batch.cols[0]).vector[r]);
-        assertEquals(1111, ((LongColumnVector) batch.cols[1]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testBasicOld() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-    long[] inp = new long[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6,
-        7, 8, 9, 10, 1, 1, 1, 1, 1, 1, 10, 9, 7, 6, 5, 4, 3, 2, 1, 1, 1, 1, 1,
-        2, 5, 1, 3, 7, 1, 9, 2, 6, 3, 7, 1, 9, 2, 6, 3, 7, 1, 9, 2, 6, 3, 7, 1,
-        9, 2, 6, 3, 7, 1, 9, 2, 6, 2000, 2, 1, 1, 1, 1, 1, 3, 7, 1, 9, 2, 6, 1,
-        1, 1, 1, 1 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .compress(CompressionKind.NONE)
-                                         .version(OrcFile.Version.V_0_11)
-                                         .bufferSize(10000)
-                                         .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    int idx = 0;
-    batch = reader.getSchema().createRowBatch();
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testBasicNew() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 5, 6,
-        7, 8, 9, 10, 1, 1, 1, 1, 1, 1, 10, 9, 7, 6, 5, 4, 3, 2, 1, 1, 1, 1, 1,
-        2, 5, 1, 3, 7, 1, 9, 2, 6, 3, 7, 1, 9, 2, 6, 3, 7, 1, 9, 2, 6, 3, 7, 1,
-        9, 2, 6, 3, 7, 1, 9, 2, 6, 2000, 2, 1, 1, 1, 1, 1, 3, 7, 1, 9, 2, 6, 1,
-        1, 1, 1, 1 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    int idx = 0;
-    batch = reader.getSchema().createRowBatch();
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-  
-  @Test
-  public void testBasicDelta1() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { -500, -400, -350, -325, -310 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testBasicDelta2() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { -500, -600, -650, -675, -710 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testBasicDelta3() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 500, 400, 350, 325, 310 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testBasicDelta4() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 500, 600, 650, 675, 710 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testDeltaOverflow() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[]{4513343538618202719l, 4513343538618202711l,
-        2911390882471569739l,
-        -9181829309989854913l};
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .compress(CompressionKind.NONE).bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile
-        .createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testDeltaOverflow2() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[]{Long.MAX_VALUE, 4513343538618202711l,
-        2911390882471569739l,
-        Long.MIN_VALUE};
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .compress(CompressionKind.NONE).bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile
-        .createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testDeltaOverflow3() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[]{-4513343538618202711l, -2911390882471569739l, -2,
-        Long.MAX_VALUE};
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .compress(CompressionKind.NONE).bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile
-        .createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testIntegerMin() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    input.add((long) Integer.MIN_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testIntegerMax() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    input.add((long) Integer.MAX_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testLongMin() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    input.add(Long.MIN_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testLongMax() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    input.add(Long.MAX_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testRandomInt() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 100000; i++) {
-      input.add((long) rand.nextInt());
-    }
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(100000);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testRandomLong() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 100000; i++) {
-      input.add(rand.nextLong());
-    }
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(100000);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseNegativeMin() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 20, 2, 3, 2, 1, 3, 17, 71, 35, 2, 1, 139, 2, 2,
-        3, 1783, 475, 2, 1, 1, 3, 1, 3, 2, 32, 1, 2, 3, 1, 8, 30, 1, 3, 414, 1,
-        1, 135, 3, 3, 1, 414, 2, 1, 2, 2, 594, 2, 5, 6, 4, 11, 1, 2, 2, 1, 1,
-        52, 4, 1, 2, 7, 1, 17, 334, 1, 2, 1, 2, 2, 6, 1, 266, 1, 2, 217, 2, 6,
-        2, 13, 2, 2, 1, 2, 3, 5, 1, 2, 1, 7244, 11813, 1, 33, 2, -13, 1, 2, 3,
-        13, 1, 92, 3, 13, 5, 14, 9, 141, 12, 6, 15, 25, 1, 1, 1, 46, 2, 1, 1,
-        141, 3, 1, 1, 1, 1, 2, 1, 4, 34, 5, 78, 8, 1, 2, 2, 1, 9, 10, 2, 1, 4,
-        13, 1, 5, 4, 4, 19, 5, 1, 1, 1, 68, 33, 399, 1, 1885, 25, 5, 2, 4, 1,
-        1, 2, 16, 1, 2966, 3, 1, 1, 25501, 1, 1, 1, 66, 1, 3, 8, 131, 14, 5, 1,
-        2, 2, 1, 1, 8, 1, 1, 2, 1, 5, 9, 2, 3, 112, 13, 2, 2, 1, 5, 10, 3, 1,
-        1, 13, 2, 3, 4, 1, 3, 1, 1, 2, 1, 1, 2, 4, 2, 207, 1, 1, 2, 4, 3, 3, 2,
-        2, 16 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseNegativeMin2() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 20, 2, 3, 2, 1, 3, 17, 71, 35, 2, 1, 139, 2, 2,
-        3, 1783, 475, 2, 1, 1, 3, 1, 3, 2, 32, 1, 2, 3, 1, 8, 30, 1, 3, 414, 1,
-        1, 135, 3, 3, 1, 414, 2, 1, 2, 2, 594, 2, 5, 6, 4, 11, 1, 2, 2, 1, 1,
-        52, 4, 1, 2, 7, 1, 17, 334, 1, 2, 1, 2, 2, 6, 1, 266, 1, 2, 217, 2, 6,
-        2, 13, 2, 2, 1, 2, 3, 5, 1, 2, 1, 7244, 11813, 1, 33, 2, -1, 1, 2, 3,
-        13, 1, 92, 3, 13, 5, 14, 9, 141, 12, 6, 15, 25, 1, 1, 1, 46, 2, 1, 1,
-        141, 3, 1, 1, 1, 1, 2, 1, 4, 34, 5, 78, 8, 1, 2, 2, 1, 9, 10, 2, 1, 4,
-        13, 1, 5, 4, 4, 19, 5, 1, 1, 1, 68, 33, 399, 1, 1885, 25, 5, 2, 4, 1,
-        1, 2, 16, 1, 2966, 3, 1, 1, 25501, 1, 1, 1, 66, 1, 3, 8, 131, 14, 5, 1,
-        2, 2, 1, 1, 8, 1, 1, 2, 1, 5, 9, 2, 3, 112, 13, 2, 2, 1, 5, 10, 3, 1,
-        1, 13, 2, 3, 4, 1, 3, 1, 1, 2, 1, 1, 2, 4, 2, 207, 1, 1, 2, 4, 3, 3, 2,
-        2, 16 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseNegativeMin3() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 20, 2, 3, 2, 1, 3, 17, 71, 35, 2, 1, 139, 2, 2,
-        3, 1783, 475, 2, 1, 1, 3, 1, 3, 2, 32, 1, 2, 3, 1, 8, 30, 1, 3, 414, 1,
-        1, 135, 3, 3, 1, 414, 2, 1, 2, 2, 594, 2, 5, 6, 4, 11, 1, 2, 2, 1, 1,
-        52, 4, 1, 2, 7, 1, 17, 334, 1, 2, 1, 2, 2, 6, 1, 266, 1, 2, 217, 2, 6,
-        2, 13, 2, 2, 1, 2, 3, 5, 1, 2, 1, 7244, 11813, 1, 33, 2, 0, 1, 2, 3,
-        13, 1, 92, 3, 13, 5, 14, 9, 141, 12, 6, 15, 25, 1, 1, 1, 46, 2, 1, 1,
-        141, 3, 1, 1, 1, 1, 2, 1, 4, 34, 5, 78, 8, 1, 2, 2, 1, 9, 10, 2, 1, 4,
-        13, 1, 5, 4, 4, 19, 5, 1, 1, 1, 68, 33, 399, 1, 1885, 25, 5, 2, 4, 1,
-        1, 2, 16, 1, 2966, 3, 1, 1, 25501, 1, 1, 1, 66, 1, 3, 8, 131, 14, 5, 1,
-        2, 2, 1, 1, 8, 1, 1, 2, 1, 5, 9, 2, 3, 112, 13, 2, 2, 1, 5, 10, 3, 1,
-        1, 13, 2, 3, 4, 1, 3, 1, 1, 2, 1, 1, 2, 4, 2, 207, 1, 1, 2, 4, 3, 3, 2,
-        2, 16 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseNegativeMin4() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { 13, 13, 11, 8, 13, 10, 10, 11, 11, 14, 11, 7, 13,
-        12, 12, 11, 15, 12, 12, 9, 8, 10, 13, 11, 8, 6, 5, 6, 11, 7, 15, 10, 7,
-        6, 8, 7, 9, 9, 11, 33, 11, 3, 7, 4, 6, 10, 14, 12, 5, 14, 7, 6 };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseAt0() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(100));
-    }
-    input.set(0, 20000L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseAt1() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(100));
-    }
-    input.set(1, 20000L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseAt255() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(100));
-    }
-    input.set(255, 20000L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseAt256() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(100));
-    }
-    input.set(256, 20000L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBase510() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(100));
-    }
-    input.set(510, 20000L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBase511() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(100));
-    }
-    input.set(511, 20000L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseMax1() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for (int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(60));
-    }
-    input.set(511, Long.MAX_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseMax2() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for (int i = 0; i < 5120; i++) {
-      input.add((long) rand.nextInt(60));
-    }
-    input.set(128, Long.MAX_VALUE);
-    input.set(256, Long.MAX_VALUE);
-    input.set(511, Long.MAX_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseMax3() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    input.add(371946367L);
-    input.add(11963367L);
-    input.add(68639400007L);
-    input.add(100233367L);
-    input.add(6367L);
-    input.add(10026367L);
-    input.add(3670000L);
-    input.add(3602367L);
-    input.add(4719226367L);
-    input.add(7196367L);
-    input.add(444442L);
-    input.add(210267L);
-    input.add(21033L);
-    input.add(160267L);
-    input.add(400267L);
-    input.add(23634347L);
-    input.add(16027L);
-    input.add(46026367L);
-    input.add(Long.MAX_VALUE);
-    input.add(33333L);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseMax4() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    for (int i = 0; i < 25; i++) {
-      input.add(371292224226367L);
-      input.add(119622332222267L);
-      input.add(686329400222007L);
-      input.add(100233333222367L);
-      input.add(636272333322222L);
-      input.add(10202633223267L);
-      input.add(36700222022230L);
-      input.add(36023226224227L);
-      input.add(47192226364427L);
-      input.add(71963622222447L);
-      input.add(22244444222222L);
-      input.add(21220263327442L);
-      input.add(21032233332232L);
-      input.add(16026322232227L);
-      input.add(40022262272212L);
-      input.add(23634342227222L);
-      input.add(16022222222227L);
-      input.add(46026362222227L);
-      input.add(46026362222227L);
-      input.add(33322222222323L);
-    }
-    input.add(Long.MAX_VALUE);
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-  @Test
-  public void testPatchedBaseTimestamp() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("ts", TypeDescription.createTimestamp());
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-
-    List<Timestamp> tslist = Lists.newArrayList();
-    tslist.add(Timestamp.valueOf("2099-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2003-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("1999-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("1995-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2002-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2010-03-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("2005-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2006-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2003-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("1996-08-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("1998-11-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("2008-10-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("1993-08-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("2008-01-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("2007-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2004-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2008-10-02 00:00:00"));
-    tslist.add(Timestamp.valueOf("2003-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2004-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2008-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2005-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("1994-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2006-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2004-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2001-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2000-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2000-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2002-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2006-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2011-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2002-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("2005-01-01 00:00:00"));
-    tslist.add(Timestamp.valueOf("1974-01-01 00:00:00"));
-    int idx = 0;
-    for (Timestamp ts : tslist) {
-      ((TimestampColumnVector) batch.cols[0]).set(idx, ts);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(tslist.get(idx++),
-            ((TimestampColumnVector) batch.cols[0]).asScratchTimestamp(r));
-      }
-    }
-  }
-
-  @Test
-  public void testDirectLargeNegatives() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch();
-
-    appendLong(batch, -7486502418706614742L);
-    appendLong(batch, 0L);
-    appendLong(batch, 1L);
-    appendLong(batch, 1L);
-    appendLong(batch, -5535739865598783616L);
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    assertEquals(true, rows.nextBatch(batch));
-    assertEquals(5, batch.size);
-    assertEquals(-7486502418706614742L,
-        ((LongColumnVector) batch.cols[0]).vector[0]);
-    assertEquals(0L,
-        ((LongColumnVector) batch.cols[0]).vector[1]);
-    assertEquals(1L,
-        ((LongColumnVector) batch.cols[0]).vector[2]);
-    assertEquals(1L,
-        ((LongColumnVector) batch.cols[0]).vector[3]);
-    assertEquals(-5535739865598783616L,
-        ((LongColumnVector) batch.cols[0]).vector[4]);
-    assertEquals(false, rows.nextBatch(batch));
-  }
-
-  @Test
-  public void testSeek() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    List<Long> input = Lists.newArrayList();
-    Random rand = new Random();
-    for(int i = 0; i < 100000; i++) {
-      input.add((long) rand.nextInt());
-    }
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .compress(CompressionKind.NONE)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .version(OrcFile.Version.V_0_11)
-            .encodingStrategy(encodingStrategy));
-    VectorizedRowBatch batch = schema.createRowBatch(100000);
-    for(Long l : input) {
-      appendLong(batch, l);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 55555;
-    rows.seekToRow(idx);
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestOrcNullOptimization.java b/orc/src/test/org/apache/orc/TestOrcNullOptimization.java
deleted file mode 100644
index 0b605c9fdc..0000000000
--- a/orc/src/test/org/apache/orc/TestOrcNullOptimization.java
+++ /dev/null
@@ -1,415 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.List;
-import java.util.Random;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-
-import org.apache.orc.impl.RecordReaderImpl;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-import com.google.common.collect.Lists;
-
-public class TestOrcNullOptimization {
-
-  TypeDescription createMyStruct() {
-    return TypeDescription.createStruct()
-        .addField("a", TypeDescription.createInt())
-        .addField("b", TypeDescription.createString())
-        .addField("c", TypeDescription.createBoolean())
-        .addField("d", TypeDescription.createList(
-            TypeDescription.createStruct()
-                .addField("z", TypeDescription.createInt())));
-  }
-
-  void addRow(Writer writer, VectorizedRowBatch batch,
-              Integer a, String b, Boolean c,
-              Integer... d) throws IOException {
-    if (batch.size == batch.getMaxSize()) {
-      writer.addRowBatch(batch);
-      batch.reset();
-    }
-    int row = batch.size++;
-    LongColumnVector aColumn = (LongColumnVector) batch.cols[0];
-    BytesColumnVector bColumn = (BytesColumnVector) batch.cols[1];
-    LongColumnVector cColumn = (LongColumnVector) batch.cols[2];
-    ListColumnVector dColumn = (ListColumnVector) batch.cols[3];
-    StructColumnVector dStruct = (StructColumnVector) dColumn.child;
-    LongColumnVector dInt = (LongColumnVector) dStruct.fields[0];
-    if (a == null) {
-      aColumn.noNulls = false;
-      aColumn.isNull[row] = true;
-    } else {
-      aColumn.vector[row] = a;
-    }
-    if (b == null) {
-      bColumn.noNulls = false;
-      bColumn.isNull[row] = true;
-    } else {
-      bColumn.setVal(row, b.getBytes());
-    }
-    if (c == null) {
-      cColumn.noNulls = false;
-      cColumn.isNull[row] = true;
-    } else {
-      cColumn.vector[row] = c ? 1 : 0;
-    }
-    if (d == null) {
-      dColumn.noNulls = false;
-      dColumn.isNull[row] = true;
-    } else {
-      dColumn.offsets[row] = dColumn.childCount;
-      dColumn.lengths[row] = d.length;
-      dColumn.childCount += d.length;
-      for(int e=0; e < d.length; ++e) {
-        dInt.vector[(int) dColumn.offsets[row] + e] = d[e];
-      }
-    }
-  }
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcNullOptimization." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @Test
-  public void testMultiStripeWithNull() throws Exception {
-    TypeDescription schema = createMyStruct();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(100000)
-                                         .compress(CompressionKind.NONE)
-                                         .bufferSize(10000));
-    Random rand = new Random(100);
-    VectorizedRowBatch batch = schema.createRowBatch();
-    addRow(writer, batch, null, null, true, 100);
-    for (int i = 2; i < 20000; i++) {
-      addRow(writer, batch, rand.nextInt(1), "a", true, 100);
-    }
-    addRow(writer, batch, null, null, true, 100);
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(20000, reader.getNumberOfRows());
-    assertEquals(20000, stats[0].getNumberOfValues());
-
-    assertEquals(0, ((IntegerColumnStatistics) stats[1]).getMaximum());
-    assertEquals(0, ((IntegerColumnStatistics) stats[1]).getMinimum());
-    assertEquals(true, ((IntegerColumnStatistics) stats[1]).isSumDefined());
-    assertEquals(0, ((IntegerColumnStatistics) stats[1]).getSum());
-    assertEquals("count: 19998 hasNull: true min: 0 max: 0 sum: 0",
-        stats[1].toString());
-
-    assertEquals("a", ((StringColumnStatistics) stats[2]).getMaximum());
-    assertEquals("a", ((StringColumnStatistics) stats[2]).getMinimum());
-    assertEquals(19998, stats[2].getNumberOfValues());
-    assertEquals("count: 19998 hasNull: true min: a max: a sum: 19998",
-        stats[2].toString());
-
-    // check the inspectors
-    assertEquals("struct<a:int,b:string,c:boolean,d:array<struct<z:int>>>",
-        reader.getSchema().toString());
-
-    RecordReader rows = reader.rows();
-
-    List<Boolean> expected = Lists.newArrayList();
-    for (StripeInformation sinfo : reader.getStripes()) {
-      expected.add(false);
-    }
-    // only the first and last stripe will have PRESENT stream
-    expected.set(0, true);
-    expected.set(expected.size() - 1, true);
-
-    List<Boolean> got = Lists.newArrayList();
-    // check if the strip footer contains PRESENT stream
-    for (StripeInformation sinfo : reader.getStripes()) {
-      OrcProto.StripeFooter sf =
-        ((RecordReaderImpl) rows).readStripeFooter(sinfo);
-      got.add(sf.toString().indexOf(OrcProto.Stream.Kind.PRESENT.toString())
-              != -1);
-    }
-    assertEquals(expected, got);
-
-    batch = reader.getSchema().createRowBatch();
-    LongColumnVector aColumn = (LongColumnVector) batch.cols[0];
-    BytesColumnVector bColumn = (BytesColumnVector) batch.cols[1];
-    LongColumnVector cColumn = (LongColumnVector) batch.cols[2];
-    ListColumnVector dColumn = (ListColumnVector) batch.cols[3];
-    LongColumnVector dElements =
-        (LongColumnVector)(((StructColumnVector) dColumn.child).fields[0]);
-    assertEquals(true , rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-
-    // row 1
-    assertEquals(true, aColumn.isNull[0]);
-    assertEquals(true, bColumn.isNull[0]);
-    assertEquals(1, cColumn.vector[0]);
-    assertEquals(0, dColumn.offsets[0]);
-    assertEquals(1, dColumn.lengths[1]);
-    assertEquals(100, dElements.vector[0]);
-
-    rows.seekToRow(19998);
-    rows.nextBatch(batch);
-    assertEquals(2, batch.size);
-
-    // last-1 row
-    assertEquals(0, aColumn.vector[0]);
-    assertEquals("a", bColumn.toString(0));
-    assertEquals(1, cColumn.vector[0]);
-    assertEquals(0, dColumn.offsets[0]);
-    assertEquals(1, dColumn.lengths[0]);
-    assertEquals(100, dElements.vector[0]);
-
-    // last row
-    assertEquals(true, aColumn.isNull[1]);
-    assertEquals(true, bColumn.isNull[1]);
-    assertEquals(1, cColumn.vector[1]);
-    assertEquals(1, dColumn.offsets[1]);
-    assertEquals(1, dColumn.lengths[1]);
-    assertEquals(100, dElements.vector[1]);
-
-    assertEquals(false, rows.nextBatch(batch));
-    rows.close();
-  }
-
-  @Test
-  public void testMultiStripeWithoutNull() throws Exception {
-    TypeDescription schema = createMyStruct();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(100000)
-                                         .compress(CompressionKind.NONE)
-                                         .bufferSize(10000));
-    Random rand = new Random(100);
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (int i = 1; i < 20000; i++) {
-      addRow(writer, batch, rand.nextInt(1), "a", true, 100);
-    }
-    addRow(writer, batch, 0, "b", true, 100);
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(20000, reader.getNumberOfRows());
-    assertEquals(20000, stats[0].getNumberOfValues());
-
-    assertEquals(0, ((IntegerColumnStatistics) stats[1]).getMaximum());
-    assertEquals(0, ((IntegerColumnStatistics) stats[1]).getMinimum());
-    assertEquals(true, ((IntegerColumnStatistics) stats[1]).isSumDefined());
-    assertEquals(0, ((IntegerColumnStatistics) stats[1]).getSum());
-    assertEquals("count: 20000 hasNull: false min: 0 max: 0 sum: 0",
-        stats[1].toString());
-
-    assertEquals("b", ((StringColumnStatistics) stats[2]).getMaximum());
-    assertEquals("a", ((StringColumnStatistics) stats[2]).getMinimum());
-    assertEquals(20000, stats[2].getNumberOfValues());
-    assertEquals("count: 20000 hasNull: false min: a max: b sum: 20000",
-        stats[2].toString());
-
-    // check the inspectors
-    Assert.assertEquals("struct<a:int,b:string,c:boolean,d:array<struct<z:int>>>",
-        reader.getSchema().toString());
-
-    RecordReader rows = reader.rows();
-
-    // none of the stripes will have PRESENT stream
-    List<Boolean> expected = Lists.newArrayList();
-    for (StripeInformation sinfo : reader.getStripes()) {
-      expected.add(false);
-    }
-
-    List<Boolean> got = Lists.newArrayList();
-    // check if the strip footer contains PRESENT stream
-    for (StripeInformation sinfo : reader.getStripes()) {
-      OrcProto.StripeFooter sf =
-        ((RecordReaderImpl) rows).readStripeFooter(sinfo);
-      got.add(sf.toString().indexOf(OrcProto.Stream.Kind.PRESENT.toString())
-              != -1);
-    }
-    assertEquals(expected, got);
-
-    rows.seekToRow(19998);
-
-    batch = reader.getSchema().createRowBatch();
-    LongColumnVector aColumn = (LongColumnVector) batch.cols[0];
-    BytesColumnVector bColumn = (BytesColumnVector) batch.cols[1];
-    LongColumnVector cColumn = (LongColumnVector) batch.cols[2];
-    ListColumnVector dColumn = (ListColumnVector) batch.cols[3];
-    LongColumnVector dElements =
-        (LongColumnVector)(((StructColumnVector) dColumn.child).fields[0]);
-
-    assertEquals(true, rows.nextBatch(batch));
-    assertEquals(2, batch.size);
-
-    // last-1 row
-    assertEquals(0, aColumn.vector[0]);
-    assertEquals("a", bColumn.toString(0));
-    assertEquals(1, cColumn.vector[0]);
-    assertEquals(0, dColumn.offsets[0]);
-    assertEquals(1, dColumn.lengths[0]);
-    assertEquals(100, dElements.vector[0]);
-
-    // last row
-    assertEquals(0, aColumn.vector[1]);
-    assertEquals("b", bColumn.toString(1));
-    assertEquals(1, cColumn.vector[1]);
-    assertEquals(1, dColumn.offsets[1]);
-    assertEquals(1, dColumn.lengths[1]);
-    assertEquals(100, dElements.vector[1]);
-    rows.close();
-  }
-
-  @Test
-  public void testColumnsWithNullAndCompression() throws Exception {
-    TypeDescription schema = createMyStruct();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(100000)
-                                         .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    addRow(writer, batch, 3, "a", true, 100);
-    addRow(writer, batch, null, "b", true, 100);
-    addRow(writer, batch, 3, null, false, 100);
-    addRow(writer, batch, 3, "d", true, 100);
-    addRow(writer, batch, 2, "e", true, 100);
-    addRow(writer, batch, 2, "f", true, 100);
-    addRow(writer, batch, 2, "g", true, 100);
-    addRow(writer, batch, 2, "h", true, 100);
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(8, reader.getNumberOfRows());
-    assertEquals(8, stats[0].getNumberOfValues());
-
-    assertEquals(3, ((IntegerColumnStatistics) stats[1]).getMaximum());
-    assertEquals(2, ((IntegerColumnStatistics) stats[1]).getMinimum());
-    assertEquals(true, ((IntegerColumnStatistics) stats[1]).isSumDefined());
-    assertEquals(17, ((IntegerColumnStatistics) stats[1]).getSum());
-    assertEquals("count: 7 hasNull: true min: 2 max: 3 sum: 17",
-        stats[1].toString());
-
-    assertEquals("h", ((StringColumnStatistics) stats[2]).getMaximum());
-    assertEquals("a", ((StringColumnStatistics) stats[2]).getMinimum());
-    assertEquals(7, stats[2].getNumberOfValues());
-    assertEquals("count: 7 hasNull: true min: a max: h sum: 7",
-        stats[2].toString());
-
-    // check the inspectors
-    batch = reader.getSchema().createRowBatch();
-    LongColumnVector aColumn = (LongColumnVector) batch.cols[0];
-    BytesColumnVector bColumn = (BytesColumnVector) batch.cols[1];
-    LongColumnVector cColumn = (LongColumnVector) batch.cols[2];
-    ListColumnVector dColumn = (ListColumnVector) batch.cols[3];
-    LongColumnVector dElements =
-        (LongColumnVector)(((StructColumnVector) dColumn.child).fields[0]);
-    Assert.assertEquals("struct<a:int,b:string,c:boolean,d:array<struct<z:int>>>",
-        reader.getSchema().toString());
-
-    RecordReader rows = reader.rows();
-    // only the last strip will have PRESENT stream
-    List<Boolean> expected = Lists.newArrayList();
-    for (StripeInformation sinfo : reader.getStripes()) {
-      expected.add(false);
-    }
-    expected.set(expected.size() - 1, true);
-
-    List<Boolean> got = Lists.newArrayList();
-    // check if the strip footer contains PRESENT stream
-    for (StripeInformation sinfo : reader.getStripes()) {
-      OrcProto.StripeFooter sf =
-        ((RecordReaderImpl) rows).readStripeFooter(sinfo);
-      got.add(sf.toString().indexOf(OrcProto.Stream.Kind.PRESENT.toString())
-              != -1);
-    }
-    assertEquals(expected, got);
-
-    assertEquals(true, rows.nextBatch(batch));
-    assertEquals(8, batch.size);
-
-    // row 1
-    assertEquals(3, aColumn.vector[0]);
-    assertEquals("a", bColumn.toString(0));
-    assertEquals(1, cColumn.vector[0]);
-    assertEquals(0, dColumn.offsets[0]);
-    assertEquals(1, dColumn.lengths[0]);
-    assertEquals(100, dElements.vector[0]);
-
-    // row 2
-    assertEquals(true, aColumn.isNull[1]);
-    assertEquals("b", bColumn.toString(1));
-    assertEquals(1, cColumn.vector[1]);
-    assertEquals(1, dColumn.offsets[1]);
-    assertEquals(1, dColumn.lengths[1]);
-    assertEquals(100, dElements.vector[1]);
-
-    // row 3
-    assertEquals(3, aColumn.vector[2]);
-    assertEquals(true, bColumn.isNull[2]);
-    assertEquals(0, cColumn.vector[2]);
-    assertEquals(2, dColumn.offsets[2]);
-    assertEquals(1, dColumn.lengths[2]);
-    assertEquals(100, dElements.vector[2]);
-
-    rows.close();
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestOrcTimezone1.java b/orc/src/test/org/apache/orc/TestOrcTimezone1.java
deleted file mode 100644
index 72dc455069..0000000000
--- a/orc/src/test/org/apache/orc/TestOrcTimezone1.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static junit.framework.Assert.assertEquals;
-import static junit.framework.Assert.assertNotNull;
-
-import java.io.File;
-import java.sql.Timestamp;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-import java.util.TimeZone;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-
-import com.google.common.collect.Lists;
-
-/**
- *
- */
-@RunWith(Parameterized.class)
-public class TestOrcTimezone1 {
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-  String writerTimeZone;
-  String readerTimeZone;
-  static TimeZone defaultTimeZone = TimeZone.getDefault();
-
-  public TestOrcTimezone1(String writerTZ, String readerTZ) {
-    this.writerTimeZone = writerTZ;
-    this.readerTimeZone = readerTZ;
-  }
-
-  @Parameterized.Parameters
-  public static Collection<Object[]> data() {
-    List<Object[]> result = Arrays.asList(new Object[][]{
-        /* Extreme timezones */
-        {"GMT-12:00", "GMT+14:00"},
-        /* No difference in DST */
-        {"America/Los_Angeles", "America/Los_Angeles"}, /* same timezone both with DST */
-        {"Europe/Berlin", "Europe/Berlin"}, /* same as above but europe */
-        {"America/Phoenix", "Asia/Kolkata"} /* Writer no DST, Reader no DST */,
-        {"Europe/Berlin", "America/Los_Angeles"} /* Writer DST, Reader DST */,
-        {"Europe/Berlin", "America/Chicago"} /* Writer DST, Reader DST */,
-        /* With DST difference */
-        {"Europe/Berlin", "UTC"},
-        {"UTC", "Europe/Berlin"} /* Writer no DST, Reader DST */,
-        {"America/Los_Angeles", "Asia/Kolkata"} /* Writer DST, Reader no DST */,
-        {"Europe/Berlin", "Asia/Kolkata"} /* Writer DST, Reader no DST */,
-        /* Timezone offsets for the reader has changed historically */
-        {"Asia/Saigon", "Pacific/Enderbury"},
-        {"UTC", "Asia/Jerusalem"},
-
-        // NOTE:
-        // "1995-01-01 03:00:00.688888888" this is not a valid time in Pacific/Enderbury timezone.
-        // On 1995-01-01 00:00:00 GMT offset moved from -11:00 hr to +13:00 which makes all values
-        // on 1995-01-01 invalid. Try this with joda time
-        // new MutableDateTime("1995-01-01", DateTimeZone.forTimeZone(readerTimeZone));
-    });
-    return result;
-  }
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @After
-  public void restoreTimeZone() {
-    TimeZone.setDefault(defaultTimeZone);
-  }
-
-  @Test
-  public void testTimestampWriter() throws Exception {
-    TypeDescription schema = TypeDescription.createTimestamp();
-
-    TimeZone.setDefault(TimeZone.getTimeZone(writerTimeZone));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .bufferSize(10000));
-    assertEquals(writerTimeZone, TimeZone.getDefault().getID());
-    List<String> ts = Lists.newArrayList();
-    ts.add("2003-01-01 01:00:00.000000222");
-    ts.add("1996-08-02 09:00:00.723100809");
-    ts.add("1999-01-01 02:00:00.999999999");
-    ts.add("1995-01-02 03:00:00.688888888");
-    ts.add("2002-01-01 04:00:00.1");
-    ts.add("2010-03-02 05:00:00.000009001");
-    ts.add("2005-01-01 06:00:00.000002229");
-    ts.add("2006-01-01 07:00:00.900203003");
-    ts.add("2003-01-01 08:00:00.800000007");
-    ts.add("1998-11-02 10:00:00.857340643");
-    ts.add("2008-10-02 11:00:00.0");
-    ts.add("2037-01-01 00:00:00.000999");
-    ts.add("2014-03-28 00:00:00.0");
-    VectorizedRowBatch batch = schema.createRowBatch();
-    TimestampColumnVector times = (TimestampColumnVector) batch.cols[0];
-    for (String t : ts) {
-      times.set(batch.size++, Timestamp.valueOf(t));
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    TimeZone.setDefault(TimeZone.getTimeZone(readerTimeZone));
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    assertEquals(readerTimeZone, TimeZone.getDefault().getID());
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    times = (TimestampColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(ts.get(idx++), times.asScratchTimestamp(r).toString());
-      }
-    }
-    rows.close();
-  }
-
-  @Test
-  public void testReadTimestampFormat_0_11() throws Exception {
-    TimeZone.setDefault(TimeZone.getTimeZone(readerTimeZone));
-    Path oldFilePath = new Path(getClass().getClassLoader().
-        getSystemResource("orc-file-11-format.orc").getPath());
-    Reader reader = OrcFile.createReader(oldFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    TypeDescription schema = reader.getSchema();
-    int col = schema.getFieldNames().indexOf("ts");
-    VectorizedRowBatch batch = schema.createRowBatch(10);
-    TimestampColumnVector ts = (TimestampColumnVector) batch.cols[col];
-
-    boolean[] include = new boolean[schema.getMaximumId() + 1];
-    include[schema.getChildren().get(col).getId()] = true;
-    RecordReader rows = reader.rows
-        (new Reader.Options().include(include));
-    assertEquals(true, rows.nextBatch(batch));
-    assertEquals(Timestamp.valueOf("2000-03-12 15:00:00"),
-        ts.asScratchTimestamp(0));
-
-    // check the contents of second row
-    rows.seekToRow(7499);
-    assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1, batch.size);
-    assertEquals(Timestamp.valueOf("2000-03-12 15:00:01"),
-        ts.asScratchTimestamp(0));
-
-    // handle the close up
-    Assert.assertEquals(false, rows.nextBatch(batch));
-    rows.close();
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestOrcTimezone2.java b/orc/src/test/org/apache/orc/TestOrcTimezone2.java
deleted file mode 100644
index 4a0285521a..0000000000
--- a/orc/src/test/org/apache/orc/TestOrcTimezone2.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.io.File;
-import java.sql.Timestamp;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-import java.util.Random;
-import java.util.TimeZone;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-
-import com.google.common.collect.Lists;
-
-/**
- *
- */
-@RunWith(Parameterized.class)
-public class TestOrcTimezone2 {
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-  String writerTimeZone;
-  String readerTimeZone;
-  static TimeZone defaultTimeZone = TimeZone.getDefault();
-
-  public TestOrcTimezone2(String writerTZ, String readerTZ) {
-    this.writerTimeZone = writerTZ;
-    this.readerTimeZone = readerTZ;
-  }
-
-  @Parameterized.Parameters
-  public static Collection<Object[]> data() {
-    String[] allTimeZones = TimeZone.getAvailableIDs();
-    Random rand = new Random(123);
-    int len = allTimeZones.length;
-    int n = 500;
-    Object[][] data = new Object[n][];
-    for (int i = 0; i < n; i++) {
-      int wIdx = rand.nextInt(len);
-      int rIdx = rand.nextInt(len);
-      data[i] = new Object[2];
-      data[i][0] = allTimeZones[wIdx];
-      data[i][1] = allTimeZones[rIdx];
-    }
-    return Arrays.asList(data);
-  }
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @After
-  public void restoreTimeZone() {
-    TimeZone.setDefault(defaultTimeZone);
-  }
-
-  @Test
-  public void testTimestampWriter() throws Exception {
-    TypeDescription schema = TypeDescription.createTimestamp();
-
-    TimeZone.setDefault(TimeZone.getTimeZone(writerTimeZone));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema)
-            .stripeSize(100000).bufferSize(10000));
-    assertEquals(writerTimeZone, TimeZone.getDefault().getID());
-    List<String> ts = Lists.newArrayList();
-    ts.add("2003-01-01 01:00:00.000000222");
-    ts.add("1999-01-01 02:00:00.999999999");
-    ts.add("1995-01-02 03:00:00.688888888");
-    ts.add("2002-01-01 04:00:00.1");
-    ts.add("2010-03-02 05:00:00.000009001");
-    ts.add("2005-01-01 06:00:00.000002229");
-    ts.add("2006-01-01 07:00:00.900203003");
-    ts.add("2003-01-01 08:00:00.800000007");
-    ts.add("1996-08-02 09:00:00.723100809");
-    ts.add("1998-11-02 10:00:00.857340643");
-    ts.add("2008-10-02 11:00:00.0");
-    ts.add("2037-01-01 00:00:00.000999");
-    VectorizedRowBatch batch = schema.createRowBatch();
-    TimestampColumnVector tsc = (TimestampColumnVector) batch.cols[0];
-    for (String t : ts) {
-      tsc.set(batch.size++, Timestamp.valueOf(t));
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    TimeZone.setDefault(TimeZone.getTimeZone(readerTimeZone));
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    assertEquals(readerTimeZone, TimeZone.getDefault().getID());
-    RecordReader rows = reader.rows();
-    int idx = 0;
-    batch = reader.getSchema().createRowBatch();
-    tsc = (TimestampColumnVector) batch.cols[0];
-    while (rows.nextBatch(batch)) {
-      for (int r=0; r < batch.size; ++r) {
-        assertEquals(ts.get(idx++), tsc.asScratchTimestamp(r).toString());
-      }
-    }
-    rows.close();
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestOrcTimezone3.java b/orc/src/test/org/apache/orc/TestOrcTimezone3.java
deleted file mode 100644
index 40ab0c929d..0000000000
--- a/orc/src/test/org/apache/orc/TestOrcTimezone3.java
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.io.File;
-import java.sql.Timestamp;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-import java.util.TimeZone;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-
-import com.google.common.collect.Lists;
-
-import junit.framework.Assert;
-
-/**
- *
- */
-@RunWith(Parameterized.class)
-public class TestOrcTimezone3 {
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-  String writerTimeZone;
-  String readerTimeZone;
-  static TimeZone defaultTimeZone = TimeZone.getDefault();
-
-  public TestOrcTimezone3(String writerTZ, String readerTZ) {
-    this.writerTimeZone = writerTZ;
-    this.readerTimeZone = readerTZ;
-  }
-
-  @Parameterized.Parameters
-  public static Collection<Object[]> data() {
-    List<Object[]> result = Arrays.asList(new Object[][]{
-        {"America/Chicago", "America/Los_Angeles"},
-    });
-    return result;
-  }
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcTimezone3." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @After
-  public void restoreTimeZone() {
-    TimeZone.setDefault(defaultTimeZone);
-  }
-
-  @Test
-  public void testTimestampWriter() throws Exception {
-    TypeDescription schema = TypeDescription.createTimestamp();
-
-    TimeZone.setDefault(TimeZone.getTimeZone(writerTimeZone));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .bufferSize(10000));
-    assertEquals(writerTimeZone, TimeZone.getDefault().getID());
-    List<String> ts = Lists.newArrayList();
-    ts.add("1969-12-31 16:00:14.007");
-    ts.add("1969-12-31 16:00:06.021");
-    ts.add("1969-12-31 16:00:03.963");
-    VectorizedRowBatch batch = schema.createRowBatch();
-    TimestampColumnVector times = (TimestampColumnVector) batch.cols[0];
-    for (String t : ts) {
-      times.set(batch.size++, Timestamp.valueOf(t));
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    TimeZone.setDefault(TimeZone.getTimeZone(readerTimeZone));
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    assertEquals(readerTimeZone, TimeZone.getDefault().getID());
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    times = (TimestampColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(ts.get(idx++), times.asScratchTimestamp(r).toString());
-      }
-    }
-    rows.close();
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestStringDictionary.java b/orc/src/test/org/apache/orc/TestStringDictionary.java
deleted file mode 100644
index 46209bbbb3..0000000000
--- a/orc/src/test/org/apache/orc/TestStringDictionary.java
+++ /dev/null
@@ -1,290 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static org.junit.Assert.assertEquals;
-
-import java.io.File;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-
-import org.apache.orc.impl.RecordReaderImpl;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-public class TestStringDictionary {
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir", "target" + File.separator + "test"
-      + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile." + testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @Test
-  public void testTooManyDistinct() throws Exception {
-    TypeDescription schema = TypeDescription.createString();
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema)
-                                   .compress(CompressionKind.NONE)
-                                   .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    BytesColumnVector col = (BytesColumnVector) batch.cols[0];
-    for (int i = 0; i < 20000; i++) {
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-      col.setVal(batch.size++, String.valueOf(i).getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    col = (BytesColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(String.valueOf(idx++), col.toString(r));
-      }
-    }
-
-    // make sure the encoding type is correct
-    for (StripeInformation stripe : reader.getStripes()) {
-      // hacky but does the job, this casting will work as long this test resides
-      // within the same package as ORC reader
-      OrcProto.StripeFooter footer = ((RecordReaderImpl) rows).readStripeFooter(stripe);
-      for (int i = 0; i < footer.getColumnsCount(); ++i) {
-        OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-        assertEquals(OrcProto.ColumnEncoding.Kind.DIRECT_V2, encoding.getKind());
-      }
-    }
-  }
-
-  @Test
-  public void testHalfDistinct() throws Exception {
-    TypeDescription schema = TypeDescription.createString();
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).compress(CompressionKind.NONE)
-            .bufferSize(10000));
-    Random rand = new Random(123);
-    int[] input = new int[20000];
-    for (int i = 0; i < 20000; i++) {
-      input[i] = rand.nextInt(10000);
-    }
-
-    VectorizedRowBatch batch = schema.createRowBatch();
-    BytesColumnVector col = (BytesColumnVector) batch.cols[0];
-    for (int i = 0; i < 20000; i++) {
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-      col.setVal(batch.size++, String.valueOf(input[i]).getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    col = (BytesColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(String.valueOf(input[idx++]), col.toString(r));
-      }
-    }
-
-    // make sure the encoding type is correct
-    for (StripeInformation stripe : reader.getStripes()) {
-      // hacky but does the job, this casting will work as long this test resides
-      // within the same package as ORC reader
-      OrcProto.StripeFooter footer = ((RecordReaderImpl) rows).readStripeFooter(stripe);
-      for (int i = 0; i < footer.getColumnsCount(); ++i) {
-        OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-        assertEquals(OrcProto.ColumnEncoding.Kind.DICTIONARY_V2, encoding.getKind());
-      }
-    }
-  }
-
-  @Test
-  public void testTooManyDistinctCheckDisabled() throws Exception {
-    TypeDescription schema = TypeDescription.createString();
-
-    conf.setBoolean(OrcConf.ROW_INDEX_STRIDE_DICTIONARY_CHECK.getAttribute(), false);
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).compress(CompressionKind.NONE)
-            .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    BytesColumnVector string = (BytesColumnVector) batch.cols[0];
-    for (int i = 0; i < 20000; i++) {
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-      string.setVal(batch.size++, String.valueOf(i).getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    string = (BytesColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(String.valueOf(idx++), string.toString(r));
-      }
-    }
-
-    // make sure the encoding type is correct
-    for (StripeInformation stripe : reader.getStripes()) {
-      // hacky but does the job, this casting will work as long this test resides
-      // within the same package as ORC reader
-      OrcProto.StripeFooter footer = ((RecordReaderImpl) rows).readStripeFooter(stripe);
-      for (int i = 0; i < footer.getColumnsCount(); ++i) {
-        OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-        assertEquals(OrcProto.ColumnEncoding.Kind.DIRECT_V2, encoding.getKind());
-      }
-    }
-  }
-
-  @Test
-  public void testHalfDistinctCheckDisabled() throws Exception {
-    TypeDescription schema = TypeDescription.createString();
-
-    conf.setBoolean(OrcConf.ROW_INDEX_STRIDE_DICTIONARY_CHECK.getAttribute(),
-        false);
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000));
-    Random rand = new Random(123);
-    int[] input = new int[20000];
-    for (int i = 0; i < 20000; i++) {
-      input[i] = rand.nextInt(10000);
-    }
-    VectorizedRowBatch batch = schema.createRowBatch();
-    BytesColumnVector string = (BytesColumnVector) batch.cols[0];
-    for (int i = 0; i < 20000; i++) {
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-      string.setVal(batch.size++, String.valueOf(input[i]).getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    string = (BytesColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(String.valueOf(input[idx++]), string.toString(r));
-      }
-    }
-
-    // make sure the encoding type is correct
-    for (StripeInformation stripe : reader.getStripes()) {
-      // hacky but does the job, this casting will work as long this test resides
-      // within the same package as ORC reader
-      OrcProto.StripeFooter footer = ((RecordReaderImpl) rows).readStripeFooter(stripe);
-      for (int i = 0; i < footer.getColumnsCount(); ++i) {
-        OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-        assertEquals(OrcProto.ColumnEncoding.Kind.DICTIONARY_V2, encoding.getKind());
-      }
-    }
-  }
-
-  @Test
-  public void testTooManyDistinctV11AlwaysDictionary() throws Exception {
-    TypeDescription schema = TypeDescription.createString();
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema)
-            .compress(CompressionKind.NONE)
-            .version(OrcFile.Version.V_0_11).bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    BytesColumnVector string = (BytesColumnVector) batch.cols[0];
-    for (int i = 0; i < 20000; i++) {
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-      string.setVal(batch.size++, String.valueOf(i).getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    batch = reader.getSchema().createRowBatch();
-    string = (BytesColumnVector) batch.cols[0];
-    RecordReader rows = reader.rows();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(String.valueOf(idx++), string.toString(r));
-      }
-    }
-
-    // make sure the encoding type is correct
-    for (StripeInformation stripe : reader.getStripes()) {
-      // hacky but does the job, this casting will work as long this test resides
-      // within the same package as ORC reader
-      OrcProto.StripeFooter footer = ((RecordReaderImpl) rows).readStripeFooter(stripe);
-      for (int i = 0; i < footer.getColumnsCount(); ++i) {
-        OrcProto.ColumnEncoding encoding = footer.getColumns(i);
-        assertEquals(OrcProto.ColumnEncoding.Kind.DICTIONARY, encoding.getKind());
-      }
-    }
-
-  }
-
-}
diff --git a/orc/src/test/org/apache/orc/TestTypeDescription.java b/orc/src/test/org/apache/orc/TestTypeDescription.java
deleted file mode 100644
index 27516be213..0000000000
--- a/orc/src/test/org/apache/orc/TestTypeDescription.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc;
-
-import static org.junit.Assert.assertEquals;
-
-import org.apache.orc.TypeDescription;
-import org.junit.Test;
-
-public class TestTypeDescription {
-
-  @Test
-  public void testJson() {
-    TypeDescription bin = TypeDescription.createBinary();
-    assertEquals("{\"category\": \"binary\", \"id\": 0, \"max\": 0}",
-        bin.toJson());
-    assertEquals("binary", bin.toString());
-    TypeDescription struct = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal());
-    assertEquals("struct<f1:int,f2:string,f3:decimal(38,10)>",
-        struct.toString());
-    assertEquals("{\"category\": \"struct\", \"id\": 0, \"max\": 3, \"fields\": [\n"
-            + "  \"f1\": {\"category\": \"int\", \"id\": 1, \"max\": 1},\n"
-            + "  \"f2\": {\"category\": \"string\", \"id\": 2, \"max\": 2},\n"
-            + "  \"f3\": {\"category\": \"decimal\", \"id\": 3, \"max\": 3, \"precision\": 38, \"scale\": 10}]}",
-        struct.toJson());
-    struct = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createByte())
-            .addUnionChild(TypeDescription.createDecimal()
-                .withPrecision(20).withScale(10)))
-        .addField("f2", TypeDescription.createStruct()
-            .addField("f3", TypeDescription.createDate())
-            .addField("f4", TypeDescription.createDouble())
-            .addField("f5", TypeDescription.createBoolean()))
-        .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    assertEquals("struct<f1:uniontype<tinyint,decimal(20,10)>,f2:struct<f3:date,f4:double,f5:boolean>,f6:char(100)>",
-        struct.toString());
-    assertEquals(
-        "{\"category\": \"struct\", \"id\": 0, \"max\": 8, \"fields\": [\n" +
-            "  \"f1\": {\"category\": \"uniontype\", \"id\": 1, \"max\": 3, \"children\": [\n" +
-            "    {\"category\": \"tinyint\", \"id\": 2, \"max\": 2},\n" +
-            "    {\"category\": \"decimal\", \"id\": 3, \"max\": 3, \"precision\": 20, \"scale\": 10}]},\n" +
-            "  \"f2\": {\"category\": \"struct\", \"id\": 4, \"max\": 7, \"fields\": [\n" +
-            "    \"f3\": {\"category\": \"date\", \"id\": 5, \"max\": 5},\n" +
-            "    \"f4\": {\"category\": \"double\", \"id\": 6, \"max\": 6},\n" +
-            "    \"f5\": {\"category\": \"boolean\", \"id\": 7, \"max\": 7}]},\n" +
-            "  \"f6\": {\"category\": \"char\", \"id\": 8, \"max\": 8, \"length\": 100}]}",
-        struct.toJson());
-  }
-
-  @Test
-  public void testEquals() {
-    TypeDescription type1 =
-        TypeDescription.createStruct()
-        .addField("a", TypeDescription.createInt())
-        .addField("b", TypeDescription.createStruct()
-                         .addField("x", TypeDescription.createString())
-                         .addField("y", TypeDescription.createBinary())
-                         .addField("z", TypeDescription.createDouble()))
-        .addField("c", TypeDescription.createString());
-    assertEquals(0, type1.getId());
-    assertEquals(6, type1.getMaximumId());
-    TypeDescription type2 =
-        TypeDescription.createStruct()
-        .addField("x", TypeDescription.createString())
-        .addField("y", TypeDescription.createBinary())
-        .addField("z", TypeDescription.createDouble());
-    assertEquals(0, type2.getId());
-    assertEquals(3, type2.getMaximumId());
-    assertEquals(type2, type1.getChildren().get(1));
-    assertEquals(type2.hashCode(), type1.getChildren().get(1).hashCode());
-  }
-}
diff --git a/orc/src/test/org/apache/orc/TestUnrolledBitPack.java b/orc/src/test/org/apache/orc/TestUnrolledBitPack.java
deleted file mode 100644
index ef8fcd0c75..0000000000
--- a/orc/src/test/org/apache/orc/TestUnrolledBitPack.java
+++ /dev/null
@@ -1,114 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import static org.junit.Assert.assertEquals;
-
-import java.io.File;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-import org.junit.runners.Parameterized.Parameters;
-
-import com.google.common.collect.Lists;
-import com.google.common.primitives.Longs;
-
-@RunWith(value = Parameterized.class)
-public class TestUnrolledBitPack {
-
-  private long val;
-
-  public TestUnrolledBitPack(long val) {
-    this.val = val;
-  }
-
-  @Parameters
-  public static Collection<Object[]> data() {
-    Object[][] data = new Object[][] { { -1 }, { 1 }, { 7 }, { -128 }, { 32000 }, { 8300000 },
-        { Integer.MAX_VALUE }, { 540000000000L }, { 140000000000000L }, { 36000000000000000L },
-        { Long.MAX_VALUE } };
-    return Arrays.asList(data);
-  }
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir", "target" + File.separator + "test"
-      + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile." + testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @Test
-  public void testBitPacking() throws Exception {
-    TypeDescription schema = TypeDescription.createLong();
-
-    long[] inp = new long[] { val, 0, val, val, 0, val, 0, val, val, 0, val, 0, val, val, 0, 0,
-        val, val, 0, val, 0, 0, val, 0, val, 0, val, 0, 0, val, 0, val, 0, val, 0, 0, val, 0, val,
-        0, val, 0, 0, val, 0, val, 0, val, 0, 0, val, 0, val, 0, val, 0, 0, val, 0, val, 0, val, 0,
-        0, val, 0, val, 0, val, 0, 0, val, 0, val, 0, val, 0, 0, val, 0, val, 0, val, 0, 0, val, 0,
-        val, 0, val, 0, 0, val, 0, val, 0, 0, val, val };
-    List<Long> input = Lists.newArrayList(Longs.asList(inp));
-
-    Writer writer = OrcFile.createWriter(
-        testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .compress(CompressionKind.NONE).bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    for (Long l : input) {
-      int row = batch.size++;
-      ((LongColumnVector) batch.cols[0]).vector[row] = l;
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath, OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(input.get(idx++).longValue(),
-            ((LongColumnVector) batch.cols[0]).vector[r]);
-      }
-    }
-  }
-
-}
diff --git a/orc/src/test/org/apache/orc/TestVectorOrcFile.java b/orc/src/test/org/apache/orc/TestVectorOrcFile.java
deleted file mode 100644
index 112edb91b7..0000000000
--- a/orc/src/test/org/apache/orc/TestVectorOrcFile.java
+++ /dev/null
@@ -1,2782 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc;
-
-import com.google.common.collect.Lists;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.UnionColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.orc.impl.DataReaderProperties;
-import org.apache.orc.impl.MemoryManager;
-import org.apache.orc.impl.OrcIndex;
-import org.apache.orc.impl.RecordReaderImpl;
-import org.apache.orc.impl.RecordReaderUtils;
-import org.apache.orc.tools.TestJsonFileDump;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-import java.io.File;
-import java.io.IOException;
-import java.math.BigInteger;
-import java.nio.ByteBuffer;
-import java.sql.Date;
-import java.sql.Timestamp;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import static junit.framework.TestCase.assertNotNull;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-
-/**
- * Tests for the vectorized reader and writer for ORC files.
- */
-public class TestVectorOrcFile {
-
-  public static class InnerStruct {
-    int int1;
-    Text string1 = new Text();
-    InnerStruct(int int1, Text string1) {
-      this.int1 = int1;
-      this.string1.set(string1);
-    }
-    InnerStruct(int int1, String string1) {
-      this.int1 = int1;
-      this.string1.set(string1);
-    }
-
-    public String toString() {
-      return "{" + int1 + ", " + string1 + "}";
-    }
-  }
-
-  public static class MiddleStruct {
-    List<InnerStruct> list = new ArrayList<InnerStruct>();
-
-    MiddleStruct(InnerStruct... items) {
-      list.clear();
-      list.addAll(Arrays.asList(items));
-    }
-  }
-
-  private static InnerStruct inner(int i, String s) {
-    return new InnerStruct(i, s);
-  }
-
-  private static Map<String, InnerStruct> map(InnerStruct... items)  {
-    Map<String, InnerStruct> result = new HashMap<String, InnerStruct>();
-    for(InnerStruct i: items) {
-      result.put(i.string1.toString(), i);
-    }
-    return result;
-  }
-
-  private static List<InnerStruct> list(InnerStruct... items) {
-    List<InnerStruct> result = new ArrayList<InnerStruct>();
-    result.addAll(Arrays.asList(items));
-    return result;
-  }
-
-  private static BytesWritable bytes(int... items) {
-    BytesWritable result = new BytesWritable();
-    result.setSize(items.length);
-    for(int i=0; i < items.length; ++i) {
-      result.getBytes()[i] = (byte) items[i];
-    }
-    return result;
-  }
-
-  private static byte[] bytesArray(int... items) {
-    byte[] result = new byte[items.length];
-    for(int i=0; i < items.length; ++i) {
-      result[i] = (byte) items[i];
-    }
-    return result;
-  }
-
-  private static ByteBuffer byteBuf(int... items) {
-    ByteBuffer result = ByteBuffer.allocate(items.length);
-    for(int item: items) {
-      result.put((byte) item);
-    }
-    result.flip();
-    return result;
-  }
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem () throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestVectorOrcFile." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @Test
-  public void testReadFormat_0_11() throws Exception {
-    Path oldFilePath =
-        new Path(TestJsonFileDump.getFileFromClasspath("orc-file-11-format.orc"));
-    Reader reader = OrcFile.createReader(oldFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    int stripeCount = 0;
-    int rowCount = 0;
-    long currentOffset = -1;
-    for(StripeInformation stripe : reader.getStripes()) {
-      stripeCount += 1;
-      rowCount += stripe.getNumberOfRows();
-      if (currentOffset < 0) {
-        currentOffset = stripe.getOffset() + stripe.getIndexLength()
-            + stripe.getDataLength() + stripe.getFooterLength();
-      } else {
-        assertEquals(currentOffset, stripe.getOffset());
-        currentOffset += stripe.getIndexLength() + stripe.getDataLength()
-            + stripe.getFooterLength();
-      }
-    }
-    Assert.assertEquals(reader.getNumberOfRows(), rowCount);
-    assertEquals(2, stripeCount);
-
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(7500, stats[1].getNumberOfValues());
-    assertEquals(3750, ((BooleanColumnStatistics) stats[1]).getFalseCount());
-    assertEquals(3750, ((BooleanColumnStatistics) stats[1]).getTrueCount());
-    assertEquals("count: 7500 hasNull: true true: 3750", stats[1].toString());
-
-    assertEquals(2048, ((IntegerColumnStatistics) stats[3]).getMaximum());
-    assertEquals(1024, ((IntegerColumnStatistics) stats[3]).getMinimum());
-    assertEquals(true, ((IntegerColumnStatistics) stats[3]).isSumDefined());
-    assertEquals(11520000, ((IntegerColumnStatistics) stats[3]).getSum());
-    assertEquals("count: 7500 hasNull: true min: 1024 max: 2048 sum: 11520000",
-        stats[3].toString());
-
-    assertEquals(Long.MAX_VALUE,
-        ((IntegerColumnStatistics) stats[5]).getMaximum());
-    assertEquals(Long.MAX_VALUE,
-        ((IntegerColumnStatistics) stats[5]).getMinimum());
-    assertEquals(false, ((IntegerColumnStatistics) stats[5]).isSumDefined());
-    assertEquals(
-        "count: 7500 hasNull: true min: 9223372036854775807 max: 9223372036854775807",
-        stats[5].toString());
-
-    assertEquals(-15.0, ((DoubleColumnStatistics) stats[7]).getMinimum(), 0.0001);
-    assertEquals(-5.0, ((DoubleColumnStatistics) stats[7]).getMaximum(), 0.0001);
-    assertEquals(-75000.0, ((DoubleColumnStatistics) stats[7]).getSum(),
-        0.00001);
-    assertEquals("count: 7500 hasNull: true min: -15.0 max: -5.0 sum: -75000.0",
-        stats[7].toString());
-
-    assertEquals("count: 7500 hasNull: true min: bye max: hi sum: 0", stats[9].toString());
-
-    // check the inspectors
-    TypeDescription schema = reader.getSchema();
-    assertEquals(TypeDescription.Category.STRUCT, schema.getCategory());
-    assertEquals("struct<boolean1:boolean,byte1:tinyint,short1:smallint,"
-        + "int1:int,long1:bigint,float1:float,double1:double,bytes1:"
-        + "binary,string1:string,middle:struct<list:array<struct<int1:int,"
-        + "string1:string>>>,list:array<struct<int1:int,string1:string>>,"
-        + "map:map<string,struct<int1:int,string1:string>>,ts:timestamp,"
-        + "decimal1:decimal(38,10)>", schema.toString());
-    VectorizedRowBatch batch = schema.createRowBatch();
-
-    RecordReader rows = reader.rows();
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-
-    // check the contents of the first row
-    assertEquals(false, getBoolean(batch, 0));
-    assertEquals(1, getByte(batch, 0));
-    assertEquals(1024, getShort(batch, 0));
-    assertEquals(65536, getInt(batch, 0));
-    assertEquals(Long.MAX_VALUE, getLong(batch, 0));
-    assertEquals(1.0, getFloat(batch, 0), 0.00001);
-    assertEquals(-15.0, getDouble(batch, 0), 0.00001);
-    assertEquals(bytes(0, 1, 2, 3, 4), getBinary(batch, 0));
-    assertEquals("hi", getText(batch, 0).toString());
-
-    StructColumnVector middle = (StructColumnVector) batch.cols[9];
-    ListColumnVector midList = (ListColumnVector) middle.fields[0];
-    StructColumnVector midListStruct = (StructColumnVector) midList.child;
-    LongColumnVector midListInt = (LongColumnVector) midListStruct.fields[0];
-    BytesColumnVector midListStr = (BytesColumnVector) midListStruct.fields[1];
-    ListColumnVector list = (ListColumnVector) batch.cols[10];
-    StructColumnVector listStruct = (StructColumnVector) list.child;
-    LongColumnVector listInts = (LongColumnVector) listStruct.fields[0];
-    BytesColumnVector listStrs = (BytesColumnVector) listStruct.fields[1];
-    MapColumnVector map = (MapColumnVector) batch.cols[11];
-    BytesColumnVector mapKey = (BytesColumnVector) map.keys;
-    StructColumnVector mapValue = (StructColumnVector) map.values;
-    LongColumnVector mapValueInts = (LongColumnVector) mapValue.fields[0];
-    BytesColumnVector mapValueStrs = (BytesColumnVector) mapValue.fields[1];
-    TimestampColumnVector timestamp = (TimestampColumnVector) batch.cols[12];
-    DecimalColumnVector decs = (DecimalColumnVector) batch.cols[13];
-
-    assertEquals(false, middle.isNull[0]);
-    assertEquals(2, midList.lengths[0]);
-    int start = (int) midList.offsets[0];
-    assertEquals(1, midListInt.vector[start]);
-    assertEquals("bye", midListStr.toString(start));
-    assertEquals(2, midListInt.vector[start + 1]);
-    assertEquals("sigh", midListStr.toString(start + 1));
-
-    assertEquals(2, list.lengths[0]);
-    start = (int) list.offsets[0];
-    assertEquals(3, listInts.vector[start]);
-    assertEquals("good", listStrs.toString(start));
-    assertEquals(4, listInts.vector[start + 1]);
-    assertEquals("bad", listStrs.toString(start + 1));
-    assertEquals(0, map.lengths[0]);
-    assertEquals(Timestamp.valueOf("2000-03-12 15:00:00"),
-        timestamp.asScratchTimestamp(0));
-    assertEquals(new HiveDecimalWritable(HiveDecimal.create("12345678.6547456")),
-        decs.vector[0]);
-
-    // check the contents of row 7499
-    rows.seekToRow(7499);
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(true, getBoolean(batch, 0));
-    assertEquals(100, getByte(batch, 0));
-    assertEquals(2048, getShort(batch, 0));
-    assertEquals(65536, getInt(batch, 0));
-    assertEquals(Long.MAX_VALUE, getLong(batch, 0));
-    assertEquals(2.0, getFloat(batch, 0), 0.00001);
-    assertEquals(-5.0, getDouble(batch, 0), 0.00001);
-    assertEquals(bytes(), getBinary(batch, 0));
-    assertEquals("bye", getText(batch, 0).toString());
-    assertEquals(false, middle.isNull[0]);
-    assertEquals(2, midList.lengths[0]);
-    start = (int) midList.offsets[0];
-    assertEquals(1, midListInt.vector[start]);
-    assertEquals("bye", midListStr.toString(start));
-    assertEquals(2, midListInt.vector[start + 1]);
-    assertEquals("sigh", midListStr.toString(start + 1));
-    assertEquals(3, list.lengths[0]);
-    start = (int) list.offsets[0];
-    assertEquals(100000000, listInts.vector[start]);
-    assertEquals("cat", listStrs.toString(start));
-    assertEquals(-100000, listInts.vector[start + 1]);
-    assertEquals("in", listStrs.toString(start + 1));
-    assertEquals(1234, listInts.vector[start + 2]);
-    assertEquals("hat", listStrs.toString(start + 2));
-    assertEquals(2, map.lengths[0]);
-    start = (int) map.offsets[0];
-    assertEquals("chani", mapKey.toString(start));
-    assertEquals(5, mapValueInts.vector[start]);
-    assertEquals("chani", mapValueStrs.toString(start));
-    assertEquals("mauddib", mapKey.toString(start + 1));
-    assertEquals(1, mapValueInts.vector[start + 1]);
-    assertEquals("mauddib", mapValueStrs.toString(start + 1));
-    assertEquals(Timestamp.valueOf("2000-03-12 15:00:01"),
-        timestamp.asScratchTimestamp(0));
-    assertEquals(new HiveDecimalWritable(HiveDecimal.create("12345678.6547457")),
-        decs.vector[0]);
-
-    // handle the close up
-    Assert.assertEquals(false, rows.nextBatch(batch));
-    rows.close();
-  }
-
-  @Test
-  public void testTimestamp() throws Exception {
-    TypeDescription schema = TypeDescription.createTimestamp();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .bufferSize(10000).version(org.apache.orc.OrcFile.Version.V_0_11));
-    List<Timestamp> tslist = Lists.newArrayList();
-    tslist.add(Timestamp.valueOf("2037-01-01 00:00:00.000999"));
-    tslist.add(Timestamp.valueOf("2003-01-01 00:00:00.000000222"));
-    tslist.add(Timestamp.valueOf("1999-01-01 00:00:00.999999999"));
-    tslist.add(Timestamp.valueOf("1995-01-01 00:00:00.688888888"));
-    tslist.add(Timestamp.valueOf("2002-01-01 00:00:00.1"));
-    tslist.add(Timestamp.valueOf("2010-03-02 00:00:00.000009001"));
-    tslist.add(Timestamp.valueOf("2005-01-01 00:00:00.000002229"));
-    tslist.add(Timestamp.valueOf("2006-01-01 00:00:00.900203003"));
-    tslist.add(Timestamp.valueOf("2003-01-01 00:00:00.800000007"));
-    tslist.add(Timestamp.valueOf("1996-08-02 00:00:00.723100809"));
-    tslist.add(Timestamp.valueOf("1998-11-02 00:00:00.857340643"));
-    tslist.add(Timestamp.valueOf("2008-10-02 00:00:00"));
-
-    VectorizedRowBatch batch = new VectorizedRowBatch(1, 1024);
-    TimestampColumnVector vec = new TimestampColumnVector(1024);
-    batch.cols[0] = vec;
-    batch.reset();
-    batch.size = tslist.size();
-    for (int i=0; i < tslist.size(); ++i) {
-      Timestamp ts = tslist.get(i);
-      vec.set(i, ts);
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    TimestampColumnVector timestamps = (TimestampColumnVector) batch.cols[0];
-    int idx = 0;
-    while (rows.nextBatch(batch)) {
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(tslist.get(idx++).getNanos(),
-            timestamps.asScratchTimestamp(r).getNanos());
-      }
-    }
-    Assert.assertEquals(tslist.size(), rows.getRowNumber());
-    assertEquals(0, writer.getSchema().getMaximumId());
-    boolean[] expected = new boolean[] {false};
-    boolean[] included = OrcUtils.includeColumns("", writer.getSchema());
-    assertEquals(true, Arrays.equals(expected, included));
-  }
-
-  @Test
-  public void testStringAndBinaryStatistics() throws Exception {
-
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("bytes1", TypeDescription.createBinary())
-        .addField("string1", TypeDescription.createString());
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(100000)
-                                         .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 4;
-    BytesColumnVector field1 = (BytesColumnVector) batch.cols[0];
-    BytesColumnVector field2 = (BytesColumnVector) batch.cols[1];
-    field1.setVal(0, bytesArray(0, 1, 2, 3, 4));
-    field1.setVal(1, bytesArray(0, 1, 2, 3));
-    field1.setVal(2, bytesArray(0, 1, 2, 3, 4, 5));
-    field1.noNulls = false;
-    field1.isNull[3] = true;
-    field2.setVal(0, "foo".getBytes());
-    field2.setVal(1, "bar".getBytes());
-    field2.noNulls = false;
-    field2.isNull[2] = true;
-    field2.setVal(3, "hi".getBytes());
-    writer.addRowBatch(batch);
-    writer.close();
-    schema = writer.getSchema();
-    assertEquals(2, schema.getMaximumId());
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    boolean[] expected = new boolean[] {false, false, true};
-    boolean[] included = OrcUtils.includeColumns("string1", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    expected = new boolean[] {false, false, false};
-    included = OrcUtils.includeColumns("", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    expected = new boolean[] {false, false, false};
-    included = OrcUtils.includeColumns(null, schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(4, stats[0].getNumberOfValues());
-    assertEquals("count: 4 hasNull: false", stats[0].toString());
-
-    assertEquals(3, stats[1].getNumberOfValues());
-    assertEquals(15, ((BinaryColumnStatistics) stats[1]).getSum());
-    assertEquals("count: 3 hasNull: true sum: 15", stats[1].toString());
-
-    assertEquals(3, stats[2].getNumberOfValues());
-    assertEquals("bar", ((StringColumnStatistics) stats[2]).getMinimum());
-    assertEquals("hi", ((StringColumnStatistics) stats[2]).getMaximum());
-    assertEquals(8, ((StringColumnStatistics) stats[2]).getSum());
-    assertEquals("count: 3 hasNull: true min: bar max: hi sum: 8",
-        stats[2].toString());
-
-    // check the inspectors
-    batch = reader.getSchema().createRowBatch();
-    BytesColumnVector bytes = (BytesColumnVector) batch.cols[0];
-    BytesColumnVector strs = (BytesColumnVector) batch.cols[1];
-    RecordReader rows = reader.rows();
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(4, batch.size);
-
-    // check the contents of the first row
-    assertEquals(bytes(0,1,2,3,4), getBinary(bytes, 0));
-    assertEquals("foo", strs.toString(0));
-
-    // check the contents of second row
-    assertEquals(bytes(0,1,2,3), getBinary(bytes, 1));
-    assertEquals("bar", strs.toString(1));
-
-    // check the contents of third row
-    assertEquals(bytes(0,1,2,3,4,5), getBinary(bytes, 2));
-    assertNull(strs.toString(2));
-
-    // check the contents of fourth row
-    assertNull(getBinary(bytes, 3));
-    assertEquals("hi", strs.toString(3));
-
-    // handle the close up
-    Assert.assertEquals(false, rows.nextBatch(batch));
-    rows.close();
-  }
-
-
-  @Test
-  public void testStripeLevelStats() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("int1", TypeDescription.createInt())
-        .addField("string1", TypeDescription.createString());
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1000;
-    LongColumnVector field1 = (LongColumnVector) batch.cols[0];
-    BytesColumnVector field2 = (BytesColumnVector) batch.cols[1];
-    field1.isRepeating = true;
-    field2.isRepeating = true;
-    for (int b = 0; b < 11; b++) {
-      if (b >= 5) {
-        if (b >= 10) {
-          field1.vector[0] = 3;
-          field2.setVal(0, "three".getBytes());
-        } else {
-          field1.vector[0] = 2;
-          field2.setVal(0, "two".getBytes());
-        }
-      } else {
-        field1.vector[0] = 1;
-        field2.setVal(0, "one".getBytes());
-      }
-      writer.addRowBatch(batch);
-    }
-
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    schema = writer.getSchema();
-    assertEquals(2, schema.getMaximumId());
-    boolean[] expected = new boolean[] {false, true, false};
-    boolean[] included = OrcUtils.includeColumns("int1", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    List<StripeStatistics> stats = reader.getStripeStatistics();
-    int numStripes = stats.size();
-    assertEquals(3, numStripes);
-    StripeStatistics ss1 = stats.get(0);
-    StripeStatistics ss2 = stats.get(1);
-    StripeStatistics ss3 = stats.get(2);
-
-    assertEquals(5000, ss1.getColumnStatistics()[0].getNumberOfValues());
-    assertEquals(5000, ss2.getColumnStatistics()[0].getNumberOfValues());
-    assertEquals(1000, ss3.getColumnStatistics()[0].getNumberOfValues());
-
-    assertEquals(5000, (ss1.getColumnStatistics()[1]).getNumberOfValues());
-    assertEquals(5000, (ss2.getColumnStatistics()[1]).getNumberOfValues());
-    assertEquals(1000, (ss3.getColumnStatistics()[1]).getNumberOfValues());
-    assertEquals(1, ((IntegerColumnStatistics)ss1.getColumnStatistics()[1]).getMinimum());
-    assertEquals(2, ((IntegerColumnStatistics)ss2.getColumnStatistics()[1]).getMinimum());
-    assertEquals(3, ((IntegerColumnStatistics)ss3.getColumnStatistics()[1]).getMinimum());
-    assertEquals(1, ((IntegerColumnStatistics)ss1.getColumnStatistics()[1]).getMaximum());
-    assertEquals(2, ((IntegerColumnStatistics)ss2.getColumnStatistics()[1]).getMaximum());
-    assertEquals(3, ((IntegerColumnStatistics)ss3.getColumnStatistics()[1]).getMaximum());
-    assertEquals(5000, ((IntegerColumnStatistics)ss1.getColumnStatistics()[1]).getSum());
-    assertEquals(10000, ((IntegerColumnStatistics)ss2.getColumnStatistics()[1]).getSum());
-    assertEquals(3000, ((IntegerColumnStatistics)ss3.getColumnStatistics()[1]).getSum());
-
-    assertEquals(5000, (ss1.getColumnStatistics()[2]).getNumberOfValues());
-    assertEquals(5000, (ss2.getColumnStatistics()[2]).getNumberOfValues());
-    assertEquals(1000, (ss3.getColumnStatistics()[2]).getNumberOfValues());
-    assertEquals("one", ((StringColumnStatistics)ss1.getColumnStatistics()[2]).getMinimum());
-    assertEquals("two", ((StringColumnStatistics)ss2.getColumnStatistics()[2]).getMinimum());
-    assertEquals("three", ((StringColumnStatistics)ss3.getColumnStatistics()[2]).getMinimum());
-    assertEquals("one", ((StringColumnStatistics)ss1.getColumnStatistics()[2]).getMaximum());
-    assertEquals("two", ((StringColumnStatistics) ss2.getColumnStatistics()[2]).getMaximum());
-    assertEquals("three", ((StringColumnStatistics)ss3.getColumnStatistics()[2]).getMaximum());
-    assertEquals(15000, ((StringColumnStatistics)ss1.getColumnStatistics()[2]).getSum());
-    assertEquals(15000, ((StringColumnStatistics)ss2.getColumnStatistics()[2]).getSum());
-    assertEquals(5000, ((StringColumnStatistics)ss3.getColumnStatistics()[2]).getSum());
-
-    RecordReaderImpl recordReader = (RecordReaderImpl) reader.rows();
-    OrcProto.RowIndex[] index = recordReader.readRowIndex(0, null, null).getRowGroupIndex();
-    assertEquals(3, index.length);
-    List<OrcProto.RowIndexEntry> items = index[1].getEntryList();
-    assertEquals(1, items.size());
-    assertEquals(3, items.get(0).getPositionsCount());
-    assertEquals(0, items.get(0).getPositions(0));
-    assertEquals(0, items.get(0).getPositions(1));
-    assertEquals(0, items.get(0).getPositions(2));
-    assertEquals(1,
-                 items.get(0).getStatistics().getIntStatistics().getMinimum());
-    index = recordReader.readRowIndex(1, null, null).getRowGroupIndex();
-    assertEquals(3, index.length);
-    items = index[1].getEntryList();
-    assertEquals(2,
-        items.get(0).getStatistics().getIntStatistics().getMaximum());
-  }
-
-  private static void setInner(StructColumnVector inner, int rowId,
-                               int i, String value) {
-    ((LongColumnVector) inner.fields[0]).vector[rowId] = i;
-    if (value != null) {
-      ((BytesColumnVector) inner.fields[1]).setVal(rowId, value.getBytes());
-    } else {
-      inner.fields[1].isNull[rowId] = true;
-      inner.fields[1].noNulls = false;
-    }
-  }
-
-  private static void checkInner(StructColumnVector inner, int rowId,
-                                 int rowInBatch, int i, String value) {
-    assertEquals("row " + rowId, i,
-        ((LongColumnVector) inner.fields[0]).vector[rowInBatch]);
-    if (value != null) {
-      assertEquals("row " + rowId, value,
-          ((BytesColumnVector) inner.fields[1]).toString(rowInBatch));
-    } else {
-      assertEquals("row " + rowId, true, inner.fields[1].isNull[rowInBatch]);
-      assertEquals("row " + rowId, false, inner.fields[1].noNulls);
-    }
-  }
-
-  private static void setInnerList(ListColumnVector list, int rowId,
-                                   List<InnerStruct> value) {
-    if (value != null) {
-      if (list.childCount + value.size() > list.child.isNull.length) {
-        list.child.ensureSize(list.childCount * 2, true);
-      }
-      list.lengths[rowId] = value.size();
-      list.offsets[rowId] = list.childCount;
-      for (int i = 0; i < list.lengths[rowId]; ++i) {
-        InnerStruct inner = value.get(i);
-        setInner((StructColumnVector) list.child, i + list.childCount,
-            inner.int1, inner.string1.toString());
-      }
-      list.childCount += value.size();
-    } else {
-      list.isNull[rowId] = true;
-      list.noNulls = false;
-    }
-  }
-
-  private static void checkInnerList(ListColumnVector list, int rowId,
-                                     int rowInBatch, List<InnerStruct> value) {
-    if (value != null) {
-      assertEquals("row " + rowId, value.size(), list.lengths[rowInBatch]);
-      int start = (int) list.offsets[rowInBatch];
-      for (int i = 0; i < list.lengths[rowInBatch]; ++i) {
-        InnerStruct inner = value.get(i);
-        checkInner((StructColumnVector) list.child, rowId, i + start,
-            inner.int1, inner.string1.toString());
-      }
-      list.childCount += value.size();
-    } else {
-      assertEquals("row " + rowId, true, list.isNull[rowInBatch]);
-      assertEquals("row " + rowId, false, list.noNulls);
-    }
-  }
-
-  private static void setInnerMap(MapColumnVector map, int rowId,
-                                  Map<String, InnerStruct> value) {
-    if (value != null) {
-      if (map.childCount >= map.keys.isNull.length) {
-        map.keys.ensureSize(map.childCount * 2, true);
-        map.values.ensureSize(map.childCount * 2, true);
-      }
-      map.lengths[rowId] = value.size();
-      int offset = map.childCount;
-      map.offsets[rowId] = offset;
-
-      for (Map.Entry<String, InnerStruct> entry : value.entrySet()) {
-        ((BytesColumnVector) map.keys).setVal(offset, entry.getKey().getBytes());
-        InnerStruct inner = entry.getValue();
-        setInner((StructColumnVector) map.values, offset, inner.int1,
-            inner.string1.toString());
-        offset += 1;
-      }
-      map.childCount = offset;
-    } else {
-      map.isNull[rowId] = true;
-      map.noNulls = false;
-    }
-  }
-
-  private static void checkInnerMap(MapColumnVector map, int rowId,
-                                    int rowInBatch,
-                                    Map<String, InnerStruct> value) {
-    if (value != null) {
-      assertEquals("row " + rowId, value.size(), map.lengths[rowInBatch]);
-      int offset = (int) map.offsets[rowInBatch];
-      for(int i=0; i < value.size(); ++i) {
-        String key = ((BytesColumnVector) map.keys).toString(offset + i);
-        InnerStruct expected = value.get(key);
-        checkInner((StructColumnVector) map.values, rowId, offset + i,
-            expected.int1, expected.string1.toString());
-      }
-    } else {
-      assertEquals("row " + rowId, true, map.isNull[rowId]);
-      assertEquals("row " + rowId, false, map.noNulls);
-    }
-  }
-
-  private static void setMiddleStruct(StructColumnVector middle, int rowId,
-                                      MiddleStruct value) {
-    if (value != null) {
-      setInnerList((ListColumnVector) middle.fields[0], rowId, value.list);
-    } else {
-      middle.isNull[rowId] = true;
-      middle.noNulls = false;
-    }
-  }
-
-  private static void checkMiddleStruct(StructColumnVector middle, int rowId,
-                                        int rowInBatch, MiddleStruct value) {
-    if (value != null) {
-      checkInnerList((ListColumnVector) middle.fields[0], rowId, rowInBatch,
-          value.list);
-    } else {
-      assertEquals("row " + rowId, true, middle.isNull[rowInBatch]);
-      assertEquals("row " + rowId, false, middle.noNulls);
-    }
-  }
-
-  private static void setBigRow(VectorizedRowBatch batch, int rowId,
-                                Boolean b1, Byte b2, Short s1,
-                                Integer i1, Long l1, Float f1,
-                                Double d1, BytesWritable b3, String s2,
-                                MiddleStruct m1, List<InnerStruct> l2,
-                                Map<String, InnerStruct> m2) {
-    ((LongColumnVector) batch.cols[0]).vector[rowId] = b1 ? 1 : 0;
-    ((LongColumnVector) batch.cols[1]).vector[rowId] = b2;
-    ((LongColumnVector) batch.cols[2]).vector[rowId] = s1;
-    ((LongColumnVector) batch.cols[3]).vector[rowId] = i1;
-    ((LongColumnVector) batch.cols[4]).vector[rowId] = l1;
-    ((DoubleColumnVector) batch.cols[5]).vector[rowId] = f1;
-    ((DoubleColumnVector) batch.cols[6]).vector[rowId] = d1;
-    if (b3 != null) {
-      ((BytesColumnVector) batch.cols[7]).setVal(rowId, b3.getBytes(), 0,
-          b3.getLength());
-    } else {
-      batch.cols[7].isNull[rowId] = true;
-      batch.cols[7].noNulls = false;
-    }
-    if (s2 != null) {
-      ((BytesColumnVector) batch.cols[8]).setVal(rowId, s2.getBytes());
-    } else {
-      batch.cols[8].isNull[rowId] = true;
-      batch.cols[8].noNulls = false;
-    }
-    setMiddleStruct((StructColumnVector) batch.cols[9], rowId, m1);
-    setInnerList((ListColumnVector) batch.cols[10], rowId, l2);
-    setInnerMap((MapColumnVector) batch.cols[11], rowId, m2);
-  }
-
-  private static void checkBigRow(VectorizedRowBatch batch,
-                                  int rowInBatch,
-                                  int rowId,
-                                  boolean b1, byte b2, short s1,
-                                  int i1, long l1, float f1,
-                                  double d1, BytesWritable b3, String s2,
-                                  MiddleStruct m1, List<InnerStruct> l2,
-                                  Map<String, InnerStruct> m2) {
-    assertEquals("row " + rowId, b1, getBoolean(batch, rowInBatch));
-    assertEquals("row " + rowId, b2, getByte(batch, rowInBatch));
-    assertEquals("row " + rowId, s1, getShort(batch, rowInBatch));
-    assertEquals("row " + rowId, i1, getInt(batch, rowInBatch));
-    assertEquals("row " + rowId, l1, getLong(batch, rowInBatch));
-    assertEquals("row " + rowId, f1, getFloat(batch, rowInBatch), 0.0001);
-    assertEquals("row " + rowId, d1, getDouble(batch, rowInBatch), 0.0001);
-    if (b3 != null) {
-      BytesColumnVector bytes = (BytesColumnVector) batch.cols[7];
-      assertEquals("row " + rowId, b3.getLength(), bytes.length[rowInBatch]);
-      for(int i=0; i < b3.getLength(); ++i) {
-        assertEquals("row " + rowId + " byte " + i, b3.getBytes()[i],
-            bytes.vector[rowInBatch][bytes.start[rowInBatch] + i]);
-      }
-    } else {
-      assertEquals("row " + rowId, true, batch.cols[7].isNull[rowInBatch]);
-      assertEquals("row " + rowId, false, batch.cols[7].noNulls);
-    }
-    if (s2 != null) {
-      assertEquals("row " + rowId, s2, getText(batch, rowInBatch).toString());
-    } else {
-      assertEquals("row " + rowId, true, batch.cols[8].isNull[rowInBatch]);
-      assertEquals("row " + rowId, false, batch.cols[8].noNulls);
-    }
-    checkMiddleStruct((StructColumnVector) batch.cols[9], rowId, rowInBatch,
-        m1);
-    checkInnerList((ListColumnVector) batch.cols[10], rowId, rowInBatch, l2);
-    checkInnerMap((MapColumnVector) batch.cols[11], rowId, rowInBatch, m2);
-  }
-
-  private static boolean getBoolean(VectorizedRowBatch batch, int rowId) {
-    return ((LongColumnVector) batch.cols[0]).vector[rowId] != 0;
-  }
-
-  private static byte getByte(VectorizedRowBatch batch, int rowId) {
-    return (byte) ((LongColumnVector) batch.cols[1]).vector[rowId];
-  }
-
-  private static short getShort(VectorizedRowBatch batch, int rowId) {
-    return (short) ((LongColumnVector) batch.cols[2]).vector[rowId];
-  }
-
-  private static int getInt(VectorizedRowBatch batch, int rowId) {
-    return (int) ((LongColumnVector) batch.cols[3]).vector[rowId];
-  }
-
-  private static long getLong(VectorizedRowBatch batch, int rowId) {
-    return ((LongColumnVector) batch.cols[4]).vector[rowId];
-  }
-
-  private static float getFloat(VectorizedRowBatch batch, int rowId) {
-    return (float) ((DoubleColumnVector) batch.cols[5]).vector[rowId];
-  }
-
-  private static double getDouble(VectorizedRowBatch batch, int rowId) {
-    return ((DoubleColumnVector) batch.cols[6]).vector[rowId];
-  }
-
-  private static BytesWritable getBinary(BytesColumnVector column, int rowId) {
-    if (column.isRepeating) {
-      rowId = 0;
-    }
-    if (column.noNulls || !column.isNull[rowId]) {
-      return new BytesWritable(Arrays.copyOfRange(column.vector[rowId],
-          column.start[rowId], column.start[rowId] + column.length[rowId]));
-    } else {
-      return null;
-    }
-  }
-
-  private static BytesWritable getBinary(VectorizedRowBatch batch, int rowId) {
-    return getBinary((BytesColumnVector) batch.cols[7], rowId);
-  }
-
-  private static Text getText(BytesColumnVector vector, int rowId) {
-    if (vector.isRepeating) {
-      rowId = 0;
-    }
-    if (vector.noNulls || !vector.isNull[rowId]) {
-      return new Text(Arrays.copyOfRange(vector.vector[rowId],
-          vector.start[rowId], vector.start[rowId] + vector.length[rowId]));
-    } else {
-      return null;
-    }
-  }
-
-  private static Text getText(VectorizedRowBatch batch, int rowId) {
-    return getText((BytesColumnVector) batch.cols[8], rowId);
-  }
-
-  private static InnerStruct getInner(StructColumnVector vector,
-                                      int rowId) {
-    return new InnerStruct(
-        (int) ((LongColumnVector) vector.fields[0]).vector[rowId],
-        getText((BytesColumnVector) vector.fields[1], rowId));
-  }
-
-  private static List<InnerStruct> getList(ListColumnVector cv,
-                                           int rowId) {
-    if (cv.isRepeating) {
-      rowId = 0;
-    }
-    if (cv.noNulls || !cv.isNull[rowId]) {
-      List<InnerStruct> result =
-          new ArrayList<InnerStruct>((int) cv.lengths[rowId]);
-      for(long i=cv.offsets[rowId];
-          i < cv.offsets[rowId] + cv.lengths[rowId]; ++i) {
-        result.add(getInner((StructColumnVector) cv.child, (int) i));
-      }
-      return result;
-    } else {
-      return null;
-    }
-  }
-
-  private static List<InnerStruct> getMidList(VectorizedRowBatch batch,
-                                              int rowId) {
-    return getList((ListColumnVector) ((StructColumnVector) batch.cols[9])
-        .fields[0], rowId);
-  }
-
-  private static List<InnerStruct> getList(VectorizedRowBatch batch,
-                                           int rowId) {
-    return getList((ListColumnVector) batch.cols[10], rowId);
-  }
-
-  private static Map<Text, InnerStruct> getMap(VectorizedRowBatch batch,
-                                               int rowId) {
-    MapColumnVector cv = (MapColumnVector) batch.cols[11];
-    if (cv.isRepeating) {
-      rowId = 0;
-    }
-    if (cv.noNulls || !cv.isNull[rowId]) {
-      Map<Text, InnerStruct> result =
-          new HashMap<Text, InnerStruct>((int) cv.lengths[rowId]);
-      for(long i=cv.offsets[rowId];
-          i < cv.offsets[rowId] + cv.lengths[rowId]; ++i) {
-        result.put(getText((BytesColumnVector) cv.keys, (int) i),
-            getInner((StructColumnVector) cv.values, (int) i));
-      }
-      return result;
-    } else {
-      return null;
-    }
-  }
-
-  private static TypeDescription createInnerSchema() {
-    return TypeDescription.createStruct()
-        .addField("int1", TypeDescription.createInt())
-        .addField("string1", TypeDescription.createString());
-  }
-
-  private static TypeDescription createBigRowSchema() {
-    return TypeDescription.createStruct()
-        .addField("boolean1", TypeDescription.createBoolean())
-        .addField("byte1", TypeDescription.createByte())
-        .addField("short1", TypeDescription.createShort())
-        .addField("int1", TypeDescription.createInt())
-        .addField("long1", TypeDescription.createLong())
-        .addField("float1", TypeDescription.createFloat())
-        .addField("double1", TypeDescription.createDouble())
-        .addField("bytes1", TypeDescription.createBinary())
-        .addField("string1", TypeDescription.createString())
-        .addField("middle", TypeDescription.createStruct()
-            .addField("list", TypeDescription.createList(createInnerSchema())))
-        .addField("list", TypeDescription.createList(createInnerSchema()))
-        .addField("map", TypeDescription.createMap(
-            TypeDescription.createString(),
-            createInnerSchema()));
-  }
-
-  static void assertArrayEquals(boolean[] expected, boolean[] actual) {
-    assertEquals(expected.length, actual.length);
-    boolean diff = false;
-    for(int i=0; i < expected.length; ++i) {
-      if (expected[i] != actual[i]) {
-        System.out.println("Difference at " + i + " expected: " + expected[i] +
-          " actual: " + actual[i]);
-        diff = true;
-      }
-    }
-    assertEquals(false, diff);
-  }
-
-  @Test
-  public void test1() throws Exception {
-    TypeDescription schema = createBigRowSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 2;
-    setBigRow(batch, 0, false, (byte) 1, (short) 1024, 65536,
-        Long.MAX_VALUE, (float) 1.0, -15.0, bytes(0, 1, 2, 3, 4), "hi",
-        new MiddleStruct(inner(1, "bye"), inner(2, "sigh")),
-        list(inner(3, "good"), inner(4, "bad")),
-        map());
-    setBigRow(batch, 1, true, (byte) 100, (short) 2048, 65536,
-        Long.MAX_VALUE, (float) 2.0, -5.0, bytes(), "bye",
-        new MiddleStruct(inner(1, "bye"), inner(2, "sigh")),
-        list(inner(100000000, "cat"), inner(-100000, "in"), inner(1234, "hat")),
-        map(inner(5, "chani"), inner(1, "mauddib")));
-    writer.addRowBatch(batch);
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    schema = writer.getSchema();
-    assertEquals(23, schema.getMaximumId());
-    boolean[] expected = new boolean[] {false, false, false, false, false,
-        false, false, false, false, false,
-        false, false, false, false, false,
-        false, false, false, false, false,
-        false, false, false, false};
-    boolean[] included = OrcUtils.includeColumns("", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    expected = new boolean[] {false, true, false, false, false,
-        false, false, false, false, true,
-        true, true, true, true, true,
-        false, false, false, false, true,
-        true, true, true, true};
-    included = OrcUtils.includeColumns("boolean1,string1,middle,map", schema);
-
-    assertArrayEquals(expected, included);
-
-    expected = new boolean[] {false, true, false, false, false,
-        false, false, false, false, true,
-        true, true, true, true, true,
-        false, false, false, false, true,
-        true, true, true, true};
-    included = OrcUtils.includeColumns("boolean1,string1,middle,map", schema);
-    assertArrayEquals(expected, included);
-
-    expected = new boolean[] {false, true, true, true, true,
-        true, true, true, true, true,
-        true, true, true, true, true,
-        true, true, true, true, true,
-        true, true, true, true};
-    included = OrcUtils.includeColumns(
-        "boolean1,byte1,short1,int1,long1,float1,double1,bytes1,string1,middle,list,map",
-        schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(2, stats[1].getNumberOfValues());
-    assertEquals(1, ((BooleanColumnStatistics) stats[1]).getFalseCount());
-    assertEquals(1, ((BooleanColumnStatistics) stats[1]).getTrueCount());
-    assertEquals("count: 2 hasNull: false true: 1", stats[1].toString());
-
-    assertEquals(2048, ((IntegerColumnStatistics) stats[3]).getMaximum());
-    assertEquals(1024, ((IntegerColumnStatistics) stats[3]).getMinimum());
-    assertEquals(true, ((IntegerColumnStatistics) stats[3]).isSumDefined());
-    assertEquals(3072, ((IntegerColumnStatistics) stats[3]).getSum());
-    assertEquals("count: 2 hasNull: false min: 1024 max: 2048 sum: 3072",
-        stats[3].toString());
-
-    StripeStatistics ss = reader.getStripeStatistics().get(0);
-    assertEquals(2, ss.getColumnStatistics()[0].getNumberOfValues());
-    assertEquals(1, ((BooleanColumnStatistics) ss.getColumnStatistics()[1]).getTrueCount());
-    assertEquals(1024, ((IntegerColumnStatistics) ss.getColumnStatistics()[3]).getMinimum());
-    assertEquals(2048, ((IntegerColumnStatistics) ss.getColumnStatistics()[3]).getMaximum());
-    assertEquals(3072, ((IntegerColumnStatistics) ss.getColumnStatistics()[3]).getSum());
-    assertEquals(-15.0, ((DoubleColumnStatistics) stats[7]).getMinimum(), 0.0001);
-    assertEquals(-5.0, ((DoubleColumnStatistics) stats[7]).getMaximum(), 0.0001);
-    assertEquals(-20.0, ((DoubleColumnStatistics) stats[7]).getSum(), 0.00001);
-    assertEquals("count: 2 hasNull: false min: -15.0 max: -5.0 sum: -20.0",
-        stats[7].toString());
-
-    assertEquals("count: 2 hasNull: false min: bye max: hi sum: 5", stats[9].toString());
-
-    // check the schema
-    TypeDescription readerSchema = reader.getSchema();
-    assertEquals(TypeDescription.Category.STRUCT, readerSchema.getCategory());
-    assertEquals("struct<boolean1:boolean,byte1:tinyint,short1:smallint,"
-        + "int1:int,long1:bigint,float1:float,double1:double,bytes1:"
-        + "binary,string1:string,middle:struct<list:array<struct<int1:int,"
-        + "string1:string>>>,list:array<struct<int1:int,string1:string>>,"
-        + "map:map<string,struct<int1:int,string1:string>>>",
-        readerSchema.toString());
-    List<String> fieldNames = readerSchema.getFieldNames();
-    List<TypeDescription> fieldTypes = readerSchema.getChildren();
-    assertEquals("boolean1", fieldNames.get(0));
-    assertEquals(TypeDescription.Category.BOOLEAN, fieldTypes.get(0).getCategory());
-    assertEquals("byte1", fieldNames.get(1));
-    assertEquals(TypeDescription.Category.BYTE, fieldTypes.get(1).getCategory());
-    assertEquals("short1", fieldNames.get(2));
-    assertEquals(TypeDescription.Category.SHORT, fieldTypes.get(2).getCategory());
-    assertEquals("int1", fieldNames.get(3));
-    assertEquals(TypeDescription.Category.INT, fieldTypes.get(3).getCategory());
-    assertEquals("long1", fieldNames.get(4));
-    assertEquals(TypeDescription.Category.LONG, fieldTypes.get(4).getCategory());
-    assertEquals("float1", fieldNames.get(5));
-    assertEquals(TypeDescription.Category.FLOAT, fieldTypes.get(5).getCategory());
-    assertEquals("double1", fieldNames.get(6));
-    assertEquals(TypeDescription.Category.DOUBLE, fieldTypes.get(6).getCategory());
-    assertEquals("bytes1", fieldNames.get(7));
-    assertEquals(TypeDescription.Category.BINARY, fieldTypes.get(7).getCategory());
-    assertEquals("string1", fieldNames.get(8));
-    assertEquals(TypeDescription.Category.STRING, fieldTypes.get(8).getCategory());
-    assertEquals("middle", fieldNames.get(9));
-    TypeDescription middle = fieldTypes.get(9);
-    assertEquals(TypeDescription.Category.STRUCT, middle.getCategory());
-    TypeDescription midList = middle.getChildren().get(0);
-    assertEquals(TypeDescription.Category.LIST, midList.getCategory());
-    TypeDescription inner = midList.getChildren().get(0);
-    assertEquals(TypeDescription.Category.STRUCT, inner.getCategory());
-    assertEquals("int1", inner.getFieldNames().get(0));
-    assertEquals("string1", inner.getFieldNames().get(1));
-
-    RecordReader rows = reader.rows();
-    // create a new batch
-    batch = readerSchema.createRowBatch();
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(2, batch.size);
-    Assert.assertEquals(false, rows.nextBatch(batch));
-
-    // check the contents of the first row
-    assertEquals(false, getBoolean(batch, 0));
-    assertEquals(1, getByte(batch, 0));
-    assertEquals(1024, getShort(batch, 0));
-    assertEquals(65536, getInt(batch, 0));
-    assertEquals(Long.MAX_VALUE, getLong(batch, 0));
-    assertEquals(1.0, getFloat(batch, 0), 0.00001);
-    assertEquals(-15.0, getDouble(batch, 0), 0.00001);
-    assertEquals(bytes(0,1,2,3,4), getBinary(batch, 0));
-    assertEquals("hi", getText(batch, 0).toString());
-    List<InnerStruct> midRow = getMidList(batch, 0);
-    assertNotNull(midRow);
-    assertEquals(2, midRow.size());
-    assertEquals(1, midRow.get(0).int1);
-    assertEquals("bye", midRow.get(0).string1.toString());
-    assertEquals(2, midRow.get(1).int1);
-    assertEquals("sigh", midRow.get(1).string1.toString());
-    List<InnerStruct> list = getList(batch, 0);
-    assertEquals(2, list.size());
-    assertEquals(3, list.get(0).int1);
-    assertEquals("good", list.get(0).string1.toString());
-    assertEquals(4, list.get(1).int1);
-    assertEquals("bad", list.get(1).string1.toString());
-    Map<Text, InnerStruct> map = getMap(batch, 0);
-    assertEquals(0, map.size());
-
-    // check the contents of second row
-    assertEquals(true, getBoolean(batch, 1));
-    assertEquals(100, getByte(batch, 1));
-    assertEquals(2048, getShort(batch, 1));
-    assertEquals(65536, getInt(batch, 1));
-    assertEquals(Long.MAX_VALUE, getLong(batch, 1));
-    assertEquals(2.0, getFloat(batch, 1), 0.00001);
-    assertEquals(-5.0, getDouble(batch, 1), 0.00001);
-    assertEquals(bytes(), getBinary(batch, 1));
-    assertEquals("bye", getText(batch, 1).toString());
-    midRow = getMidList(batch, 1);
-    assertNotNull(midRow);
-    assertEquals(2, midRow.size());
-    assertEquals(1, midRow.get(0).int1);
-    assertEquals("bye", midRow.get(0).string1.toString());
-    assertEquals(2, midRow.get(1).int1);
-    assertEquals("sigh", midRow.get(1).string1.toString());
-    list = getList(batch, 1);
-    assertEquals(3, list.size());
-    assertEquals(100000000, list.get(0).int1);
-    assertEquals("cat", list.get(0).string1.toString());
-    assertEquals(-100000, list.get(1).int1);
-    assertEquals("in", list.get(1).string1.toString());
-    assertEquals(1234, list.get(2).int1);
-    assertEquals("hat", list.get(2).string1.toString());
-    map = getMap(batch, 1);
-    assertEquals(2, map.size());
-    InnerStruct value = map.get(new Text("chani"));
-    assertEquals(5, value.int1);
-    assertEquals("chani", value.string1.toString());
-    value = map.get(new Text("mauddib"));
-    assertEquals(1, value.int1);
-    assertEquals("mauddib", value.string1.toString());
-
-    // handle the close up
-    Assert.assertEquals(false, rows.nextBatch(batch));
-    rows.close();
-  }
-
-  @Test
-  public void testColumnProjection() throws Exception {
-    TypeDescription schema = createInnerSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(1000)
-                                         .compress(CompressionKind.NONE)
-                                         .bufferSize(100)
-                                         .rowIndexStride(1000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    Random r1 = new Random(1);
-    Random r2 = new Random(2);
-    int x;
-    int minInt=0, maxInt=0;
-    String y;
-    String minStr = null, maxStr = null;
-    batch.size = 1000;
-    boolean first = true;
-    for(int b=0; b < 21; ++b) {
-      for(int r=0; r < 1000; ++r) {
-        x = r1.nextInt();
-        y = Long.toHexString(r2.nextLong());
-        if (first || x < minInt) {
-          minInt = x;
-        }
-        if (first || x > maxInt) {
-          maxInt = x;
-        }
-        if (first || y.compareTo(minStr) < 0) {
-          minStr = y;
-        }
-        if (first || y.compareTo(maxStr) > 0) {
-          maxStr = y;
-        }
-        first = false;
-        ((LongColumnVector) batch.cols[0]).vector[r] = x;
-        ((BytesColumnVector) batch.cols[1]).setVal(r, y.getBytes());
-      }
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    // check out the statistics
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(3, stats.length);
-    for(ColumnStatistics s: stats) {
-      assertEquals(21000, s.getNumberOfValues());
-      if (s instanceof IntegerColumnStatistics) {
-        assertEquals(minInt, ((IntegerColumnStatistics) s).getMinimum());
-        assertEquals(maxInt, ((IntegerColumnStatistics) s).getMaximum());
-      } else if (s instanceof  StringColumnStatistics) {
-        assertEquals(maxStr, ((StringColumnStatistics) s).getMaximum());
-        assertEquals(minStr, ((StringColumnStatistics) s).getMinimum());
-      }
-    }
-
-    // check out the types
-    TypeDescription type = reader.getSchema();
-    assertEquals(TypeDescription.Category.STRUCT, type.getCategory());
-    assertEquals(2, type.getChildren().size());
-    TypeDescription type1 = type.getChildren().get(0);
-    TypeDescription type2 = type.getChildren().get(1);
-    assertEquals(TypeDescription.Category.INT, type1.getCategory());
-    assertEquals(TypeDescription.Category.STRING, type2.getCategory());
-    assertEquals("struct<int1:int,string1:string>", type.toString());
-
-    // read the contents and make sure they match
-    RecordReader rows1 = reader.rows(
-        new Reader.Options().include(new boolean[]{true, true, false}));
-    RecordReader rows2 = reader.rows(
-        new Reader.Options().include(new boolean[]{true, false, true}));
-    r1 = new Random(1);
-    r2 = new Random(2);
-    VectorizedRowBatch batch1 = reader.getSchema().createRowBatch(1000);
-    VectorizedRowBatch batch2 = reader.getSchema().createRowBatch(1000);
-    for(int i = 0; i < 21000; i += 1000) {
-      Assert.assertEquals(true, rows1.nextBatch(batch1));
-      Assert.assertEquals(true, rows2.nextBatch(batch2));
-      assertEquals(1000, batch1.size);
-      assertEquals(1000, batch2.size);
-      for(int j=0; j < 1000; ++j) {
-        assertEquals(r1.nextInt(),
-            ((LongColumnVector) batch1.cols[0]).vector[j]);
-        assertEquals(Long.toHexString(r2.nextLong()),
-            ((BytesColumnVector) batch2.cols[1]).toString(j));
-      }
-    }
-    Assert.assertEquals(false, rows1.nextBatch(batch1));
-    Assert.assertEquals(false, rows2.nextBatch(batch2));
-    rows1.close();
-    rows2.close();
-  }
-
-  @Test
-  public void testEmptyFile() throws Exception {
-    TypeDescription schema = createBigRowSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(1000)
-                                         .compress(CompressionKind.NONE)
-                                         .bufferSize(100));
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    VectorizedRowBatch batch = reader.getSchema().createRowBatch();
-    Assert.assertEquals(false, reader.rows().nextBatch(batch));
-    Assert.assertEquals(CompressionKind.NONE, reader.getCompressionKind());
-    Assert.assertEquals(0, reader.getNumberOfRows());
-    Assert.assertEquals(0, reader.getCompressionSize());
-    Assert.assertEquals(false, reader.getMetadataKeys().iterator().hasNext());
-    Assert.assertEquals(3, reader.getContentLength());
-    Assert.assertEquals(false, reader.getStripes().iterator().hasNext());
-  }
-
-  @Test
-  public void metaData() throws Exception {
-    TypeDescription schema = createBigRowSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(1000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(100));
-    writer.addUserMetadata("my.meta", byteBuf(1, 2, 3, 4, 5, 6, 7, -1, -2, 127,
-                                              -128));
-    writer.addUserMetadata("clobber", byteBuf(1, 2, 3));
-    writer.addUserMetadata("clobber", byteBuf(4, 3, 2, 1));
-    ByteBuffer bigBuf = ByteBuffer.allocate(40000);
-    Random random = new Random(0);
-    random.nextBytes(bigBuf.array());
-    writer.addUserMetadata("big", bigBuf);
-    bigBuf.position(0);
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1;
-    setBigRow(batch, 0, true, (byte) 127, (short) 1024, 42,
-        42L * 1024 * 1024 * 1024, (float) 3.1415, -2.713, null,
-        null, null, null, null);
-    writer.addRowBatch(batch);
-    writer.addUserMetadata("clobber", byteBuf(5,7,11,13,17,19));
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    Assert.assertEquals(byteBuf(5, 7, 11, 13, 17, 19), reader.getMetadataValue("clobber"));
-    Assert.assertEquals(byteBuf(1, 2, 3, 4, 5, 6, 7, -1, -2, 127, -128),
-        reader.getMetadataValue("my.meta"));
-    Assert.assertEquals(bigBuf, reader.getMetadataValue("big"));
-    try {
-      reader.getMetadataValue("unknown");
-      assertTrue(false);
-    } catch (IllegalArgumentException iae) {
-      // PASS
-    }
-    int i = 0;
-    for(String key: reader.getMetadataKeys()) {
-      if ("my.meta".equals(key) ||
-          "clobber".equals(key) ||
-          "big".equals(key)) {
-        i += 1;
-      } else {
-        throw new IllegalArgumentException("unknown key " + key);
-      }
-    }
-    assertEquals(3, i);
-    int numStripes = reader.getStripeStatistics().size();
-    assertEquals(1, numStripes);
-  }
-
-  /**
-   * Generate an ORC file with a range of dates and times.
-   */
-  public void createOrcDateFile(Path file, int minYear, int maxYear
-                                ) throws IOException {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("time", TypeDescription.createTimestamp())
-        .addField("date", TypeDescription.createDate());
-    Writer writer = OrcFile.createWriter(file,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .bufferSize(10000)
-            .blockPadding(false));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1000;
-    for (int year = minYear; year < maxYear; ++year) {
-      for (int ms = 1000; ms < 2000; ++ms) {
-        TimestampColumnVector timestampColVector = (TimestampColumnVector) batch.cols[0];
-        timestampColVector.set(ms - 1000,
-            Timestamp.valueOf(year +
-                "-05-05 12:34:56." + ms));
-        ((LongColumnVector) batch.cols[1]).vector[ms - 1000] =
-            new DateWritable(new Date(year - 1900, 11, 25)).getDays();
-      }
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    Reader reader = OrcFile.createReader(file,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch(1000);
-    TimestampColumnVector times = (TimestampColumnVector) batch.cols[0];
-    LongColumnVector dates = (LongColumnVector) batch.cols[1];
-    for (int year = minYear; year < maxYear; ++year) {
-      rows.nextBatch(batch);
-      assertEquals(1000, batch.size);
-      for(int ms = 1000; ms < 2000; ++ms) {
-        StringBuilder buffer = new StringBuilder();
-        times.stringifyValue(buffer, ms - 1000);
-        String expected = Integer.toString(year) + "-05-05 12:34:56.";
-        // suppress the final zeros on the string by dividing by the largest
-        // power of 10 that divides evenly.
-        int roundedMs = ms;
-        for(int round = 1000; round > 0; round /= 10) {
-          if (ms % round == 0) {
-            roundedMs = ms / round;
-            break;
-          }
-        }
-        expected += roundedMs;
-        assertEquals(expected, buffer.toString());
-        assertEquals(Integer.toString(year) + "-12-25",
-            new DateWritable((int) dates.vector[ms - 1000]).toString());
-      }
-    }
-    rows.nextBatch(batch);
-    assertEquals(0, batch.size);
-  }
-
-  @Test
-  public void testDate1900() throws Exception {
-    createOrcDateFile(testFilePath, 1900, 1970);
-  }
-
-  @Test
-  public void testDate2038() throws Exception {
-    createOrcDateFile(testFilePath, 2038, 2250);
-  }
-
-  private static void setUnion(VectorizedRowBatch batch, int rowId,
-                               Timestamp ts, Integer tag, Integer i, String s,
-                               HiveDecimalWritable dec) {
-    UnionColumnVector union = (UnionColumnVector) batch.cols[1];
-    if (ts != null) {
-      TimestampColumnVector timestampColVector = (TimestampColumnVector) batch.cols[0];
-      timestampColVector.set(rowId, ts);
-    } else {
-      batch.cols[0].isNull[rowId] = true;
-      batch.cols[0].noNulls = false;
-    }
-    if (tag != null) {
-      union.tags[rowId] = tag;
-      if (tag == 0) {
-        if (i != null) {
-          ((LongColumnVector) union.fields[tag]).vector[rowId] = i;
-        } else {
-          union.fields[tag].isNull[rowId] = true;
-          union.fields[tag].noNulls = false;
-        }
-      } else if (tag == 1) {
-        if (s != null) {
-          ((BytesColumnVector) union.fields[tag]).setVal(rowId, s.getBytes());
-        } else {
-          union.fields[tag].isNull[rowId] = true;
-          union.fields[tag].noNulls = false;
-        }
-      } else {
-        throw new IllegalArgumentException("Bad tag " + tag);
-      }
-    } else {
-      batch.cols[1].isNull[rowId] = true;
-      batch.cols[1].noNulls = false;
-    }
-    if (dec != null) {
-      ((DecimalColumnVector) batch.cols[2]).vector[rowId] = dec;
-    } else {
-      batch.cols[2].isNull[rowId] = true;
-      batch.cols[2].noNulls = false;
-    }
-  }
-
-  /**
-     * We test union, timestamp, and decimal separately since we need to make the
-     * object inspector manually. (The Hive reflection-based doesn't handle
-     * them properly.)
-     */
-  @Test
-  public void testUnionAndTimestamp() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("time", TypeDescription.createTimestamp())
-        .addField("union", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createInt())
-            .addUnionChild(TypeDescription.createString()))
-        .addField("decimal", TypeDescription.createDecimal()
-            .withPrecision(38)
-            .withScale(18));
-    HiveDecimal maxValue = HiveDecimal.create("10000000000000000000");
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(1000)
-                                         .compress(CompressionKind.NONE)
-                                         .bufferSize(100)
-                                         .blockPadding(false));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 6;
-    setUnion(batch, 0, Timestamp.valueOf("2000-03-12 15:00:00"), 0, 42, null,
-             new HiveDecimalWritable("12345678.6547456"));
-    setUnion(batch, 1, Timestamp.valueOf("2000-03-20 12:00:00.123456789"),
-        1, null, "hello", new HiveDecimalWritable("-5643.234"));
-
-    setUnion(batch, 2, null, null, null, null, null);
-    setUnion(batch, 3, null, 0, null, null, null);
-    setUnion(batch, 4, null, 1, null, null, null);
-
-    setUnion(batch, 5, Timestamp.valueOf("1970-01-01 00:00:00"), 0, 200000,
-        null, new HiveDecimalWritable("10000000000000000000"));
-    writer.addRowBatch(batch);
-
-    batch.reset();
-    Random rand = new Random(42);
-    for(int i=1970; i < 2038; ++i) {
-      Timestamp ts = Timestamp.valueOf(i + "-05-05 12:34:56." + i);
-      HiveDecimal dec =
-          HiveDecimal.create(new BigInteger(64, rand), rand.nextInt(18));
-      if ((i & 1) == 0) {
-        setUnion(batch, batch.size++, ts, 0, i*i, null,
-            new HiveDecimalWritable(dec));
-      } else {
-        setUnion(batch, batch.size++, ts, 1, null, Integer.toString(i*i),
-            new HiveDecimalWritable(dec));
-      }
-      if (maxValue.compareTo(dec) < 0) {
-        maxValue = dec;
-      }
-    }
-    writer.addRowBatch(batch);
-    batch.reset();
-
-    // let's add a lot of constant rows to test the rle
-    batch.size = 1000;
-    for(int c=0; c < batch.cols.length; ++c) {
-      batch.cols[c].setRepeating(true);
-    }
-    ((UnionColumnVector) batch.cols[1]).fields[0].isRepeating = true;
-    setUnion(batch, 0, null, 0, 1732050807, null, null);
-    for(int i=0; i < 5; ++i) {
-      writer.addRowBatch(batch);
-    }
-
-    batch.reset();
-    batch.size = 3;
-    setUnion(batch, 0, null, 0, 0, null, null);
-    setUnion(batch, 1, null, 0, 10, null, null);
-    setUnion(batch, 2, null, 0, 138, null, null);
-    writer.addRowBatch(batch);
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    schema = writer.getSchema();
-    assertEquals(5, schema.getMaximumId());
-    boolean[] expected = new boolean[] {false, false, false, false, false, false};
-    boolean[] included = OrcUtils.includeColumns("", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    expected = new boolean[] {false, true, false, false, false, true};
-    included = OrcUtils.includeColumns("time,decimal", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    expected = new boolean[] {false, false, true, true, true, false};
-    included = OrcUtils.includeColumns("union", schema);
-    assertEquals(true, Arrays.equals(expected, included));
-
-    Assert.assertEquals(false, reader.getMetadataKeys().iterator().hasNext());
-    Assert.assertEquals(5077, reader.getNumberOfRows());
-    DecimalColumnStatistics stats =
-        (DecimalColumnStatistics) reader.getStatistics()[5];
-    assertEquals(71, stats.getNumberOfValues());
-    assertEquals(HiveDecimal.create("-5643.234"), stats.getMinimum());
-    assertEquals(maxValue, stats.getMaximum());
-    // TODO: fix this
-//    assertEquals(null,stats.getSum());
-    int stripeCount = 0;
-    int rowCount = 0;
-    long currentOffset = -1;
-    for(StripeInformation stripe: reader.getStripes()) {
-      stripeCount += 1;
-      rowCount += stripe.getNumberOfRows();
-      if (currentOffset < 0) {
-        currentOffset = stripe.getOffset() + stripe.getLength();
-      } else {
-        assertEquals(currentOffset, stripe.getOffset());
-        currentOffset += stripe.getLength();
-      }
-    }
-    Assert.assertEquals(reader.getNumberOfRows(), rowCount);
-    assertEquals(2, stripeCount);
-    Assert.assertEquals(reader.getContentLength(), currentOffset);
-    RecordReader rows = reader.rows();
-    Assert.assertEquals(0, rows.getRowNumber());
-    Assert.assertEquals(0.0, rows.getProgress(), 0.000001);
-
-    schema = reader.getSchema();
-    batch = schema.createRowBatch(74);
-    Assert.assertEquals(0, rows.getRowNumber());
-    rows.nextBatch(batch);
-    assertEquals(74, batch.size);
-    Assert.assertEquals(74, rows.getRowNumber());
-    TimestampColumnVector ts = (TimestampColumnVector) batch.cols[0];
-    UnionColumnVector union = (UnionColumnVector) batch.cols[1];
-    LongColumnVector longs = (LongColumnVector) union.fields[0];
-    BytesColumnVector strs = (BytesColumnVector) union.fields[1];
-    DecimalColumnVector decs = (DecimalColumnVector) batch.cols[2];
-
-    assertEquals("struct<time:timestamp,union:uniontype<int,string>,decimal:decimal(38,18)>",
-        schema.toString());
-    assertEquals("2000-03-12 15:00:00.0", ts.asScratchTimestamp(0).toString());
-    assertEquals(0, union.tags[0]);
-    assertEquals(42, longs.vector[0]);
-    assertEquals("12345678.6547456", decs.vector[0].toString());
-
-    assertEquals("2000-03-20 12:00:00.123456789", ts.asScratchTimestamp(1).toString());
-    assertEquals(1, union.tags[1]);
-    assertEquals("hello", strs.toString(1));
-    assertEquals("-5643.234", decs.vector[1].toString());
-
-    assertEquals(false, ts.noNulls);
-    assertEquals(false, union.noNulls);
-    assertEquals(false, decs.noNulls);
-    assertEquals(true, ts.isNull[2]);
-    assertEquals(true, union.isNull[2]);
-    assertEquals(true, decs.isNull[2]);
-
-    assertEquals(true, ts.isNull[3]);
-    assertEquals(false, union.isNull[3]);
-    assertEquals(0, union.tags[3]);
-    assertEquals(true, longs.isNull[3]);
-    assertEquals(true, decs.isNull[3]);
-
-    assertEquals(true, ts.isNull[4]);
-    assertEquals(false, union.isNull[4]);
-    assertEquals(1, union.tags[4]);
-    assertEquals(true, strs.isNull[4]);
-    assertEquals(true, decs.isNull[4]);
-
-    assertEquals(false, ts.isNull[5]);
-    assertEquals("1970-01-01 00:00:00.0", ts.asScratchTimestamp(5).toString());
-    assertEquals(false, union.isNull[5]);
-    assertEquals(0, union.tags[5]);
-    assertEquals(false, longs.isNull[5]);
-    assertEquals(200000, longs.vector[5]);
-    assertEquals(false, decs.isNull[5]);
-    assertEquals("10000000000000000000", decs.vector[5].toString());
-
-    rand = new Random(42);
-    for(int i=1970; i < 2038; ++i) {
-      int row = 6 + i - 1970;
-      assertEquals(Timestamp.valueOf(i + "-05-05 12:34:56." + i),
-          ts.asScratchTimestamp(row));
-      if ((i & 1) == 0) {
-        assertEquals(0, union.tags[row]);
-        assertEquals(i*i, longs.vector[row]);
-      } else {
-        assertEquals(1, union.tags[row]);
-        assertEquals(Integer.toString(i * i), strs.toString(row));
-      }
-      assertEquals(new HiveDecimalWritable(HiveDecimal.create(new BigInteger(64, rand),
-                                   rand.nextInt(18))), decs.vector[row]);
-    }
-
-    // rebuild the row batch, so that we can read by 1000 rows
-    batch = schema.createRowBatch(1000);
-    ts = (TimestampColumnVector) batch.cols[0];
-    union = (UnionColumnVector) batch.cols[1];
-    longs = (LongColumnVector) union.fields[0];
-    strs = (BytesColumnVector) union.fields[1];
-    decs = (DecimalColumnVector) batch.cols[2];
-
-    for(int i=0; i < 5; ++i) {
-      rows.nextBatch(batch);
-      assertEquals("batch " + i, 1000, batch.size);
-      assertEquals("batch " + i, false, union.isRepeating);
-      assertEquals("batch " + i, true, union.noNulls);
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals("bad tag at " + i + "." +r, 0, union.tags[r]);
-      }
-      assertEquals("batch " + i, true, longs.isRepeating);
-      assertEquals("batch " + i, 1732050807, longs.vector[0]);
-    }
-
-    rows.nextBatch(batch);
-    assertEquals(3, batch.size);
-    assertEquals(0, union.tags[0]);
-    assertEquals(0, longs.vector[0]);
-    assertEquals(0, union.tags[1]);
-    assertEquals(10, longs.vector[1]);
-    assertEquals(0, union.tags[2]);
-    assertEquals(138, longs.vector[2]);
-
-    rows.nextBatch(batch);
-    assertEquals(0, batch.size);
-    Assert.assertEquals(1.0, rows.getProgress(), 0.00001);
-    Assert.assertEquals(reader.getNumberOfRows(), rows.getRowNumber());
-    rows.seekToRow(1);
-    rows.nextBatch(batch);
-    assertEquals(1000, batch.size);
-    assertEquals(Timestamp.valueOf("2000-03-20 12:00:00.123456789"), ts.asScratchTimestamp(0));
-    assertEquals(1, union.tags[0]);
-    assertEquals("hello", strs.toString(0));
-    assertEquals(new HiveDecimalWritable(HiveDecimal.create("-5643.234")), decs.vector[0]);
-    rows.close();
-  }
-
-  /**
-   * Read and write a randomly generated snappy file.
-   * @throws Exception
-   */
-  @Test
-  public void testSnappy() throws Exception {
-    TypeDescription schema = createInnerSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(1000)
-                                         .compress(CompressionKind.SNAPPY)
-                                         .bufferSize(100));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    Random rand = new Random(12);
-    batch.size = 1000;
-    for(int b=0; b < 10; ++b) {
-      for (int r=0; r < 1000; ++r) {
-        ((LongColumnVector) batch.cols[0]).vector[r] = rand.nextInt();
-        ((BytesColumnVector) batch.cols[1]).setVal(r,
-            Integer.toHexString(rand.nextInt()).getBytes());
-      }
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    Assert.assertEquals(CompressionKind.SNAPPY, reader.getCompressionKind());
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch(1000);
-    rand = new Random(12);
-    LongColumnVector longs = (LongColumnVector) batch.cols[0];
-    BytesColumnVector strs = (BytesColumnVector) batch.cols[1];
-    for(int b=0; b < 10; ++b) {
-      rows.nextBatch(batch);
-      assertEquals(1000, batch.size);
-      for(int r=0; r < batch.size; ++r) {
-        assertEquals(rand.nextInt(), longs.vector[r]);
-        assertEquals(Integer.toHexString(rand.nextInt()), strs.toString(r));
-      }
-    }
-    rows.nextBatch(batch);
-    assertEquals(0, batch.size);
-    rows.close();
-  }
-
-  /**
-   * Read and write a randomly generated snappy file.
-   * @throws Exception
-   */
-  @Test
-  public void testWithoutIndex() throws Exception {
-    TypeDescription schema = createInnerSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(5000)
-                                         .compress(CompressionKind.SNAPPY)
-                                         .bufferSize(1000)
-                                         .rowIndexStride(0));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    Random rand = new Random(24);
-    batch.size = 5;
-    for(int c=0; c < batch.cols.length; ++c) {
-      batch.cols[c].setRepeating(true);
-    }
-    for(int i=0; i < 10000; ++i) {
-      ((LongColumnVector) batch.cols[0]).vector[0] = rand.nextInt();
-      ((BytesColumnVector) batch.cols[1])
-          .setVal(0, Integer.toBinaryString(rand.nextInt()).getBytes());
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    Assert.assertEquals(50000, reader.getNumberOfRows());
-    Assert.assertEquals(0, reader.getRowIndexStride());
-    StripeInformation stripe = reader.getStripes().iterator().next();
-    assertEquals(true, stripe.getDataLength() != 0);
-    assertEquals(0, stripe.getIndexLength());
-    RecordReader rows = reader.rows();
-    rand = new Random(24);
-    batch = reader.getSchema().createRowBatch(1000);
-    LongColumnVector longs = (LongColumnVector) batch.cols[0];
-    BytesColumnVector strs = (BytesColumnVector) batch.cols[1];
-    for(int i=0; i < 50; ++i) {
-      rows.nextBatch(batch);
-      assertEquals("batch " + i, 1000, batch.size);
-      for(int j=0; j < 200; ++j) {
-        int intVal = rand.nextInt();
-        String strVal = Integer.toBinaryString(rand.nextInt());
-        for (int k = 0; k < 5; ++k) {
-          assertEquals(intVal, longs.vector[j * 5 + k]);
-          assertEquals(strVal, strs.toString(j * 5 + k));
-        }
-      }
-    }
-    rows.nextBatch(batch);
-    assertEquals(0, batch.size);
-    rows.close();
-  }
-
-  @Test
-  public void testSeek() throws Exception {
-    TypeDescription schema = createBigRowSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .stripeSize(200000)
-                                         .bufferSize(65536)
-                                         .rowIndexStride(1000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    Random rand = new Random(42);
-    final int COUNT=32768;
-    long[] intValues= new long[COUNT];
-    double[] doubleValues = new double[COUNT];
-    String[] stringValues = new String[COUNT];
-    BytesWritable[] byteValues = new BytesWritable[COUNT];
-    String[] words = new String[128];
-    for(int i=0; i < words.length; ++i) {
-      words[i] = Integer.toHexString(rand.nextInt());
-    }
-    for(int i=0; i < COUNT/2; ++i) {
-      intValues[2*i] = rand.nextLong();
-      intValues[2*i+1] = intValues[2*i];
-      stringValues[2*i] = words[rand.nextInt(words.length)];
-      stringValues[2*i+1] = stringValues[2*i];
-    }
-    for(int i=0; i < COUNT; ++i) {
-      doubleValues[i] = rand.nextDouble();
-      byte[] buf = new byte[20];
-      rand.nextBytes(buf);
-      byteValues[i] = new BytesWritable(buf);
-    }
-    for(int i=0; i < COUNT; ++i) {
-      appendRandomRow(batch, intValues, doubleValues, stringValues,
-          byteValues, words, i);
-      if (batch.size == 1024) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-    }
-    if (batch.size != 0) {
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    Assert.assertEquals(COUNT, reader.getNumberOfRows());
-    RecordReader rows = reader.rows();
-    // get the row index
-    DataReader meta = RecordReaderUtils.createDefaultDataReader(
-        DataReaderProperties.builder()
-            .withBufferSize(reader.getCompressionSize())
-            .withFileSystem(fs)
-            .withPath(testFilePath)
-            .withCompression(reader.getCompressionKind())
-            .withTypeCount(reader.getSchema().getMaximumId() + 1)
-            .withZeroCopy(false)
-            .build());
-    OrcIndex index =
-        meta.readRowIndex(reader.getStripes().get(0), null, null, null, null,
-            null);
-    // check the primitive columns to make sure they have the right number of
-    // items in the first row group
-    for(int c=1; c < 9; ++c) {
-      OrcProto.RowIndex colIndex = index.getRowGroupIndex()[c];
-      assertEquals(1000,
-          colIndex.getEntry(0).getStatistics().getNumberOfValues());
-    }
-    batch = reader.getSchema().createRowBatch();
-    int nextRowInBatch = -1;
-    for(int i=COUNT-1; i >= 0; --i, --nextRowInBatch) {
-      // if we have consumed the previous batch read a new one
-      if (nextRowInBatch < 0) {
-        long base = Math.max(i - 1023, 0);
-        rows.seekToRow(base);
-        Assert.assertEquals("row " + i, true, rows.nextBatch(batch));
-        nextRowInBatch = batch.size - 1;
-      }
-      checkRandomRow(batch, intValues, doubleValues,
-          stringValues, byteValues, words, i, nextRowInBatch);
-    }
-    rows.close();
-    Iterator<StripeInformation> stripeIterator =
-      reader.getStripes().iterator();
-    long offsetOfStripe2 = 0;
-    long offsetOfStripe4 = 0;
-    long lastRowOfStripe2 = 0;
-    for(int i = 0; i < 5; ++i) {
-      StripeInformation stripe = stripeIterator.next();
-      if (i < 2) {
-        lastRowOfStripe2 += stripe.getNumberOfRows();
-      } else if (i == 2) {
-        offsetOfStripe2 = stripe.getOffset();
-        lastRowOfStripe2 += stripe.getNumberOfRows() - 1;
-      } else if (i == 4) {
-        offsetOfStripe4 = stripe.getOffset();
-      }
-    }
-    boolean[] columns = new boolean[reader.getStatistics().length];
-    columns[5] = true; // long colulmn
-    columns[9] = true; // text column
-    rows = reader.rows(new Reader.Options()
-        .range(offsetOfStripe2, offsetOfStripe4 - offsetOfStripe2)
-        .include(columns));
-    rows.seekToRow(lastRowOfStripe2);
-    // we only want two rows
-    batch = reader.getSchema().createRowBatch(2);
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1, batch.size);
-    assertEquals(intValues[(int) lastRowOfStripe2], getLong(batch, 0));
-    assertEquals(stringValues[(int) lastRowOfStripe2],
-        getText(batch, 0).toString());
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(intValues[(int) lastRowOfStripe2 + 1], getLong(batch, 0));
-    assertEquals(stringValues[(int) lastRowOfStripe2 + 1],
-        getText(batch, 0).toString());
-    rows.close();
-  }
-
-  private void appendRandomRow(VectorizedRowBatch batch,
-                               long[] intValues, double[] doubleValues,
-                               String[] stringValues,
-                               BytesWritable[] byteValues,
-                               String[] words, int i) {
-    InnerStruct inner = new InnerStruct((int) intValues[i], stringValues[i]);
-    InnerStruct inner2 = new InnerStruct((int) (intValues[i] >> 32),
-        words[i % words.length] + "-x");
-    setBigRow(batch, batch.size++, (intValues[i] & 1) == 0, (byte) intValues[i],
-        (short) intValues[i], (int) intValues[i], intValues[i],
-        (float) doubleValues[i], doubleValues[i], byteValues[i], stringValues[i],
-        new MiddleStruct(inner, inner2), list(), map(inner, inner2));
-  }
-
-  private void checkRandomRow(VectorizedRowBatch batch,
-                              long[] intValues, double[] doubleValues,
-                              String[] stringValues,
-                              BytesWritable[] byteValues,
-                              String[] words, int i, int rowInBatch) {
-    InnerStruct inner = new InnerStruct((int) intValues[i], stringValues[i]);
-    InnerStruct inner2 = new InnerStruct((int) (intValues[i] >> 32),
-        words[i % words.length] + "-x");
-    checkBigRow(batch, rowInBatch, i, (intValues[i] & 1) == 0, (byte) intValues[i],
-        (short) intValues[i], (int) intValues[i], intValues[i],
-        (float) doubleValues[i], doubleValues[i], byteValues[i], stringValues[i],
-        new MiddleStruct(inner, inner2), list(), map(inner, inner2));
-  }
-
-  private static class MyMemoryManager extends MemoryManager {
-    final long totalSpace;
-    double rate;
-    Path path = null;
-    long lastAllocation = 0;
-    int rows = 0;
-    Callback callback;
-
-    MyMemoryManager(Configuration conf, long totalSpace, double rate) {
-      super(conf);
-      this.totalSpace = totalSpace;
-      this.rate = rate;
-    }
-
-    @Override
-    public void addWriter(Path path, long requestedAllocation,
-                   Callback callback) {
-      this.path = path;
-      this.lastAllocation = requestedAllocation;
-      this.callback = callback;
-    }
-
-    @Override
-    public synchronized void removeWriter(Path path) {
-      this.path = null;
-      this.lastAllocation = 0;
-    }
-
-    @Override
-    public long getTotalMemoryPool() {
-      return totalSpace;
-    }
-
-    @Override
-    public double getAllocationScale() {
-      return rate;
-    }
-
-    @Override
-    public void addedRow(int count) throws IOException {
-      rows += count;
-      if (rows % 100 == 0) {
-        callback.checkMemory(rate);
-      }
-    }
-  }
-
-  @Test
-  public void testMemoryManagementV11() throws Exception {
-    TypeDescription schema = createInnerSchema();
-    MyMemoryManager memory = new MyMemoryManager(conf, 10000, 0.1);
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .compress(CompressionKind.NONE)
-            .stripeSize(50000)
-            .bufferSize(100)
-            .rowIndexStride(0)
-            .memory(memory)
-            .version(OrcFile.Version.V_0_11));
-    assertEquals(testFilePath, memory.path);
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1;
-    for(int i=0; i < 2500; ++i) {
-      ((LongColumnVector) batch.cols[0]).vector[0] = i * 300;
-      ((BytesColumnVector) batch.cols[1]).setVal(0,
-          Integer.toHexString(10*i).getBytes());
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    assertEquals(null, memory.path);
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    int i = 0;
-    for(StripeInformation stripe: reader.getStripes()) {
-      i += 1;
-      assertTrue("stripe " + i + " is too long at " + stripe.getDataLength(),
-          stripe.getDataLength() < 5000);
-    }
-    assertEquals(25, i);
-    assertEquals(2500, reader.getNumberOfRows());
-  }
-
-  @Test
-  public void testMemoryManagementV12() throws Exception {
-    TypeDescription schema = createInnerSchema();
-    MyMemoryManager memory = new MyMemoryManager(conf, 10000, 0.1);
-    Writer writer = OrcFile.createWriter(testFilePath,
-                                         OrcFile.writerOptions(conf)
-                                         .setSchema(schema)
-                                         .compress(CompressionKind.NONE)
-                                         .stripeSize(50000)
-                                         .bufferSize(100)
-                                         .rowIndexStride(0)
-                                         .memory(memory)
-                                         .version(OrcFile.Version.V_0_12));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    assertEquals(testFilePath, memory.path);
-    batch.size = 1;
-    for(int i=0; i < 2500; ++i) {
-      ((LongColumnVector) batch.cols[0]).vector[0] = i * 300;
-      ((BytesColumnVector) batch.cols[1]).setVal(0,
-          Integer.toHexString(10*i).getBytes());
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    assertEquals(null, memory.path);
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    int i = 0;
-    for(StripeInformation stripe: reader.getStripes()) {
-      i += 1;
-      assertTrue("stripe " + i + " is too long at " + stripe.getDataLength(),
-          stripe.getDataLength() < 5000);
-    }
-    // with HIVE-7832, the dictionaries will be disabled after writing the first
-    // stripe as there are too many distinct values. Hence only 3 stripes as
-    // compared to 25 stripes in version 0.11 (above test case)
-    assertEquals(3, i);
-    assertEquals(2500, reader.getNumberOfRows());
-  }
-
-  @Test
-  public void testPredicatePushdown() throws Exception {
-    TypeDescription schema = createInnerSchema();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .stripeSize(400000L)
-            .compress(CompressionKind.NONE)
-            .bufferSize(500)
-            .rowIndexStride(1000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.ensureSize(3500);
-    batch.size = 3500;
-    for(int i=0; i < 3500; ++i) {
-      ((LongColumnVector) batch.cols[0]).vector[i] = i * 300;
-      ((BytesColumnVector) batch.cols[1]).setVal(i,
-          Integer.toHexString(10*i).getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    assertEquals(3500, reader.getNumberOfRows());
-
-    SearchArgument sarg = SearchArgumentFactory.newBuilder()
-        .startAnd()
-          .startNot()
-             .lessThan("int1", PredicateLeaf.Type.LONG, 300000L)
-          .end()
-          .lessThan("int1", PredicateLeaf.Type.LONG, 600000L)
-        .end()
-        .build();
-    RecordReader rows = reader.rows(new Reader.Options()
-        .range(0L, Long.MAX_VALUE)
-        .include(new boolean[]{true, true, true})
-        .searchArgument(sarg, new String[]{null, "int1", "string1"}));
-    batch = reader.getSchema().createRowBatch(2000);
-    LongColumnVector ints = (LongColumnVector) batch.cols[0];
-    BytesColumnVector strs = (BytesColumnVector) batch.cols[1];
-
-    Assert.assertEquals(1000L, rows.getRowNumber());
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1000, batch.size);
-
-    for(int i=1000; i < 2000; ++i) {
-      assertEquals(300 * i, ints.vector[i - 1000]);
-      assertEquals(Integer.toHexString(10*i), strs.toString(i - 1000));
-    }
-    Assert.assertEquals(false, rows.nextBatch(batch));
-    Assert.assertEquals(3500, rows.getRowNumber());
-
-    // look through the file with no rows selected
-    sarg = SearchArgumentFactory.newBuilder()
-        .startAnd()
-          .lessThan("int1", PredicateLeaf.Type.LONG, 0L)
-        .end()
-        .build();
-    rows = reader.rows(new Reader.Options()
-        .range(0L, Long.MAX_VALUE)
-        .include(new boolean[]{true, true, true})
-        .searchArgument(sarg, new String[]{null, "int1", "string1"}));
-    Assert.assertEquals(3500L, rows.getRowNumber());
-    assertTrue(!rows.nextBatch(batch));
-
-    // select first 100 and last 100 rows
-    sarg = SearchArgumentFactory.newBuilder()
-        .startOr()
-          .lessThan("int1", PredicateLeaf.Type.LONG, 300L * 100)
-          .startNot()
-            .lessThan("int1", PredicateLeaf.Type.LONG, 300L * 3400)
-          .end()
-        .end()
-        .build();
-    rows = reader.rows(new Reader.Options()
-        .range(0L, Long.MAX_VALUE)
-        .include(new boolean[]{true, true, true})
-        .searchArgument(sarg, new String[]{null, "int1", "string1"}));
-    Assert.assertEquals(0, rows.getRowNumber());
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1000, batch.size);
-    Assert.assertEquals(3000, rows.getRowNumber());
-    for(int i=0; i < 1000; ++i) {
-      assertEquals(300 * i, ints.vector[i]);
-      assertEquals(Integer.toHexString(10*i), strs.toString(i));
-    }
-
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(500, batch.size);
-    Assert.assertEquals(3500, rows.getRowNumber());
-    for(int i=3000; i < 3500; ++i) {
-      assertEquals(300 * i, ints.vector[i - 3000]);
-      assertEquals(Integer.toHexString(10*i), strs.toString(i - 3000));
-    }
-    Assert.assertEquals(false, rows.nextBatch(batch));
-    Assert.assertEquals(3500, rows.getRowNumber());
-  }
-
-  /**
-   * Test all of the types that have distinct ORC writers using the vectorized
-   * writer with different combinations of repeating and null values.
-   * @throws Exception
-   */
-  @Test
-  public void testRepeating() throws Exception {
-    // create a row type with each type that has a unique writer
-    // really just folds short, int, and long together
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("bin", TypeDescription.createBinary())
-        .addField("bool", TypeDescription.createBoolean())
-        .addField("byte", TypeDescription.createByte())
-        .addField("long", TypeDescription.createLong())
-        .addField("float", TypeDescription.createFloat())
-        .addField("double", TypeDescription.createDouble())
-        .addField("date", TypeDescription.createDate())
-        .addField("time", TypeDescription.createTimestamp())
-        .addField("dec", TypeDescription.createDecimal()
-            .withPrecision(20).withScale(6))
-        .addField("string", TypeDescription.createString())
-        .addField("char", TypeDescription.createChar().withMaxLength(10))
-        .addField("vc", TypeDescription.createVarchar().withMaxLength(10))
-        .addField("struct", TypeDescription.createStruct()
-            .addField("sub1", TypeDescription.createInt()))
-        .addField("union", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createString())
-            .addUnionChild(TypeDescription.createInt()))
-        .addField("list", TypeDescription
-            .createList(TypeDescription.createInt()))
-        .addField("map",
-            TypeDescription.createMap(TypeDescription.createString(),
-                TypeDescription.createString()));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .rowIndexStride(1000));
-
-    // write 1024 repeating nulls
-    batch.size = 1024;
-    for(int c = 0; c < batch.cols.length; ++c) {
-      batch.cols[c].setRepeating(true);
-      batch.cols[c].noNulls = false;
-      batch.cols[c].isNull[0] = true;
-    }
-    writer.addRowBatch(batch);
-
-    // write 1024 repeating non-null
-    for(int c =0; c < batch.cols.length; ++c) {
-      batch.cols[c].isNull[0] = false;
-    }
-    ((BytesColumnVector) batch.cols[0]).setVal(0, "Horton".getBytes());
-    ((LongColumnVector) batch.cols[1]).vector[0] = 1;
-    ((LongColumnVector) batch.cols[2]).vector[0] = 130;
-    ((LongColumnVector) batch.cols[3]).vector[0] = 0x123456789abcdef0L;
-    ((DoubleColumnVector) batch.cols[4]).vector[0] = 1.125;
-    ((DoubleColumnVector) batch.cols[5]).vector[0] = 0.0009765625;
-    ((LongColumnVector) batch.cols[6]).vector[0] =
-        new DateWritable(new Date(111, 6, 1)).getDays();
-    ((TimestampColumnVector) batch.cols[7]).set(0,
-        new Timestamp(115, 9, 23, 10, 11, 59,
-            999999999));
-    ((DecimalColumnVector) batch.cols[8]).vector[0] =
-        new HiveDecimalWritable("1.234567");
-    ((BytesColumnVector) batch.cols[9]).setVal(0, "Echelon".getBytes());
-    ((BytesColumnVector) batch.cols[10]).setVal(0, "Juggernaut".getBytes());
-    ((BytesColumnVector) batch.cols[11]).setVal(0, "Dreadnaught".getBytes());
-    ((LongColumnVector) ((StructColumnVector) batch.cols[12]).fields[0])
-        .vector[0] = 123;
-    ((UnionColumnVector) batch.cols[13]).tags[0] = 1;
-    ((LongColumnVector) ((UnionColumnVector) batch.cols[13]).fields[1])
-        .vector[0] = 1234;
-    ((ListColumnVector) batch.cols[14]).offsets[0] = 0;
-    ((ListColumnVector) batch.cols[14]).lengths[0] = 3;
-    ((ListColumnVector) batch.cols[14]).child.isRepeating = true;
-    ((LongColumnVector) ((ListColumnVector) batch.cols[14]).child).vector[0]
-        = 31415;
-    ((MapColumnVector) batch.cols[15]).offsets[0] = 0;
-    ((MapColumnVector) batch.cols[15]).lengths[0] = 3;
-    ((MapColumnVector) batch.cols[15]).values.isRepeating = true;
-    ((BytesColumnVector) ((MapColumnVector) batch.cols[15]).keys)
-        .setVal(0, "ORC".getBytes());
-    ((BytesColumnVector) ((MapColumnVector) batch.cols[15]).keys)
-        .setVal(1, "Hive".getBytes());
-    ((BytesColumnVector) ((MapColumnVector) batch.cols[15]).keys)
-        .setVal(2, "LLAP".getBytes());
-    ((BytesColumnVector) ((MapColumnVector) batch.cols[15]).values)
-        .setVal(0, "fast".getBytes());
-    writer.addRowBatch(batch);
-
-    // write 1024 null without repeat
-    for(int c = 0; c < batch.cols.length; ++c) {
-      batch.cols[c].setRepeating(false);
-      batch.cols[c].noNulls = false;
-      Arrays.fill(batch.cols[c].isNull, true);
-    }
-    writer.addRowBatch(batch);
-
-    // add 1024 rows of non-null, non-repeating
-    batch.reset();
-    batch.size = 1024;
-    ((ListColumnVector) batch.cols[14]).child.ensureSize(3 * 1024, false);
-    ((MapColumnVector) batch.cols[15]).keys.ensureSize(3 * 1024, false);
-    ((MapColumnVector) batch.cols[15]).values.ensureSize(3 * 1024, false);
-    for(int r=0; r < 1024; ++r) {
-      ((BytesColumnVector) batch.cols[0]).setVal(r,
-          Integer.toHexString(r).getBytes());
-      ((LongColumnVector) batch.cols[1]).vector[r] = r % 2;
-      ((LongColumnVector) batch.cols[2]).vector[r] = (r % 255);
-      ((LongColumnVector) batch.cols[3]).vector[r] = 31415L * r;
-      ((DoubleColumnVector) batch.cols[4]).vector[r] = 1.125 * r;
-      ((DoubleColumnVector) batch.cols[5]).vector[r] = 0.0009765625 * r;
-      ((LongColumnVector) batch.cols[6]).vector[r] =
-          new DateWritable(new Date(111, 6, 1)).getDays() + r;
-
-      Timestamp ts = new Timestamp(115, 9, 25, 10, 11, 59 + r, 999999999);
-      ((TimestampColumnVector) batch.cols[7]).set(r, ts);
-      ((DecimalColumnVector) batch.cols[8]).vector[r] =
-          new HiveDecimalWritable("1.234567");
-      ((BytesColumnVector) batch.cols[9]).setVal(r,
-          Integer.toString(r).getBytes());
-      ((BytesColumnVector) batch.cols[10]).setVal(r,
-          Integer.toHexString(r).getBytes());
-      ((BytesColumnVector) batch.cols[11]).setVal(r,
-          Integer.toHexString(r * 128).getBytes());
-      ((LongColumnVector) ((StructColumnVector) batch.cols[12]).fields[0])
-          .vector[r] = r + 13;
-      ((UnionColumnVector) batch.cols[13]).tags[r] = 1;
-      ((LongColumnVector) ((UnionColumnVector) batch.cols[13]).fields[1])
-          .vector[r] = r + 42;
-      ((ListColumnVector) batch.cols[14]).offsets[r] = 3 * r;
-      ((ListColumnVector) batch.cols[14]).lengths[r] = 3;
-      for(int i=0; i < 3; ++i) {
-        ((LongColumnVector) ((ListColumnVector) batch.cols[14]).child)
-            .vector[3 * r + i] = 31415 + i;
-      }
-      ((MapColumnVector) batch.cols[15]).offsets[r] = 3 * r;
-      ((MapColumnVector) batch.cols[15]).lengths[r] = 3;
-      for(int i=0; i < 3; ++i) {
-        ((BytesColumnVector) ((MapColumnVector) batch.cols[15]).keys)
-            .setVal(3 * r + i, Integer.toHexString(3 * r + i).getBytes());
-        ((BytesColumnVector) ((MapColumnVector) batch.cols[15]).values)
-            .setVal(3 * r + i, Integer.toString(3 * r + i).getBytes());
-      }
-    }
-    writer.addRowBatch(batch);
-
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-
-    // check the stats
-    ColumnStatistics[] stats = reader.getStatistics();
-    assertEquals(4096, stats[0].getNumberOfValues());
-    assertEquals(false, stats[0].hasNull());
-    for(TypeDescription colType: schema.getChildren()) {
-      assertEquals("count on " + colType.getId(),
-          2048, stats[colType.getId()].getNumberOfValues());
-      assertEquals("hasNull on " + colType.getId(),
-          true, stats[colType.getId()].hasNull());
-    }
-    assertEquals(8944, ((BinaryColumnStatistics) stats[1]).getSum());
-    assertEquals(1536, ((BooleanColumnStatistics) stats[2]).getTrueCount());
-    assertEquals(512, ((BooleanColumnStatistics) stats[2]).getFalseCount());
-    assertEquals(false, ((IntegerColumnStatistics) stats[4]).isSumDefined());
-    assertEquals(0, ((IntegerColumnStatistics) stats[4]).getMinimum());
-    assertEquals(0x123456789abcdef0L,
-        ((IntegerColumnStatistics) stats[4]).getMaximum());
-    assertEquals("0", ((StringColumnStatistics) stats[10]).getMinimum());
-    assertEquals("Echelon", ((StringColumnStatistics) stats[10]).getMaximum());
-    assertEquals(10154, ((StringColumnStatistics) stats[10]).getSum());
-    assertEquals("0         ",
-        ((StringColumnStatistics) stats[11]).getMinimum());
-    assertEquals("ff        ",
-        ((StringColumnStatistics) stats[11]).getMaximum());
-    assertEquals(20480, ((StringColumnStatistics) stats[11]).getSum());
-    assertEquals("0",
-        ((StringColumnStatistics) stats[12]).getMinimum());
-    assertEquals("ff80",
-        ((StringColumnStatistics) stats[12]).getMaximum());
-    assertEquals(14813, ((StringColumnStatistics) stats[12]).getSum());
-
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch(1024);
-    BytesColumnVector bins = (BytesColumnVector) batch.cols[0];
-    LongColumnVector bools = (LongColumnVector) batch.cols[1];
-    LongColumnVector bytes = (LongColumnVector) batch.cols[2];
-    LongColumnVector longs = (LongColumnVector) batch.cols[3];
-    DoubleColumnVector floats = (DoubleColumnVector) batch.cols[4];
-    DoubleColumnVector doubles = (DoubleColumnVector) batch.cols[5];
-    LongColumnVector dates = (LongColumnVector) batch.cols[6];
-    TimestampColumnVector times = (TimestampColumnVector) batch.cols[7];
-    DecimalColumnVector decs = (DecimalColumnVector) batch.cols[8];
-    BytesColumnVector strs = (BytesColumnVector) batch.cols[9];
-    BytesColumnVector chars = (BytesColumnVector) batch.cols[10];
-    BytesColumnVector vcs = (BytesColumnVector) batch.cols[11];
-    StructColumnVector structs = (StructColumnVector) batch.cols[12];
-    UnionColumnVector unions = (UnionColumnVector) batch.cols[13];
-    ListColumnVector lists = (ListColumnVector) batch.cols[14];
-    MapColumnVector maps = (MapColumnVector) batch.cols[15];
-    LongColumnVector structInts = (LongColumnVector) structs.fields[0];
-    LongColumnVector unionInts = (LongColumnVector) unions.fields[1];
-    LongColumnVector listInts = (LongColumnVector) lists.child;
-    BytesColumnVector mapKeys = (BytesColumnVector) maps.keys;
-    BytesColumnVector mapValues = (BytesColumnVector) maps.values;
-
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-
-    // read the 1024 nulls
-    for(int f=0; f < batch.cols.length; ++f) {
-      assertEquals("field " + f,
-          true, batch.cols[f].isRepeating);
-      assertEquals("field " + f,
-          false, batch.cols[f].noNulls);
-      assertEquals("field " + f,
-          true, batch.cols[f].isNull[0]);
-    }
-
-    // read the 1024 repeat values
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      assertEquals("row " + r, "Horton", bins.toString(r));
-      assertEquals("row " + r, 1, bools.vector[r]);
-      assertEquals("row " + r, -126, bytes.vector[r]);
-      assertEquals("row " + r, 1311768467463790320L, longs.vector[r]);
-      assertEquals("row " + r, 1.125, floats.vector[r], 0.00001);
-      assertEquals("row " + r, 9.765625E-4, doubles.vector[r], 0.000001);
-      assertEquals("row " + r, "2011-07-01",
-          new DateWritable((int) dates.vector[r]).toString());
-      assertEquals("row " + r, "2015-10-23 10:11:59.999999999",
-          times.asScratchTimestamp(r).toString());
-      assertEquals("row " + r, "1.234567", decs.vector[r].toString());
-      assertEquals("row " + r, "Echelon", strs.toString(r));
-      assertEquals("row " + r, "Juggernaut", chars.toString(r));
-      assertEquals("row " + r, "Dreadnaugh", vcs.toString(r));
-      assertEquals("row " + r, 123, structInts.vector[r]);
-      assertEquals("row " + r, 1, unions.tags[r]);
-      assertEquals("row " + r, 1234, unionInts.vector[r]);
-      assertEquals("row " + r, 3, lists.lengths[r]);
-      assertEquals("row " + r, true, listInts.isRepeating);
-      assertEquals("row " + r, 31415, listInts.vector[0]);
-      assertEquals("row " + r, 3, maps.lengths[r]);
-      assertEquals("row " + r, "ORC", mapKeys.toString((int) maps.offsets[r]));
-      assertEquals("row " + r, "Hive", mapKeys.toString((int) maps.offsets[r] + 1));
-      assertEquals("row " + r, "LLAP", mapKeys.toString((int) maps.offsets[r] + 2));
-      assertEquals("row " + r, "fast", mapValues.toString((int) maps.offsets[r]));
-      assertEquals("row " + r, "fast", mapValues.toString((int) maps.offsets[r] + 1));
-      assertEquals("row " + r, "fast", mapValues.toString((int) maps.offsets[r] + 2));
-    }
-
-    // read the second set of 1024 nulls
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-    for(int f=0; f < batch.cols.length; ++f) {
-      assertEquals("field " + f,
-          true, batch.cols[f].isRepeating);
-      assertEquals("field " + f,
-          false, batch.cols[f].noNulls);
-      assertEquals("field " + f,
-          true, batch.cols[f].isNull[0]);
-    }
-
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      String hex = Integer.toHexString(r);
-
-      assertEquals("row " + r, hex, bins.toString(r));
-      assertEquals("row " + r, r % 2 == 1 ? 1 : 0, bools.vector[r]);
-      assertEquals("row " + r, (byte) (r % 255), bytes.vector[r]);
-      assertEquals("row " + r, 31415L * r, longs.vector[r]);
-      assertEquals("row " + r, 1.125F * r, floats.vector[r], 0.0001);
-      assertEquals("row " + r, 0.0009765625 * r, doubles.vector[r], 0.000001);
-      assertEquals("row " + r, new DateWritable(new Date(111, 6, 1 + r)),
-          new DateWritable((int) dates.vector[r]));
-      assertEquals("row " + r,
-          new Timestamp(115, 9, 25, 10, 11, 59 + r, 999999999),
-          times.asScratchTimestamp(r));
-      assertEquals("row " + r, "1.234567", decs.vector[r].toString());
-      assertEquals("row " + r, Integer.toString(r), strs.toString(r));
-      assertEquals("row " + r, Integer.toHexString(r), chars.toString(r));
-      assertEquals("row " + r, Integer.toHexString(r * 128), vcs.toString(r));
-      assertEquals("row " + r, r + 13, structInts.vector[r]);
-      assertEquals("row " + r, 1, unions.tags[r]);
-      assertEquals("row " + r, r + 42, unionInts.vector[r]);
-      assertEquals("row " + r, 3, lists.lengths[r]);
-      assertEquals("row " + r, 31415, listInts.vector[(int) lists.offsets[r]]);
-      assertEquals("row " + r, 31416, listInts.vector[(int) lists.offsets[r] + 1]);
-      assertEquals("row " + r, 31417, listInts.vector[(int) lists.offsets[r] + 2]);
-      assertEquals("row " + r, 3, maps.lengths[3]);
-      assertEquals("row " + r, Integer.toHexString(3 * r), mapKeys.toString((int) maps.offsets[r]));
-      assertEquals("row " + r, Integer.toString(3 * r), mapValues.toString((int) maps.offsets[r]));
-      assertEquals("row " + r, Integer.toHexString(3 * r + 1), mapKeys.toString((int) maps.offsets[r] + 1));
-      assertEquals("row " + r, Integer.toString(3 * r + 1), mapValues.toString((int) maps.offsets[r] + 1));
-      assertEquals("row " + r, Integer.toHexString(3 * r + 2), mapKeys.toString((int) maps.offsets[r] + 2));
-      assertEquals("row " + r, Integer.toString(3 * r + 2), mapValues.toString((int) maps.offsets[r] + 2));
-    }
-
-    // should have no more rows
-    Assert.assertEquals(false, rows.nextBatch(batch));
-  }
-
-  private static String makeString(BytesColumnVector vector, int row) {
-    if (vector.isRepeating) {
-      row = 0;
-    }
-    if (vector.noNulls || !vector.isNull[row]) {
-      return new String(vector.vector[row], vector.start[row],
-          vector.length[row]);
-    } else {
-      return null;
-    }
-  }
-
-  /**
-   * Test the char and varchar padding and truncation.
-   * @throws Exception
-   */
-  @Test
-  public void testStringPadding() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("char", TypeDescription.createChar().withMaxLength(10))
-        .addField("varchar", TypeDescription.createVarchar().withMaxLength(10));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 4;
-    for(int c=0; c < batch.cols.length; ++c) {
-      ((BytesColumnVector) batch.cols[c]).setVal(0, "".getBytes());
-      ((BytesColumnVector) batch.cols[c]).setVal(1, "xyz".getBytes());
-      ((BytesColumnVector) batch.cols[c]).setVal(2, "0123456789".getBytes());
-      ((BytesColumnVector) batch.cols[c]).setVal(3,
-          "0123456789abcdef".getBytes());
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(4, batch.size);
-    // ORC currently trims the output strings. See HIVE-12286
-    assertEquals("",
-        makeString((BytesColumnVector) batch.cols[0], 0));
-    assertEquals("xyz",
-        makeString((BytesColumnVector) batch.cols[0], 1));
-    assertEquals("0123456789",
-        makeString((BytesColumnVector) batch.cols[0], 2));
-    assertEquals("0123456789",
-        makeString((BytesColumnVector) batch.cols[0], 3));
-    assertEquals("",
-        makeString((BytesColumnVector) batch.cols[1], 0));
-    assertEquals("xyz",
-        makeString((BytesColumnVector) batch.cols[1], 1));
-    assertEquals("0123456789",
-        makeString((BytesColumnVector) batch.cols[1], 2));
-    assertEquals("0123456789",
-        makeString((BytesColumnVector) batch.cols[1], 3));
-  }
-
-  /**
-   * A test case that tests the case where you add a repeating batch
-   * to a column that isn't using dictionary encoding.
-   * @throws Exception
-   */
-  @Test
-  public void testNonDictionaryRepeatingString() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("str", TypeDescription.createString());
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .setSchema(schema)
-            .rowIndexStride(1000));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1024;
-    for(int r=0; r < batch.size; ++r) {
-      ((BytesColumnVector) batch.cols[0]).setVal(r,
-          Integer.toString(r * 10001).getBytes());
-    }
-    writer.addRowBatch(batch);
-    batch.cols[0].isRepeating = true;
-    ((BytesColumnVector) batch.cols[0]).setVal(0, "Halloween".getBytes());
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      assertEquals(Integer.toString(r * 10001),
-          makeString((BytesColumnVector) batch.cols[0], r));
-    }
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      assertEquals("Halloween",
-          makeString((BytesColumnVector) batch.cols[0], r));
-    }
-    Assert.assertEquals(false, rows.nextBatch(batch));
-  }
-
-  @Test
-  public void testStructs() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("struct", TypeDescription.createStruct()
-            .addField("inner", TypeDescription.createLong()));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1024;
-    StructColumnVector outer = (StructColumnVector) batch.cols[0];
-    outer.noNulls = false;
-    for(int r=0; r < 1024; ++r) {
-      if (r < 200 || (r >= 400 && r < 600) || r >= 800) {
-        outer.isNull[r] = true;
-      }
-      ((LongColumnVector) outer.fields[0]).vector[r] = r;
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    rows.nextBatch(batch);
-    assertEquals(1024, batch.size);
-    StructColumnVector inner = (StructColumnVector) batch.cols[0];
-    LongColumnVector vec = (LongColumnVector) inner.fields[0];
-    for(int r=0; r < 1024; ++r) {
-      if (r < 200 || (r >= 400 && r < 600) || r >= 800) {
-        assertEquals("row " + r, true, inner.isNull[r]);
-      } else {
-        assertEquals("row " + r, false, inner.isNull[r]);
-        assertEquals("row " + r, r, vec.vector[r]);
-      }
-    }
-    rows.nextBatch(batch);
-    assertEquals(0, batch.size);
-  }
-
-  /**
-   * Test Unions.
-   * @throws Exception
-   */
-  @Test
-  public void testUnions() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("outer", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createInt())
-            .addUnionChild(TypeDescription.createLong()));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1024;
-    UnionColumnVector outer = (UnionColumnVector) batch.cols[0];
-    batch.cols[0].noNulls = false;
-    for(int r=0; r < 1024; ++r) {
-      if (r < 200) {
-        outer.isNull[r] = true;
-      } else if (r < 300) {
-        outer.tags[r] = 0;
-      } else if (r < 400) {
-        outer.tags[r] = 1;
-      } else if (r < 600) {
-        outer.isNull[r] = true;
-      } else if (r < 800) {
-        outer.tags[r] = 1;
-      } else if (r < 1000) {
-        outer.isNull[r] = true;
-      } else {
-        outer.tags[r] = 1;
-      }
-      ((LongColumnVector) outer.fields[0]).vector[r] = r;
-      ((LongColumnVector) outer.fields[1]).vector[r] = -r;
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch(1024);
-    UnionColumnVector union = (UnionColumnVector) batch.cols[0];
-    LongColumnVector ints = (LongColumnVector) union.fields[0];
-    LongColumnVector longs = (LongColumnVector) union.fields[1];
-    Assert.assertEquals(true, rows.nextBatch(batch));
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      if (r < 200) {
-        assertEquals("row " + r, true, union.isNull[r]);
-      } else if (r < 300) {
-        assertEquals("row " + r, false, union.isNull[r]);
-        assertEquals("row " + r, 0, union.tags[r]);
-        assertEquals("row " + r, r, ints.vector[r]);
-      } else if (r < 400) {
-        assertEquals("row " + r, false, union.isNull[r]);
-        assertEquals("row " + r, 1, union.tags[r]);
-        assertEquals("row " + r, -r, longs.vector[r]);
-      } else if (r < 600) {
-        assertEquals("row " + r, true, union.isNull[r]);
-      } else if (r < 800) {
-        assertEquals("row " + r, false, union.isNull[r]);
-        assertEquals("row " + r, 1, union.tags[r]);
-        assertEquals("row " + r, -r, longs.vector[r]);
-      } else if (r < 1000) {
-        assertEquals("row " + r, true, union.isNull[r]);
-      } else {
-        assertEquals("row " + r, false, union.isNull[r]);
-        assertEquals("row " + r, 1, union.tags[r]);
-        assertEquals("row " + r, -r, longs.vector[r]);
-      }
-    }
-    Assert.assertEquals(false, rows.nextBatch(batch));
-  }
-
-  /**
-   * Test lists and how they interact with the child column. In particular,
-   * put nulls between back to back lists and then make some lists that
-   * oper lap.
-   * @throws Exception
-   */
-  @Test
-  public void testLists() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("list",
-            TypeDescription.createList(TypeDescription.createLong()));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1024;
-    ListColumnVector list = (ListColumnVector) batch.cols[0];
-    list.noNulls = false;
-    for(int r=0; r < 1024; ++r) {
-      if (r < 200) {
-        list.isNull[r] = true;
-      } else if (r < 300) {
-        list.offsets[r] = r - 200;
-        list.lengths[r] = 1;
-      } else if (r < 400) {
-        list.isNull[r] = true;
-      } else if (r < 500) {
-        list.offsets[r] = r - 300;
-        list.lengths[r] = 1;
-      } else if (r < 600) {
-        list.isNull[r] = true;
-      } else if (r < 700) {
-        list.offsets[r] = r;
-        list.lengths[r] = 2;
-      } else {
-        list.isNull[r] = true;
-      }
-      ((LongColumnVector) list.child).vector[r] = r * 10;
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch(1024);
-    list = (ListColumnVector) batch.cols[0];
-    rows.nextBatch(batch);
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      StringBuilder actual = new StringBuilder();
-      list.stringifyValue(actual, r);
-      if (r < 200) {
-        assertEquals("row " + r, "null", actual.toString());
-      } else if (r < 300) {
-        assertEquals("row " + r, "[" + ((r - 200) * 10) + "]",
-            actual.toString());
-      } else if (r < 400) {
-        assertEquals("row " + r, "null", actual.toString());
-      } else if (r < 500) {
-        assertEquals("row " + r, "[" + ((r - 300) * 10) + "]",
-            actual.toString());
-      } else if (r < 600) {
-        assertEquals("row " + r, "null", actual.toString());
-      } else if (r < 700) {
-        assertEquals("row " + r, "[" + (10 * r) + ", " + (10 * (r + 1)) + "]",
-            actual.toString());
-      } else {
-        assertEquals("row " + r, "null", actual.toString());
-      }
-    }
-    Assert.assertEquals(false, rows.nextBatch(batch));
-  }
-
-  /**
-   * Test maps and how they interact with the child column. In particular,
-   * put nulls between back to back lists and then make some lists that
-   * oper lap.
-   * @throws Exception
-   */
-  @Test
-  public void testMaps() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("map",
-            TypeDescription.createMap(TypeDescription.createLong(),
-                TypeDescription.createLong()));
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema));
-    VectorizedRowBatch batch = schema.createRowBatch();
-    batch.size = 1024;
-    MapColumnVector map = (MapColumnVector) batch.cols[0];
-    map.noNulls = false;
-    for(int r=0; r < 1024; ++r) {
-      if (r < 200) {
-        map.isNull[r] = true;
-      } else if (r < 300) {
-        map.offsets[r] = r - 200;
-        map.lengths[r] = 1;
-      } else if (r < 400) {
-        map.isNull[r] = true;
-      } else if (r < 500) {
-        map.offsets[r] = r - 300;
-        map.lengths[r] = 1;
-      } else if (r < 600) {
-        map.isNull[r] = true;
-      } else if (r < 700) {
-        map.offsets[r] = r;
-        map.lengths[r] = 2;
-      } else {
-        map.isNull[r] = true;
-      }
-      ((LongColumnVector) map.keys).vector[r] = r;
-      ((LongColumnVector) map.values).vector[r] = r * 10;
-    }
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf));
-    RecordReader rows = reader.rows();
-    batch = reader.getSchema().createRowBatch();
-    map = (MapColumnVector) batch.cols[0];
-    rows.nextBatch(batch);
-    assertEquals(1024, batch.size);
-    for(int r=0; r < 1024; ++r) {
-      StringBuilder buffer = new StringBuilder();
-      map.stringifyValue(buffer, r);
-      String actual = buffer.toString();
-      if (r < 200) {
-        assertEquals("row " + r, "null", actual);
-      } else if (r < 300) {
-        assertEquals("row " + r, "[{\"key\": " + (r - 200) +
-                ", \"value\": " + ((r - 200) * 10) + "}]",
-            actual);
-      } else if (r < 400) {
-        assertEquals("row " + r, "null", actual);
-      } else if (r < 500) {
-        assertEquals("row " + r, "[{\"key\": " + (r - 300) +
-                ", \"value\": " + ((r - 300) * 10) + "}]", actual);
-      } else if (r < 600) {
-        assertEquals("row " + r, "null", actual);
-      } else if (r < 700) {
-        assertEquals("row " + r, "[{\"key\": " + r + ", \"value\": " + (r * 10)
-                + "}, {\"key\": " + (r + 1) + ", \"value\": " + (10 * (r + 1))
-                + "}]", actual);
-      } else {
-        assertEquals("row " + r, "null", actual);
-      }
-    }
-    rows.nextBatch(batch);
-    assertEquals(0, batch.size);
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestBitFieldReader.java b/orc/src/test/org/apache/orc/impl/TestBitFieldReader.java
deleted file mode 100644
index e4c6f6be2e..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestBitFieldReader.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.nio.ByteBuffer;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-
-public class TestBitFieldReader {
-
-  public void runSeekTest(CompressionCodec codec) throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    final int COUNT = 16384;
-    BitFieldWriter out = new BitFieldWriter(
-        new OutStream("test", 500, codec, collect), 1);
-    TestInStream.PositionCollector[] positions =
-        new TestInStream.PositionCollector[COUNT];
-    for(int i=0; i < COUNT; ++i) {
-      positions[i] = new TestInStream.PositionCollector();
-      out.getPosition(positions[i]);
-      // test runs, non-runs
-      if (i < COUNT / 2) {
-        out.write(i & 1);
-      } else {
-        out.write((i/3) & 1);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    BitFieldReader in = new BitFieldReader(InStream.create("test",
-        new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(),
-        codec, 500), 1);
-    for(int i=0; i < COUNT; ++i) {
-      int x = in.next();
-      if (i < COUNT / 2) {
-        assertEquals(i & 1, x);
-      } else {
-        assertEquals((i/3) & 1, x);
-      }
-    }
-    for(int i=COUNT-1; i >= 0; --i) {
-      in.seek(positions[i]);
-      int x = in.next();
-      if (i < COUNT / 2) {
-        assertEquals(i & 1, x);
-      } else {
-        assertEquals((i/3) & 1, x);
-      }
-    }
-  }
-
-  @Test
-  public void testUncompressedSeek() throws Exception {
-    runSeekTest(null);
-  }
-
-  @Test
-  public void testCompressedSeek() throws Exception {
-    runSeekTest(new ZlibCodec());
-  }
-
-  @Test
-  public void testBiggerItems() throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    final int COUNT = 16384;
-    BitFieldWriter out = new BitFieldWriter(
-        new OutStream("test", 500, null, collect), 3);
-    for(int i=0; i < COUNT; ++i) {
-      // test runs, non-runs
-      if (i < COUNT / 2) {
-        out.write(i & 7);
-      } else {
-        out.write((i/3) & 7);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    BitFieldReader in = new BitFieldReader(InStream.create("test",
-        new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(),
-        null, 500), 3);
-    for(int i=0; i < COUNT; ++i) {
-      int x = in.next();
-      if (i < COUNT / 2) {
-        assertEquals(i & 7, x);
-      } else {
-        assertEquals((i/3) & 7, x);
-      }
-    }
-  }
-
-  @Test
-  public void testSkips() throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    BitFieldWriter out = new BitFieldWriter(
-        new OutStream("test", 100, null, collect), 1);
-    final int COUNT = 16384;
-    for(int i=0; i < COUNT; ++i) {
-      if (i < COUNT/2) {
-        out.write(i & 1);
-      } else {
-        out.write((i/3) & 1);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    BitFieldReader in = new BitFieldReader(InStream.create("test", new ByteBuffer[]{inBuf},
-        new long[]{0}, inBuf.remaining(), null, 100), 1);
-    for(int i=0; i < COUNT; i += 5) {
-      int x = (int) in.next();
-      if (i < COUNT/2) {
-        assertEquals(i & 1, x);
-      } else {
-        assertEquals((i/3) & 1, x);
-      }
-      if (i < COUNT - 5) {
-        in.skip(4);
-      }
-      in.skip(0);
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestBitPack.java b/orc/src/test/org/apache/orc/impl/TestBitPack.java
deleted file mode 100644
index f2d3d6449a..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestBitPack.java
+++ /dev/null
@@ -1,279 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-
-import java.io.File;
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.Collections;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-import com.google.common.primitives.Longs;
-
-public class TestBitPack {
-
-  private static final int SIZE = 100;
-  private static Random rand = new Random(100);
-  Path workDir = new Path(System.getProperty("test.tmp.dir", "target" + File.separator + "test"
-      + File.separator + "tmp"));
-
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile." + testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  private long[] deltaEncode(long[] inp) {
-    long[] output = new long[inp.length];
-    SerializationUtils utils = new SerializationUtils();
-    for (int i = 0; i < inp.length; i++) {
-      output[i] = utils.zigzagEncode(inp[i]);
-    }
-    return output;
-  }
-
-  private long nextLong(Random rng, long n) {
-    long bits, val;
-    do {
-      bits = (rng.nextLong() << 1) >>> 1;
-      val = bits % n;
-    } while (bits - val + (n - 1) < 0L);
-    return val;
-  }
-
-  private void runTest(int numBits) throws IOException {
-    long[] inp = new long[SIZE];
-    for (int i = 0; i < SIZE; i++) {
-      long val = 0;
-      if (numBits <= 32) {
-        if (numBits == 1) {
-          val = -1 * rand.nextInt(2);
-        } else {
-          val = rand.nextInt((int) Math.pow(2, numBits - 1));
-        }
-      } else {
-        val = nextLong(rand, (long) Math.pow(2, numBits - 2));
-      }
-      if (val % 2 == 0) {
-        val = -val;
-      }
-      inp[i] = val;
-    }
-    long[] deltaEncoded = deltaEncode(inp);
-    long minInput = Collections.min(Longs.asList(deltaEncoded));
-    long maxInput = Collections.max(Longs.asList(deltaEncoded));
-    long rangeInput = maxInput - minInput;
-    SerializationUtils utils = new SerializationUtils();
-    int fixedWidth = utils.findClosestNumBits(rangeInput);
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    OutStream output = new OutStream("test", SIZE, null, collect);
-    utils.writeInts(deltaEncoded, 0, deltaEncoded.length, fixedWidth, output);
-    output.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    long[] buff = new long[SIZE];
-    utils.readInts(buff, 0, SIZE, fixedWidth, InStream.create("test", new ByteBuffer[] { inBuf },
-        new long[] { 0 }, inBuf.remaining(), null, SIZE));
-    for (int i = 0; i < SIZE; i++) {
-      buff[i] = utils.zigzagDecode(buff[i]);
-    }
-    assertEquals(numBits, fixedWidth);
-    assertArrayEquals(inp, buff);
-  }
-
-  @Test
-  public void test01BitPacking1Bit() throws IOException {
-    runTest(1);
-  }
-
-  @Test
-  public void test02BitPacking2Bit() throws IOException {
-    runTest(2);
-  }
-
-  @Test
-  public void test03BitPacking3Bit() throws IOException {
-    runTest(3);
-  }
-
-  @Test
-  public void test04BitPacking4Bit() throws IOException {
-    runTest(4);
-  }
-
-  @Test
-  public void test05BitPacking5Bit() throws IOException {
-    runTest(5);
-  }
-
-  @Test
-  public void test06BitPacking6Bit() throws IOException {
-    runTest(6);
-  }
-
-  @Test
-  public void test07BitPacking7Bit() throws IOException {
-    runTest(7);
-  }
-
-  @Test
-  public void test08BitPacking8Bit() throws IOException {
-    runTest(8);
-  }
-
-  @Test
-  public void test09BitPacking9Bit() throws IOException {
-    runTest(9);
-  }
-
-  @Test
-  public void test10BitPacking10Bit() throws IOException {
-    runTest(10);
-  }
-
-  @Test
-  public void test11BitPacking11Bit() throws IOException {
-    runTest(11);
-  }
-
-  @Test
-  public void test12BitPacking12Bit() throws IOException {
-    runTest(12);
-  }
-
-  @Test
-  public void test13BitPacking13Bit() throws IOException {
-    runTest(13);
-  }
-
-  @Test
-  public void test14BitPacking14Bit() throws IOException {
-    runTest(14);
-  }
-
-  @Test
-  public void test15BitPacking15Bit() throws IOException {
-    runTest(15);
-  }
-
-  @Test
-  public void test16BitPacking16Bit() throws IOException {
-    runTest(16);
-  }
-
-  @Test
-  public void test17BitPacking17Bit() throws IOException {
-    runTest(17);
-  }
-
-  @Test
-  public void test18BitPacking18Bit() throws IOException {
-    runTest(18);
-  }
-
-  @Test
-  public void test19BitPacking19Bit() throws IOException {
-    runTest(19);
-  }
-
-  @Test
-  public void test20BitPacking20Bit() throws IOException {
-    runTest(20);
-  }
-
-  @Test
-  public void test21BitPacking21Bit() throws IOException {
-    runTest(21);
-  }
-
-  @Test
-  public void test22BitPacking22Bit() throws IOException {
-    runTest(22);
-  }
-
-  @Test
-  public void test23BitPacking23Bit() throws IOException {
-    runTest(23);
-  }
-
-  @Test
-  public void test24BitPacking24Bit() throws IOException {
-    runTest(24);
-  }
-
-  @Test
-  public void test26BitPacking26Bit() throws IOException {
-    runTest(26);
-  }
-
-  @Test
-  public void test28BitPacking28Bit() throws IOException {
-    runTest(28);
-  }
-
-  @Test
-  public void test30BitPacking30Bit() throws IOException {
-    runTest(30);
-  }
-
-  @Test
-  public void test32BitPacking32Bit() throws IOException {
-    runTest(32);
-  }
-
-  @Test
-  public void test40BitPacking40Bit() throws IOException {
-    runTest(40);
-  }
-
-  @Test
-  public void test48BitPacking48Bit() throws IOException {
-    runTest(48);
-  }
-
-  @Test
-  public void test56BitPacking56Bit() throws IOException {
-    runTest(56);
-  }
-
-  @Test
-  public void test64BitPacking64Bit() throws IOException {
-    runTest(64);
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestColumnStatisticsImpl.java b/orc/src/test/org/apache/orc/impl/TestColumnStatisticsImpl.java
deleted file mode 100644
index 6165526171..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestColumnStatisticsImpl.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.orc.OrcProto;
-import org.apache.orc.TypeDescription;
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-public class TestColumnStatisticsImpl {
-
-  @Test
-  public void testUpdateDate() throws Exception {
-    ColumnStatisticsImpl stat = ColumnStatisticsImpl.create(TypeDescription.createDate());
-    DateWritable date = new DateWritable(16400);
-    stat.increment();
-    stat.updateDate(date);
-    assertDateStatistics(stat, 1, 16400, 16400);
-
-    date.set(16410);
-    stat.increment();
-    stat.updateDate(date);
-    assertDateStatistics(stat, 2, 16400, 16410);
-
-    date.set(16420);
-    stat.increment();
-    stat.updateDate(date);
-    assertDateStatistics(stat, 3, 16400, 16420);
-  }
-
-  private void assertDateStatistics(ColumnStatisticsImpl stat, int count, int minimum, int maximum) {
-    OrcProto.ColumnStatistics.Builder builder = stat.serialize();
-
-    assertEquals(count, builder.getNumberOfValues());
-    assertTrue(builder.hasDateStatistics());
-    assertFalse(builder.hasStringStatistics());
-
-    OrcProto.DateStatistics protoStat = builder.getDateStatistics();
-    assertTrue(protoStat.hasMinimum());
-    assertEquals(minimum, protoStat.getMinimum());
-    assertTrue(protoStat.hasMaximum());
-    assertEquals(maximum, protoStat.getMaximum());
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestDataReaderProperties.java b/orc/src/test/org/apache/orc/impl/TestDataReaderProperties.java
deleted file mode 100644
index 46546b04b0..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestDataReaderProperties.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.CompressionCodec;
-import org.apache.orc.CompressionKind;
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNull;
-import static org.mockito.Mockito.mock;
-
-public class TestDataReaderProperties {
-
-  private FileSystem mockedFileSystem = mock(FileSystem.class);
-  private Path mockedPath = mock(Path.class);
-  private boolean mockedZeroCopy = false;
-
-  @Test
-  public void testCompleteBuild() {
-    DataReaderProperties properties = DataReaderProperties.builder()
-      .withFileSystem(mockedFileSystem)
-      .withPath(mockedPath)
-      .withCompression(CompressionKind.ZLIB)
-      .withZeroCopy(mockedZeroCopy)
-      .build();
-    assertEquals(mockedFileSystem, properties.getFileSystem());
-    assertEquals(mockedPath, properties.getPath());
-    assertEquals(CompressionKind.ZLIB, properties.getCompression());
-    assertEquals(mockedZeroCopy, properties.getZeroCopy());
-  }
-
-  @Test
-  public void testMissingNonRequiredArgs() {
-    DataReaderProperties properties = DataReaderProperties.builder()
-      .withFileSystem(mockedFileSystem)
-      .withPath(mockedPath)
-      .build();
-    assertEquals(mockedFileSystem, properties.getFileSystem());
-    assertEquals(mockedPath, properties.getPath());
-    assertNull(properties.getCompression());
-    assertFalse(properties.getZeroCopy());
-  }
-
-  @Test(expected = java.lang.NullPointerException.class)
-  public void testEmptyBuild() {
-    DataReaderProperties.builder().build();
-  }
-
-  @Test(expected = java.lang.NullPointerException.class)
-  public void testMissingPath() {
-    DataReaderProperties.builder()
-      .withFileSystem(mockedFileSystem)
-      .withCompression(CompressionKind.NONE)
-      .withZeroCopy(mockedZeroCopy)
-      .build();
-  }
-
-  @Test(expected = java.lang.NullPointerException.class)
-  public void testMissingFileSystem() {
-    DataReaderProperties.builder()
-      .withPath(mockedPath)
-      .withCompression(CompressionKind.NONE)
-      .withZeroCopy(mockedZeroCopy)
-      .build();
-  }
-
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestDynamicArray.java b/orc/src/test/org/apache/orc/impl/TestDynamicArray.java
deleted file mode 100644
index af583f7d60..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestDynamicArray.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.util.Random;
-
-import org.apache.orc.impl.DynamicByteArray;
-import org.apache.orc.impl.DynamicIntArray;
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-
-public class TestDynamicArray {
-
-  @Test
-  public void testByteArray() throws Exception {
-    DynamicByteArray dba = new DynamicByteArray(3, 10);
-    dba.add((byte) 0);
-    dba.add((byte) 1);
-    dba.set(3, (byte) 3);
-    dba.set(2, (byte) 2);
-    dba.add((byte) 4);
-    assertEquals("{0,1,2,3,4}", dba.toString());
-    assertEquals(5, dba.size());
-    byte[] val;
-    val = new byte[0];
-    assertEquals(0, dba.compare(val, 0, 0, 2, 0));
-    assertEquals(-1, dba.compare(val, 0, 0, 2, 1));
-    val = new byte[]{3,42};
-    assertEquals(1, dba.compare(val, 0, 1, 2, 0));
-    assertEquals(1, dba.compare(val, 0, 1, 2, 1));
-    assertEquals(0, dba.compare(val, 0, 1, 3, 1));
-    assertEquals(-1, dba.compare(val, 0, 1, 3, 2));
-    assertEquals(1, dba.compare(val, 0, 2, 3, 1));
-    val = new byte[256];
-    for(int b=-128; b < 128; ++b) {
-      dba.add((byte) b);
-      val[b+128] = (byte) b;
-    }
-    assertEquals(0, dba.compare(val, 0, 256, 5, 256));
-    assertEquals(1, dba.compare(val, 0, 1, 0, 1));
-    assertEquals(1, dba.compare(val, 254, 1, 0, 1));
-    assertEquals(1, dba.compare(val, 120, 1, 64, 1));
-    val = new byte[1024];
-    Random rand = new Random(1701);
-    for(int i = 0; i < val.length; ++i) {
-      rand.nextBytes(val);
-    }
-    dba.add(val, 0, 1024);
-    assertEquals(1285, dba.size());
-    assertEquals(0, dba.compare(val, 0, 1024, 261, 1024));
-  }
-
-  @Test
-  public void testIntArray() throws Exception {
-    DynamicIntArray dia = new DynamicIntArray(10);
-    for(int i=0; i < 10000; ++i) {
-      dia.add(2*i);
-    }
-    assertEquals(10000, dia.size());
-    for(int i=0; i < 10000; ++i) {
-      assertEquals(2*i, dia.get(i));
-    }
-    dia.clear();
-    assertEquals(0, dia.size());
-    dia.add(3);
-    dia.add(12);
-    dia.add(65);
-    assertEquals("{3,12,65}", dia.toString());
-    for(int i=0; i < 5; ++i) {
-      dia.increment(i, 3);
-    }
-    assertEquals("{6,15,68,3,3}", dia.toString());
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestInStream.java b/orc/src/test/org/apache/orc/impl/TestInStream.java
deleted file mode 100644
index 9e65345640..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestInStream.java
+++ /dev/null
@@ -1,314 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import static junit.framework.Assert.assertEquals;
-import static junit.framework.Assert.fail;
-
-import java.io.DataInputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-
-public class TestInStream {
-
-  static class OutputCollector implements OutStream.OutputReceiver {
-    DynamicByteArray buffer = new DynamicByteArray();
-
-    @Override
-    public void output(ByteBuffer buffer) throws IOException {
-      this.buffer.add(buffer.array(), buffer.arrayOffset() + buffer.position(),
-          buffer.remaining());
-    }
-  }
-
-  static class PositionCollector
-      implements PositionProvider, PositionRecorder {
-    private List<Long> positions = new ArrayList<Long>();
-    private int index = 0;
-
-    @Override
-    public long getNext() {
-      return positions.get(index++);
-    }
-
-    @Override
-    public void addPosition(long offset) {
-      positions.add(offset);
-    }
-
-    public void reset() {
-      index = 0;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder builder = new StringBuilder("position: ");
-      for(int i=0; i < positions.size(); ++i) {
-        if (i != 0) {
-          builder.append(", ");
-        }
-        builder.append(positions.get(i));
-      }
-      return builder.toString();
-    }
-  }
-
-  @Test
-  public void testUncompressed() throws Exception {
-    OutputCollector collect = new OutputCollector();
-    OutStream out = new OutStream("test", 100, null, collect);
-    PositionCollector[] positions = new PositionCollector[1024];
-    for(int i=0; i < 1024; ++i) {
-      positions[i] = new PositionCollector();
-      out.getPosition(positions[i]);
-      out.write(i);
-    }
-    out.flush();
-    assertEquals(1024, collect.buffer.size());
-    for(int i=0; i < 1024; ++i) {
-      assertEquals((byte) i, collect.buffer.get(i));
-    }
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    InStream in = InStream.create("test", new ByteBuffer[]{inBuf},
-        new long[]{0}, inBuf.remaining(), null, 100);
-    assertEquals("uncompressed stream test position: 0 length: 1024" +
-                 " range: 0 offset: 0 limit: 0",
-                 in.toString());
-    for(int i=0; i < 1024; ++i) {
-      int x = in.read();
-      assertEquals(i & 0xff, x);
-    }
-    for(int i=1023; i >= 0; --i) {
-      in.seek(positions[i]);
-      assertEquals(i & 0xff, in.read());
-    }
-  }
-
-  @Test
-  public void testCompressed() throws Exception {
-    OutputCollector collect = new OutputCollector();
-    CompressionCodec codec = new ZlibCodec();
-    OutStream out = new OutStream("test", 300, codec, collect);
-    PositionCollector[] positions = new PositionCollector[1024];
-    for(int i=0; i < 1024; ++i) {
-      positions[i] = new PositionCollector();
-      out.getPosition(positions[i]);
-      out.write(i);
-    }
-    out.flush();
-    assertEquals("test", out.toString());
-    assertEquals(961, collect.buffer.size());
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    InStream in = InStream.create("test", new ByteBuffer[]{inBuf},
-        new long[]{0}, inBuf.remaining(), codec, 300);
-    assertEquals("compressed stream test position: 0 length: 961 range: 0" +
-                 " offset: 0 limit: 0 range 0 = 0 to 961",
-                 in.toString());
-    for(int i=0; i < 1024; ++i) {
-      int x = in.read();
-      assertEquals(i & 0xff, x);
-    }
-    assertEquals(0, in.available());
-    for(int i=1023; i >= 0; --i) {
-      in.seek(positions[i]);
-      assertEquals(i & 0xff, in.read());
-    }
-  }
-
-  @Test
-  public void testCorruptStream() throws Exception {
-    OutputCollector collect = new OutputCollector();
-    CompressionCodec codec = new ZlibCodec();
-    OutStream out = new OutStream("test", 500, codec, collect);
-    PositionCollector[] positions = new PositionCollector[1024];
-    for(int i=0; i < 1024; ++i) {
-      positions[i] = new PositionCollector();
-      out.getPosition(positions[i]);
-      out.write(i);
-    }
-    out.flush();
-
-    // now try to read the stream with a buffer that is too small
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    InStream in = InStream.create("test", new ByteBuffer[]{inBuf},
-        new long[]{0}, inBuf.remaining(), codec, 100);
-    byte[] contents = new byte[1024];
-    try {
-      in.read(contents);
-      fail();
-    } catch(IllegalArgumentException iae) {
-      // EXPECTED
-    }
-
-    // make a corrupted header
-    inBuf.clear();
-    inBuf.put((byte) 32);
-    inBuf.put((byte) 0);
-    inBuf.flip();
-    in = InStream.create("test2", new ByteBuffer[]{inBuf}, new long[]{0},
-        inBuf.remaining(), codec, 300);
-    try {
-      in.read();
-      fail();
-    } catch (IllegalStateException ise) {
-      // EXPECTED
-    }
-  }
-
-  @Test
-  public void testDisjointBuffers() throws Exception {
-    OutputCollector collect = new OutputCollector();
-    CompressionCodec codec = new ZlibCodec();
-    OutStream out = new OutStream("test", 400, codec, collect);
-    PositionCollector[] positions = new PositionCollector[1024];
-    DataOutput stream = new DataOutputStream(out);
-    for(int i=0; i < 1024; ++i) {
-      positions[i] = new PositionCollector();
-      out.getPosition(positions[i]);
-      stream.writeInt(i);
-    }
-    out.flush();
-    assertEquals("test", out.toString());
-    assertEquals(1674, collect.buffer.size());
-    ByteBuffer[] inBuf = new ByteBuffer[3];
-    inBuf[0] = ByteBuffer.allocate(500);
-    inBuf[1] = ByteBuffer.allocate(1200);
-    inBuf[2] = ByteBuffer.allocate(500);
-    collect.buffer.setByteBuffer(inBuf[0], 0, 483);
-    collect.buffer.setByteBuffer(inBuf[1], 483, 1625 - 483);
-    collect.buffer.setByteBuffer(inBuf[2], 1625, 1674 - 1625);
-
-    for(int i=0; i < inBuf.length; ++i) {
-      inBuf[i].flip();
-    }
-    InStream in = InStream.create("test", inBuf,
-        new long[]{0,483, 1625}, 1674, codec, 400);
-    assertEquals("compressed stream test position: 0 length: 1674 range: 0" +
-                 " offset: 0 limit: 0 range 0 = 0 to 483;" +
-                 "  range 1 = 483 to 1142;  range 2 = 1625 to 49",
-                 in.toString());
-    DataInputStream inStream = new DataInputStream(in);
-    for(int i=0; i < 1024; ++i) {
-      int x = inStream.readInt();
-      assertEquals(i, x);
-    }
-    assertEquals(0, in.available());
-    for(int i=1023; i >= 0; --i) {
-      in.seek(positions[i]);
-      assertEquals(i, inStream.readInt());
-    }
-
-    in = InStream.create("test", new ByteBuffer[]{inBuf[1], inBuf[2]},
-        new long[]{483, 1625}, 1674, codec, 400);
-    inStream = new DataInputStream(in);
-    positions[303].reset();
-    in.seek(positions[303]);
-    for(int i=303; i < 1024; ++i) {
-      assertEquals(i, inStream.readInt());
-    }
-
-    in = InStream.create("test", new ByteBuffer[]{inBuf[0], inBuf[2]},
-        new long[]{0, 1625}, 1674, codec, 400);
-    inStream = new DataInputStream(in);
-    positions[1001].reset();
-    for(int i=0; i < 300; ++i) {
-      assertEquals(i, inStream.readInt());
-    }
-    in.seek(positions[1001]);
-    for(int i=1001; i < 1024; ++i) {
-      assertEquals(i, inStream.readInt());
-    }
-  }
-
-  @Test
-  public void testUncompressedDisjointBuffers() throws Exception {
-    OutputCollector collect = new OutputCollector();
-    OutStream out = new OutStream("test", 400, null, collect);
-    PositionCollector[] positions = new PositionCollector[1024];
-    DataOutput stream = new DataOutputStream(out);
-    for(int i=0; i < 1024; ++i) {
-      positions[i] = new PositionCollector();
-      out.getPosition(positions[i]);
-      stream.writeInt(i);
-    }
-    out.flush();
-    assertEquals("test", out.toString());
-    assertEquals(4096, collect.buffer.size());
-    ByteBuffer[] inBuf = new ByteBuffer[3];
-    inBuf[0] = ByteBuffer.allocate(1100);
-    inBuf[1] = ByteBuffer.allocate(2200);
-    inBuf[2] = ByteBuffer.allocate(1100);
-    collect.buffer.setByteBuffer(inBuf[0], 0, 1024);
-    collect.buffer.setByteBuffer(inBuf[1], 1024, 2048);
-    collect.buffer.setByteBuffer(inBuf[2], 3072, 1024);
-
-    for(int i=0; i < inBuf.length; ++i) {
-      inBuf[i].flip();
-    }
-    InStream in = InStream.create("test", inBuf,
-        new long[]{0, 1024, 3072}, 4096, null, 400);
-    assertEquals("uncompressed stream test position: 0 length: 4096" +
-                 " range: 0 offset: 0 limit: 0",
-                 in.toString());
-    DataInputStream inStream = new DataInputStream(in);
-    for(int i=0; i < 1024; ++i) {
-      int x = inStream.readInt();
-      assertEquals(i, x);
-    }
-    assertEquals(0, in.available());
-    for(int i=1023; i >= 0; --i) {
-      in.seek(positions[i]);
-      assertEquals(i, inStream.readInt());
-    }
-
-    in = InStream.create("test", new ByteBuffer[]{inBuf[1], inBuf[2]},
-        new long[]{1024, 3072}, 4096, null, 400);
-    inStream = new DataInputStream(in);
-    positions[256].reset();
-    in.seek(positions[256]);
-    for(int i=256; i < 1024; ++i) {
-      assertEquals(i, inStream.readInt());
-    }
-
-    in = InStream.create("test", new ByteBuffer[]{inBuf[0], inBuf[2]},
-        new long[]{0, 3072}, 4096, null, 400);
-    inStream = new DataInputStream(in);
-    positions[768].reset();
-    for(int i=0; i < 256; ++i) {
-      assertEquals(i, inStream.readInt());
-    }
-    in.seek(positions[768]);
-    for(int i=768; i < 1024; ++i) {
-      assertEquals(i, inStream.readInt());
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestIntegerCompressionReader.java b/orc/src/test/org/apache/orc/impl/TestIntegerCompressionReader.java
deleted file mode 100644
index 399f35e73a..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestIntegerCompressionReader.java
+++ /dev/null
@@ -1,130 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.nio.ByteBuffer;
-import java.util.Random;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-
-public class TestIntegerCompressionReader {
-
-  public void runSeekTest(CompressionCodec codec) throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthIntegerWriterV2 out = new RunLengthIntegerWriterV2(
-        new OutStream("test", 1000, codec, collect), true);
-    TestInStream.PositionCollector[] positions =
-        new TestInStream.PositionCollector[4096];
-    Random random = new Random(99);
-    int[] junk = new int[2048];
-    for(int i=0; i < junk.length; ++i) {
-      junk[i] = random.nextInt();
-    }
-    for(int i=0; i < 4096; ++i) {
-      positions[i] = new TestInStream.PositionCollector();
-      out.getPosition(positions[i]);
-      // test runs, incrementing runs, non-runs
-      if (i < 1024) {
-        out.write(i/4);
-      } else if (i < 2048) {
-        out.write(2*i);
-      } else {
-        out.write(junk[i-2048]);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthIntegerReaderV2 in =
-      new RunLengthIntegerReaderV2(InStream.create
-                                   ("test", new ByteBuffer[]{inBuf},
-                                    new long[]{0}, inBuf.remaining(),
-                                    codec, 1000), true, false);
-    for(int i=0; i < 2048; ++i) {
-      int x = (int) in.next();
-      if (i < 1024) {
-        assertEquals(i/4, x);
-      } else if (i < 2048) {
-        assertEquals(2*i, x);
-      } else {
-        assertEquals(junk[i-2048], x);
-      }
-    }
-    for(int i=2047; i >= 0; --i) {
-      in.seek(positions[i]);
-      int x = (int) in.next();
-      if (i < 1024) {
-        assertEquals(i/4, x);
-      } else if (i < 2048) {
-        assertEquals(2*i, x);
-      } else {
-        assertEquals(junk[i-2048], x);
-      }
-    }
-  }
-
-  @Test
-  public void testUncompressedSeek() throws Exception {
-    runSeekTest(null);
-  }
-
-  @Test
-  public void testCompressedSeek() throws Exception {
-    runSeekTest(new ZlibCodec());
-  }
-
-  @Test
-  public void testSkips() throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthIntegerWriterV2 out = new RunLengthIntegerWriterV2(
-        new OutStream("test", 100, null, collect), true);
-    for(int i=0; i < 2048; ++i) {
-      if (i < 1024) {
-        out.write(i);
-      } else {
-        out.write(256 * i);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthIntegerReaderV2 in =
-      new RunLengthIntegerReaderV2(InStream.create("test",
-                                                   new ByteBuffer[]{inBuf},
-                                                   new long[]{0},
-                                                   inBuf.remaining(),
-                                                   null, 100), true, false);
-    for(int i=0; i < 2048; i += 10) {
-      int x = (int) in.next();
-      if (i < 1024) {
-        assertEquals(i, x);
-      } else {
-        assertEquals(256 * i, x);
-      }
-      if (i < 2038) {
-        in.skip(9);
-      }
-      in.skip(0);
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestMemoryManager.java b/orc/src/test/org/apache/orc/impl/TestMemoryManager.java
deleted file mode 100644
index f48c545098..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestMemoryManager.java
+++ /dev/null
@@ -1,133 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.orc.impl.MemoryManager;
-import org.hamcrest.BaseMatcher;
-import org.hamcrest.Description;
-import org.junit.Test;
-import org.mockito.Matchers;
-import org.mockito.Mockito;
-
-import java.lang.management.ManagementFactory;
-
-import static junit.framework.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.verify;
-
-/**
- * Test the ORC memory manager.
- */
-public class TestMemoryManager {
-  private static final double ERROR = 0.000001;
-
-  private static class NullCallback implements MemoryManager.Callback {
-    public boolean checkMemory(double newScale) {
-      return false;
-    }
-  }
-
-  @Test
-  public void testBasics() throws Exception {
-    Configuration conf = new Configuration();
-    MemoryManager mgr = new MemoryManager(conf);
-    NullCallback callback = new NullCallback();
-    long poolSize = mgr.getTotalMemoryPool();
-    assertEquals(Math.round(ManagementFactory.getMemoryMXBean().
-        getHeapMemoryUsage().getMax() * 0.5d), poolSize);
-    assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
-    mgr.addWriter(new Path("p1"), 1000, callback);
-    assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
-    mgr.addWriter(new Path("p1"), poolSize / 2, callback);
-    assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
-    mgr.addWriter(new Path("p2"), poolSize / 2, callback);
-    assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
-    mgr.addWriter(new Path("p3"), poolSize / 2, callback);
-    assertEquals(0.6666667, mgr.getAllocationScale(), 0.00001);
-    mgr.addWriter(new Path("p4"), poolSize / 2, callback);
-    assertEquals(0.5, mgr.getAllocationScale(), 0.000001);
-    mgr.addWriter(new Path("p4"), 3 * poolSize / 2, callback);
-    assertEquals(0.3333333, mgr.getAllocationScale(), 0.000001);
-    mgr.removeWriter(new Path("p1"));
-    mgr.removeWriter(new Path("p2"));
-    assertEquals(0.5, mgr.getAllocationScale(), 0.00001);
-    mgr.removeWriter(new Path("p4"));
-    assertEquals(1.0, mgr.getAllocationScale(), 0.00001);
-  }
-
-  @Test
-  public void testConfig() throws Exception {
-    Configuration conf = new Configuration();
-    conf.set("hive.exec.orc.memory.pool", "0.9");
-    MemoryManager mgr = new MemoryManager(conf);
-    long mem =
-        ManagementFactory.getMemoryMXBean().getHeapMemoryUsage().getMax();
-    System.err.print("Memory = " + mem);
-    long pool = mgr.getTotalMemoryPool();
-    assertTrue("Pool too small: " + pool, mem * 0.899 < pool);
-    assertTrue("Pool too big: " + pool, pool < mem * 0.901);
-  }
-
-  private static class DoubleMatcher extends BaseMatcher<Double> {
-    final double expected;
-    final double error;
-    DoubleMatcher(double expected, double error) {
-      this.expected = expected;
-      this.error = error;
-    }
-
-    @Override
-    public boolean matches(Object val) {
-      double dbl = (Double) val;
-      return Math.abs(dbl - expected) <= error;
-    }
-
-    @Override
-    public void describeTo(Description description) {
-      description.appendText("not sufficiently close to ");
-      description.appendText(Double.toString(expected));
-    }
-  }
-
-  private static DoubleMatcher closeTo(double value, double error) {
-    return new DoubleMatcher(value, error);
-  }
-
-  @Test
-  public void testCallback() throws Exception {
-    Configuration conf = new Configuration();
-    MemoryManager mgr = new MemoryManager(conf);
-    long pool = mgr.getTotalMemoryPool();
-    MemoryManager.Callback[] calls = new MemoryManager.Callback[20];
-    for(int i=0; i < calls.length; ++i) {
-      calls[i] = Mockito.mock(MemoryManager.Callback.class);
-      mgr.addWriter(new Path(Integer.toString(i)), pool/4, calls[i]);
-    }
-    // add enough rows to get the memory manager to check the limits
-    for(int i=0; i < 10000; ++i) {
-      mgr.addedRow(1);
-    }
-    for(int call=0; call < calls.length; ++call) {
-      Mockito.verify(calls[call], Mockito.times(2))
-          .checkMemory(Matchers.doubleThat(closeTo(0.2, ERROR)));
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestOrcWideTable.java b/orc/src/test/org/apache/orc/impl/TestOrcWideTable.java
deleted file mode 100644
index efa3ffb6bf..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestOrcWideTable.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import static org.junit.Assert.assertEquals;
-
-import java.io.IOException;
-
-import org.junit.Test;
-
-public class TestOrcWideTable {
-
-  @Test
-  public void testBufferSizeFor1Col() throws IOException {
-    assertEquals(128 * 1024, PhysicalFsWriter.getEstimatedBufferSize(512 * 1024 * 1024,
-        1, 128*1024));
-  }
-
-  @Test
-  public void testBufferSizeFor50Col() throws IOException {
-    assertEquals(256 * 1024, PhysicalFsWriter.getEstimatedBufferSize(256 * 1024 * 1024,
-        50, 256*1024));
-  }
-
-  @Test
-  public void testBufferSizeFor1000Col() throws IOException {
-    assertEquals(32 * 1024, PhysicalFsWriter.getEstimatedBufferSize(512 * 1024 * 1024,
-        1000, 128*1024));
-  }
-
-  @Test
-  public void testBufferSizeFor2000Col() throws IOException {
-    assertEquals(16 * 1024, PhysicalFsWriter.getEstimatedBufferSize(512 * 1024 * 1024,
-        2000, 256*1024));
-  }
-
-  @Test
-  public void testBufferSizeFor4000Col() throws IOException {
-    assertEquals(8 * 1024, PhysicalFsWriter.getEstimatedBufferSize(512 * 1024 * 1024,
-        4000, 256*1024));
-  }
-
-  @Test
-  public void testBufferSizeFor25000Col() throws IOException {
-    assertEquals(4 * 1024, PhysicalFsWriter.getEstimatedBufferSize(512 * 1024 * 1024,
-        25000, 256*1024));
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestOutStream.java b/orc/src/test/org/apache/orc/impl/TestOutStream.java
deleted file mode 100644
index e9614d51f2..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestOutStream.java
+++ /dev/null
@@ -1,43 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-import org.mockito.Mockito;
-
-import java.nio.ByteBuffer;
-
-import static org.junit.Assert.assertEquals;
-
-public class TestOutStream {
-
-  @Test
-  public void testFlush() throws Exception {
-    OutStream.OutputReceiver receiver =
-        Mockito.mock(OutStream.OutputReceiver.class);
-    CompressionCodec codec = new ZlibCodec();
-    OutStream stream = new OutStream("test", 128*1024, codec, receiver);
-    assertEquals(0L, stream.getBufferSize());
-    stream.write(new byte[]{0, 1, 2});
-    stream.flush();
-    Mockito.verify(receiver).output(Mockito.any(ByteBuffer.class));
-    assertEquals(0L, stream.getBufferSize());
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestRLEv2.java b/orc/src/test/org/apache/orc/impl/TestRLEv2.java
deleted file mode 100644
index e139619c92..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestRLEv2.java
+++ /dev/null
@@ -1,307 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static org.junit.Assert.assertEquals;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.PrintStream;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcFile;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.apache.orc.tools.FileDump;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-public class TestRLEv2 {
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-  Path testFilePath;
-  Configuration conf;
-  FileSystem fs;
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  @Before
-  public void openFileSystem () throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestRLEv2." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  void appendInt(VectorizedRowBatch batch, int i) {
-    ((LongColumnVector) batch.cols[0]).vector[batch.size++] = i;
-  }
-
-  @Test
-  public void testFixedDeltaZero() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (int i = 0; i < 5120; ++i) {
-      appendInt(batch, 123);
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // 10 runs of 512 elements. Each run has 2 bytes header, 2 bytes base (base = 123,
-    // zigzag encoded varint) and 1 byte delta (delta = 0). In total, 5 bytes per run.
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 50"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testFixedDeltaOne() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (int i = 0; i < 5120; ++i) {
-      appendInt(batch, i % 512);
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // 10 runs of 512 elements. Each run has 2 bytes header, 1 byte base (base = 0)
-    // and 1 byte delta (delta = 1). In total, 4 bytes per run.
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 40"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testFixedDeltaOneDescending() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (int i = 0; i < 5120; ++i) {
-      appendInt(batch, 512 - (i % 512));
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // 10 runs of 512 elements. Each run has 2 bytes header, 2 byte base (base = 512, zigzag + varint)
-    // and 1 byte delta (delta = 1). In total, 5 bytes per run.
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 50"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testFixedDeltaLarge() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (int i = 0; i < 5120; ++i) {
-      appendInt(batch, i % 512 + ((i % 512) * 100));
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // 10 runs of 512 elements. Each run has 2 bytes header, 1 byte base (base = 0)
-    // and 2 bytes delta (delta = 100, zigzag encoded varint). In total, 5 bytes per run.
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 50"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testFixedDeltaLargeDescending() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (int i = 0; i < 5120; ++i) {
-      appendInt(batch, (512 - i % 512) + ((i % 512) * 100));
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // 10 runs of 512 elements. Each run has 2 bytes header, 2 byte base (base = 512, zigzag + varint)
-    // and 2 bytes delta (delta = 100, zigzag encoded varint). In total, 6 bytes per run.
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 60"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testShortRepeat() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    for (int i = 0; i < 5; ++i) {
-      appendInt(batch, 10);
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // 1 byte header + 1 byte value
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 2"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testDeltaUnknownSign() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    appendInt(batch, 0);
-    for (int i = 0; i < 511; ++i) {
-      appendInt(batch, i);
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // monotonicity will be undetermined for this sequence 0,0,1,2,3,...510. Hence DIRECT encoding
-    // will be used. 2 bytes for header and 640 bytes for data (512 values with fixed bit of 10 bits
-    // each, 5120/8 = 640). Total bytes 642
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 642"));
-    System.setOut(origOut);
-  }
-
-  @Test
-  public void testPatchedBase() throws Exception {
-    TypeDescription schema = TypeDescription.createInt();
-    Writer w = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .compress(CompressionKind.NONE)
-            .setSchema(schema)
-            .rowIndexStride(0)
-            .encodingStrategy(OrcFile.EncodingStrategy.COMPRESSION)
-            .version(OrcFile.Version.V_0_12)
-    );
-
-    Random rand = new Random(123);
-    VectorizedRowBatch batch = schema.createRowBatch(5120);
-    appendInt(batch, 10000000);
-    for (int i = 0; i < 511; ++i) {
-      appendInt(batch, rand.nextInt(i+1));
-    }
-    w.addRowBatch(batch);
-    w.close();
-
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toUri().toString()});
-    System.out.flush();
-    String outDump = new String(myOut.toByteArray());
-    // use PATCHED_BASE encoding
-    assertEquals(true, outDump.contains("Stream: column 0 section DATA start: 3 length 583"));
-    System.setOut(origOut);
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestReaderImpl.java b/orc/src/test/org/apache/orc/impl/TestReaderImpl.java
deleted file mode 100644
index 23d0dab2c4..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestReaderImpl.java
+++ /dev/null
@@ -1,152 +0,0 @@
-/*
- * Copyright 2016 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import java.io.ByteArrayInputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.nio.charset.CharacterCodingException;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PositionedReadable;
-import org.apache.hadoop.fs.Seekable;
-import org.apache.orc.FileFormatException;
-import org.apache.hadoop.io.Text;
-import org.apache.orc.OrcFile;
-import org.junit.Test;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.rules.ExpectedException;
-
-public class TestReaderImpl {
-
-  @Rule
-  public ExpectedException thrown = ExpectedException.none();
-
-  private final Path path = new Path("test-file.orc");
-  private FSDataInputStream in;
-  private int psLen;
-  private ByteBuffer buffer;
-
-  @Before
-  public void setup() {
-    in = null;
-  }
-
-  @Test
-  public void testEnsureOrcFooterSmallTextFile() throws IOException {
-    prepareTestCase("1".getBytes());
-    thrown.expect(FileFormatException.class);
-    ReaderImpl.ensureOrcFooter(in, path, psLen, buffer);
-  }
-
-  @Test
-  public void testEnsureOrcFooterLargeTextFile() throws IOException {
-    prepareTestCase("This is Some Text File".getBytes());
-    thrown.expect(FileFormatException.class);
-    ReaderImpl.ensureOrcFooter(in, path, psLen, buffer);
-  }
-
-  @Test
-  public void testEnsureOrcFooter011ORCFile() throws IOException {
-    prepareTestCase(composeContent(OrcFile.MAGIC, "FOOTER"));
-    ReaderImpl.ensureOrcFooter(in, path, psLen, buffer);
-  }
-
-  @Test
-  public void testEnsureOrcFooterCorrectORCFooter() throws IOException {
-    prepareTestCase(composeContent("", OrcFile.MAGIC));
-    ReaderImpl.ensureOrcFooter(in, path, psLen, buffer);
-  }
-
-  private void prepareTestCase(byte[] bytes) {
-    buffer = ByteBuffer.wrap(bytes);
-    psLen = buffer.get(bytes.length - 1) & 0xff;
-    in = new FSDataInputStream(new SeekableByteArrayInputStream(bytes));
-  }
-
-  private byte[] composeContent(String headerStr, String footerStr) throws CharacterCodingException {
-    ByteBuffer header = Text.encode(headerStr);
-    ByteBuffer footer = Text.encode(footerStr);
-    int headerLen = header.remaining();
-    int footerLen = footer.remaining() + 1;
-
-    ByteBuffer buf = ByteBuffer.allocate(headerLen + footerLen);
-
-    buf.put(header);
-    buf.put(footer);
-    buf.put((byte) footerLen);
-    return buf.array();
-  }
-
-  private static final class SeekableByteArrayInputStream extends ByteArrayInputStream
-          implements Seekable, PositionedReadable {
-
-    public SeekableByteArrayInputStream(byte[] buf) {
-      super(buf);
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-      this.reset();
-      this.skip(pos);
-    }
-
-    @Override
-    public long getPos() throws IOException {
-      return pos;
-    }
-
-    @Override
-    public boolean seekToNewSource(long targetPos) throws IOException {
-      return false;
-    }
-
-    @Override
-    public int read(long position, byte[] buffer, int offset, int length)
-            throws IOException {
-      long oldPos = getPos();
-      int nread = -1;
-      try {
-        seek(position);
-        nread = read(buffer, offset, length);
-      } finally {
-        seek(oldPos);
-      }
-      return nread;
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer, int offset, int length)
-            throws IOException {
-      int nread = 0;
-      while (nread < length) {
-        int nbytes = read(position + nread, buffer, offset + nread, length - nread);
-        if (nbytes < 0) {
-          throw new EOFException("End of file reached before reading fully.");
-        }
-        nread += nbytes;
-      }
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer)
-            throws IOException {
-      readFully(position, buffer, 0, buffer.length);
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestRecordReaderImpl.java b/orc/src/test/org/apache/orc/impl/TestRecordReaderImpl.java
deleted file mode 100644
index 30b42ee001..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestRecordReaderImpl.java
+++ /dev/null
@@ -1,1691 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import static junit.framework.Assert.assertEquals;
-import static org.hamcrest.core.Is.is;
-import static org.junit.Assert.*;
-import static org.mockito.Mockito.any;
-import static org.mockito.Mockito.atLeastOnce;
-import static org.mockito.Mockito.doThrow;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.verify;
-import static org.mockito.Mockito.when;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStream;
-import java.sql.Timestamp;
-import java.util.ArrayList;
-import java.util.List;
-
-import junit.framework.Assert;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PositionedReadable;
-import org.apache.hadoop.fs.Seekable;
-import org.apache.hadoop.hive.common.io.DiskRangeList;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl;
-import org.apache.orc.BloomFilterIO;
-import org.apache.orc.DataReader;
-import org.apache.orc.RecordReader;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.apache.orc.impl.RecordReaderImpl.Location;
-import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.orc.ColumnStatistics;
-import org.apache.orc.OrcFile;
-import org.apache.orc.Reader;
-import org.apache.orc.OrcProto;
-
-import org.junit.Test;
-import org.mockito.MockSettings;
-import org.mockito.Mockito;
-
-public class TestRecordReaderImpl {
-  /**
-   * Create a predicate leaf. This is used by another test.
-   */
-  public static PredicateLeaf createPredicateLeaf(PredicateLeaf.Operator operator,
-                                                  PredicateLeaf.Type type,
-                                                  String columnName,
-                                                  Object literal,
-                                                  List<Object> literalList) {
-    return new SearchArgumentImpl.PredicateLeafImpl(operator, type, columnName,
-        literal, literalList, null);
-  }
-
-  // can add .verboseLogging() to cause Mockito to log invocations
-  private final MockSettings settings = Mockito.withSettings().verboseLogging();
-
-  static class BufferInStream
-      extends InputStream implements PositionedReadable, Seekable {
-    private final byte[] buffer;
-    private final int length;
-    private int position = 0;
-
-    BufferInStream(byte[] bytes, int length) {
-      this.buffer = bytes;
-      this.length = length;
-    }
-
-    @Override
-    public int read() {
-      if (position < length) {
-        return buffer[position++];
-      }
-      return -1;
-    }
-
-    @Override
-    public int read(byte[] bytes, int offset, int length) {
-      int lengthToRead = Math.min(length, this.length - this.position);
-      if (lengthToRead >= 0) {
-        for(int i=0; i < lengthToRead; ++i) {
-          bytes[offset + i] = buffer[position++];
-        }
-        return lengthToRead;
-      } else {
-        return -1;
-      }
-    }
-
-    @Override
-    public int read(long position, byte[] bytes, int offset, int length) {
-      this.position = (int) position;
-      return read(bytes, offset, length);
-    }
-
-    @Override
-    public void readFully(long position, byte[] bytes, int offset,
-                          int length) throws IOException {
-      this.position = (int) position;
-      while (length > 0) {
-        int result = read(bytes, offset, length);
-        offset += result;
-        length -= result;
-        if (result < 0) {
-          throw new IOException("Read past end of buffer at " + offset);
-        }
-      }
-    }
-
-    @Override
-    public void readFully(long position, byte[] bytes) throws IOException {
-      readFully(position, bytes, 0, bytes.length);
-    }
-
-    @Override
-    public void seek(long position) {
-      this.position = (int) position;
-    }
-
-    @Override
-    public long getPos() {
-      return position;
-    }
-
-    @Override
-    public boolean seekToNewSource(long position) throws IOException {
-      this.position = (int) position;
-      return false;
-    }
-  }
-
-  @Test
-  public void testMaxLengthToReader() throws Exception {
-    Configuration conf = new Configuration();
-    OrcProto.Type rowType = OrcProto.Type.newBuilder()
-        .setKind(OrcProto.Type.Kind.STRUCT).build();
-    OrcProto.Footer footer = OrcProto.Footer.newBuilder()
-        .setHeaderLength(0).setContentLength(0).setNumberOfRows(0)
-        .setRowIndexStride(0).addTypes(rowType).build();
-    OrcProto.PostScript ps = OrcProto.PostScript.newBuilder()
-        .setCompression(OrcProto.CompressionKind.NONE)
-        .setFooterLength(footer.getSerializedSize())
-        .setMagic("ORC").addVersion(0).addVersion(11).build();
-    DataOutputBuffer buffer = new DataOutputBuffer();
-    footer.writeTo(buffer);
-    ps.writeTo(buffer);
-    buffer.write(ps.getSerializedSize());
-    FileSystem fs = mock(FileSystem.class, settings);
-    FSDataInputStream file =
-        new FSDataInputStream(new BufferInStream(buffer.getData(),
-            buffer.getLength()));
-    Path p = new Path("/dir/file.orc");
-    when(fs.open(p)).thenReturn(file);
-    OrcFile.ReaderOptions options = OrcFile.readerOptions(conf);
-    options.filesystem(fs);
-    options.maxLength(buffer.getLength());
-    when(fs.getFileStatus(p))
-        .thenReturn(new FileStatus(10, false, 3, 3000, 0, p));
-    Reader reader = OrcFile.createReader(p, options);
-  }
-
-  @Test
-  public void testCompareToRangeInt() throws Exception {
-    assertEquals(Location.BEFORE,
-      RecordReaderImpl.compareToRange(19L, 20L, 40L));
-    assertEquals(Location.AFTER,
-      RecordReaderImpl.compareToRange(41L, 20L, 40L));
-    assertEquals(Location.MIN,
-        RecordReaderImpl.compareToRange(20L, 20L, 40L));
-    assertEquals(Location.MIDDLE,
-        RecordReaderImpl.compareToRange(21L, 20L, 40L));
-    assertEquals(Location.MAX,
-      RecordReaderImpl.compareToRange(40L, 20L, 40L));
-    assertEquals(Location.BEFORE,
-      RecordReaderImpl.compareToRange(0L, 1L, 1L));
-    assertEquals(Location.MIN,
-      RecordReaderImpl.compareToRange(1L, 1L, 1L));
-    assertEquals(Location.AFTER,
-      RecordReaderImpl.compareToRange(2L, 1L, 1L));
-  }
-
-  @Test
-  public void testCompareToRangeString() throws Exception {
-    assertEquals(Location.BEFORE,
-        RecordReaderImpl.compareToRange("a", "b", "c"));
-    assertEquals(Location.AFTER,
-        RecordReaderImpl.compareToRange("d", "b", "c"));
-    assertEquals(Location.MIN,
-        RecordReaderImpl.compareToRange("b", "b", "c"));
-    assertEquals(Location.MIDDLE,
-        RecordReaderImpl.compareToRange("bb", "b", "c"));
-    assertEquals(Location.MAX,
-        RecordReaderImpl.compareToRange("c", "b", "c"));
-    assertEquals(Location.BEFORE,
-        RecordReaderImpl.compareToRange("a", "b", "b"));
-    assertEquals(Location.MIN,
-        RecordReaderImpl.compareToRange("b", "b", "b"));
-    assertEquals(Location.AFTER,
-        RecordReaderImpl.compareToRange("c", "b", "b"));
-  }
-
-  @Test
-  public void testCompareToCharNeedConvert() throws Exception {
-    assertEquals(Location.BEFORE,
-      RecordReaderImpl.compareToRange("apple", "hello", "world"));
-    assertEquals(Location.AFTER,
-      RecordReaderImpl.compareToRange("zombie", "hello", "world"));
-    assertEquals(Location.MIN,
-        RecordReaderImpl.compareToRange("hello", "hello", "world"));
-    assertEquals(Location.MIDDLE,
-        RecordReaderImpl.compareToRange("pilot", "hello", "world"));
-    assertEquals(Location.MAX,
-      RecordReaderImpl.compareToRange("world", "hello", "world"));
-    assertEquals(Location.BEFORE,
-      RecordReaderImpl.compareToRange("apple", "hello", "hello"));
-    assertEquals(Location.MIN,
-      RecordReaderImpl.compareToRange("hello", "hello", "hello"));
-    assertEquals(Location.AFTER,
-      RecordReaderImpl.compareToRange("zombie", "hello", "hello"));
-  }
-
-  @Test
-  public void testGetMin() throws Exception {
-    assertEquals(10L, RecordReaderImpl.getMin(
-      ColumnStatisticsImpl.deserialize(createIntStats(10L, 100L))));
-    assertEquals(10.0d, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(
-      OrcProto.ColumnStatistics.newBuilder()
-        .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder()
-          .setMinimum(10.0d).setMaximum(100.0d).build()).build())));
-    assertEquals(null, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(
-      OrcProto.ColumnStatistics.newBuilder()
-        .setStringStatistics(OrcProto.StringStatistics.newBuilder().build())
-        .build())));
-    assertEquals("a", RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(
-      OrcProto.ColumnStatistics.newBuilder()
-        .setStringStatistics(OrcProto.StringStatistics.newBuilder()
-          .setMinimum("a").setMaximum("b").build()).build())));
-    assertEquals("hello", RecordReaderImpl.getMin(ColumnStatisticsImpl
-      .deserialize(createStringStats("hello", "world"))));
-    assertEquals(HiveDecimal.create("111.1"), RecordReaderImpl.getMin(ColumnStatisticsImpl
-      .deserialize(createDecimalStats("111.1", "112.1"))));
-  }
-
-  private static OrcProto.ColumnStatistics createIntStats(Long min,
-                                                          Long max) {
-    OrcProto.IntegerStatistics.Builder intStats =
-        OrcProto.IntegerStatistics.newBuilder();
-    if (min != null) {
-      intStats.setMinimum(min);
-    }
-    if (max != null) {
-      intStats.setMaximum(max);
-    }
-    return OrcProto.ColumnStatistics.newBuilder()
-        .setIntStatistics(intStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createBooleanStats(int n, int trueCount) {
-    OrcProto.BucketStatistics.Builder boolStats = OrcProto.BucketStatistics.newBuilder();
-    boolStats.addCount(trueCount);
-    return OrcProto.ColumnStatistics.newBuilder().setNumberOfValues(n).setBucketStatistics(
-      boolStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createIntStats(int min, int max) {
-    OrcProto.IntegerStatistics.Builder intStats = OrcProto.IntegerStatistics.newBuilder();
-    intStats.setMinimum(min);
-    intStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setIntStatistics(intStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createDoubleStats(double min, double max) {
-    OrcProto.DoubleStatistics.Builder dblStats = OrcProto.DoubleStatistics.newBuilder();
-    dblStats.setMinimum(min);
-    dblStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setDoubleStatistics(dblStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createStringStats(String min, String max,
-      boolean hasNull) {
-    OrcProto.StringStatistics.Builder strStats = OrcProto.StringStatistics.newBuilder();
-    strStats.setMinimum(min);
-    strStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setStringStatistics(strStats.build())
-        .setHasNull(hasNull).build();
-  }
-
-  private static OrcProto.ColumnStatistics createStringStats(String min, String max) {
-    OrcProto.StringStatistics.Builder strStats = OrcProto.StringStatistics.newBuilder();
-    strStats.setMinimum(min);
-    strStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setStringStatistics(strStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createDateStats(int min, int max) {
-    OrcProto.DateStatistics.Builder dateStats = OrcProto.DateStatistics.newBuilder();
-    dateStats.setMinimum(min);
-    dateStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setDateStatistics(dateStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createTimestampStats(long min, long max) {
-    OrcProto.TimestampStatistics.Builder tsStats = OrcProto.TimestampStatistics.newBuilder();
-    tsStats.setMinimum(min);
-    tsStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setTimestampStatistics(tsStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createDecimalStats(String min, String max) {
-    OrcProto.DecimalStatistics.Builder decStats = OrcProto.DecimalStatistics.newBuilder();
-    decStats.setMinimum(min);
-    decStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setDecimalStatistics(decStats.build()).build();
-  }
-
-  private static OrcProto.ColumnStatistics createDecimalStats(String min, String max,
-      boolean hasNull) {
-    OrcProto.DecimalStatistics.Builder decStats = OrcProto.DecimalStatistics.newBuilder();
-    decStats.setMinimum(min);
-    decStats.setMaximum(max);
-    return OrcProto.ColumnStatistics.newBuilder().setDecimalStatistics(decStats.build())
-        .setHasNull(hasNull).build();
-  }
-
-  @Test
-  public void testGetMax() throws Exception {
-    assertEquals(100L, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(createIntStats(10L, 100L))));
-    assertEquals(100.0d, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(
-        OrcProto.ColumnStatistics.newBuilder()
-            .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder()
-                .setMinimum(10.0d).setMaximum(100.0d).build()).build())));
-    assertEquals(null, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(
-        OrcProto.ColumnStatistics.newBuilder()
-            .setStringStatistics(OrcProto.StringStatistics.newBuilder().build())
-            .build())));
-    assertEquals("b", RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(
-        OrcProto.ColumnStatistics.newBuilder()
-            .setStringStatistics(OrcProto.StringStatistics.newBuilder()
-                .setMinimum("a").setMaximum("b").build()).build())));
-    assertEquals("world", RecordReaderImpl.getMax(ColumnStatisticsImpl
-      .deserialize(createStringStats("hello", "world"))));
-    assertEquals(HiveDecimal.create("112.1"), RecordReaderImpl.getMax(ColumnStatisticsImpl
-      .deserialize(createDecimalStats("111.1", "112.1"))));
-  }
-
-  @Test
-  public void testPredEvalWithBooleanStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.BOOLEAN, "x", true, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createBooleanStats(10, 10), pred, null));
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createBooleanStats(10, 0), pred, null));
-
-    pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.BOOLEAN, "x", true, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createBooleanStats(10, 10), pred, null));
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createBooleanStats(10, 0), pred, null));
-
-    pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.BOOLEAN, "x", false, null);
-    assertEquals(TruthValue.NO,
-      RecordReaderImpl.evaluatePredicateProto(createBooleanStats(10, 10), pred, null));
-    assertEquals(TruthValue.YES_NO,
-      RecordReaderImpl.evaluatePredicateProto(createBooleanStats(10, 0), pred, null));
-  }
-
-  @Test
-  public void testPredEvalWithIntStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10, 100), pred, null));
-
-    // Stats gets converted to column type. "15" is outside of "10" and "100"
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "15", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10, 100), pred, null));
-
-    // Integer stats will not be converted date because of days/seconds/millis ambiguity
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(15).get(), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DECIMAL, "x", new HiveDecimalWritable("15"), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15), null);
-    assertEquals(TruthValue.YES_NO,
-      RecordReaderImpl.evaluatePredicateProto(createIntStats(10, 100), pred, null));
-  }
-
-  @Test
-  public void testPredEvalWithDoubleStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-
-    // Stats gets converted to column type. "15.0" is outside of "10.0" and "100.0"
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "15", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-
-    // Double is not converted to date type because of days/seconds/millis ambiguity
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(15).get(), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DECIMAL, "x", new HiveDecimalWritable("15"), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15*1000L), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(150*1000L), null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDoubleStats(10.0, 100.0), pred, null));
-  }
-
-  @Test
-  public void testPredEvalWithStringStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 100L, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("10", "1000"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.FLOAT, "x", 100.0, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("10", "1000"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "100", null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("10", "1000"), pred, null));
-
-    // IllegalArgumentException is thrown when converting String to Date, hence YES_NO
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(100).get(), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 1000), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DECIMAL, "x", new HiveDecimalWritable("100"), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("10", "1000"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(100), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("10", "1000"), pred, null));
-  }
-
-  @Test
-  public void testPredEvalWithDateStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    // Date to Integer conversion is not possible.
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    // Date to Float conversion is also not possible.
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "15", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "1970-01-11", null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "15.1", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "__a15__1", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "2000-01-16", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "1970-01-16", null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(15).get(), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(150).get(), null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    // Date to Decimal conversion is also not possible.
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DECIMAL, "x", new HiveDecimalWritable("15"), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15), null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15L * 24L * 60L * 60L * 1000L), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDateStats(10, 100), pred, null));
-  }
-
-  @Test
-  public void testPredEvalWithDecimalStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-
-    // "15" out of range of "10.0" and "100.0"
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "15", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-
-    // Decimal to Date not possible.
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(15).get(), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DECIMAL, "x", new HiveDecimalWritable("15"), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15 * 1000L), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(150 * 1000L), null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createDecimalStats("10.0", "100.0"), pred, null));
-  }
-
-  @Test
-  public void testPredEvalWithTimestampStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10000, 100000), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", "15", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.STRING, "x", new Timestamp(15).toString(), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DATE, "x", new DateWritable(15).get(), null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10 * 24L * 60L * 60L * 1000L,
-          100 * 24L * 60L * 60L * 1000L), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.DECIMAL, "x", new HiveDecimalWritable("15"), null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10000, 100000), pred, null));
-
-    pred = createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,
-        PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15), null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10, 100), pred, null));
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createTimestampStats(10000, 100000), pred, null));
-  }
-
-  @Test
-  public void testEquals() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.LONG,
-            "x", 15L, null);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(20L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 15L), pred, null));
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(0L, 10L), pred, null));
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 15L), pred, null));
-  }
-
-  @Test
-  public void testNullSafeEquals() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG,
-            "x", 15L, null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(20L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 15L), pred, null));
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(0L, 10L), pred, null));
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 15L), pred, null));
-  }
-
-  @Test
-  public void testLessThan() throws Exception {
-    PredicateLeaf lessThan = createPredicateLeaf
-        (PredicateLeaf.Operator.LESS_THAN, PredicateLeaf.Type.LONG,
-            "x", 15L, null);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(20L, 30L), lessThan, null));
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 30L), lessThan, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 30L), lessThan, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 15L), lessThan, null));
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(0L, 10L), lessThan, null));
-  }
-
-  @Test
-  public void testLessThanEquals() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.LESS_THAN_EQUALS, PredicateLeaf.Type.LONG,
-            "x", 15L, null);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(20L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 15L), pred, null));
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(0L, 10L), pred, null));
-  }
-
-  @Test
-  public void testIn() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(10L);
-    args.add(20L);
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.LONG,
-            "x", null, args);
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(20L, 20L), pred, null));
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(30L, 30L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 30L), pred, null));
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(12L, 18L), pred, null));
-  }
-
-  @Test
-  public void testBetween() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(10L);
-    args.add(20L);
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.BETWEEN, PredicateLeaf.Type.LONG,
-            "x", null, args);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(0L, 5L), pred, null));
-    assertEquals(TruthValue.NO_NULL,
-      RecordReaderImpl.evaluatePredicateProto(createIntStats(30L, 40L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-      RecordReaderImpl.evaluatePredicateProto(createIntStats(5L, 15L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(15L, 25L), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(5L, 25L), pred, null));
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(10L, 20L), pred, null));
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(12L, 18L), pred, null));
-  }
-
-  @Test
-  public void testIsNull() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IS_NULL, PredicateLeaf.Type.LONG,
-            "x", null, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createIntStats(20L, 30L), pred, null));
-  }
-
-
-  @Test
-  public void testEqualsWithNullInStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.STRING,
-            "x", "c", null);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("d", "e", true), pred, null)); // before
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "b", true), pred, null)); // after
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "c", true), pred, null)); // max
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", true), pred, null)); // min
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "d", true), pred, null)); // middle
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "c", true), pred, null)); // same
-  }
-
-  @Test
-  public void testNullSafeEqualsWithNullInStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.STRING,
-            "x", "c", null);
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("d", "e", true), pred, null)); // before
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "b", true), pred, null)); // after
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "c", true), pred, null)); // max
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", true), pred, null)); // min
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "d", true), pred, null)); // middle
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "c", true), pred, null)); // same
-  }
-
-  @Test
-  public void testLessThanWithNullInStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.LESS_THAN, PredicateLeaf.Type.STRING,
-            "x", "c", null);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("d", "e", true), pred, null)); // before
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "b", true), pred, null)); // after
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "c", true), pred, null)); // max
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", true), pred, null)); // min
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "d", true), pred, null)); // middle
-    assertEquals(TruthValue.NO_NULL, // min, same stats
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "c", true), pred, null));
-  }
-
-  @Test
-  public void testLessThanEqualsWithNullInStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.LESS_THAN_EQUALS, PredicateLeaf.Type.STRING,
-            "x", "c", null);
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("d", "e", true), pred, null)); // before
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "b", true), pred, null)); // after
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "c", true), pred, null)); // max
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", true), pred, null)); // min
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "d", true), pred, null)); // middle
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "c", true), pred, null)); // same
-  }
-
-  @Test
-  public void testInWithNullInStats() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add("c");
-    args.add("f");
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.STRING,
-            "x", null, args);
-    assertEquals(TruthValue.NO_NULL, // before & after
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("d", "e", true), pred, null));
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "b", true), pred, null)); // after
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("e", "f", true), pred, null)); // max
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", true), pred, null)); // min
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "d", true), pred, null)); // middle
-    assertEquals(TruthValue.YES_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "c", true), pred, null)); // same
-  }
-
-  @Test
-  public void testBetweenWithNullInStats() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add("c");
-    args.add("f");
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.BETWEEN, PredicateLeaf.Type.STRING,
-            "x", null, args);
-    assertEquals(TruthValue.YES_NULL, // before & after
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("d", "e", true), pred, null));
-    assertEquals(TruthValue.YES_NULL, // before & max
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("e", "f", true), pred, null));
-    assertEquals(TruthValue.NO_NULL, // before & before
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("h", "g", true), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL, // before & min
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("f", "g", true), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL, // before & middle
-      RecordReaderImpl.evaluatePredicateProto(createStringStats("e", "g", true), pred, null));
-
-    assertEquals(TruthValue.YES_NULL, // min & after
-      RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "e", true), pred, null));
-    assertEquals(TruthValue.YES_NULL, // min & max
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "f", true), pred, null));
-    assertEquals(TruthValue.YES_NO_NULL, // min & middle
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "g", true), pred, null));
-
-    assertEquals(TruthValue.NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "b", true), pred, null)); // after
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("a", "c", true), pred, null)); // max
-    assertEquals(TruthValue.YES_NO_NULL,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("b", "d", true), pred, null)); // middle
-    assertEquals(TruthValue.YES_NULL, // min & after, same stats
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "c", true), pred, null));
-  }
-
-  @Test
-  public void testIsNullWithNullInStats() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IS_NULL, PredicateLeaf.Type.STRING,
-            "x", null, null);
-    assertEquals(TruthValue.YES_NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", true), pred, null));
-    assertEquals(TruthValue.NO,
-        RecordReaderImpl.evaluatePredicateProto(createStringStats("c", "d", false), pred, null));
-  }
-
-  @Test
-  public void testOverlap() throws Exception {
-    assertTrue(!RecordReaderUtils.overlap(0, 10, -10, -1));
-    assertTrue(RecordReaderUtils.overlap(0, 10, -1, 0));
-    assertTrue(RecordReaderUtils.overlap(0, 10, -1, 1));
-    assertTrue(RecordReaderUtils.overlap(0, 10, 2, 8));
-    assertTrue(RecordReaderUtils.overlap(0, 10, 5, 10));
-    assertTrue(RecordReaderUtils.overlap(0, 10, 10, 11));
-    assertTrue(RecordReaderUtils.overlap(0, 10, 0, 10));
-    assertTrue(RecordReaderUtils.overlap(0, 10, -1, 11));
-    assertTrue(!RecordReaderUtils.overlap(0, 10, 11, 12));
-  }
-
-  private static DiskRangeList diskRanges(Integer... points) {
-    DiskRangeList head = null, tail = null;
-    for(int i = 0; i < points.length; i += 2) {
-      DiskRangeList range = new DiskRangeList(points[i], points[i+1]);
-      if (tail == null) {
-        head = tail = range;
-      } else {
-        tail = tail.insertAfter(range);
-      }
-    }
-    return head;
-  }
-
-  @Test
-  public void testGetIndexPosition() throws Exception {
-    assertEquals(0, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
-            OrcProto.Stream.Kind.PRESENT, true, true));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
-            OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
-            OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(0, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.INT,
-            OrcProto.Stream.Kind.DATA, true, false));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DICTIONARY, OrcProto.Type.Kind.STRING,
-            OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
-            OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
-            OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(6, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
-            OrcProto.Stream.Kind.LENGTH, true, true));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.BINARY,
-            OrcProto.Stream.Kind.LENGTH, false, true));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
-            OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
-            OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(6, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
-            OrcProto.Stream.Kind.SECONDARY, true, true));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.DECIMAL,
-            OrcProto.Stream.Kind.SECONDARY, false, true));
-    assertEquals(4, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
-            OrcProto.Stream.Kind.DATA, true, true));
-    assertEquals(3, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
-            OrcProto.Stream.Kind.DATA, false, true));
-    assertEquals(7, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
-            OrcProto.Stream.Kind.SECONDARY, true, true));
-    assertEquals(5, RecordReaderUtils.getIndexPosition
-        (OrcProto.ColumnEncoding.Kind.DIRECT, OrcProto.Type.Kind.TIMESTAMP,
-            OrcProto.Stream.Kind.SECONDARY, false, true));
-  }
-
-  @Test
-  public void testPartialPlan() throws Exception {
-    DiskRangeList result;
-
-    // set the streams
-    List<OrcProto.Stream> streams = new ArrayList<OrcProto.Stream>();
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.PRESENT)
-        .setColumn(1).setLength(1000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DATA)
-        .setColumn(1).setLength(99000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.PRESENT)
-        .setColumn(2).setLength(2000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DATA)
-        .setColumn(2).setLength(98000).build());
-
-    boolean[] columns = new boolean[]{true, true, false};
-    boolean[] rowGroups = new boolean[]{true, true, false, false, true, false};
-
-    // set the index
-    OrcProto.RowIndex[] indexes = new OrcProto.RowIndex[columns.length];
-    indexes[1] = OrcProto.RowIndex.newBuilder()
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(0).addPositions(-1).addPositions(-1)
-            .addPositions(0)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(100).addPositions(-1).addPositions(-1)
-            .addPositions(10000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(200).addPositions(-1).addPositions(-1)
-            .addPositions(20000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(300).addPositions(-1).addPositions(-1)
-            .addPositions(30000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(400).addPositions(-1).addPositions(-1)
-            .addPositions(40000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(500).addPositions(-1).addPositions(-1)
-            .addPositions(50000)
-            .build())
-        .build();
-
-    // set encodings
-    List<OrcProto.ColumnEncoding> encodings =
-        new ArrayList<OrcProto.ColumnEncoding>();
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-                    .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-
-    // set types struct{x: int, y: int}
-    List<OrcProto.Type> types = new ArrayList<OrcProto.Type>();
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.STRUCT)
-                .addSubtypes(1).addSubtypes(2).addFieldNames("x")
-                .addFieldNames("y").build());
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.INT).build());
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.INT).build());
-
-    // filter by rows and groups
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(0, 1000, 100, 1000, 400, 1000,
-        1000, 11000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
-        11000, 21000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
-        41000, 51000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, true);
-    assertThat(result, is(diskRanges(0, 21000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
-        41000, 51000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
-
-    // if we read no rows, don't read any bytes
-    rowGroups = new boolean[]{false, false, false, false, false, false};
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, false);
-    assertNull(result);
-
-    // all rows, but only columns 0 and 2.
-    rowGroups = null;
-    columns = new boolean[]{true, false, true};
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, null, false, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(100000, 102000, 102000, 200000)));
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, null, false, encodings, types, 32768, true);
-    assertThat(result, is(diskRanges(100000, 200000)));
-
-    rowGroups = new boolean[]{false, true, false, false, false, false};
-    indexes[2] = indexes[1];
-    indexes[1] = null;
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(100100, 102000,
-        112000, 122000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, true);
-    assertThat(result, is(diskRanges(100100, 102000,
-        112000, 122000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP)));
-
-    rowGroups = new boolean[]{false, false, false, false, false, true};
-    indexes[1] = indexes[2];
-    columns = new boolean[]{true, true, true};
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(500, 1000, 51000, 100000, 100500, 102000,
-        152000, 200000)));
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, true);
-    assertThat(result, is(diskRanges(500, 1000, 51000, 100000, 100500, 102000,
-        152000, 200000)));
-  }
-
-
-  @Test
-  public void testPartialPlanCompressed() throws Exception {
-    DiskRangeList result;
-
-    // set the streams
-    List<OrcProto.Stream> streams = new ArrayList<OrcProto.Stream>();
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.PRESENT)
-        .setColumn(1).setLength(1000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DATA)
-        .setColumn(1).setLength(99000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.PRESENT)
-        .setColumn(2).setLength(2000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DATA)
-        .setColumn(2).setLength(98000).build());
-
-    boolean[] columns = new boolean[]{true, true, false};
-    boolean[] rowGroups = new boolean[]{true, true, false, false, true, false};
-
-    // set the index
-    OrcProto.RowIndex[] indexes = new OrcProto.RowIndex[columns.length];
-    indexes[1] = OrcProto.RowIndex.newBuilder()
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(0).addPositions(-1).addPositions(-1).addPositions(-1)
-            .addPositions(0)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(100).addPositions(-1).addPositions(-1).addPositions(-1)
-            .addPositions(10000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(200).addPositions(-1).addPositions(-1).addPositions(-1)
-            .addPositions(20000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(300).addPositions(-1).addPositions(-1).addPositions(-1)
-            .addPositions(30000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(400).addPositions(-1).addPositions(-1).addPositions(-1)
-            .addPositions(40000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(500).addPositions(-1).addPositions(-1).addPositions(-1)
-            .addPositions(50000)
-            .build())
-        .build();
-
-    // set encodings
-    List<OrcProto.ColumnEncoding> encodings =
-        new ArrayList<OrcProto.ColumnEncoding>();
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-
-    // set types struct{x: int, y: int}
-    List<OrcProto.Type> types = new ArrayList<OrcProto.Type>();
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.STRUCT)
-        .addSubtypes(1).addSubtypes(2).addFieldNames("x")
-        .addFieldNames("y").build());
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.INT).build());
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.INT).build());
-
-    // filter by rows and groups
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, true, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(0, 1000, 100, 1000,
-        400, 1000, 1000, 11000+(2*32771),
-        11000, 21000+(2*32771), 41000, 100000)));
-
-    rowGroups = new boolean[]{false, false, false, false, false, true};
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, true, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(500, 1000, 51000, 100000)));
-  }
-
-  @Test
-  public void testPartialPlanString() throws Exception {
-    DiskRangeList result;
-
-    // set the streams
-    List<OrcProto.Stream> streams = new ArrayList<OrcProto.Stream>();
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.PRESENT)
-        .setColumn(1).setLength(1000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DATA)
-        .setColumn(1).setLength(94000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.LENGTH)
-        .setColumn(1).setLength(2000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DICTIONARY_DATA)
-        .setColumn(1).setLength(3000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.PRESENT)
-        .setColumn(2).setLength(2000).build());
-    streams.add(OrcProto.Stream.newBuilder()
-        .setKind(OrcProto.Stream.Kind.DATA)
-        .setColumn(2).setLength(98000).build());
-
-    boolean[] columns = new boolean[]{true, true, false};
-    boolean[] rowGroups = new boolean[]{false, true, false, false, true, true};
-
-    // set the index
-    OrcProto.RowIndex[] indexes = new OrcProto.RowIndex[columns.length];
-    indexes[1] = OrcProto.RowIndex.newBuilder()
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(0).addPositions(-1).addPositions(-1)
-            .addPositions(0)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(100).addPositions(-1).addPositions(-1)
-            .addPositions(10000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(200).addPositions(-1).addPositions(-1)
-            .addPositions(20000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(300).addPositions(-1).addPositions(-1)
-            .addPositions(30000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(400).addPositions(-1).addPositions(-1)
-            .addPositions(40000)
-            .build())
-        .addEntry(OrcProto.RowIndexEntry.newBuilder()
-            .addPositions(500).addPositions(-1).addPositions(-1)
-            .addPositions(50000)
-            .build())
-        .build();
-
-    // set encodings
-    List<OrcProto.ColumnEncoding> encodings =
-        new ArrayList<OrcProto.ColumnEncoding>();
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DICTIONARY).build());
-    encodings.add(OrcProto.ColumnEncoding.newBuilder()
-        .setKind(OrcProto.ColumnEncoding.Kind.DIRECT).build());
-
-    // set types struct{x: string, y: int}
-    List<OrcProto.Type> types = new ArrayList<OrcProto.Type>();
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.STRUCT)
-        .addSubtypes(1).addSubtypes(2).addFieldNames("x")
-        .addFieldNames("y").build());
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.STRING).build());
-    types.add(OrcProto.Type.newBuilder().setKind(OrcProto.Type.Kind.INT).build());
-
-    // filter by rows and groups
-    result = RecordReaderImpl.planReadPartialDataStreams(streams, indexes,
-        columns, rowGroups, false, encodings, types, 32768, false);
-    assertThat(result, is(diskRanges(100, 1000, 400, 1000, 500, 1000,
-        11000, 21000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
-        41000, 51000 + RecordReaderUtils.WORST_UNCOMPRESSED_SLOP,
-        51000, 95000, 95000, 97000, 97000, 100000)));
-  }
-
-  @Test
-  public void testIntNullSafeEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong(i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createIntStats(10, 100));
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong(15);
-    assertEquals(TruthValue.YES_NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testIntEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.LONG, "x", 15L, null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong(i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createIntStats(10, 100));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong(15);
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testIntInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(15L);
-    args.add(19L);
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.LONG,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong(i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createIntStats(10, 100));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong(19);
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong(15);
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDoubleNullSafeEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addDouble(i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDoubleStats(10.0, 100.0));
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addDouble(15.0);
-    assertEquals(TruthValue.YES_NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDoubleEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.FLOAT, "x", 15.0, null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addDouble(i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDoubleStats(10.0, 100.0));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addDouble(15.0);
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDoubleInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(15.0);
-    args.add(19.0);
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.FLOAT,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addDouble(i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDoubleStats(10.0, 100.0));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addDouble(19.0);
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addDouble(15.0);
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testStringNullSafeEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.STRING, "x", "str_15", null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString("str_" + i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createStringStats("str_10", "str_200"));
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString("str_15");
-    assertEquals(TruthValue.YES_NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testStringEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.STRING, "x", "str_15", null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString("str_" + i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createStringStats("str_10", "str_200"));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString("str_15");
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testStringInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add("str_15");
-    args.add("str_19");
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.STRING,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString("str_" + i);
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createStringStats("str_10", "str_200"));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString("str_19");
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString("str_15");
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDateWritableNullSafeEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.DATE, "x",
-        new DateWritable(15).get(), null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong((new DateWritable(i)).getDays());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDateStats(10, 100));
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new DateWritable(15)).getDays());
-    assertEquals(TruthValue.YES_NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDateWritableEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.DATE, "x",
-        new DateWritable(15).get(), null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong((new DateWritable(i)).getDays());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDateStats(10, 100));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new DateWritable(15)).getDays());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDateWritableInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(new DateWritable(15).get());
-    args.add(new DateWritable(19).get());
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.DATE,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong((new DateWritable(i)).getDays());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDateStats(10, 100));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new DateWritable(19)).getDays());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new DateWritable(15)).getDays());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testTimestampNullSafeEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.TIMESTAMP, "x",
-        new Timestamp(15),
-        null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong((new Timestamp(i)).getTime());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createTimestampStats(10, 100));
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new Timestamp(15)).getTime());
-    assertEquals(TruthValue.YES_NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testTimestampEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.TIMESTAMP, "x", new Timestamp(15), null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong((new Timestamp(i)).getTime());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createTimestampStats(10, 100));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new Timestamp(15)).getTime());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testTimestampInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(new Timestamp(15));
-    args.add(new Timestamp(19));
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.TIMESTAMP,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addLong((new Timestamp(i)).getTime());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createTimestampStats(10, 100));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new Timestamp(19)).getTime());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addLong((new Timestamp(15)).getTime());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDecimalNullSafeEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.NULL_SAFE_EQUALS, PredicateLeaf.Type.DECIMAL, "x",
-        new HiveDecimalWritable("15"),
-        null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString(HiveDecimal.create(i).toString());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDecimalStats("10", "200"));
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString(HiveDecimal.create(15).toString());
-    assertEquals(TruthValue.YES_NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDecimalEqualsBloomFilter() throws Exception {
-    PredicateLeaf pred = createPredicateLeaf(
-        PredicateLeaf.Operator.EQUALS, PredicateLeaf.Type.DECIMAL, "x",
-        new HiveDecimalWritable("15"),
-        null);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString(HiveDecimal.create(i).toString());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDecimalStats("10", "200"));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString(HiveDecimal.create(15).toString());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testDecimalInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(new HiveDecimalWritable("15"));
-    args.add(new HiveDecimalWritable("19"));
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.DECIMAL,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString(HiveDecimal.create(i).toString());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDecimalStats("10", "200"));
-    assertEquals(TruthValue.NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString(HiveDecimal.create(19).toString());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString(HiveDecimal.create(15).toString());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testNullsInBloomFilter() throws Exception {
-    List<Object> args = new ArrayList<Object>();
-    args.add(new HiveDecimalWritable("15"));
-    args.add(null);
-    args.add(new HiveDecimalWritable("19"));
-    PredicateLeaf pred = createPredicateLeaf
-        (PredicateLeaf.Operator.IN, PredicateLeaf.Type.DECIMAL,
-            "x", null, args);
-    BloomFilterIO bf = new BloomFilterIO(10000);
-    for (int i = 20; i < 1000; i++) {
-      bf.addString(HiveDecimal.create(i).toString());
-    }
-    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(createDecimalStats("10", "200", false));
-    // hasNull is false, so bloom filter should return NO
-    assertEquals(TruthValue.NO, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    cs = ColumnStatisticsImpl.deserialize(createDecimalStats("10", "200", true));
-    // hasNull is true, so bloom filter should return YES_NO_NULL
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString(HiveDecimal.create(19).toString());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-
-    bf.addString(HiveDecimal.create(15).toString());
-    assertEquals(TruthValue.YES_NO_NULL, RecordReaderImpl.evaluatePredicate(cs, pred, bf));
-  }
-
-  @Test
-  public void testClose() throws Exception {
-    DataReader mockedDataReader = mock(DataReader.class);
-    closeMockedRecordReader(mockedDataReader);
-
-    verify(mockedDataReader, atLeastOnce()).close();
-  }
-
-  @Test
-  public void testCloseWithException() throws Exception {
-    DataReader mockedDataReader = mock(DataReader.class);
-    doThrow(IOException.class).when(mockedDataReader).close();
-
-    try {
-      closeMockedRecordReader(mockedDataReader);
-      fail("Exception should have been thrown when Record Reader was closed");
-    } catch (IOException expected) {
-
-    }
-
-    verify(mockedDataReader, atLeastOnce()).close();
-  }
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-
-  private void closeMockedRecordReader(DataReader mockedDataReader) throws IOException {
-    Configuration conf = new Configuration();
-    Path path = new Path(workDir, "empty.orc");
-    FileSystem.get(conf).delete(path, true);
-    Writer writer = OrcFile.createWriter(path, OrcFile.writerOptions(conf)
-        .setSchema(TypeDescription.createLong()));
-    writer.close();
-    Reader reader = OrcFile.createReader(path, OrcFile.readerOptions(conf));
-
-    RecordReader recordReader = reader.rows(new Reader.Options()
-        .dataReader(mockedDataReader));
-
-    recordReader.close();
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestRunLengthByteReader.java b/orc/src/test/org/apache/orc/impl/TestRunLengthByteReader.java
deleted file mode 100644
index a14bef16a3..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestRunLengthByteReader.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.nio.ByteBuffer;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-
-public class TestRunLengthByteReader {
-
-  @Test
-  public void testUncompressedSeek() throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthByteWriter out = new RunLengthByteWriter(new OutStream("test", 100,
-        null, collect));
-    TestInStream.PositionCollector[] positions =
-        new TestInStream.PositionCollector[2048];
-    for(int i=0; i < 2048; ++i) {
-      positions[i] = new TestInStream.PositionCollector();
-      out.getPosition(positions[i]);
-      if (i < 1024) {
-        out.write((byte) (i/4));
-      } else {
-        out.write((byte) i);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthByteReader in = new RunLengthByteReader(InStream.create("test",
-        new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(), null, 100));
-    for(int i=0; i < 2048; ++i) {
-      int x = in.next() & 0xff;
-      if (i < 1024) {
-        assertEquals((i/4) & 0xff, x);
-      } else {
-        assertEquals(i & 0xff, x);
-      }
-    }
-    for(int i=2047; i >= 0; --i) {
-      in.seek(positions[i]);
-      int x = in.next() & 0xff;
-      if (i < 1024) {
-        assertEquals((i/4) & 0xff, x);
-      } else {
-        assertEquals(i & 0xff, x);
-      }
-    }
-  }
-
-  @Test
-  public void testCompressedSeek() throws Exception {
-    CompressionCodec codec = new SnappyCodec();
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthByteWriter out = new RunLengthByteWriter(new OutStream("test", 500,
-        codec, collect));
-    TestInStream.PositionCollector[] positions =
-        new TestInStream.PositionCollector[2048];
-    for(int i=0; i < 2048; ++i) {
-      positions[i] = new TestInStream.PositionCollector();
-      out.getPosition(positions[i]);
-      if (i < 1024) {
-        out.write((byte) (i/4));
-      } else {
-        out.write((byte) i);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthByteReader in = new RunLengthByteReader(InStream.create("test",
-        new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(), codec, 500));
-    for(int i=0; i < 2048; ++i) {
-      int x = in.next() & 0xff;
-      if (i < 1024) {
-        assertEquals((i/4) & 0xff, x);
-      } else {
-        assertEquals(i & 0xff, x);
-      }
-    }
-    for(int i=2047; i >= 0; --i) {
-      in.seek(positions[i]);
-      int x = in.next() & 0xff;
-      if (i < 1024) {
-        assertEquals((i/4) & 0xff, x);
-      } else {
-        assertEquals(i & 0xff, x);
-      }
-    }
-  }
-
-  @Test
-  public void testSkips() throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthByteWriter out = new RunLengthByteWriter(new OutStream("test", 100,
-        null, collect));
-    for(int i=0; i < 2048; ++i) {
-      if (i < 1024) {
-        out.write((byte) (i/16));
-      } else {
-        out.write((byte) i);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthByteReader in = new RunLengthByteReader(InStream.create("test",
-        new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(), null, 100));
-    for(int i=0; i < 2048; i += 10) {
-      int x = in.next() & 0xff;
-      if (i < 1024) {
-        assertEquals((i/16) & 0xff, x);
-      } else {
-        assertEquals(i & 0xff, x);
-      }
-      if (i < 2038) {
-        in.skip(9);
-      }
-      in.skip(0);
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestRunLengthIntegerReader.java b/orc/src/test/org/apache/orc/impl/TestRunLengthIntegerReader.java
deleted file mode 100644
index 28239ba427..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestRunLengthIntegerReader.java
+++ /dev/null
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static junit.framework.Assert.assertEquals;
-
-import java.nio.ByteBuffer;
-import java.util.Random;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-
-public class TestRunLengthIntegerReader {
-
-  public void runSeekTest(CompressionCodec codec) throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthIntegerWriter out = new RunLengthIntegerWriter(
-        new OutStream("test", 1000, codec, collect), true);
-    TestInStream.PositionCollector[] positions =
-        new TestInStream.PositionCollector[4096];
-    Random random = new Random(99);
-    int[] junk = new int[2048];
-    for(int i=0; i < junk.length; ++i) {
-      junk[i] = random.nextInt();
-    }
-    for(int i=0; i < 4096; ++i) {
-      positions[i] = new TestInStream.PositionCollector();
-      out.getPosition(positions[i]);
-      // test runs, incrementing runs, non-runs
-      if (i < 1024) {
-        out.write(i/4);
-      } else if (i < 2048) {
-        out.write(2*i);
-      } else {
-        out.write(junk[i-2048]);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthIntegerReader in = new RunLengthIntegerReader(InStream.create
-        ("test", new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(),
-            codec, 1000), true);
-    for(int i=0; i < 2048; ++i) {
-      int x = (int) in.next();
-      if (i < 1024) {
-        assertEquals(i/4, x);
-      } else if (i < 2048) {
-        assertEquals(2*i, x);
-      } else {
-        assertEquals(junk[i-2048], x);
-      }
-    }
-    for(int i=2047; i >= 0; --i) {
-      in.seek(positions[i]);
-      int x = (int) in.next();
-      if (i < 1024) {
-        assertEquals(i/4, x);
-      } else if (i < 2048) {
-        assertEquals(2*i, x);
-      } else {
-        assertEquals(junk[i-2048], x);
-      }
-    }
-  }
-
-  @Test
-  public void testUncompressedSeek() throws Exception {
-    runSeekTest(null);
-  }
-
-  @Test
-  public void testCompressedSeek() throws Exception {
-    runSeekTest(new ZlibCodec());
-  }
-
-  @Test
-  public void testSkips() throws Exception {
-    TestInStream.OutputCollector collect = new TestInStream.OutputCollector();
-    RunLengthIntegerWriter out = new RunLengthIntegerWriter(
-        new OutStream("test", 100, null, collect), true);
-    for(int i=0; i < 2048; ++i) {
-      if (i < 1024) {
-        out.write(i);
-      } else {
-        out.write(256 * i);
-      }
-    }
-    out.flush();
-    ByteBuffer inBuf = ByteBuffer.allocate(collect.buffer.size());
-    collect.buffer.setByteBuffer(inBuf, 0, collect.buffer.size());
-    inBuf.flip();
-    RunLengthIntegerReader in = new RunLengthIntegerReader(InStream.create
-        ("test", new ByteBuffer[]{inBuf}, new long[]{0}, inBuf.remaining(),
-            null, 100), true);
-    for(int i=0; i < 2048; i += 10) {
-      int x = (int) in.next();
-      if (i < 1024) {
-        assertEquals(i, x);
-      } else {
-        assertEquals(256 * i, x);
-      }
-      if (i < 2038) {
-        in.skip(9);
-      }
-      in.skip(0);
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestSchemaEvolution.java b/orc/src/test/org/apache/orc/impl/TestSchemaEvolution.java
deleted file mode 100644
index c28af94d76..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestSchemaEvolution.java
+++ /dev/null
@@ -1,469 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.orc.OrcFile;
-import org.apache.orc.Reader;
-import org.apache.orc.RecordReader;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.junit.Before;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-
-public class TestSchemaEvolution {
-
-  @Rule
-  public TestName testCaseName = new TestName();
-
-  Configuration conf;
-  Path testFilePath;
-  FileSystem fs;
-  Path workDir = new Path(System.getProperty("test.tmp.dir",
-      "target" + File.separator + "test" + File.separator + "tmp"));
-
-  @Before
-  public void setup() throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    testFilePath = new Path(workDir, "TestOrcFile." +
-        testCaseName.getMethodName() + ".orc");
-    fs.delete(testFilePath, false);
-  }
-
-  @Test
-  public void testDataTypeConversion1() throws IOException {
-    TypeDescription fileStruct1 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10));
-    SchemaEvolution same1 = new SchemaEvolution(fileStruct1, null);
-    assertFalse(same1.hasConversion());
-    TypeDescription readerStruct1 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10));
-    SchemaEvolution both1 = new SchemaEvolution(fileStruct1, readerStruct1, null);
-    assertFalse(both1.hasConversion());
-    TypeDescription readerStruct1diff = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createLong())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10));
-    SchemaEvolution both1diff = new SchemaEvolution(fileStruct1, readerStruct1diff, null);
-    assertTrue(both1diff.hasConversion());
-    TypeDescription readerStruct1diffPrecision = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(12).withScale(10));
-    SchemaEvolution both1diffPrecision = new SchemaEvolution(fileStruct1, readerStruct1diffPrecision, null);
-    assertTrue(both1diffPrecision.hasConversion());
-  }
-
-  @Test
-  public void testDataTypeConversion2() throws IOException {
-    TypeDescription fileStruct2 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createByte())
-            .addUnionChild(TypeDescription.createDecimal()
-                .withPrecision(20).withScale(10)))
-        .addField("f2", TypeDescription.createStruct()
-            .addField("f3", TypeDescription.createDate())
-            .addField("f4", TypeDescription.createDouble())
-            .addField("f5", TypeDescription.createBoolean()))
-        .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    SchemaEvolution same2 = new SchemaEvolution(fileStruct2, null);
-    assertFalse(same2.hasConversion());
-    TypeDescription readerStruct2 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createByte())
-            .addUnionChild(TypeDescription.createDecimal()
-                .withPrecision(20).withScale(10)))
-        .addField("f2", TypeDescription.createStruct()
-            .addField("f3", TypeDescription.createDate())
-            .addField("f4", TypeDescription.createDouble())
-            .addField("f5", TypeDescription.createBoolean()))
-        .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    SchemaEvolution both2 = new SchemaEvolution(fileStruct2, readerStruct2, null);
-    assertFalse(both2.hasConversion());
-    TypeDescription readerStruct2diff = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createByte())
-            .addUnionChild(TypeDescription.createDecimal()
-                .withPrecision(20).withScale(10)))
-        .addField("f2", TypeDescription.createStruct()
-            .addField("f3", TypeDescription.createDate())
-            .addField("f4", TypeDescription.createDouble())
-            .addField("f5", TypeDescription.createByte()))
-        .addField("f6", TypeDescription.createChar().withMaxLength(100));
-    SchemaEvolution both2diff = new SchemaEvolution(fileStruct2, readerStruct2diff, null);
-    assertTrue(both2diff.hasConversion());
-    TypeDescription readerStruct2diffChar = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createUnion()
-            .addUnionChild(TypeDescription.createByte())
-            .addUnionChild(TypeDescription.createDecimal()
-                .withPrecision(20).withScale(10)))
-        .addField("f2", TypeDescription.createStruct()
-            .addField("f3", TypeDescription.createDate())
-            .addField("f4", TypeDescription.createDouble())
-            .addField("f5", TypeDescription.createBoolean()))
-        .addField("f6", TypeDescription.createChar().withMaxLength(80));
-    SchemaEvolution both2diffChar = new SchemaEvolution(fileStruct2, readerStruct2diffChar, null);
-    assertTrue(both2diffChar.hasConversion());
-  }
-
-  @Test
-  public void testFloatToDoubleEvolution() throws Exception {
-    testFilePath = new Path(workDir, "TestOrcFile." +
-        testCaseName.getMethodName() + ".orc");
-    TypeDescription schema = TypeDescription.createFloat();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf).setSchema(schema).stripeSize(100000)
-            .bufferSize(10000));
-    VectorizedRowBatch batch = new VectorizedRowBatch(1, 1024);
-    DoubleColumnVector dcv = new DoubleColumnVector(1024);
-    batch.cols[0] = dcv;
-    batch.reset();
-    batch.size = 1;
-    dcv.vector[0] = 74.72f;
-    writer.addRowBatch(batch);
-    writer.close();
-
-    Reader reader = OrcFile.createReader(testFilePath,
-        OrcFile.readerOptions(conf).filesystem(fs));
-    TypeDescription schemaOnRead = TypeDescription.createDouble();
-    RecordReader rows = reader.rows(new Reader.Options().schema(schemaOnRead));
-    batch = schemaOnRead.createRowBatch();
-    rows.nextBatch(batch);
-    assertEquals(74.72, ((DoubleColumnVector) batch.cols[0]).vector[0], 0.00000000001);
-    rows.close();
-  }
-
-  @Test
-  public void testSafePpdEvaluation() throws IOException {
-    TypeDescription fileStruct1 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10));
-    SchemaEvolution same1 = new SchemaEvolution(fileStruct1, null);
-    assertTrue(same1.isPPDSafeConversion(0));
-    assertFalse(same1.hasConversion());
-    TypeDescription readerStruct1 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10));
-    SchemaEvolution both1 = new SchemaEvolution(fileStruct1, readerStruct1, null);
-    assertFalse(both1.hasConversion());
-    assertTrue(both1.isPPDSafeConversion(0));
-    assertTrue(both1.isPPDSafeConversion(1));
-    assertTrue(both1.isPPDSafeConversion(2));
-    assertTrue(both1.isPPDSafeConversion(3));
-
-    // int -> long
-    TypeDescription readerStruct1diff = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createLong())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10));
-    SchemaEvolution both1diff = new SchemaEvolution(fileStruct1, readerStruct1diff, null);
-    assertTrue(both1diff.hasConversion());
-    assertFalse(both1diff.isPPDSafeConversion(0));
-    assertTrue(both1diff.isPPDSafeConversion(1));
-    assertTrue(both1diff.isPPDSafeConversion(2));
-    assertTrue(both1diff.isPPDSafeConversion(3));
-
-    // decimal(38,10) -> decimal(12, 10)
-    TypeDescription readerStruct1diffPrecision = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(12).withScale(10));
-    SchemaEvolution both1diffPrecision = new SchemaEvolution(fileStruct1, readerStruct1diffPrecision,
-        new boolean[] {true, false, false, true});
-    assertTrue(both1diffPrecision.hasConversion());
-    assertFalse(both1diffPrecision.isPPDSafeConversion(0));
-    assertFalse(both1diffPrecision.isPPDSafeConversion(1)); // column not included
-    assertFalse(both1diffPrecision.isPPDSafeConversion(2)); // column not included
-    assertFalse(both1diffPrecision.isPPDSafeConversion(3));
-
-    // add columns
-    readerStruct1 = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt())
-        .addField("f2", TypeDescription.createString())
-        .addField("f3", TypeDescription.createDecimal().withPrecision(38).withScale(10))
-        .addField("f4", TypeDescription.createBoolean());
-    both1 = new SchemaEvolution(fileStruct1, readerStruct1, null);
-    assertTrue(both1.hasConversion());
-    assertFalse(both1.isPPDSafeConversion(0));
-    assertTrue(both1.isPPDSafeConversion(1));
-    assertTrue(both1.isPPDSafeConversion(2));
-    assertTrue(both1.isPPDSafeConversion(3));
-    assertFalse(both1.isPPDSafeConversion(4));
-  }
-
-  @Test
-  public void testSafePpdEvaluationForInts() throws IOException {
-    // byte -> short -> int -> long
-    TypeDescription fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createByte());
-    SchemaEvolution schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertFalse(schemaEvolution.hasConversion());
-
-    // byte -> short
-    TypeDescription readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createShort());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // byte -> int
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // byte -> long
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createLong());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // short -> int -> long
-    fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createShort());
-    schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertFalse(schemaEvolution.hasConversion());
-
-    // unsafe conversion short -> byte
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createByte());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // short -> int
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // short -> long
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createLong());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // int -> long
-    fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt());
-    schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertFalse(schemaEvolution.hasConversion());
-
-    // unsafe conversion int -> byte
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createByte());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // unsafe conversion int -> short
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createShort());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // int -> long
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createLong());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // long
-    fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createLong());
-    schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertTrue(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.hasConversion());
-
-    // unsafe conversion long -> byte
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createByte());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // unsafe conversion long -> short
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createShort());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // unsafe conversion long -> int
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // invalid
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createString());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // invalid
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createFloat());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // invalid
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createTimestamp());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-  }
-
-  @Test
-  public void testSafePpdEvaluationForStrings() throws IOException {
-    TypeDescription fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createString());
-    SchemaEvolution schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertTrue(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.hasConversion());
-
-    // string -> char
-    TypeDescription readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createChar());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // string -> varchar
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createVarchar());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createChar());
-    schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertTrue(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.hasConversion());
-
-    // char -> string
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createString());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // char -> varchar
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createVarchar());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    fileSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createVarchar());
-    schemaEvolution = new SchemaEvolution(fileSchema, null);
-    assertTrue(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.hasConversion());
-
-    // varchar -> string
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createString());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertTrue(schemaEvolution.isPPDSafeConversion(1));
-
-    // varchar -> char
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createChar());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // invalid
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createDecimal());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // invalid
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createDate());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-
-    // invalid
-    readerSchema = TypeDescription.createStruct()
-        .addField("f1", TypeDescription.createInt());
-    schemaEvolution = new SchemaEvolution(fileSchema, readerSchema, null);
-    assertTrue(schemaEvolution.hasConversion());
-    assertFalse(schemaEvolution.isPPDSafeConversion(0));
-    assertFalse(schemaEvolution.isPPDSafeConversion(1));
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestSerializationUtils.java b/orc/src/test/org/apache/orc/impl/TestSerializationUtils.java
deleted file mode 100644
index 4a8a0f25dc..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestSerializationUtils.java
+++ /dev/null
@@ -1,201 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.orc.impl;
-
-import static org.junit.Assert.assertArrayEquals;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.fail;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.InputStream;
-import java.math.BigInteger;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-
-import org.junit.Test;
-
-import com.google.common.math.LongMath;
-
-public class TestSerializationUtils {
-
-  private InputStream fromBuffer(ByteArrayOutputStream buffer) {
-    return new ByteArrayInputStream(buffer.toByteArray());
-  }
-
-  @Test
-  public void testDoubles() throws Exception {
-    double tolerance = 0.0000000000000001;
-    ByteArrayOutputStream buffer = new ByteArrayOutputStream();
-    SerializationUtils utils = new SerializationUtils();
-    utils.writeDouble(buffer, 1343822337.759);
-    assertEquals(1343822337.759, utils.readDouble(fromBuffer(buffer)), tolerance);
-    buffer = new ByteArrayOutputStream();
-    utils.writeDouble(buffer, 0.8);
-    double got = utils.readDouble(fromBuffer(buffer));
-    assertEquals(0.8, got, tolerance);
-  }
-
-  @Test
-  public void testBigIntegers() throws Exception {
-    ByteArrayOutputStream buffer = new ByteArrayOutputStream();
-    SerializationUtils.writeBigInteger(buffer, BigInteger.valueOf(0));
-    assertArrayEquals(new byte[]{0}, buffer.toByteArray());
-    assertEquals(0L,
-        SerializationUtils.readBigInteger(fromBuffer(buffer)).longValue());
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer, BigInteger.valueOf(1));
-    assertArrayEquals(new byte[]{2}, buffer.toByteArray());
-    assertEquals(1L,
-        SerializationUtils.readBigInteger(fromBuffer(buffer)).longValue());
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer, BigInteger.valueOf(-1));
-    assertArrayEquals(new byte[]{1}, buffer.toByteArray());
-    assertEquals(-1L,
-        SerializationUtils.readBigInteger(fromBuffer(buffer)).longValue());
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer, BigInteger.valueOf(50));
-    assertArrayEquals(new byte[]{100}, buffer.toByteArray());
-    assertEquals(50L,
-        SerializationUtils.readBigInteger(fromBuffer(buffer)).longValue());
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer, BigInteger.valueOf(-50));
-    assertArrayEquals(new byte[]{99}, buffer.toByteArray());
-    assertEquals(-50L,
-        SerializationUtils.readBigInteger(fromBuffer(buffer)).longValue());
-    for(int i=-8192; i < 8192; ++i) {
-      buffer.reset();
-        SerializationUtils.writeBigInteger(buffer, BigInteger.valueOf(i));
-      assertEquals("compare length for " + i,
-            i >= -64 && i < 64 ? 1 : 2, buffer.size());
-      assertEquals("compare result for " + i,
-          i, SerializationUtils.readBigInteger(fromBuffer(buffer)).intValue());
-    }
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer,
-        new BigInteger("123456789abcdef0",16));
-    assertEquals(new BigInteger("123456789abcdef0",16),
-        SerializationUtils.readBigInteger(fromBuffer(buffer)));
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer,
-        new BigInteger("-123456789abcdef0",16));
-    assertEquals(new BigInteger("-123456789abcdef0",16),
-        SerializationUtils.readBigInteger(fromBuffer(buffer)));
-    StringBuilder buf = new StringBuilder();
-    for(int i=0; i < 256; ++i) {
-      String num = Integer.toHexString(i);
-      if (num.length() == 1) {
-        buf.append('0');
-      }
-      buf.append(num);
-    }
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer,
-        new BigInteger(buf.toString(),16));
-    assertEquals(new BigInteger(buf.toString(),16),
-        SerializationUtils.readBigInteger(fromBuffer(buffer)));
-    buffer.reset();
-    SerializationUtils.writeBigInteger(buffer,
-        new BigInteger("ff000000000000000000000000000000000000000000ff",16));
-    assertEquals(
-        new BigInteger("ff000000000000000000000000000000000000000000ff",16),
-        SerializationUtils.readBigInteger(fromBuffer(buffer)));
-  }
-
-  @Test
-  public void testSubtractionOverflow() {
-    // cross check results with Guava results below
-    SerializationUtils utils = new SerializationUtils();
-    assertEquals(false, utils.isSafeSubtract(22222222222L, Long.MIN_VALUE));
-    assertEquals(false, utils.isSafeSubtract(-22222222222L, Long.MAX_VALUE));
-    assertEquals(false, utils.isSafeSubtract(Long.MIN_VALUE, Long.MAX_VALUE));
-    assertEquals(true, utils.isSafeSubtract(-1553103058346370095L, 6553103058346370095L));
-    assertEquals(true, utils.isSafeSubtract(0, Long.MAX_VALUE));
-    assertEquals(true, utils.isSafeSubtract(Long.MIN_VALUE, 0));
-  }
-
-  @Test
-  public void testSubtractionOverflowGuava() {
-    try {
-      LongMath.checkedSubtract(22222222222L, Long.MIN_VALUE);
-      fail("expected ArithmeticException for overflow");
-    } catch (ArithmeticException ex) {
-      assertEquals(ex.getMessage(), "overflow");
-    }
-
-    try {
-      LongMath.checkedSubtract(-22222222222L, Long.MAX_VALUE);
-      fail("expected ArithmeticException for overflow");
-    } catch (ArithmeticException ex) {
-      assertEquals(ex.getMessage(), "overflow");
-    }
-
-    try {
-      LongMath.checkedSubtract(Long.MIN_VALUE, Long.MAX_VALUE);
-      fail("expected ArithmeticException for overflow");
-    } catch (ArithmeticException ex) {
-      assertEquals(ex.getMessage(), "overflow");
-    }
-
-    assertEquals(-8106206116692740190L,
-        LongMath.checkedSubtract(-1553103058346370095L, 6553103058346370095L));
-    assertEquals(-Long.MAX_VALUE, LongMath.checkedSubtract(0, Long.MAX_VALUE));
-    assertEquals(Long.MIN_VALUE, LongMath.checkedSubtract(Long.MIN_VALUE, 0));
-  }
-
-  @Test
-  public void testRandomFloats() throws Exception {
-    float tolerance = 0.0000000000000001f;
-    ByteArrayOutputStream buffer = new ByteArrayOutputStream();
-    SerializationUtils utils = new SerializationUtils();
-    Random rand = new Random();
-    int n = 100_000;
-    float[] expected = new float[n];
-    for (int i = 0; i < n; i++) {
-      float f = rand.nextFloat();
-      expected[i] = f;
-      utils.writeFloat(buffer, f);
-    }
-    InputStream newBuffer = fromBuffer(buffer);
-    for (int i = 0; i < n; i++) {
-      float got = utils.readFloat(newBuffer);
-      assertEquals(expected[i], got, tolerance);
-    }
-  }
-
-  @Test
-  public void testRandomDoubles() throws Exception {
-    double tolerance = 0.0000000000000001;
-    ByteArrayOutputStream buffer = new ByteArrayOutputStream();
-    SerializationUtils utils = new SerializationUtils();
-    Random rand = new Random();
-    int n = 100_000;
-    double[] expected = new double[n];
-    for (int i = 0; i < n; i++) {
-      double d = rand.nextDouble();
-      expected[i] = d;
-      utils.writeDouble(buffer, d);
-    }
-    InputStream newBuffer = fromBuffer(buffer);
-    for (int i = 0; i < n; i++) {
-      double got = utils.readDouble(newBuffer);
-      assertEquals(expected[i], got, tolerance);
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestStreamName.java b/orc/src/test/org/apache/orc/impl/TestStreamName.java
deleted file mode 100644
index be58d4c3fe..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestStreamName.java
+++ /dev/null
@@ -1,49 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.OrcProto;
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-
-public class TestStreamName {
-
-  @Test
-  public void test1() throws Exception {
-    StreamName s1 = new StreamName(3, OrcProto.Stream.Kind.DATA);
-    StreamName s2 = new StreamName(3,
-        OrcProto.Stream.Kind.DICTIONARY_DATA);
-    StreamName s3 = new StreamName(5, OrcProto.Stream.Kind.DATA);
-    StreamName s4 = new StreamName(5,
-        OrcProto.Stream.Kind.DICTIONARY_DATA);
-    StreamName s1p = new StreamName(3, OrcProto.Stream.Kind.DATA);
-    assertEquals(true, s1.equals(s1));
-    assertEquals(false, s1.equals(s2));
-    assertEquals(false, s1.equals(s3));
-    assertEquals(true, s1.equals(s1p));
-    assertEquals(true, s1.compareTo(null) < 0);
-    assertEquals(false, s1.equals(null));
-    assertEquals(true, s1.compareTo(s2) < 0);
-    assertEquals(true, s2.compareTo(s3) < 0);
-    assertEquals(true, s3.compareTo(s4) < 0);
-    assertEquals(true, s4.compareTo(s1p) > 0);
-    assertEquals(0, s1p.compareTo(s1));
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestStringRedBlackTree.java b/orc/src/test/org/apache/orc/impl/TestStringRedBlackTree.java
deleted file mode 100644
index 3d4612cdd9..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestStringRedBlackTree.java
+++ /dev/null
@@ -1,234 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.orc.impl.RedBlackTree;
-import org.apache.orc.impl.StringRedBlackTree;
-import org.junit.Test;
-
-import java.io.IOException;
-
-import static junit.framework.Assert.assertEquals;
-
-/**
- * Test the red-black tree with string keys.
- */
-public class TestStringRedBlackTree {
-
-  /**
-   * Checks the red-black tree rules to make sure that we have correctly built
-   * a valid tree.
-   *
-   * Properties:
-   *   1. Red nodes must have black children
-   *   2. Each node must have the same black height on both sides.
-   *
-   * @param node The id of the root of the subtree to check for the red-black
-   *        tree properties.
-   * @return The black-height of the subtree.
-   */
-  private int checkSubtree(RedBlackTree tree, int node, IntWritable count
-                          ) throws IOException {
-    if (node == RedBlackTree.NULL) {
-      return 1;
-    }
-    count.set(count.get() + 1);
-    boolean is_red = tree.isRed(node);
-    int left = tree.getLeft(node);
-    int right = tree.getRight(node);
-    if (is_red) {
-      if (tree.isRed(left)) {
-        printTree(tree, "", tree.root);
-        throw new IllegalStateException("Left node of " + node + " is " + left +
-          " and both are red.");
-      }
-      if (tree.isRed(right)) {
-        printTree(tree, "", tree.root);
-        throw new IllegalStateException("Right node of " + node + " is " +
-          right + " and both are red.");
-      }
-    }
-    int left_depth = checkSubtree(tree, left, count);
-    int right_depth = checkSubtree(tree, right, count);
-    if (left_depth != right_depth) {
-      printTree(tree, "", tree.root);
-      throw new IllegalStateException("Lopsided tree at node " + node +
-        " with depths " + left_depth + " and " + right_depth);
-    }
-    if (is_red) {
-      return left_depth;
-    } else {
-      return left_depth + 1;
-    }
-  }
-
-  /**
-   * Checks the validity of the entire tree. Also ensures that the number of
-   * nodes visited is the same as the size of the set.
-   */
-  void checkTree(RedBlackTree tree) throws IOException {
-    IntWritable count = new IntWritable(0);
-    if (tree.isRed(tree.root)) {
-      printTree(tree, "", tree.root);
-      throw new IllegalStateException("root is red");
-    }
-    checkSubtree(tree, tree.root, count);
-    if (count.get() != tree.size) {
-      printTree(tree, "", tree.root);
-      throw new IllegalStateException("Broken tree! visited= " + count.get() +
-        " size=" + tree.size);
-    }
-  }
-
-  void printTree(RedBlackTree tree, String indent, int node
-                ) throws IOException {
-    if (node == RedBlackTree.NULL) {
-      System.err.println(indent + "NULL");
-    } else {
-      System.err.println(indent + "Node " + node + " color " +
-        (tree.isRed(node) ? "red" : "black"));
-      printTree(tree, indent + "  ", tree.getLeft(node));
-      printTree(tree, indent + "  ", tree.getRight(node));
-    }
-  }
-
-  private static class MyVisitor implements StringRedBlackTree.Visitor {
-    private final String[] words;
-    private final int[] order;
-    private final DataOutputBuffer buffer = new DataOutputBuffer();
-    int current = 0;
-
-    MyVisitor(String[] args, int[] order) {
-      words = args;
-      this.order = order;
-    }
-
-    @Override
-    public void visit(StringRedBlackTree.VisitorContext context
-                     ) throws IOException {
-      String word = context.getText().toString();
-      assertEquals("in word " + current, words[current], word);
-      assertEquals("in word " + current, order[current],
-        context.getOriginalPosition());
-      buffer.reset();
-      context.writeBytes(buffer);
-      assertEquals(word, new String(buffer.getData(),0,buffer.getLength()));
-      current += 1;
-    }
-  }
-
-  void checkContents(StringRedBlackTree tree, int[] order,
-                     String... params
-                    ) throws IOException {
-    tree.visit(new MyVisitor(params, order));
-  }
-
-  StringRedBlackTree buildTree(String... params) throws IOException {
-    StringRedBlackTree result = new StringRedBlackTree(1000);
-    for(String word: params) {
-      result.add(word);
-      checkTree(result);
-    }
-    return result;
-  }
-
-  @Test
-  public void test1() throws Exception {
-    StringRedBlackTree tree = new StringRedBlackTree(5);
-    assertEquals(0, tree.getSizeInBytes());
-    checkTree(tree);
-    assertEquals(0, tree.add("owen"));
-    checkTree(tree);
-    assertEquals(1, tree.add("ashutosh"));
-    checkTree(tree);
-    assertEquals(0, tree.add("owen"));
-    checkTree(tree);
-    assertEquals(2, tree.add("alan"));
-    checkTree(tree);
-    assertEquals(2, tree.add("alan"));
-    checkTree(tree);
-    assertEquals(1, tree.add("ashutosh"));
-    checkTree(tree);
-    assertEquals(3, tree.add("greg"));
-    checkTree(tree);
-    assertEquals(4, tree.add("eric"));
-    checkTree(tree);
-    assertEquals(5, tree.add("arun"));
-    checkTree(tree);
-    assertEquals(6, tree.size());
-    checkTree(tree);
-    assertEquals(6, tree.add("eric14"));
-    checkTree(tree);
-    assertEquals(7, tree.add("o"));
-    checkTree(tree);
-    assertEquals(8, tree.add("ziggy"));
-    checkTree(tree);
-    assertEquals(9, tree.add("z"));
-    checkTree(tree);
-    checkContents(tree, new int[]{2,5,1,4,6,3,7,0,9,8},
-      "alan", "arun", "ashutosh", "eric", "eric14", "greg",
-      "o", "owen", "z", "ziggy");
-    assertEquals(32888, tree.getSizeInBytes());
-    // check that adding greg again bumps the count
-    assertEquals(3, tree.add("greg"));
-    assertEquals(41, tree.getCharacterSize());
-    // add some more strings to test the different branches of the
-    // rebalancing
-    assertEquals(10, tree.add("zak"));
-    checkTree(tree);
-    assertEquals(11, tree.add("eric1"));
-    checkTree(tree);
-    assertEquals(12, tree.add("ash"));
-    checkTree(tree);
-    assertEquals(13, tree.add("harry"));
-    checkTree(tree);
-    assertEquals(14, tree.add("john"));
-    checkTree(tree);
-    tree.clear();
-    checkTree(tree);
-    assertEquals(0, tree.getSizeInBytes());
-    assertEquals(0, tree.getCharacterSize());
-  }
-
-  @Test
-  public void test2() throws Exception {
-    StringRedBlackTree tree =
-      buildTree("a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l",
-        "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z");
-    assertEquals(26, tree.size());
-    checkContents(tree, new int[]{0,1,2, 3,4,5, 6,7,8, 9,10,11, 12,13,14,
-      15,16,17, 18,19,20, 21,22,23, 24,25},
-      "a", "b", "c", "d", "e", "f", "g", "h", "i", "j","k", "l", "m", "n", "o",
-      "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z");
-  }
-
-  @Test
-  public void test3() throws Exception {
-    StringRedBlackTree tree =
-      buildTree("z", "y", "x", "w", "v", "u", "t", "s", "r", "q", "p", "o", "n",
-        "m", "l", "k", "j", "i", "h", "g", "f", "e", "d", "c", "b", "a");
-    assertEquals(26, tree.size());
-    checkContents(tree, new int[]{25,24,23, 22,21,20, 19,18,17, 16,15,14,
-      13,12,11, 10,9,8, 7,6,5, 4,3,2, 1,0},
-      "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o",
-      "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z");
-  }
-}
diff --git a/orc/src/test/org/apache/orc/impl/TestZlib.java b/orc/src/test/org/apache/orc/impl/TestZlib.java
deleted file mode 100644
index 327ecfcce5..0000000000
--- a/orc/src/test/org/apache/orc/impl/TestZlib.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.impl;
-
-import org.apache.orc.CompressionCodec;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-import static junit.framework.Assert.assertEquals;
-import static junit.framework.Assert.fail;
-
-public class TestZlib {
-
-  @Test
-  public void testNoOverflow() throws Exception {
-    ByteBuffer in = ByteBuffer.allocate(10);
-    ByteBuffer out = ByteBuffer.allocate(10);
-    in.put(new byte[]{1,2,3,4,5,6,7,10});
-    in.flip();
-    CompressionCodec codec = new ZlibCodec();
-    assertEquals(false, codec.compress(in, out, null));
-  }
-
-  @Test
-  public void testCorrupt() throws Exception {
-    ByteBuffer buf = ByteBuffer.allocate(1000);
-    buf.put(new byte[]{127,-128,0,99,98,-1});
-    buf.flip();
-    CompressionCodec codec = new ZlibCodec();
-    ByteBuffer out = ByteBuffer.allocate(1000);
-    try {
-      codec.decompress(buf, out);
-      fail();
-    } catch (IOException ioe) {
-      // EXPECTED
-    }
-  }
-}
diff --git a/orc/src/test/org/apache/orc/tools/TestFileDump.java b/orc/src/test/org/apache/orc/tools/TestFileDump.java
deleted file mode 100644
index ce3381e763..0000000000
--- a/orc/src/test/org/apache/orc/tools/TestFileDump.java
+++ /dev/null
@@ -1,486 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.tools;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNull;
-
-import java.io.BufferedReader;
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.FileReader;
-import java.io.PrintStream;
-import java.sql.Date;
-import java.sql.Timestamp;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.ListColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.MapColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.StructColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcConf;
-import org.apache.orc.OrcFile;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestFileDump {
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir"));
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Before
-  public void openFileSystem () throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    fs.setWorkingDirectory(workDir);
-    testFilePath = new Path("TestFileDump.testDump.orc");
-    fs.delete(testFilePath, false);
-  }
-
-  static TypeDescription getMyRecordType() {
-    return TypeDescription.createStruct()
-        .addField("i", TypeDescription.createInt())
-        .addField("l", TypeDescription.createLong())
-        .addField("s", TypeDescription.createString());
-  }
-
-  static void appendMyRecord(VectorizedRowBatch batch,
-                             int i,
-                             long l,
-                             String str) {
-    ((LongColumnVector) batch.cols[0]).vector[batch.size] = i;
-    ((LongColumnVector) batch.cols[1]).vector[batch.size] = l;
-    if (str == null) {
-      batch.cols[2].noNulls = false;
-      batch.cols[2].isNull[batch.size] = true;
-    } else {
-      ((BytesColumnVector) batch.cols[2]).setVal(batch.size,
-          str.getBytes());
-    }
-    batch.size += 1;
-  }
-
-  static TypeDescription getAllTypesType() {
-    return TypeDescription.createStruct()
-        .addField("b", TypeDescription.createBoolean())
-        .addField("bt", TypeDescription.createByte())
-        .addField("s", TypeDescription.createShort())
-        .addField("i", TypeDescription.createInt())
-        .addField("l", TypeDescription.createLong())
-        .addField("f", TypeDescription.createFloat())
-        .addField("d", TypeDescription.createDouble())
-        .addField("de", TypeDescription.createDecimal())
-        .addField("t", TypeDescription.createTimestamp())
-        .addField("dt", TypeDescription.createDate())
-        .addField("str", TypeDescription.createString())
-        .addField("c", TypeDescription.createChar().withMaxLength(5))
-        .addField("vc", TypeDescription.createVarchar().withMaxLength(10))
-        .addField("m", TypeDescription.createMap(
-            TypeDescription.createString(),
-            TypeDescription.createString()))
-        .addField("a", TypeDescription.createList(TypeDescription.createInt()))
-        .addField("st", TypeDescription.createStruct()
-                .addField("i", TypeDescription.createInt())
-                .addField("s", TypeDescription.createString()));
-  }
-
-  static void appendAllTypes(VectorizedRowBatch batch,
-                             boolean b,
-                             byte bt,
-                             short s,
-                             int i,
-                             long l,
-                             float f,
-                             double d,
-                             HiveDecimalWritable de,
-                             Timestamp t,
-                             DateWritable dt,
-                             String str,
-                             String c,
-                             String vc,
-                             Map<String, String> m,
-                             List<Integer> a,
-                             int sti,
-                             String sts) {
-    int row = batch.size++;
-    ((LongColumnVector) batch.cols[0]).vector[row] = b ? 1 : 0;
-    ((LongColumnVector) batch.cols[1]).vector[row] = bt;
-    ((LongColumnVector) batch.cols[2]).vector[row] = s;
-    ((LongColumnVector) batch.cols[3]).vector[row] = i;
-    ((LongColumnVector) batch.cols[4]).vector[row] = l;
-    ((DoubleColumnVector) batch.cols[5]).vector[row] = f;
-    ((DoubleColumnVector) batch.cols[6]).vector[row] = d;
-    ((DecimalColumnVector) batch.cols[7]).vector[row].set(de);
-    ((TimestampColumnVector) batch.cols[8]).set(row, t);
-    ((LongColumnVector) batch.cols[9]).vector[row] = dt.getDays();
-    ((BytesColumnVector) batch.cols[10]).setVal(row, str.getBytes());
-    ((BytesColumnVector) batch.cols[11]).setVal(row, c.getBytes());
-    ((BytesColumnVector) batch.cols[12]).setVal(row, vc.getBytes());
-    MapColumnVector map = (MapColumnVector) batch.cols[13];
-    int offset = map.childCount;
-    map.offsets[row] = offset;
-    map.lengths[row] = m.size();
-    map.childCount += map.lengths[row];
-    for(Map.Entry<String, String> entry: m.entrySet()) {
-      ((BytesColumnVector) map.keys).setVal(offset, entry.getKey().getBytes());
-      ((BytesColumnVector) map.values).setVal(offset++,
-          entry.getValue().getBytes());
-    }
-    ListColumnVector list = (ListColumnVector) batch.cols[14];
-    offset = list.childCount;
-    list.offsets[row] = offset;
-    list.lengths[row] = a.size();
-    list.childCount += list.lengths[row];
-    for(int e=0; e < a.size(); ++e) {
-      ((LongColumnVector) list.child).vector[offset + e] = a.get(e);
-    }
-    StructColumnVector struct = (StructColumnVector) batch.cols[15];
-    ((LongColumnVector) struct.fields[0]).vector[row] = sti;
-    ((BytesColumnVector) struct.fields[1]).setVal(row, sts.getBytes());
-  }
-
-  public static void checkOutput(String expected,
-                                 String actual) throws Exception {
-    BufferedReader eStream =
-        new BufferedReader(new FileReader
-            (TestJsonFileDump.getFileFromClasspath(expected)));
-    BufferedReader aStream =
-        new BufferedReader(new FileReader(actual));
-    String expectedLine = eStream.readLine().trim();
-    while (expectedLine != null) {
-      String actualLine = aStream.readLine().trim();
-      System.out.println("actual:   " + actualLine);
-      System.out.println("expected: " + expectedLine);
-      Assert.assertEquals(expectedLine, actualLine);
-      expectedLine = eStream.readLine();
-      expectedLine = expectedLine == null ? null : expectedLine.trim();
-    }
-    Assert.assertNull(eStream.readLine());
-    Assert.assertNull(aStream.readLine());
-    eStream.close();
-    aStream.close();
-  }
-
-  @Test
-  public void testDump() throws Exception {
-    TypeDescription schema = getMyRecordType();
-    conf.set(OrcConf.ENCODING_STRATEGY.getAttribute(), "COMPRESSION");
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .fileSystem(fs)
-            .setSchema(schema)
-            .compress(CompressionKind.ZLIB)
-            .stripeSize(100000)
-            .rowIndexStride(1000));
-    Random r1 = new Random(1);
-    String[] words = new String[]{"It", "was", "the", "best", "of", "times,",
-        "it", "was", "the", "worst", "of", "times,", "it", "was", "the", "age",
-        "of", "wisdom,", "it", "was", "the", "age", "of", "foolishness,", "it",
-        "was", "the", "epoch", "of", "belief,", "it", "was", "the", "epoch",
-        "of", "incredulity,", "it", "was", "the", "season", "of", "Light,",
-        "it", "was", "the", "season", "of", "Darkness,", "it", "was", "the",
-        "spring", "of", "hope,", "it", "was", "the", "winter", "of", "despair,",
-        "we", "had", "everything", "before", "us,", "we", "had", "nothing",
-        "before", "us,", "we", "were", "all", "going", "direct", "to",
-        "Heaven,", "we", "were", "all", "going", "direct", "the", "other",
-        "way"};
-    VectorizedRowBatch batch = schema.createRowBatch(1000);
-    for(int i=0; i < 21000; ++i) {
-      appendMyRecord(batch, r1.nextInt(), r1.nextLong(),
-          words[r1.nextInt(words.length)]);
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-    }
-    if (batch.size > 0) {
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    PrintStream origOut = System.out;
-    String outputFilename = "orc-file-dump.out";
-    FileOutputStream myOut = new FileOutputStream(workDir + File.separator + outputFilename);
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "--rowindex=1,2,3"});
-    System.out.flush();
-    System.setOut(origOut);
-
-
-    checkOutput(outputFilename, workDir + File.separator + outputFilename);
-  }
-
-  @Test
-  public void testDataDump() throws Exception {
-    TypeDescription schema = getAllTypesType();
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .fileSystem(fs)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.NONE)
-            .bufferSize(10000)
-            .rowIndexStride(1000));
-    VectorizedRowBatch batch = schema.createRowBatch(1000);
-    Map<String, String> m = new HashMap<String, String>(2);
-    m.put("k1", "v1");
-    appendAllTypes(batch,
-        true,
-        (byte) 10,
-        (short) 100,
-        1000,
-        10000L,
-        4.0f,
-        20.0,
-        new HiveDecimalWritable("4.2222"),
-        new Timestamp(1416967764000L),
-        new DateWritable(new Date(1416967764000L)),
-        "string",
-        "hello",
-       "hello",
-        m,
-        Arrays.asList(100, 200),
-        10, "foo");
-    m.clear();
-    m.put("k3", "v3");
-    appendAllTypes(
-        batch,
-        false,
-        (byte)20,
-        (short)200,
-        2000,
-        20000L,
-        8.0f,
-        40.0,
-        new HiveDecimalWritable("2.2222"),
-        new Timestamp(1416967364000L),
-        new DateWritable(new Date(1411967764000L)),
-        "abcd",
-        "world",
-        "world",
-        m,
-        Arrays.asList(200, 300),
-        20, "bar");
-    writer.addRowBatch(batch);
-
-    writer.close();
-    PrintStream origOut = System.out;
-    ByteArrayOutputStream myOut = new ByteArrayOutputStream();
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "-d"});
-    System.out.flush();
-    System.setOut(origOut);
-    String[] lines = myOut.toString().split("\n");
-    Assert.assertEquals("{\"b\":true,\"bt\":10,\"s\":100,\"i\":1000,\"l\":10000,\"f\":4,\"d\":20,\"de\":\"4.2222\",\"t\":\"2014-11-25 18:09:24.0\",\"dt\":\"2014-11-25\",\"str\":\"string\",\"c\":\"hello\",\"vc\":\"hello\",\"m\":[{\"_key\":\"k1\",\"_value\":\"v1\"}],\"a\":[100,200],\"st\":{\"i\":10,\"s\":\"foo\"}}", lines[0]);
-    Assert.assertEquals("{\"b\":false,\"bt\":20,\"s\":200,\"i\":2000,\"l\":20000,\"f\":8,\"d\":40,\"de\":\"2.2222\",\"t\":\"2014-11-25 18:02:44.0\",\"dt\":\"2014-09-28\",\"str\":\"abcd\",\"c\":\"world\",\"vc\":\"world\",\"m\":[{\"_key\":\"k3\",\"_value\":\"v3\"}],\"a\":[200,300],\"st\":{\"i\":20,\"s\":\"bar\"}}", lines[1]);
-  }
-  
-  // Test that if the fraction of rows that have distinct strings is greater than the configured
-  // threshold dictionary encoding is turned off.  If dictionary encoding is turned off the length
-  // of the dictionary stream for the column will be 0 in the ORC file dump.
-  @Test
-  public void testDictionaryThreshold() throws Exception {
-    TypeDescription schema = getMyRecordType();
-    Configuration conf = new Configuration();
-    conf.set(OrcConf.ENCODING_STRATEGY.getAttribute(), "COMPRESSION");
-    conf.setFloat(OrcConf.DICTIONARY_KEY_SIZE_THRESHOLD.getAttribute(), 0.49f);
-    Writer writer = OrcFile.createWriter(testFilePath,
-        OrcFile.writerOptions(conf)
-            .fileSystem(fs)
-            .setSchema(schema)
-            .stripeSize(100000)
-            .compress(CompressionKind.ZLIB)
-            .rowIndexStride(1000)
-            .bufferSize(10000));
-    VectorizedRowBatch batch = schema.createRowBatch(1000);
-    Random r1 = new Random(1);
-    String[] words = new String[]{"It", "was", "the", "best", "of", "times,",
-        "it", "was", "the", "worst", "of", "times,", "it", "was", "the", "age",
-        "of", "wisdom,", "it", "was", "the", "age", "of", "foolishness,", "it",
-        "was", "the", "epoch", "of", "belief,", "it", "was", "the", "epoch",
-        "of", "incredulity,", "it", "was", "the", "season", "of", "Light,",
-        "it", "was", "the", "season", "of", "Darkness,", "it", "was", "the",
-        "spring", "of", "hope,", "it", "was", "the", "winter", "of", "despair,",
-        "we", "had", "everything", "before", "us,", "we", "had", "nothing",
-        "before", "us,", "we", "were", "all", "going", "direct", "to",
-        "Heaven,", "we", "were", "all", "going", "direct", "the", "other",
-        "way"};
-    int nextInt = 0;
-    for(int i=0; i < 21000; ++i) {
-      // Write out the same string twice, this guarantees the fraction of rows with
-      // distinct strings is 0.5
-      if (i % 2 == 0) {
-        nextInt = r1.nextInt(words.length);
-        // Append the value of i to the word, this guarantees when an index or word is repeated
-        // the actual string is unique.
-        words[nextInt] += "-" + i;
-      }
-      appendMyRecord(batch, r1.nextInt(), r1.nextLong(), words[nextInt]);
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-    }
-    if (batch.size != 0) {
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    PrintStream origOut = System.out;
-    String outputFilename = "orc-file-dump-dictionary-threshold.out";
-    FileOutputStream myOut = new FileOutputStream(workDir + File.separator + outputFilename);
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "--rowindex=1,2,3"});
-    System.out.flush();
-    System.setOut(origOut);
-
-    checkOutput(outputFilename, workDir + File.separator + outputFilename);
-  }
-
-  @Test
-  public void testBloomFilter() throws Exception {
-    TypeDescription schema = getMyRecordType();
-    conf.set(OrcConf.ENCODING_STRATEGY.getAttribute(), "COMPRESSION");
-    OrcFile.WriterOptions options = OrcFile.writerOptions(conf)
-        .fileSystem(fs)
-        .setSchema(schema)
-        .stripeSize(100000)
-        .compress(CompressionKind.ZLIB)
-        .bufferSize(10000)
-        .rowIndexStride(1000)
-        .bloomFilterColumns("S");
-    Writer writer = OrcFile.createWriter(testFilePath, options);
-    Random r1 = new Random(1);
-    String[] words = new String[]{"It", "was", "the", "best", "of", "times,",
-        "it", "was", "the", "worst", "of", "times,", "it", "was", "the", "age",
-        "of", "wisdom,", "it", "was", "the", "age", "of", "foolishness,", "it",
-        "was", "the", "epoch", "of", "belief,", "it", "was", "the", "epoch",
-        "of", "incredulity,", "it", "was", "the", "season", "of", "Light,",
-        "it", "was", "the", "season", "of", "Darkness,", "it", "was", "the",
-        "spring", "of", "hope,", "it", "was", "the", "winter", "of", "despair,",
-        "we", "had", "everything", "before", "us,", "we", "had", "nothing",
-        "before", "us,", "we", "were", "all", "going", "direct", "to",
-        "Heaven,", "we", "were", "all", "going", "direct", "the", "other",
-        "way"};
-    VectorizedRowBatch batch = schema.createRowBatch(1000);
-    for(int i=0; i < 21000; ++i) {
-      appendMyRecord(batch, r1.nextInt(), r1.nextLong(),
-          words[r1.nextInt(words.length)]);
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-    }
-    if (batch.size > 0) {
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    PrintStream origOut = System.out;
-    String outputFilename = "orc-file-dump-bloomfilter.out";
-    FileOutputStream myOut = new FileOutputStream(workDir + File.separator + outputFilename);
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "--rowindex=3"});
-    System.out.flush();
-    System.setOut(origOut);
-
-
-    checkOutput(outputFilename, workDir + File.separator + outputFilename);
-  }
-
-  @Test
-  public void testBloomFilter2() throws Exception {
-    TypeDescription schema = getMyRecordType();
-    conf.set(OrcConf.ENCODING_STRATEGY.getAttribute(), "COMPRESSION");
-    OrcFile.WriterOptions options = OrcFile.writerOptions(conf)
-        .fileSystem(fs)
-        .setSchema(schema)
-        .stripeSize(100000)
-        .compress(CompressionKind.ZLIB)
-        .bufferSize(10000)
-        .rowIndexStride(1000)
-        .bloomFilterColumns("l")
-        .bloomFilterFpp(0.01);
-    VectorizedRowBatch batch = schema.createRowBatch(1000);
-    Writer writer = OrcFile.createWriter(testFilePath, options);
-    Random r1 = new Random(1);
-    String[] words = new String[]{"It", "was", "the", "best", "of", "times,",
-        "it", "was", "the", "worst", "of", "times,", "it", "was", "the", "age",
-        "of", "wisdom,", "it", "was", "the", "age", "of", "foolishness,", "it",
-        "was", "the", "epoch", "of", "belief,", "it", "was", "the", "epoch",
-        "of", "incredulity,", "it", "was", "the", "season", "of", "Light,",
-        "it", "was", "the", "season", "of", "Darkness,", "it", "was", "the",
-        "spring", "of", "hope,", "it", "was", "the", "winter", "of", "despair,",
-        "we", "had", "everything", "before", "us,", "we", "had", "nothing",
-        "before", "us,", "we", "were", "all", "going", "direct", "to",
-        "Heaven,", "we", "were", "all", "going", "direct", "the", "other",
-        "way"};
-    for(int i=0; i < 21000; ++i) {
-      appendMyRecord(batch, r1.nextInt(), r1.nextLong(),
-          words[r1.nextInt(words.length)]);
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-    }
-    if (batch.size > 0) {
-      writer.addRowBatch(batch);
-    }
-    writer.close();
-    PrintStream origOut = System.out;
-    String outputFilename = "orc-file-dump-bloomfilter2.out";
-    FileOutputStream myOut = new FileOutputStream(workDir + File.separator + outputFilename);
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "--rowindex=2"});
-    System.out.flush();
-    System.setOut(origOut);
-
-
-    checkOutput(outputFilename, workDir + File.separator + outputFilename);
-  }
-}
diff --git a/orc/src/test/org/apache/orc/tools/TestJsonFileDump.java b/orc/src/test/org/apache/orc/tools/TestJsonFileDump.java
deleted file mode 100644
index a514824f40..0000000000
--- a/orc/src/test/org/apache/orc/tools/TestJsonFileDump.java
+++ /dev/null
@@ -1,150 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.orc.tools;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNull;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.FileReader;
-import java.io.PrintStream;
-import java.net.URL;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.orc.CompressionKind;
-import org.apache.orc.OrcConf;
-import org.apache.orc.OrcFile;
-import org.apache.orc.TypeDescription;
-import org.apache.orc.Writer;
-import org.junit.Before;
-import org.junit.Test;
-
-public class TestJsonFileDump {
-  public static String getFileFromClasspath(String name) {
-    URL url = ClassLoader.getSystemResource(name);
-    if (url == null) {
-      throw new IllegalArgumentException("Could not find " + name);
-    }
-    return url.getPath();
-  }
-
-  Path workDir = new Path(System.getProperty("test.tmp.dir"));
-  Configuration conf;
-  FileSystem fs;
-  Path testFilePath;
-
-  @Before
-  public void openFileSystem () throws Exception {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    fs.setWorkingDirectory(workDir);
-    testFilePath = new Path("TestFileDump.testDump.orc");
-    fs.delete(testFilePath, false);
-  }
-
-  static void checkOutput(String expected,
-                                  String actual) throws Exception {
-    BufferedReader eStream =
-        new BufferedReader(new FileReader(getFileFromClasspath(expected)));
-    BufferedReader aStream =
-        new BufferedReader(new FileReader(actual));
-    String expectedLine = eStream.readLine();
-    while (expectedLine != null) {
-      String actualLine = aStream.readLine();
-      System.out.println("actual:   " + actualLine);
-      System.out.println("expected: " + expectedLine);
-      assertEquals(expectedLine, actualLine);
-      expectedLine = eStream.readLine();
-    }
-    assertNull(eStream.readLine());
-    assertNull(aStream.readLine());
-  }
-
-  @Test
-  public void testJsonDump() throws Exception {
-    TypeDescription schema = TypeDescription.createStruct()
-        .addField("i", TypeDescription.createInt())
-        .addField("l", TypeDescription.createLong())
-        .addField("s", TypeDescription.createString());
-    conf.set(OrcConf.ENCODING_STRATEGY.getAttribute(), "COMPRESSION");
-    OrcFile.WriterOptions options = OrcFile.writerOptions(conf)
-        .fileSystem(fs)
-        .setSchema(schema)
-        .stripeSize(100000)
-        .compress(CompressionKind.ZLIB)
-        .bufferSize(10000)
-        .rowIndexStride(1000)
-        .bloomFilterColumns("s");
-    Writer writer = OrcFile.createWriter(testFilePath, options);
-    Random r1 = new Random(1);
-    String[] words = new String[]{"It", "was", "the", "best", "of", "times,",
-        "it", "was", "the", "worst", "of", "times,", "it", "was", "the", "age",
-        "of", "wisdom,", "it", "was", "the", "age", "of", "foolishness,", "it",
-        "was", "the", "epoch", "of", "belief,", "it", "was", "the", "epoch",
-        "of", "incredulity,", "it", "was", "the", "season", "of", "Light,",
-        "it", "was", "the", "season", "of", "Darkness,", "it", "was", "the",
-        "spring", "of", "hope,", "it", "was", "the", "winter", "of", "despair,",
-        "we", "had", "everything", "before", "us,", "we", "had", "nothing",
-        "before", "us,", "we", "were", "all", "going", "direct", "to",
-        "Heaven,", "we", "were", "all", "going", "direct", "the", "other",
-        "way"};
-    VectorizedRowBatch batch = schema.createRowBatch(1000);
-    for(int i=0; i < 21000; ++i) {
-      ((LongColumnVector) batch.cols[0]).vector[batch.size] = r1.nextInt();
-      ((LongColumnVector) batch.cols[1]).vector[batch.size] = r1.nextLong();
-      if (i % 100 == 0) {
-        batch.cols[2].noNulls = false;
-        batch.cols[2].isNull[batch.size] = true;
-      } else {
-        ((BytesColumnVector) batch.cols[2]).setVal(batch.size,
-            words[r1.nextInt(words.length)].getBytes());
-      }
-      batch.size += 1;
-      if (batch.size == batch.getMaxSize()) {
-        writer.addRowBatch(batch);
-        batch.reset();
-      }
-    }
-    if (batch.size > 0) {
-      writer.addRowBatch(batch);
-    }
-
-    writer.close();
-    PrintStream origOut = System.out;
-    String outputFilename = "orc-file-dump.json";
-    FileOutputStream myOut = new FileOutputStream(workDir + File.separator + outputFilename);
-
-    // replace stdout and run command
-    System.setOut(new PrintStream(myOut));
-    FileDump.main(new String[]{testFilePath.toString(), "-j", "-p", "--rowindex=3"});
-    System.out.flush();
-    System.setOut(origOut);
-
-
-    checkOutput(outputFilename, workDir + File.separator + outputFilename);
-  }
-}
diff --git a/orc/src/test/resources/orc-file-11-format.orc b/orc/src/test/resources/orc-file-11-format.orc
deleted file mode 100644
index 41653c8403..0000000000
Binary files a/orc/src/test/resources/orc-file-11-format.orc and /dev/null differ
diff --git a/orc/src/test/resources/orc-file-dump-bloomfilter.out b/orc/src/test/resources/orc-file-dump-bloomfilter.out
deleted file mode 100644
index 18fd2fb093..0000000000
--- a/orc/src/test/resources/orc-file-dump-bloomfilter.out
+++ /dev/null
@@ -1,179 +0,0 @@
-Structure for TestFileDump.testDump.orc
-File Version: 0.12 with HIVE_13083
-Rows: 21000
-Compression: ZLIB
-Compression size: 4096
-Type: struct<i:int,l:bigint,s:string>
-
-Stripe Statistics:
-  Stripe 1:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146021688 max: 2147223299 sum: 515792826
-    Column 2: count: 5000 hasNull: false min: -9218592812243954469 max: 9221614132680747961
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19280
-  Stripe 2:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146733128 max: 2147001622 sum: 7673427
-    Column 2: count: 5000 hasNull: false min: -9220818777591257749 max: 9222259462014003839
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19504
-  Stripe 3:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146993718 max: 2147378179 sum: 132660742551
-    Column 2: count: 5000 hasNull: false min: -9218342074710552826 max: 9222303228623055266
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19641
-  Stripe 4:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146658006 max: 2145520931 sum: 8533549236
-    Column 2: count: 5000 hasNull: false min: -9222758097219661129 max: 9221043130193737406
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19470
-  Stripe 5:
-    Column 0: count: 1000 hasNull: false
-    Column 1: count: 1000 hasNull: false min: -2146245500 max: 2146378640 sum: 51299706363
-    Column 2: count: 1000 hasNull: false min: -9208193203370316142 max: 9218567213558056476
-    Column 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3866
-
-File Statistics:
-  Column 0: count: 21000 hasNull: false
-  Column 1: count: 21000 hasNull: false min: -2146993718 max: 2147378179 sum: 193017464403
-  Column 2: count: 21000 hasNull: false min: -9222758097219661129 max: 9222303228623055266
-  Column 3: count: 21000 hasNull: false min: Darkness, max: worst sum: 81761
-
-Stripes:
-  Stripe: offset: 3 data: 63786 rows: 5000 tail: 86 index: 951
-    Stream: column 0 section ROW_INDEX start: 3 length 17
-    Stream: column 1 section ROW_INDEX start: 20 length 166
-    Stream: column 2 section ROW_INDEX start: 186 length 169
-    Stream: column 3 section ROW_INDEX start: 355 length 87
-    Stream: column 3 section BLOOM_FILTER start: 442 length 512
-    Stream: column 1 section DATA start: 954 length 20035
-    Stream: column 2 section DATA start: 20989 length 40050
-    Stream: column 3 section DATA start: 61039 length 3543
-    Stream: column 3 section LENGTH start: 64582 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 64607 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3862 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 3884 positions: 0,659,149
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3893 positions: 0,1531,3
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3798 positions: 0,2281,32
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 3843 positions: 0,3033,45
-    Bloom filters for column 3:
-      Entry 0: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 1: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 2: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 3: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 4: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-  Stripe: offset: 64826 data: 63775 rows: 5000 tail: 86 index: 944
-    Stream: column 0 section ROW_INDEX start: 64826 length 17
-    Stream: column 1 section ROW_INDEX start: 64843 length 164
-    Stream: column 2 section ROW_INDEX start: 65007 length 168
-    Stream: column 3 section ROW_INDEX start: 65175 length 83
-    Stream: column 3 section BLOOM_FILTER start: 65258 length 512
-    Stream: column 1 section DATA start: 65770 length 20035
-    Stream: column 2 section DATA start: 85805 length 40050
-    Stream: column 3 section DATA start: 125855 length 3532
-    Stream: column 3 section LENGTH start: 129387 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 129412 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3923 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 3869 positions: 0,761,12
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3817 positions: 0,1472,70
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3931 positions: 0,2250,43
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 3964 positions: 0,2978,88
-    Bloom filters for column 3:
-      Entry 0: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 1: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 2: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 3: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 4: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-  Stripe: offset: 129631 data: 63787 rows: 5000 tail: 86 index: 950
-    Stream: column 0 section ROW_INDEX start: 129631 length 17
-    Stream: column 1 section ROW_INDEX start: 129648 length 163
-    Stream: column 2 section ROW_INDEX start: 129811 length 168
-    Stream: column 3 section ROW_INDEX start: 129979 length 90
-    Stream: column 3 section BLOOM_FILTER start: 130069 length 512
-    Stream: column 1 section DATA start: 130581 length 20035
-    Stream: column 2 section DATA start: 150616 length 40050
-    Stream: column 3 section DATA start: 190666 length 3544
-    Stream: column 3 section LENGTH start: 194210 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 194235 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3817 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 4008 positions: 0,634,174
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3999 positions: 0,1469,69
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3817 positions: 0,2133,194
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 4000 positions: 0,3005,43
-    Bloom filters for column 3:
-      Entry 0: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 1: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 2: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 3: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 4: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-  Stripe: offset: 194454 data: 63817 rows: 5000 tail: 86 index: 952
-    Stream: column 0 section ROW_INDEX start: 194454 length 17
-    Stream: column 1 section ROW_INDEX start: 194471 length 165
-    Stream: column 2 section ROW_INDEX start: 194636 length 167
-    Stream: column 3 section ROW_INDEX start: 194803 length 91
-    Stream: column 3 section BLOOM_FILTER start: 194894 length 512
-    Stream: column 1 section DATA start: 195406 length 20035
-    Stream: column 2 section DATA start: 215441 length 40050
-    Stream: column 3 section DATA start: 255491 length 3574
-    Stream: column 3 section LENGTH start: 259065 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 259090 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3901 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 3900 positions: 0,431,431
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3909 positions: 0,1485,52
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3947 positions: 0,2196,104
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 3813 positions: 0,2934,131
-    Bloom filters for column 3:
-      Entry 0: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 1: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 2: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 3: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Entry 4: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-  Stripe: offset: 259309 data: 12943 rows: 1000 tail: 78 index: 432
-    Stream: column 0 section ROW_INDEX start: 259309 length 12
-    Stream: column 1 section ROW_INDEX start: 259321 length 38
-    Stream: column 2 section ROW_INDEX start: 259359 length 41
-    Stream: column 3 section ROW_INDEX start: 259400 length 40
-    Stream: column 3 section BLOOM_FILTER start: 259440 length 301
-    Stream: column 1 section DATA start: 259741 length 4007
-    Stream: column 2 section DATA start: 263748 length 8010
-    Stream: column 3 section DATA start: 271758 length 768
-    Stream: column 3 section LENGTH start: 272526 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 272551 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3866 positions: 0,0,0
-    Bloom filters for column 3:
-      Entry 0: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-      Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 138 loadFactor: 0.022 expectedFpp: 2.343647E-7
-
-File length: 273307 bytes
-Padding length: 0 bytes
-Padding ratio: 0%
-________________________________________________________________________________________________________________________
-
diff --git a/orc/src/test/resources/orc-file-dump-bloomfilter2.out b/orc/src/test/resources/orc-file-dump-bloomfilter2.out
deleted file mode 100644
index fa5cc2d154..0000000000
--- a/orc/src/test/resources/orc-file-dump-bloomfilter2.out
+++ /dev/null
@@ -1,179 +0,0 @@
-Structure for TestFileDump.testDump.orc
-File Version: 0.12 with HIVE_13083
-Rows: 21000
-Compression: ZLIB
-Compression size: 4096
-Type: struct<i:int,l:bigint,s:string>
-
-Stripe Statistics:
-  Stripe 1:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146021688 max: 2147223299 sum: 515792826
-    Column 2: count: 5000 hasNull: false min: -9218592812243954469 max: 9221614132680747961
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19280
-  Stripe 2:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146733128 max: 2147001622 sum: 7673427
-    Column 2: count: 5000 hasNull: false min: -9220818777591257749 max: 9222259462014003839
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19504
-  Stripe 3:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146993718 max: 2147378179 sum: 132660742551
-    Column 2: count: 5000 hasNull: false min: -9218342074710552826 max: 9222303228623055266
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19641
-  Stripe 4:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146658006 max: 2145520931 sum: 8533549236
-    Column 2: count: 5000 hasNull: false min: -9222758097219661129 max: 9221043130193737406
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19470
-  Stripe 5:
-    Column 0: count: 1000 hasNull: false
-    Column 1: count: 1000 hasNull: false min: -2146245500 max: 2146378640 sum: 51299706363
-    Column 2: count: 1000 hasNull: false min: -9208193203370316142 max: 9218567213558056476
-    Column 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3866
-
-File Statistics:
-  Column 0: count: 21000 hasNull: false
-  Column 1: count: 21000 hasNull: false min: -2146993718 max: 2147378179 sum: 193017464403
-  Column 2: count: 21000 hasNull: false min: -9222758097219661129 max: 9222303228623055266
-  Column 3: count: 21000 hasNull: false min: Darkness, max: worst sum: 81761
-
-Stripes:
-  Stripe: offset: 3 data: 63786 rows: 5000 tail: 85 index: 6974
-    Stream: column 0 section ROW_INDEX start: 3 length 17
-    Stream: column 1 section ROW_INDEX start: 20 length 166
-    Stream: column 2 section ROW_INDEX start: 186 length 169
-    Stream: column 2 section BLOOM_FILTER start: 355 length 6535
-    Stream: column 3 section ROW_INDEX start: 6890 length 87
-    Stream: column 1 section DATA start: 6977 length 20035
-    Stream: column 2 section DATA start: 27012 length 40050
-    Stream: column 3 section DATA start: 67062 length 3543
-    Stream: column 3 section LENGTH start: 70605 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 70630 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9200577545527640566 max: 9175500305011173751 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9203618157670445774 max: 9208123824411178101 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9218592812243954469 max: 9221351515892923972 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9206585617947511272 max: 9167703224425685487 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9206645795733282496 max: 9221614132680747961 positions: 28693,14,416
-    Bloom filters for column 2:
-      Entry 0: numHashFunctions: 7 bitCount: 9600 popCount: 4931 loadFactor: 0.5136 expectedFpp: 0.009432924
-      Entry 1: numHashFunctions: 7 bitCount: 9600 popCount: 4956 loadFactor: 0.5163 expectedFpp: 0.009772834
-      Entry 2: numHashFunctions: 7 bitCount: 9600 popCount: 4971 loadFactor: 0.5178 expectedFpp: 0.009981772
-      Entry 3: numHashFunctions: 7 bitCount: 9600 popCount: 4971 loadFactor: 0.5178 expectedFpp: 0.009981772
-      Entry 4: numHashFunctions: 7 bitCount: 9600 popCount: 4949 loadFactor: 0.5155 expectedFpp: 0.009676614
-      Stripe level merge: numHashFunctions: 7 bitCount: 9600 popCount: 9347 loadFactor: 0.9736 expectedFpp: 0.829482
-  Stripe: offset: 70848 data: 63775 rows: 5000 tail: 85 index: 6965
-    Stream: column 0 section ROW_INDEX start: 70848 length 17
-    Stream: column 1 section ROW_INDEX start: 70865 length 164
-    Stream: column 2 section ROW_INDEX start: 71029 length 168
-    Stream: column 2 section BLOOM_FILTER start: 71197 length 6533
-    Stream: column 3 section ROW_INDEX start: 77730 length 83
-    Stream: column 1 section DATA start: 77813 length 20035
-    Stream: column 2 section DATA start: 97848 length 40050
-    Stream: column 3 section DATA start: 137898 length 3532
-    Stream: column 3 section LENGTH start: 141430 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 141455 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9218450653857701562 max: 9189819526332228512 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9220818777591257749 max: 9178821722829648113 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9220031433030423388 max: 9210838931786956852 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9208195729739635607 max: 9222259462014003839 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9174271499932339698 max: 9212277876771676916 positions: 28693,14,416
-    Bloom filters for column 2:
-      Entry 0: numHashFunctions: 7 bitCount: 9600 popCount: 4971 loadFactor: 0.5178 expectedFpp: 0.009981772
-      Entry 1: numHashFunctions: 7 bitCount: 9600 popCount: 4988 loadFactor: 0.5196 expectedFpp: 0.010223193
-      Entry 2: numHashFunctions: 7 bitCount: 9600 popCount: 5002 loadFactor: 0.521 expectedFpp: 0.01042575
-      Entry 3: numHashFunctions: 7 bitCount: 9600 popCount: 4962 loadFactor: 0.5169 expectedFpp: 0.009855959
-      Entry 4: numHashFunctions: 7 bitCount: 9600 popCount: 4966 loadFactor: 0.5173 expectedFpp: 0.009911705
-      Stripe level merge: numHashFunctions: 7 bitCount: 9600 popCount: 9344 loadFactor: 0.9733 expectedFpp: 0.8276205
-  Stripe: offset: 141673 data: 63787 rows: 5000 tail: 85 index: 6971
-    Stream: column 0 section ROW_INDEX start: 141673 length 17
-    Stream: column 1 section ROW_INDEX start: 141690 length 163
-    Stream: column 2 section ROW_INDEX start: 141853 length 168
-    Stream: column 2 section BLOOM_FILTER start: 142021 length 6533
-    Stream: column 3 section ROW_INDEX start: 148554 length 90
-    Stream: column 1 section DATA start: 148644 length 20035
-    Stream: column 2 section DATA start: 168679 length 40050
-    Stream: column 3 section DATA start: 208729 length 3544
-    Stream: column 3 section LENGTH start: 212273 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 212298 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9211978436552246208 max: 9179058898902097152 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9195645160817780503 max: 9189147759444307708 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9202888157616520823 max: 9193561362676960747 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9216318198067839390 max: 9221286760675829363 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9218342074710552826 max: 9222303228623055266 positions: 28693,14,416
-    Bloom filters for column 2:
-      Entry 0: numHashFunctions: 7 bitCount: 9600 popCount: 4967 loadFactor: 0.5174 expectedFpp: 0.009925688
-      Entry 1: numHashFunctions: 7 bitCount: 9600 popCount: 5002 loadFactor: 0.521 expectedFpp: 0.01042575
-      Entry 2: numHashFunctions: 7 bitCount: 9600 popCount: 4964 loadFactor: 0.5171 expectedFpp: 0.009883798
-      Entry 3: numHashFunctions: 7 bitCount: 9600 popCount: 4943 loadFactor: 0.5149 expectedFpp: 0.009594797
-      Entry 4: numHashFunctions: 7 bitCount: 9600 popCount: 4930 loadFactor: 0.5135 expectedFpp: 0.009419539
-      Stripe level merge: numHashFunctions: 7 bitCount: 9600 popCount: 9333 loadFactor: 0.9722 expectedFpp: 0.82082444
-  Stripe: offset: 212516 data: 63817 rows: 5000 tail: 85 index: 6964
-    Stream: column 0 section ROW_INDEX start: 212516 length 17
-    Stream: column 1 section ROW_INDEX start: 212533 length 165
-    Stream: column 2 section ROW_INDEX start: 212698 length 167
-    Stream: column 2 section BLOOM_FILTER start: 212865 length 6524
-    Stream: column 3 section ROW_INDEX start: 219389 length 91
-    Stream: column 1 section DATA start: 219480 length 20035
-    Stream: column 2 section DATA start: 239515 length 40050
-    Stream: column 3 section DATA start: 279565 length 3574
-    Stream: column 3 section LENGTH start: 283139 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 283164 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9222731174895935707 max: 9214167447015056056 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9222758097219661129 max: 9221043130193737406 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9174483776261243438 max: 9208134757538374043 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9174329712613510612 max: 9197412874152820822 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9221162005892422758 max: 9220625004936875965 positions: 28693,14,416
-    Bloom filters for column 2:
-      Entry 0: numHashFunctions: 7 bitCount: 9600 popCount: 4951 loadFactor: 0.5157 expectedFpp: 0.009704026
-      Entry 1: numHashFunctions: 7 bitCount: 9600 popCount: 4969 loadFactor: 0.5176 expectedFpp: 0.009953696
-      Entry 2: numHashFunctions: 7 bitCount: 9600 popCount: 4994 loadFactor: 0.5202 expectedFpp: 0.010309587
-      Entry 3: numHashFunctions: 7 bitCount: 9600 popCount: 4941 loadFactor: 0.5147 expectedFpp: 0.009567649
-      Entry 4: numHashFunctions: 7 bitCount: 9600 popCount: 4993 loadFactor: 0.5201 expectedFpp: 0.010295142
-      Stripe level merge: numHashFunctions: 7 bitCount: 9600 popCount: 9353 loadFactor: 0.9743 expectedFpp: 0.8332165
-  Stripe: offset: 283382 data: 12943 rows: 1000 tail: 78 index: 1468
-    Stream: column 0 section ROW_INDEX start: 283382 length 12
-    Stream: column 1 section ROW_INDEX start: 283394 length 38
-    Stream: column 2 section ROW_INDEX start: 283432 length 41
-    Stream: column 2 section BLOOM_FILTER start: 283473 length 1337
-    Stream: column 3 section ROW_INDEX start: 284810 length 40
-    Stream: column 1 section DATA start: 284850 length 4007
-    Stream: column 2 section DATA start: 288857 length 8010
-    Stream: column 3 section DATA start: 296867 length 768
-    Stream: column 3 section LENGTH start: 297635 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 297660 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9208193203370316142 max: 9218567213558056476 positions: 0,0,0
-    Bloom filters for column 2:
-      Entry 0: numHashFunctions: 7 bitCount: 9600 popCount: 4948 loadFactor: 0.5154 expectedFpp: 0.00966294
-      Stripe level merge: numHashFunctions: 7 bitCount: 9600 popCount: 4948 loadFactor: 0.5154 expectedFpp: 0.00966294
-
-File length: 298416 bytes
-Padding length: 0 bytes
-Padding ratio: 0%
-________________________________________________________________________________________________________________________
-
diff --git a/orc/src/test/resources/orc-file-dump-dictionary-threshold.out b/orc/src/test/resources/orc-file-dump-dictionary-threshold.out
deleted file mode 100644
index 17a964b31d..0000000000
--- a/orc/src/test/resources/orc-file-dump-dictionary-threshold.out
+++ /dev/null
@@ -1,190 +0,0 @@
-Structure for TestFileDump.testDump.orc
-File Version: 0.12 with HIVE_13083
-Rows: 21000
-Compression: ZLIB
-Compression size: 4096
-Type: struct<i:int,l:bigint,s:string>
-
-Stripe Statistics:
-  Stripe 1:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2147115959 max: 2145911404 sum: 159677169195
-    Column 2: count: 5000 hasNull: false min: -9216505819108477308 max: 9217851628057711416
-    Column 3: count: 5000 hasNull: false min: Darkness,-230 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744 sum: 381254
-  Stripe 2:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2147390285 max: 2147224606 sum: -14961457759
-    Column 2: count: 5000 hasNull: false min: -9222178666167296739 max: 9221301751385928177
-    Column 3: count: 5000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938 sum: 1117994
-  Stripe 3:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2145842720 max: 2146718321 sum: 141092475520
-    Column 2: count: 5000 hasNull: false min: -9221963099397084326 max: 9222722740629726770
-    Column 3: count: 5000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974 sum: 1925226
-  Stripe 4:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2145378214 max: 2147453086 sum: -153680004530
-    Column 2: count: 5000 hasNull: false min: -9222731174895935707 max: 9222919052987871506
-    Column 3: count: 5000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878-18010-18410-18524-18788-19204-19254-19518-19596-19786-19874-19904 sum: 2815002
-  Stripe 5:
-    Column 0: count: 1000 hasNull: false
-    Column 1: count: 1000 hasNull: false min: -2143595397 max: 2136858458 sum: -22999664100
-    Column 2: count: 1000 hasNull: false min: -9212379634781416464 max: 9197412874152820822
-    Column 3: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048-15494-15674-15726-16006-16056-16180-16304-16332-16452-16598-16730-16810-16994-17210-17268-17786-17962-18214-18444-18446-18724-18912-18952-19164-19348-19400-19546-19776-19896-20084 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878-18010-18410-18524-18788-19204-19254-19518-19596-19786-19874-19904-20390-20752-20936 sum: 670762
-
-File Statistics:
-  Column 0: count: 21000 hasNull: false
-  Column 1: count: 21000 hasNull: false min: -2147390285 max: 2147453086 sum: 109128518326
-  Column 2: count: 21000 hasNull: false min: -9222731174895935707 max: 9222919052987871506
-  Column 3: count: 21000 hasNull: false min: Darkness,-230 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878-18010-18410-18524-18788-19204-19254-19518-19596-19786-19874-19904-20390-20752-20936 sum: 6910238
-
-Stripes:
-  Stripe: offset: 3 data: 163602 rows: 5000 tail: 68 index: 720
-    Stream: column 0 section ROW_INDEX start: 3 length 17
-    Stream: column 1 section ROW_INDEX start: 20 length 166
-    Stream: column 2 section ROW_INDEX start: 186 length 171
-    Stream: column 3 section ROW_INDEX start: 357 length 366
-    Stream: column 1 section DATA start: 723 length 20035
-    Stream: column 2 section DATA start: 20758 length 40050
-    Stream: column 3 section DATA start: 60808 length 99226
-    Stream: column 3 section LENGTH start: 160034 length 4291
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DIRECT_V2
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2132329551 max: 2145911404 sum: 61941331718 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2138433136 max: 2145210552 sum: 14574030042 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2147115959 max: 2137805337 sum: -2032493169 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2137828953 max: 2145877119 sum: -3167202608 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2146452517 max: 2142394906 sum: 88361503212 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9206837518492372266 max: 9169230975203934579 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9188878639954124284 max: 9213664245516510068 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9211329013123260308 max: 9217851628057711416 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9185745718227889962 max: 9181722705210917931 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9216505819108477308 max: 9196474183833079923 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness,-230 max: worst-54-290-346-648-908-996 sum: 18442 positions: 0,0,0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966 sum: 46338 positions: 4767,2058,0,695,18
-      Entry 2: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660 sum: 75448 positions: 16464,3340,0,1554,14
-      Entry 3: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788 sum: 104868 positions: 36532,964,0,2372,90
-      Entry 4: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744 sum: 136158 positions: 63067,3432,0,3354,108
-  Stripe: offset: 164393 data: 368335 rows: 5000 tail: 69 index: 956
-    Stream: column 0 section ROW_INDEX start: 164393 length 17
-    Stream: column 1 section ROW_INDEX start: 164410 length 157
-    Stream: column 2 section ROW_INDEX start: 164567 length 166
-    Stream: column 3 section ROW_INDEX start: 164733 length 616
-    Stream: column 1 section DATA start: 165349 length 20035
-    Stream: column 2 section DATA start: 185384 length 40050
-    Stream: column 3 section DATA start: 225434 length 302715
-    Stream: column 3 section LENGTH start: 528149 length 5535
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DIRECT_V2
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2146021688 max: 2146838901 sum: -50979197646 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2143569489 max: 2141223179 sum: 22810066834 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2140649392 max: 2146301701 sum: -31694882346 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2147390285 max: 2146299933 sum: 79371934221 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2145928262 max: 2147224606 sum: -34469378822 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9222178666167296739 max: 9191250610515369723 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9220148577547102875 max: 9213945522531717278 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9220818777591257749 max: 9221301751385928177 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9220031433030423388 max: 9207856144487414148 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9201438531577205959 max: 9212462124593119846 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726 sum: 166320 positions: 0,0,0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994 sum: 193436 positions: 43833,2480,0,967,90
-      Entry 2: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988 sum: 224740 positions: 94117,3404,0,1945,222
-      Entry 3: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984 sum: 252094 positions: 155111,2864,0,3268,48
-      Entry 4: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938 sum: 281404 positions: 224570,1006,0,4064,342
-  Stripe: offset: 533753 data: 606074 rows: 5000 tail: 69 index: 1427
-    Stream: column 0 section ROW_INDEX start: 533753 length 17
-    Stream: column 1 section ROW_INDEX start: 533770 length 167
-    Stream: column 2 section ROW_INDEX start: 533937 length 168
-    Stream: column 3 section ROW_INDEX start: 534105 length 1075
-    Stream: column 1 section DATA start: 535180 length 20035
-    Stream: column 2 section DATA start: 555215 length 40050
-    Stream: column 3 section DATA start: 595265 length 540210
-    Stream: column 3 section LENGTH start: 1135475 length 5779
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DIRECT_V2
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2138229212 max: 2144818981 sum: -22823642812 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2145842720 max: 2144179881 sum: -12562754334 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2143045885 max: 2146718321 sum: 82993638644 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2144745617 max: 2146570474 sum: 25138722367 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2140127150 max: 2135081620 sum: 68346511655 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9204340807292138409 max: 9208698732685326961 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9221963099397084326 max: 9222722740629726770 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9210480084701091299 max: 9207767402467343058 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9195038026813631215 max: 9199201928563274421 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9215483580266514322 max: 9220102792864959501 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876 sum: 313880 positions: 0,0,0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964 sum: 349542 positions: 87800,2584,0,1097,28
-      Entry 2: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976 sum: 386538 positions: 185635,3966,0,2077,162
-      Entry 3: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766 sum: 421660 positions: 295550,1384,0,3369,16
-      Entry 4: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974 sum: 453606 positions: 412768,1156,0,4041,470
-  Stripe: offset: 1141323 data: 864001 rows: 5000 tail: 69 index: 1975
-    Stream: column 0 section ROW_INDEX start: 1141323 length 17
-    Stream: column 1 section ROW_INDEX start: 1141340 length 156
-    Stream: column 2 section ROW_INDEX start: 1141496 length 168
-    Stream: column 3 section ROW_INDEX start: 1141664 length 1634
-    Stream: column 1 section DATA start: 1143298 length 20035
-    Stream: column 2 section DATA start: 1163333 length 40050
-    Stream: column 3 section DATA start: 1203383 length 798014
-    Stream: column 3 section LENGTH start: 2001397 length 5902
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DIRECT_V2
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2145319330 max: 2146998132 sum: -50856753363 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2134288866 max: 2147453086 sum: -17911019023 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2139010804 max: 2144727593 sum: -24993151857 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2145378214 max: 2144098933 sum: -18055164052 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2140494429 max: 2144595861 sum: -41863916235 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9172774601303513941 max: 9212917101275642143 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9218164880949195469 max: 9222919052987871506 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9222731174895935707 max: 9214167447015056056 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9196276654247395117 max: 9210639275226058005 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9197393848859294562 max: 9208134757538374043 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610 sum: 492916 positions: 0,0,0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048-15494-15674-15726-16006 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936 sum: 527290 positions: 139298,1396,0,1077,140
-      Entry 2: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048-15494-15674-15726-16006-16056-16180-16304-16332-16452-16598-16730-16810-16994-17210 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878 sum: 568274 positions: 286457,302,0,1926,462
-      Entry 3: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048-15494-15674-15726-16006-16056-16180-16304-16332-16452-16598-16730-16810-16994-17210-17268-17786-17962-18214 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878-18010-18410-18524-18788 sum: 594578 positions: 447943,3328,0,3444,250
-      Entry 4: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048-15494-15674-15726-16006-16056-16180-16304-16332-16452-16598-16730-16810-16994-17210-17268-17786-17962-18214-18444-18446-18724-18912-18952-19164 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878-18010-18410-18524-18788-19204-19254-19518-19596-19786-19874-19904 sum: 631944 positions: 616471,3986,3778,547,292
-  Stripe: offset: 2007368 data: 207295 rows: 1000 tail: 67 index: 841
-    Stream: column 0 section ROW_INDEX start: 2007368 length 12
-    Stream: column 1 section ROW_INDEX start: 2007380 length 38
-    Stream: column 2 section ROW_INDEX start: 2007418 length 41
-    Stream: column 3 section ROW_INDEX start: 2007459 length 750
-    Stream: column 1 section DATA start: 2008209 length 4007
-    Stream: column 2 section DATA start: 2012216 length 8010
-    Stream: column 3 section DATA start: 2020226 length 194018
-    Stream: column 3 section LENGTH start: 2214244 length 1260
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DIRECT_V2
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2143595397 max: 2136858458 sum: -22999664100 positions: 0,0,0
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9212379634781416464 max: 9197412874152820822 positions: 0,0,0
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness,-230-368-488-586-862-930-1686-2044-2636-2652-2872-3108-3162-3192-3404-3442-3508-3542-3550-3712-3980-4146-4204-4336-4390-4418-4424-4490-4512-4650-4768-4924-4950-5210-5524-5630-5678-5710-5758-5952-6238-6252-6300-6366-6668-6712-6926-6942-7100-7194-7802-8030-8452-8608-8640-8862-8868-9134-9234-9412-9602-9608-9642-9678-9740-9780-10426-10510-10514-10706-10814-10870-10942-11028-11244-11326-11462-11496-11656-11830-12022-12178-12418-12832-13304-13448-13590-13618-13908-14188-14246-14340-14364-14394-14762-14850-14964-15048-15494-15674-15726-16006-16056-16180-16304-16332-16452-16598-16730-16810-16994-17210-17268-17786-17962-18214-18444-18446-18724-18912-18952-19164-19348-19400-19546-19776-19896-20084 max: worst-54-290-346-648-908-996-1038-1080-1560-1584-1620-1744-1770-1798-1852-1966-2162-2244-2286-2296-2534-2660-3114-3676-3788-4068-4150-4706-4744-5350-5420-5582-5696-5726-6006-6020-6024-6098-6184-6568-6636-6802-6994-7004-7318-7498-7758-7780-7798-7920-7952-7960-7988-8232-8256-8390-8416-8478-8620-8840-8984-9038-9128-9236-9248-9344-9594-9650-9714-9928-9938-10178-10368-10414-10502-10732-10876-11008-11158-11410-11722-11836-11964-12054-12096-12126-12136-12202-12246-12298-12616-12774-12782-12790-12802-12976-13216-13246-13502-13766-14454-14974-15004-15124-15252-15294-15356-15530-15610-16316-16936-17024-17122-17214-17310-17528-17682-17742-17870-17878-18010-18410-18524-18788-19204-19254-19518-19596-19786-19874-19904-20390-20752-20936 sum: 670762 positions: 0,0,0,0,0
-
-File length: 2217685 bytes
-Padding length: 0 bytes
-Padding ratio: 0%
-________________________________________________________________________________________________________________________
-
diff --git a/orc/src/test/resources/orc-file-dump.json b/orc/src/test/resources/orc-file-dump.json
deleted file mode 100644
index bf654a13f1..0000000000
--- a/orc/src/test/resources/orc-file-dump.json
+++ /dev/null
@@ -1,1355 +0,0 @@
-{
-  "fileName": "TestFileDump.testDump.orc",
-  "fileVersion": "0.12",
-  "writerVersion": "HIVE_13083",
-  "numberOfRows": 21000,
-  "compression": "ZLIB",
-  "compressionBufferSize": 4096,
-  "schemaString": "struct<i:int,l:bigint,s:string>",
-  "schema": [
-    {
-      "columnId": 0,
-      "columnType": "STRUCT",
-      "childColumnNames": [
-        "i",
-        "l",
-        "s"
-      ],
-      "childColumnIds": [
-        1,
-        2,
-        3
-      ]
-    },
-    {
-      "columnId": 1,
-      "columnType": "INT"
-    },
-    {
-      "columnId": 2,
-      "columnType": "LONG"
-    },
-    {
-      "columnId": 3,
-      "columnType": "STRING"
-    }
-  ],
-  "stripeStatistics": [
-    {
-      "stripeNumber": 1,
-      "columnStatistics": [
-        {
-          "columnId": 0,
-          "count": 5000,
-          "hasNull": false
-        },
-        {
-          "columnId": 1,
-          "count": 5000,
-          "hasNull": false,
-          "min": -2147115959,
-          "max": 2145210552,
-          "sum": 50111854553,
-          "type": "LONG"
-        },
-        {
-          "columnId": 2,
-          "count": 5000,
-          "hasNull": false,
-          "min": -9223180583305557329,
-          "max": 9221614132680747961,
-          "type": "LONG"
-        },
-        {
-          "columnId": 3,
-          "count": 4950,
-          "hasNull": true,
-          "min": "Darkness,",
-          "max": "worst",
-          "totalLength": 19283,
-          "type": "STRING"
-        }
-      ]
-    },
-    {
-      "stripeNumber": 2,
-      "columnStatistics": [
-        {
-          "columnId": 0,
-          "count": 5000,
-          "hasNull": false
-        },
-        {
-          "columnId": 1,
-          "count": 5000,
-          "hasNull": false,
-          "min": -2147390285,
-          "max": 2147224606,
-          "sum": -22290798217,
-          "type": "LONG"
-        },
-        {
-          "columnId": 2,
-          "count": 5000,
-          "hasNull": false,
-          "min": -9219295160509160427,
-          "max": 9217571024994660020,
-          "type": "LONG"
-        },
-        {
-          "columnId": 3,
-          "count": 4950,
-          "hasNull": true,
-          "min": "Darkness,",
-          "max": "worst",
-          "totalLength": 19397,
-          "type": "STRING"
-        }
-      ]
-    },
-    {
-      "stripeNumber": 3,
-      "columnStatistics": [
-        {
-          "columnId": 0,
-          "count": 5000,
-          "hasNull": false
-        },
-        {
-          "columnId": 1,
-          "count": 5000,
-          "hasNull": false,
-          "min": -2146954065,
-          "max": 2146722468,
-          "sum": 20639652136,
-          "type": "LONG"
-        },
-        {
-          "columnId": 2,
-          "count": 5000,
-          "hasNull": false,
-          "min": -9214076359988107846,
-          "max": 9222919052987871506,
-          "type": "LONG"
-        },
-        {
-          "columnId": 3,
-          "count": 4950,
-          "hasNull": true,
-          "min": "Darkness,",
-          "max": "worst",
-          "totalLength": 19031,
-          "type": "STRING"
-        }
-      ]
-    },
-    {
-      "stripeNumber": 4,
-      "columnStatistics": [
-        {
-          "columnId": 0,
-          "count": 5000,
-          "hasNull": false
-        },
-        {
-          "columnId": 1,
-          "count": 5000,
-          "hasNull": false,
-          "min": -2146969085,
-          "max": 2146025044,
-          "sum": -5156814387,
-          "type": "LONG"
-        },
-        {
-          "columnId": 2,
-          "count": 5000,
-          "hasNull": false,
-          "min": -9222731174895935707,
-          "max": 9220625004936875965,
-          "type": "LONG"
-        },
-        {
-          "columnId": 3,
-          "count": 4950,
-          "hasNull": true,
-          "min": "Darkness,",
-          "max": "worst",
-          "totalLength": 19459,
-          "type": "STRING"
-        }
-      ]
-    },
-    {
-      "stripeNumber": 5,
-      "columnStatistics": [
-        {
-          "columnId": 0,
-          "count": 1000,
-          "hasNull": false
-        },
-        {
-          "columnId": 1,
-          "count": 1000,
-          "hasNull": false,
-          "min": -2144303438,
-          "max": 2127599049,
-          "sum": 62841564778,
-          "type": "LONG"
-        },
-        {
-          "columnId": 2,
-          "count": 1000,
-          "hasNull": false,
-          "min": -9195133638801798919,
-          "max": 9218626063131504414,
-          "type": "LONG"
-        },
-        {
-          "columnId": 3,
-          "count": 990,
-          "hasNull": true,
-          "min": "Darkness,",
-          "max": "worst",
-          "totalLength": 3963,
-          "type": "STRING"
-        }
-      ]
-    }
-  ],
-  "fileStatistics": [
-    {
-      "columnId": 0,
-      "count": 21000,
-      "hasNull": false
-    },
-    {
-      "columnId": 1,
-      "count": 21000,
-      "hasNull": false,
-      "min": -2147390285,
-      "max": 2147224606,
-      "sum": 106145458863,
-      "type": "LONG"
-    },
-    {
-      "columnId": 2,
-      "count": 21000,
-      "hasNull": false,
-      "min": -9223180583305557329,
-      "max": 9222919052987871506,
-      "type": "LONG"
-    },
-    {
-      "columnId": 3,
-      "count": 20790,
-      "hasNull": true,
-      "min": "Darkness,",
-      "max": "worst",
-      "totalLength": 81133,
-      "type": "STRING"
-    }
-  ],
-  "stripes": [
-    {
-      "stripeNumber": 1,
-      "stripeInformation": {
-        "offset": 3,
-        "indexLength": 970,
-        "dataLength": 63770,
-        "footerLength": 90,
-        "rowCount": 5000
-      },
-      "streams": [
-        {
-          "columnId": 0,
-          "section": "ROW_INDEX",
-          "startOffset": 3,
-          "length": 17
-        },
-        {
-          "columnId": 1,
-          "section": "ROW_INDEX",
-          "startOffset": 20,
-          "length": 167
-        },
-        {
-          "columnId": 2,
-          "section": "ROW_INDEX",
-          "startOffset": 187,
-          "length": 171
-        },
-        {
-          "columnId": 3,
-          "section": "ROW_INDEX",
-          "startOffset": 358,
-          "length": 103
-        },
-        {
-          "columnId": 3,
-          "section": "BLOOM_FILTER",
-          "startOffset": 461,
-          "length": 512
-        },
-        {
-          "columnId": 1,
-          "section": "DATA",
-          "startOffset": 973,
-          "length": 20035
-        },
-        {
-          "columnId": 2,
-          "section": "DATA",
-          "startOffset": 21008,
-          "length": 40050
-        },
-        {
-          "columnId": 3,
-          "section": "PRESENT",
-          "startOffset": 61058,
-          "length": 17
-        },
-        {
-          "columnId": 3,
-          "section": "DATA",
-          "startOffset": 61075,
-          "length": 3510
-        },
-        {
-          "columnId": 3,
-          "section": "LENGTH",
-          "startOffset": 64585,
-          "length": 25
-        },
-        {
-          "columnId": 3,
-          "section": "DICTIONARY_DATA",
-          "startOffset": 64610,
-          "length": 133
-        }
-      ],
-      "encodings": [
-        {
-          "columnId": 0,
-          "kind": "DIRECT"
-        },
-        {
-          "columnId": 1,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 2,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 3,
-          "kind": "DICTIONARY_V2",
-          "dictionarySize": 35
-        }
-      ],
-      "indexes": [{
-        "columnId": 3,
-        "rowGroupIndexes": [
-          {
-            "entryId": 0,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3873,
-            "type": "STRING",
-            "positions": [
-              0,
-              0,
-              0,
-              0,
-              0,
-              0,
-              0
-            ]
-          },
-          {
-            "entryId": 1,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3861,
-            "type": "STRING",
-            "positions": [
-              0,
-              38,
-              12,
-              0,
-              0,
-              736,
-              23
-            ]
-          },
-          {
-            "entryId": 2,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3946,
-            "type": "STRING",
-            "positions": [
-              0,
-              78,
-              12,
-              0,
-              0,
-              1473,
-              43
-            ]
-          },
-          {
-            "entryId": 3,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3774,
-            "type": "STRING",
-            "positions": [
-              0,
-              118,
-              12,
-              0,
-              0,
-              2067,
-              261
-            ]
-          },
-          {
-            "entryId": 4,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3829,
-            "type": "STRING",
-            "positions": [
-              0,
-              158,
-              12,
-              0,
-              0,
-              2992,
-              35
-            ]
-          }
-        ],
-        "bloomFilterIndexes": [
-          {
-            "entryId": 0,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 1,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 2,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 3,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 4,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          }
-        ],
-        "stripeLevelBloomFilter": {
-          "numHashFunctions": 4,
-          "bitCount": 6272,
-          "popCount": 138,
-          "loadFactor": 0.022002551704645157,
-          "expectedFpp": 2.3436470542037569E-7
-        }
-      }]
-    },
-    {
-      "stripeNumber": 2,
-      "stripeInformation": {
-        "offset": 64833,
-        "indexLength": 961,
-        "dataLength": 63763,
-        "footerLength": 88,
-        "rowCount": 5000
-      },
-      "streams": [
-        {
-          "columnId": 0,
-          "section": "ROW_INDEX",
-          "startOffset": 64833,
-          "length": 17
-        },
-        {
-          "columnId": 1,
-          "section": "ROW_INDEX",
-          "startOffset": 64850,
-          "length": 166
-        },
-        {
-          "columnId": 2,
-          "section": "ROW_INDEX",
-          "startOffset": 65016,
-          "length": 166
-        },
-        {
-          "columnId": 3,
-          "section": "ROW_INDEX",
-          "startOffset": 65182,
-          "length": 100
-        },
-        {
-          "columnId": 3,
-          "section": "BLOOM_FILTER",
-          "startOffset": 65282,
-          "length": 512
-        },
-        {
-          "columnId": 1,
-          "section": "DATA",
-          "startOffset": 65794,
-          "length": 20035
-        },
-        {
-          "columnId": 2,
-          "section": "DATA",
-          "startOffset": 85829,
-          "length": 40050
-        },
-        {
-          "columnId": 3,
-          "section": "PRESENT",
-          "startOffset": 125879,
-          "length": 17
-        },
-        {
-          "columnId": 3,
-          "section": "DATA",
-          "startOffset": 125896,
-          "length": 3503
-        },
-        {
-          "columnId": 3,
-          "section": "LENGTH",
-          "startOffset": 129399,
-          "length": 25
-        },
-        {
-          "columnId": 3,
-          "section": "DICTIONARY_DATA",
-          "startOffset": 129424,
-          "length": 133
-        }
-      ],
-      "encodings": [
-        {
-          "columnId": 0,
-          "kind": "DIRECT"
-        },
-        {
-          "columnId": 1,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 2,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 3,
-          "kind": "DICTIONARY_V2",
-          "dictionarySize": 35
-        }
-      ],
-      "indexes": [{
-        "columnId": 3,
-        "rowGroupIndexes": [
-          {
-            "entryId": 0,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3946,
-            "type": "STRING",
-            "positions": [
-              0,
-              0,
-              0,
-              0,
-              0,
-              0,
-              0
-            ]
-          },
-          {
-            "entryId": 1,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3836,
-            "type": "STRING",
-            "positions": [
-              0,
-              38,
-              12,
-              0,
-              0,
-              746,
-              11
-            ]
-          },
-          {
-            "entryId": 2,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3791,
-            "type": "STRING",
-            "positions": [
-              0,
-              78,
-              12,
-              0,
-              0,
-              1430,
-              95
-            ]
-          },
-          {
-            "entryId": 3,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3904,
-            "type": "STRING",
-            "positions": [
-              0,
-              118,
-              12,
-              0,
-              0,
-              2239,
-              23
-            ]
-          },
-          {
-            "entryId": 4,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3920,
-            "type": "STRING",
-            "positions": [
-              0,
-              158,
-              12,
-              0,
-              0,
-              2994,
-              17
-            ]
-          }
-        ],
-        "bloomFilterIndexes": [
-          {
-            "entryId": 0,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 1,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 2,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 3,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 4,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          }
-        ],
-        "stripeLevelBloomFilter": {
-          "numHashFunctions": 4,
-          "bitCount": 6272,
-          "popCount": 138,
-          "loadFactor": 0.022002551704645157,
-          "expectedFpp": 2.3436470542037569E-7
-        }
-      }]
-    },
-    {
-      "stripeNumber": 3,
-      "stripeInformation": {
-        "offset": 129645,
-        "indexLength": 962,
-        "dataLength": 63770,
-        "footerLength": 91,
-        "rowCount": 5000
-      },
-      "streams": [
-        {
-          "columnId": 0,
-          "section": "ROW_INDEX",
-          "startOffset": 129645,
-          "length": 17
-        },
-        {
-          "columnId": 1,
-          "section": "ROW_INDEX",
-          "startOffset": 129662,
-          "length": 164
-        },
-        {
-          "columnId": 2,
-          "section": "ROW_INDEX",
-          "startOffset": 129826,
-          "length": 167
-        },
-        {
-          "columnId": 3,
-          "section": "ROW_INDEX",
-          "startOffset": 129993,
-          "length": 102
-        },
-        {
-          "columnId": 3,
-          "section": "BLOOM_FILTER",
-          "startOffset": 130095,
-          "length": 512
-        },
-        {
-          "columnId": 1,
-          "section": "DATA",
-          "startOffset": 130607,
-          "length": 20035
-        },
-        {
-          "columnId": 2,
-          "section": "DATA",
-          "startOffset": 150642,
-          "length": 40050
-        },
-        {
-          "columnId": 3,
-          "section": "PRESENT",
-          "startOffset": 190692,
-          "length": 17
-        },
-        {
-          "columnId": 3,
-          "section": "DATA",
-          "startOffset": 190709,
-          "length": 3510
-        },
-        {
-          "columnId": 3,
-          "section": "LENGTH",
-          "startOffset": 194219,
-          "length": 25
-        },
-        {
-          "columnId": 3,
-          "section": "DICTIONARY_DATA",
-          "startOffset": 194244,
-          "length": 133
-        }
-      ],
-      "encodings": [
-        {
-          "columnId": 0,
-          "kind": "DIRECT"
-        },
-        {
-          "columnId": 1,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 2,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 3,
-          "kind": "DICTIONARY_V2",
-          "dictionarySize": 35
-        }
-      ],
-      "indexes": [{
-        "columnId": 3,
-        "rowGroupIndexes": [
-          {
-            "entryId": 0,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3829,
-            "type": "STRING",
-            "positions": [
-              0,
-              0,
-              0,
-              0,
-              0,
-              0,
-              0
-            ]
-          },
-          {
-            "entryId": 1,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3853,
-            "type": "STRING",
-            "positions": [
-              0,
-              38,
-              12,
-              0,
-              0,
-              698,
-              74
-            ]
-          },
-          {
-            "entryId": 2,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3796,
-            "type": "STRING",
-            "positions": [
-              0,
-              78,
-              12,
-              0,
-              0,
-              1483,
-              39
-            ]
-          },
-          {
-            "entryId": 3,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3736,
-            "type": "STRING",
-            "positions": [
-              0,
-              118,
-              12,
-              0,
-              0,
-              2148,
-              155
-            ]
-          },
-          {
-            "entryId": 4,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3817,
-            "type": "STRING",
-            "positions": [
-              0,
-              158,
-              12,
-              0,
-              0,
-              3018,
-              8
-            ]
-          }
-        ],
-        "bloomFilterIndexes": [
-          {
-            "entryId": 0,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 1,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 2,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 3,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 4,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          }
-        ],
-        "stripeLevelBloomFilter": {
-          "numHashFunctions": 4,
-          "bitCount": 6272,
-          "popCount": 138,
-          "loadFactor": 0.022002551704645157,
-          "expectedFpp": 2.3436470542037569E-7
-        }
-      }]
-    },
-    {
-      "stripeNumber": 4,
-      "stripeInformation": {
-        "offset": 194468,
-        "indexLength": 973,
-        "dataLength": 63756,
-        "footerLength": 91,
-        "rowCount": 5000
-      },
-      "streams": [
-        {
-          "columnId": 0,
-          "section": "ROW_INDEX",
-          "startOffset": 194468,
-          "length": 17
-        },
-        {
-          "columnId": 1,
-          "section": "ROW_INDEX",
-          "startOffset": 194485,
-          "length": 166
-        },
-        {
-          "columnId": 2,
-          "section": "ROW_INDEX",
-          "startOffset": 194651,
-          "length": 171
-        },
-        {
-          "columnId": 3,
-          "section": "ROW_INDEX",
-          "startOffset": 194822,
-          "length": 107
-        },
-        {
-          "columnId": 3,
-          "section": "BLOOM_FILTER",
-          "startOffset": 194929,
-          "length": 512
-        },
-        {
-          "columnId": 1,
-          "section": "DATA",
-          "startOffset": 195441,
-          "length": 20035
-        },
-        {
-          "columnId": 2,
-          "section": "DATA",
-          "startOffset": 215476,
-          "length": 40050
-        },
-        {
-          "columnId": 3,
-          "section": "PRESENT",
-          "startOffset": 255526,
-          "length": 17
-        },
-        {
-          "columnId": 3,
-          "section": "DATA",
-          "startOffset": 255543,
-          "length": 3496
-        },
-        {
-          "columnId": 3,
-          "section": "LENGTH",
-          "startOffset": 259039,
-          "length": 25
-        },
-        {
-          "columnId": 3,
-          "section": "DICTIONARY_DATA",
-          "startOffset": 259064,
-          "length": 133
-        }
-      ],
-      "encodings": [
-        {
-          "columnId": 0,
-          "kind": "DIRECT"
-        },
-        {
-          "columnId": 1,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 2,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 3,
-          "kind": "DICTIONARY_V2",
-          "dictionarySize": 35
-        }
-      ],
-      "indexes": [{
-        "columnId": 3,
-        "rowGroupIndexes": [
-          {
-            "entryId": 0,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3959,
-            "type": "STRING",
-            "positions": [
-              0,
-              0,
-              0,
-              0,
-              0,
-              0,
-              0
-            ]
-          },
-          {
-            "entryId": 1,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3816,
-            "type": "STRING",
-            "positions": [
-              0,
-              38,
-              12,
-              0,
-              0,
-              495,
-              338
-            ]
-          },
-          {
-            "entryId": 2,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3883,
-            "type": "STRING",
-            "positions": [
-              0,
-              78,
-              12,
-              0,
-              0,
-              1449,
-              71
-            ]
-          },
-          {
-            "entryId": 3,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3938,
-            "type": "STRING",
-            "positions": [
-              0,
-              118,
-              12,
-              0,
-              0,
-              2207,
-              59
-            ]
-          },
-          {
-            "entryId": 4,
-            "count": 990,
-            "hasNull": true,
-            "min": "Darkness,",
-            "max": "worst",
-            "totalLength": 3863,
-            "type": "STRING",
-            "positions": [
-              0,
-              158,
-              12,
-              0,
-              0,
-              2838,
-              223
-            ]
-          }
-        ],
-        "bloomFilterIndexes": [
-          {
-            "entryId": 0,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 1,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 2,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 3,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          },
-          {
-            "entryId": 4,
-            "numHashFunctions": 4,
-            "bitCount": 6272,
-            "popCount": 138,
-            "loadFactor": 0.022002551704645157,
-            "expectedFpp": 2.3436470542037569E-7
-          }
-        ],
-        "stripeLevelBloomFilter": {
-          "numHashFunctions": 4,
-          "bitCount": 6272,
-          "popCount": 138,
-          "loadFactor": 0.022002551704645157,
-          "expectedFpp": 2.3436470542037569E-7
-        }
-      }]
-    },
-    {
-      "stripeNumber": 5,
-      "stripeInformation": {
-        "offset": 259288,
-        "indexLength": 433,
-        "dataLength": 12943,
-        "footerLength": 83,
-        "rowCount": 1000
-      },
-      "streams": [
-        {
-          "columnId": 0,
-          "section": "ROW_INDEX",
-          "startOffset": 259288,
-          "length": 12
-        },
-        {
-          "columnId": 1,
-          "section": "ROW_INDEX",
-          "startOffset": 259300,
-          "length": 38
-        },
-        {
-          "columnId": 2,
-          "section": "ROW_INDEX",
-          "startOffset": 259338,
-          "length": 41
-        },
-        {
-          "columnId": 3,
-          "section": "ROW_INDEX",
-          "startOffset": 259379,
-          "length": 41
-        },
-        {
-          "columnId": 3,
-          "section": "BLOOM_FILTER",
-          "startOffset": 259420,
-          "length": 301
-        },
-        {
-          "columnId": 1,
-          "section": "DATA",
-          "startOffset": 259721,
-          "length": 4007
-        },
-        {
-          "columnId": 2,
-          "section": "DATA",
-          "startOffset": 263728,
-          "length": 8010
-        },
-        {
-          "columnId": 3,
-          "section": "PRESENT",
-          "startOffset": 271738,
-          "length": 16
-        },
-        {
-          "columnId": 3,
-          "section": "DATA",
-          "startOffset": 271754,
-          "length": 752
-        },
-        {
-          "columnId": 3,
-          "section": "LENGTH",
-          "startOffset": 272506,
-          "length": 25
-        },
-        {
-          "columnId": 3,
-          "section": "DICTIONARY_DATA",
-          "startOffset": 272531,
-          "length": 133
-        }
-      ],
-      "encodings": [
-        {
-          "columnId": 0,
-          "kind": "DIRECT"
-        },
-        {
-          "columnId": 1,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 2,
-          "kind": "DIRECT_V2"
-        },
-        {
-          "columnId": 3,
-          "kind": "DICTIONARY_V2",
-          "dictionarySize": 35
-        }
-      ],
-      "indexes": [{
-        "columnId": 3,
-        "rowGroupIndexes": [{
-          "entryId": 0,
-          "count": 990,
-          "hasNull": true,
-          "min": "Darkness,",
-          "max": "worst",
-          "totalLength": 3963,
-          "type": "STRING",
-          "positions": [
-            0,
-            0,
-            0,
-            0,
-            0,
-            0,
-            0
-          ]
-        }],
-        "bloomFilterIndexes": [{
-          "entryId": 0,
-          "numHashFunctions": 4,
-          "bitCount": 6272,
-          "popCount": 138,
-          "loadFactor": 0.022002551704645157,
-          "expectedFpp": 2.3436470542037569E-7
-        }],
-        "stripeLevelBloomFilter": {
-          "numHashFunctions": 4,
-          "bitCount": 6272,
-          "popCount": 138,
-          "loadFactor": 0.022002551704645157,
-          "expectedFpp": 2.3436470542037569E-7
-        }
-      }]
-    }
-  ],
-  "fileLength": 273300,
-  "paddingLength": 0,
-  "paddingRatio": 0,
-  "status": "OK"
-}
diff --git a/orc/src/test/resources/orc-file-dump.out b/orc/src/test/resources/orc-file-dump.out
deleted file mode 100644
index 70f7fbd058..0000000000
--- a/orc/src/test/resources/orc-file-dump.out
+++ /dev/null
@@ -1,195 +0,0 @@
-Structure for TestFileDump.testDump.orc
-File Version: 0.12 with HIVE_13083
-Rows: 21000
-Compression: ZLIB
-Compression size: 4096
-Type: struct<i:int,l:bigint,s:string>
-
-Stripe Statistics:
-  Stripe 1:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146021688 max: 2147223299 sum: 515792826
-    Column 2: count: 5000 hasNull: false min: -9218592812243954469 max: 9221614132680747961
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19280
-  Stripe 2:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146733128 max: 2147001622 sum: 7673427
-    Column 2: count: 5000 hasNull: false min: -9220818777591257749 max: 9222259462014003839
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19504
-  Stripe 3:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146993718 max: 2147378179 sum: 132660742551
-    Column 2: count: 5000 hasNull: false min: -9218342074710552826 max: 9222303228623055266
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19641
-  Stripe 4:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false min: -2146658006 max: 2145520931 sum: 8533549236
-    Column 2: count: 5000 hasNull: false min: -9222758097219661129 max: 9221043130193737406
-    Column 3: count: 5000 hasNull: false min: Darkness, max: worst sum: 19470
-  Stripe 5:
-    Column 0: count: 1000 hasNull: false
-    Column 1: count: 1000 hasNull: false min: -2146245500 max: 2146378640 sum: 51299706363
-    Column 2: count: 1000 hasNull: false min: -9208193203370316142 max: 9218567213558056476
-    Column 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3866
-
-File Statistics:
-  Column 0: count: 21000 hasNull: false
-  Column 1: count: 21000 hasNull: false min: -2146993718 max: 2147378179 sum: 193017464403
-  Column 2: count: 21000 hasNull: false min: -9222758097219661129 max: 9222303228623055266
-  Column 3: count: 21000 hasNull: false min: Darkness, max: worst sum: 81761
-
-Stripes:
-  Stripe: offset: 3 data: 63786 rows: 5000 tail: 79 index: 439
-    Stream: column 0 section ROW_INDEX start: 3 length 17
-    Stream: column 1 section ROW_INDEX start: 20 length 166
-    Stream: column 2 section ROW_INDEX start: 186 length 169
-    Stream: column 3 section ROW_INDEX start: 355 length 87
-    Stream: column 1 section DATA start: 442 length 20035
-    Stream: column 2 section DATA start: 20477 length 40050
-    Stream: column 3 section DATA start: 60527 length 3543
-    Stream: column 3 section LENGTH start: 64070 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 64095 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2145365268 max: 2135491313 sum: 7521792925 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2139452528 max: 2147223299 sum: -12923774313 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2142420586 max: 2143898386 sum: -25521983511 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2137233441 max: 2144267163 sum: 40993386199 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2146021688 max: 2146838901 sum: -9553628474 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9200577545527640566 max: 9175500305011173751 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9203618157670445774 max: 9208123824411178101 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9218592812243954469 max: 9221351515892923972 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9206585617947511272 max: 9167703224425685487 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9206645795733282496 max: 9221614132680747961 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3862 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 3884 positions: 0,659,149
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3893 positions: 0,1531,3
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3798 positions: 0,2281,32
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 3843 positions: 0,3033,45
-  Stripe: offset: 64307 data: 63775 rows: 5000 tail: 79 index: 432
-    Stream: column 0 section ROW_INDEX start: 64307 length 17
-    Stream: column 1 section ROW_INDEX start: 64324 length 164
-    Stream: column 2 section ROW_INDEX start: 64488 length 168
-    Stream: column 3 section ROW_INDEX start: 64656 length 83
-    Stream: column 1 section DATA start: 64739 length 20035
-    Stream: column 2 section DATA start: 84774 length 40050
-    Stream: column 3 section DATA start: 124824 length 3532
-    Stream: column 3 section LENGTH start: 128356 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 128381 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2143799121 max: 2145249879 sum: -6966266181 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2146733128 max: 2147001622 sum: -35930106333 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2144302712 max: 2146299933 sum: 6944230435 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2145172948 max: 2144335014 sum: -29624404959 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2146428427 max: 2144067253 sum: 65584220465 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9218450653857701562 max: 9189819526332228512 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9220818777591257749 max: 9178821722829648113 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9220031433030423388 max: 9210838931786956852 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9208195729739635607 max: 9222259462014003839 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9174271499932339698 max: 9212277876771676916 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3923 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 3869 positions: 0,761,12
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3817 positions: 0,1472,70
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3931 positions: 0,2250,43
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 3964 positions: 0,2978,88
-  Stripe: offset: 128593 data: 63787 rows: 5000 tail: 79 index: 438
-    Stream: column 0 section ROW_INDEX start: 128593 length 17
-    Stream: column 1 section ROW_INDEX start: 128610 length 163
-    Stream: column 2 section ROW_INDEX start: 128773 length 168
-    Stream: column 3 section ROW_INDEX start: 128941 length 90
-    Stream: column 1 section DATA start: 129031 length 20035
-    Stream: column 2 section DATA start: 149066 length 40050
-    Stream: column 3 section DATA start: 189116 length 3544
-    Stream: column 3 section LENGTH start: 192660 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 192685 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2146993718 max: 2144179881 sum: -7829543271 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2144095505 max: 2144883384 sum: 51623839692 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2144113995 max: 2143773575 sum: 56574412741 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2146954065 max: 2146794873 sum: 4336083432 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2135511523 max: 2147378179 sum: 27955949957 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9211978436552246208 max: 9179058898902097152 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9195645160817780503 max: 9189147759444307708 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9202888157616520823 max: 9193561362676960747 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9216318198067839390 max: 9221286760675829363 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9218342074710552826 max: 9222303228623055266 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3817 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 4008 positions: 0,634,174
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3999 positions: 0,1469,69
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3817 positions: 0,2133,194
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 4000 positions: 0,3005,43
-  Stripe: offset: 192897 data: 63817 rows: 5000 tail: 79 index: 440
-    Stream: column 0 section ROW_INDEX start: 192897 length 17
-    Stream: column 1 section ROW_INDEX start: 192914 length 165
-    Stream: column 2 section ROW_INDEX start: 193079 length 167
-    Stream: column 3 section ROW_INDEX start: 193246 length 91
-    Stream: column 1 section DATA start: 193337 length 20035
-    Stream: column 2 section DATA start: 213372 length 40050
-    Stream: column 3 section DATA start: 253422 length 3574
-    Stream: column 3 section LENGTH start: 256996 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 257021 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2141355639 max: 2145520931 sum: 2726719912 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -2138324170 max: 2140167376 sum: -23606674002 positions: 0,2050,488
-      Entry 2: count: 1000 hasNull: false min: -2146658006 max: 2144329742 sum: -41530109703 positions: 4099,2054,464
-      Entry 3: count: 1000 hasNull: false min: -2144207593 max: 2139456355 sum: 13559842458 positions: 8198,2058,440
-      Entry 4: count: 1000 hasNull: false min: -2145744719 max: 2145417153 sum: 57383770571 positions: 12297,2062,416
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9222731174895935707 max: 9214167447015056056 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: -9222758097219661129 max: 9221043130193737406 positions: 4099,2,488
-      Entry 2: count: 1000 hasNull: false min: -9174483776261243438 max: 9208134757538374043 positions: 12297,6,464
-      Entry 3: count: 1000 hasNull: false min: -9174329712613510612 max: 9197412874152820822 positions: 20495,10,440
-      Entry 4: count: 1000 hasNull: false min: -9221162005892422758 max: 9220625004936875965 positions: 28693,14,416
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3901 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: Darkness, max: worst sum: 3900 positions: 0,431,431
-      Entry 2: count: 1000 hasNull: false min: Darkness, max: worst sum: 3909 positions: 0,1485,52
-      Entry 3: count: 1000 hasNull: false min: Darkness, max: worst sum: 3947 positions: 0,2196,104
-      Entry 4: count: 1000 hasNull: false min: Darkness, max: worst sum: 3813 positions: 0,2934,131
-  Stripe: offset: 257233 data: 12943 rows: 1000 tail: 71 index: 131
-    Stream: column 0 section ROW_INDEX start: 257233 length 12
-    Stream: column 1 section ROW_INDEX start: 257245 length 38
-    Stream: column 2 section ROW_INDEX start: 257283 length 41
-    Stream: column 3 section ROW_INDEX start: 257324 length 40
-    Stream: column 1 section DATA start: 257364 length 4007
-    Stream: column 2 section DATA start: 261371 length 8010
-    Stream: column 3 section DATA start: 269381 length 768
-    Stream: column 3 section LENGTH start: 270149 length 25
-    Stream: column 3 section DICTIONARY_DATA start: 270174 length 133
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DIRECT_V2
-    Encoding column 3: DICTIONARY_V2[35]
-    Row group indices for column 1:
-      Entry 0: count: 1000 hasNull: false min: -2146245500 max: 2146378640 sum: 51299706363 positions: 0,0,0
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: -9208193203370316142 max: 9218567213558056476 positions: 0,0,0
-    Row group indices for column 3:
-      Entry 0: count: 1000 hasNull: false min: Darkness, max: worst sum: 3866 positions: 0,0,0
-
-File length: 270923 bytes
-Padding length: 0 bytes
-Padding ratio: 0%
-________________________________________________________________________________________________________________________
-
diff --git a/orc/src/test/resources/orc-file-has-null.out b/orc/src/test/resources/orc-file-has-null.out
deleted file mode 100644
index e98a73fd9c..0000000000
--- a/orc/src/test/resources/orc-file-has-null.out
+++ /dev/null
@@ -1,112 +0,0 @@
-Structure for TestOrcFile.testHasNull.orc
-File Version: 0.12 with HIVE_13083
-Rows: 20000
-Compression: ZLIB
-Compression size: 4096
-Type: struct<bytes1:binary,string1:string>
-
-Stripe Statistics:
-  Stripe 1:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false sum: 15000
-    Column 2: count: 2000 hasNull: true min: RG1 max: RG3 sum: 6000
-  Stripe 2:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false sum: 15000
-    Column 2: count: 0 hasNull: true
-  Stripe 3:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false sum: 15000
-    Column 2: count: 5000 hasNull: false min: STRIPE-3 max: STRIPE-3 sum: 40000
-  Stripe 4:
-    Column 0: count: 5000 hasNull: false
-    Column 1: count: 5000 hasNull: false sum: 15000
-    Column 2: count: 0 hasNull: true
-
-File Statistics:
-  Column 0: count: 20000 hasNull: false
-  Column 1: count: 20000 hasNull: false sum: 60000
-  Column 2: count: 7000 hasNull: true min: RG1 max: STRIPE-3 sum: 46000
-
-Stripes:
-  Stripe: offset: 3 data: 220 rows: 5000 tail: 65 index: 154
-    Stream: column 0 section ROW_INDEX start: 3 length 17
-    Stream: column 1 section ROW_INDEX start: 20 length 60
-    Stream: column 2 section ROW_INDEX start: 80 length 77
-    Stream: column 1 section DATA start: 157 length 159
-    Stream: column 1 section LENGTH start: 316 length 15
-    Stream: column 2 section PRESENT start: 331 length 13
-    Stream: column 2 section DATA start: 344 length 18
-    Stream: column 2 section LENGTH start: 362 length 6
-    Stream: column 2 section DICTIONARY_DATA start: 368 length 9
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DICTIONARY_V2[2]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: RG1 max: RG1 sum: 3000 positions: 0,0,0,0,0,0,0
-      Entry 1: count: 0 hasNull: true positions: 0,0,125,0,0,4,488
-      Entry 2: count: 1000 hasNull: false min: RG3 max: RG3 sum: 3000 positions: 0,2,125,0,0,4,488
-      Entry 3: count: 0 hasNull: true positions: 0,4,125,0,0,12,488
-      Entry 4: count: 0 hasNull: true positions: 0,6,125,0,0,12,488
-  Stripe: offset: 442 data: 185 rows: 5000 tail: 64 index: 116
-    Stream: column 0 section ROW_INDEX start: 442 length 17
-    Stream: column 1 section ROW_INDEX start: 459 length 60
-    Stream: column 2 section ROW_INDEX start: 519 length 39
-    Stream: column 1 section DATA start: 558 length 159
-    Stream: column 1 section LENGTH start: 717 length 15
-    Stream: column 2 section PRESENT start: 732 length 11
-    Stream: column 2 section DATA start: 743 length 0
-    Stream: column 2 section LENGTH start: 743 length 0
-    Stream: column 2 section DICTIONARY_DATA start: 743 length 0
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DICTIONARY_V2[0]
-    Row group indices for column 2:
-      Entry 0: count: 0 hasNull: true positions: 0,0,0,0,0,0,0
-      Entry 1: count: 0 hasNull: true positions: 0,0,125,0,0,0,0
-      Entry 2: count: 0 hasNull: true positions: 0,2,120,0,0,0,0
-      Entry 3: count: 0 hasNull: true positions: 0,4,115,0,0,0,0
-      Entry 4: count: 0 hasNull: true positions: 0,6,110,0,0,0,0
-  Stripe: offset: 807 data: 206 rows: 5000 tail: 60 index: 137
-    Stream: column 0 section ROW_INDEX start: 807 length 17
-    Stream: column 1 section ROW_INDEX start: 824 length 60
-    Stream: column 2 section ROW_INDEX start: 884 length 60
-    Stream: column 1 section DATA start: 944 length 159
-    Stream: column 1 section LENGTH start: 1103 length 15
-    Stream: column 2 section DATA start: 1118 length 15
-    Stream: column 2 section LENGTH start: 1133 length 6
-    Stream: column 2 section DICTIONARY_DATA start: 1139 length 11
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DICTIONARY_V2[1]
-    Row group indices for column 2:
-      Entry 0: count: 1000 hasNull: false min: STRIPE-3 max: STRIPE-3 sum: 8000 positions: 0,0,0
-      Entry 1: count: 1000 hasNull: false min: STRIPE-3 max: STRIPE-3 sum: 8000 positions: 0,4,488
-      Entry 2: count: 1000 hasNull: false min: STRIPE-3 max: STRIPE-3 sum: 8000 positions: 0,12,464
-      Entry 3: count: 1000 hasNull: false min: STRIPE-3 max: STRIPE-3 sum: 8000 positions: 0,20,440
-      Entry 4: count: 1000 hasNull: false min: STRIPE-3 max: STRIPE-3 sum: 8000 positions: 0,28,416
-  Stripe: offset: 1210 data: 185 rows: 5000 tail: 64 index: 116
-    Stream: column 0 section ROW_INDEX start: 1210 length 17
-    Stream: column 1 section ROW_INDEX start: 1227 length 60
-    Stream: column 2 section ROW_INDEX start: 1287 length 39
-    Stream: column 1 section DATA start: 1326 length 159
-    Stream: column 1 section LENGTH start: 1485 length 15
-    Stream: column 2 section PRESENT start: 1500 length 11
-    Stream: column 2 section DATA start: 1511 length 0
-    Stream: column 2 section LENGTH start: 1511 length 0
-    Stream: column 2 section DICTIONARY_DATA start: 1511 length 0
-    Encoding column 0: DIRECT
-    Encoding column 1: DIRECT_V2
-    Encoding column 2: DICTIONARY_V2[0]
-    Row group indices for column 2:
-      Entry 0: count: 0 hasNull: true positions: 0,0,0,0,0,0,0
-      Entry 1: count: 0 hasNull: true positions: 0,0,125,0,0,0,0
-      Entry 2: count: 0 hasNull: true positions: 0,2,120,0,0,0,0
-      Entry 3: count: 0 hasNull: true positions: 0,4,115,0,0,0,0
-      Entry 4: count: 0 hasNull: true positions: 0,6,110,0,0,0,0
-
-File length: 1823 bytes
-Padding length: 0 bytes
-Padding ratio: 0%
-________________________________________________________________________________________________________________________
-
diff --git a/packaging/pom.xml b/packaging/pom.xml
index bfe36374de..2439e19b32 100644
--- a/packaging/pom.xml
+++ b/packaging/pom.xml
@@ -233,11 +233,6 @@
       <artifactId>hive-hplsql</artifactId>
       <version>${project.version}</version>
     </dependency>
-    <dependency>
-      <groupId>org.apache.hive</groupId>
-      <artifactId>hive-orc</artifactId>
-      <version>${project.version}</version>
-    </dependency>
     <dependency>
       <groupId>org.apache.hive</groupId>
       <artifactId>hive-storage-api</artifactId>
diff --git a/pom.xml b/pom.xml
index b4c0b819cf..5121770896 100644
--- a/pom.xml
+++ b/pom.xml
@@ -43,7 +43,6 @@
     <module>hplsql</module>
     <module>jdbc</module>
     <module>metastore</module>
-    <module>orc</module>
     <module>ql</module>
     <module>serde</module>
     <module>service-rpc</module>
@@ -173,6 +172,7 @@
     <libthrift.version>0.9.3</libthrift.version>
     <log4j2.version>2.6.2</log4j2.version>
     <opencsv.version>2.3</opencsv.version>
+    <orc.version>1.3.1</orc.version>
     <mockito-all.version>1.9.5</mockito-all.version>
     <mina.version>2.0.0-M5</mina.version>
     <netty.version>4.0.29.Final</netty.version>
@@ -506,6 +506,21 @@
         <groupId>stax</groupId>
         <artifactId>stax-api</artifactId>
         <version>${stax.version}</version>
+      </dependency>
+       <dependency>
+        <groupId>org.apache.orc</groupId>
+        <artifactId>orc-core</artifactId>
+        <version>${orc.version}</version>
+        <exclusions>
+          <exclusion>
+            <groupId>org.apache.hadoop</groupId>
+            <artifactId>hadoop-common</artifactId>
+          </exclusion>
+          <exclusion>
+            <groupId>org.apache.hive</groupId>
+            <artifactId>hive-storage-api</artifactId>
+          </exclusion>
+        </exclusions>
       </dependency>
       <dependency>
         <groupId>org.apache.hive</groupId>
@@ -619,11 +634,6 @@
         <artifactId>javax.jdo</artifactId>
         <version>${datanucleus-jdo.version}</version>
       </dependency>
-      <dependency>
-        <groupId>org.iq80.snappy</groupId>
-        <artifactId>snappy</artifactId>
-        <version>${snappy.version}</version>
-      </dependency>
       <dependency>
         <groupId>org.json</groupId>
         <artifactId>json</artifactId>
diff --git a/ql/pom.xml b/ql/pom.xml
index b17288cb30..84e83ee3aa 100644
--- a/ql/pom.xml
+++ b/ql/pom.xml
@@ -139,11 +139,6 @@
        <artifactId>commons-lang3</artifactId>
        <version>${commons-lang3.version}</version>
     </dependency>
-    <dependency>
-      <groupId>org.iq80.snappy</groupId>
-      <artifactId>snappy</artifactId>
-      <version>${snappy.version}</version>
-    </dependency>
     <dependency>
       <groupId>commons-lang</groupId>
       <artifactId>commons-lang</artifactId>
@@ -291,6 +286,11 @@
       <version>${hadoop.version}</version>
       <optional>true</optional>
     </dependency>
+    <dependency>
+      <groupId>org.apache.orc</groupId>
+      <artifactId>orc-tools</artifactId>
+      <version>${orc.version}</version>
+    </dependency>
 
     <dependency>
       <groupId>org.apache.ivy</groupId>
@@ -861,14 +861,15 @@
                   <include>com.googlecode.javaewah:JavaEWAH</include>
                   <include>javolution:javolution</include>
                   <include>com.google.protobuf:protobuf-java</include>
-                  <include>org.iq80.snappy:snappy</include>
+                  <include>io.airlift:aircompressor</include>
                   <include>org.codehaus.jackson:jackson-core-asl</include>
                   <include>org.codehaus.jackson:jackson-mapper-asl</include>
                   <include>com.google.guava:guava</include>
                   <include>net.sf.opencsv:opencsv</include>
                   <include>org.apache.hive:spark-client</include>
                   <include>org.apache.hive:hive-storage-api</include>
-                  <include>org.apache.hive:hive-orc</include>
+                  <include>org.apache.orc:orc-core</include>
+                  <include>org.apache.orc:orc-tools</include>
                   <include>joda-time:joda-time</include>
                 </includes>
               </artifactSet>
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
index 53660206e3..96ca73626f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFile.java
@@ -28,6 +28,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.orc.FileMetadata;
+import org.apache.orc.PhysicalWriter;
 import org.apache.orc.impl.MemoryManager;
 import org.apache.orc.TypeDescription;
 import org.apache.orc.impl.OrcTail;
@@ -267,6 +268,11 @@ protected WriterOptions batchSize(int maxSize) {
       return this;
     }
 
+    public WriterOptions physicalWriter(PhysicalWriter writer) {
+      super.physicalWriter(writer);
+      return this;
+    }
+
     ObjectInspector getInspector() {
       return inspector;
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
index 6281eddab3..99cc506c75 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java
@@ -1490,14 +1490,15 @@ private void populateAndCacheStripeDetails() throws IOException {
       stripeStats = orcTail.getStripeStatistics();
       fileTypes = orcTail.getTypes();
       TypeDescription fileSchema = OrcUtils.convertTypeFromProtobuf(fileTypes, 0);
+      Reader.Options readerOptions = new Reader.Options(context.conf);
       if (readerTypes == null) {
         readerIncluded = genIncludedColumns(fileSchema, context.conf);
-        evolution = new SchemaEvolution(fileSchema, readerIncluded);
+        evolution = new SchemaEvolution(fileSchema, readerOptions.include(readerIncluded));
       } else {
         // The reader schema always comes in without ACID columns.
         TypeDescription readerSchema = OrcUtils.convertTypeFromProtobuf(readerTypes, 0);
         readerIncluded = genIncludedColumns(readerSchema, context.conf);
-        evolution = new SchemaEvolution(fileSchema, readerSchema, readerIncluded);
+        evolution = new SchemaEvolution(fileSchema, readerSchema, readerOptions.include(readerIncluded));
         if (!isOriginal) {
           // The SchemaEvolution class has added the ACID metadata columns.  Let's update our
           // readerTypes so PPD code will work correctly.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java
index 492c64c29e..65f4a24750 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java
@@ -85,7 +85,8 @@ public class OrcRecordUpdater implements RecordUpdater {
   private Path deleteEventPath;
   private final FileSystem fs;
   private OrcFile.WriterOptions writerOptions;
-  private Writer writer;
+  private Writer writer = null;
+  private boolean writerClosed = false;
   private Writer deleteEventWriter = null;
   private final FSDataOutputStream flushLengths;
   private final OrcStruct item;
@@ -247,6 +248,14 @@ static StructObjectInspector createEventSchema(ObjectInspector rowInspector) {
         writerOptions = OrcFile.writerOptions(optionsCloneForDelta.getTableProperties(),
             optionsCloneForDelta.getConfiguration());
       }
+      if (this.acidOperationalProperties.isSplitUpdate()) {
+        // If this is a split-update, we initialize a delete delta file path in anticipation that
+        // they would write update/delete events to that separate file.
+        // This writes to a file in directory which starts with "delete_delta_..."
+        // The actual initialization of a writer only happens if any delete events are written.
+        this.deleteEventPath = AcidUtils.createFilename(path,
+            optionsCloneForDelta.writingDeleteDelta(true));
+      }
 
       // get buffer size and stripe size for base writer
       int baseBufferSizeValue = writerOptions.getBufferSize();
@@ -262,14 +271,6 @@ static StructObjectInspector createEventSchema(ObjectInspector rowInspector) {
     rowInspector = (StructObjectInspector)options.getInspector();
     writerOptions.inspector(createEventSchema(findRecId(options.getInspector(),
         options.getRecordIdColumn())));
-    this.writer = OrcFile.createWriter(this.path, writerOptions);
-    if (this.acidOperationalProperties.isSplitUpdate()) {
-      // If this is a split-update, we initialize a delete delta file path in anticipation that
-      // they would write update/delete events to that separate file.
-      // This writes to a file in directory which starts with "delete_delta_..."
-      // The actual initialization of a writer only happens if any delete events are written.
-      this.deleteEventPath = AcidUtils.createFilename(path, options.writingDeleteDelta(true));
-    }
     item = new OrcStruct(FIELDS);
     item.setFieldValue(OPERATION, operation);
     item.setFieldValue(CURRENT_TRANSACTION, currentTransaction);
@@ -367,6 +368,9 @@ else if(operation == INSERT_OPERATION) {
     item.setFieldValue(OrcRecordUpdater.OPERATION, new IntWritable(operation));
     item.setFieldValue(OrcRecordUpdater.ROW, (operation == DELETE_OPERATION ? null : row));
     indexBuilder.addKey(operation, originalTransaction, bucket.get(), rowId);
+    if (writer == null) {
+      writer = OrcFile.createWriter(path, writerOptions);
+    }
     writer.addRow(item);
   }
 
@@ -469,6 +473,9 @@ public void flush() throws IOException {
       throw new IllegalStateException("Attempting to flush a RecordUpdater on "
          + path + " with a single transaction.");
     }
+    if (writer == null) {
+      writer = OrcFile.createWriter(path, writerOptions);
+    }
     long len = writer.writeIntermediateFooter();
     flushLengths.writeLong(len);
     OrcInputFormat.SHIMS.hflush(flushLengths);
@@ -480,21 +487,19 @@ public void close(boolean abort) throws IOException {
       if (flushLengths == null) {
         fs.delete(path, false);
       }
-    } else {
-      if (writer != null) {
-        if (acidOperationalProperties.isSplitUpdate()) {
-          // When split-update is enabled, we can choose not to write
-          // any delta files when there are no inserts. In such cases only the delete_deltas
-          // would be written & they are closed separately below.
-          if (indexBuilder.acidStats.inserts > 0) {
-            writer.close(); // normal close, when there are inserts.
-          } else {
-            // Just remove insert delta paths, when there are no insert events.
-            fs.delete(path, false);
-          }
-        } else {
-          writer.close(); // normal close.
+    } else if (!writerClosed) {
+      if (acidOperationalProperties.isSplitUpdate()) {
+        // When split-update is enabled, we can choose not to write
+        // any delta files when there are no inserts. In such cases only the delete_deltas
+        // would be written & they are closed separately below.
+        if (writer != null && indexBuilder.acidStats.inserts > 0) {
+          writer.close(); // normal close, when there are inserts.
+        }
+      } else {
+        if (writer == null) {
+          writer = OrcFile.createWriter(path, writerOptions);
         }
+        writer.close(); // normal close.
       }
       if (deleteEventWriter != null) {
         if (deleteEventIndexBuilder.acidStats.deletes > 0) {
@@ -505,7 +510,6 @@ public void close(boolean abort) throws IOException {
           fs.delete(deleteEventPath, false);
         }
       }
-
     }
     if (flushLengths != null) {
       flushLengths.close();
@@ -513,6 +517,7 @@ public void close(boolean abort) throws IOException {
     }
     writer = null;
     deleteEventWriter = null;
+    writerClosed = true;
   }
 
   @Override
@@ -524,11 +529,6 @@ public SerDeStats getStats() {
     return stats;
   }
 
-  @VisibleForTesting
-  Writer getWriter() {
-    return writer;
-  }
-
   private static final Charset utf8 = Charset.forName("UTF-8");
   private static final CharsetDecoder utf8Decoder = utf8.newDecoder();
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
index 3e4ec2e657..dcefada2a9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java
@@ -60,9 +60,9 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
-import org.apache.orc.impl.PhysicalWriter;
 
 import com.google.common.annotations.VisibleForTesting;
+import org.apache.orc.PhysicalWriter;
 
 /**
  * An ORC file writer. The file is divided into stripes, which is the natural
@@ -97,15 +97,6 @@ public class WriterImpl extends org.apache.orc.impl.WriterImpl implements Writer
     this.fields = initializeFieldsFromOi(inspector);
   }
 
-  public WriterImpl(PhysicalWriter writer,
-                    Path pathForMem,
-                    OrcFile.WriterOptions opts) throws IOException {
-    super(writer, pathForMem, opts);
-    this.inspector = opts.getInspector();
-    this.internalBatch = opts.getSchema().createRowBatch(opts.getBatchSize());
-    this.fields = initializeFieldsFromOi(inspector);
-  }
-
   private static StructField[] initializeFieldsFromOi(ObjectInspector inspector) {
     if (inspector instanceof StructObjectInspector) {
       List<? extends StructField> fieldList =
@@ -329,9 +320,4 @@ public void close() throws IOException {
     flushInternalBatch();
     super.close();
   }
-
-  @VisibleForTesting
-  PhysicalWriter getPhysicalWriter() {
-    return physWriter;
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
index 0dba1a01c5..a434763aa0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.io.orc.encoded;
 
 import org.apache.orc.impl.RunLengthByteReader;
+import org.apache.orc.impl.SchemaEvolution;
 import org.apache.orc.impl.StreamName;
 
 import java.io.IOException;
@@ -57,8 +58,8 @@ public static class TimestampStreamReader extends TimestampTreeReader
     private TimestampStreamReader(int columnId, SettableUncompressedStream present,
                                   SettableUncompressedStream data, SettableUncompressedStream nanos,
                                   boolean isFileCompressed, OrcProto.ColumnEncoding encoding,
-                                  boolean skipCorrupt, String writerTimezoneId) throws IOException {
-      super(columnId, present, data, nanos, encoding, skipCorrupt, writerTimezoneId);
+                                  TreeReaderFactory.Context context) throws IOException {
+      super(columnId, present, data, nanos, encoding, context);
       this.isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._secondsStream = data;
@@ -117,8 +118,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData nanosStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
-      private boolean skipCorrupt;
-      private String writerTimezone;
+      private TreeReaderFactory.Context context;
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -150,13 +150,8 @@ public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         return this;
       }
 
-      public StreamReaderBuilder setWriterTimezone(String writerTimezoneId) {
-        this.writerTimezone = writerTimezoneId;
-        return this;
-      }
-
-      public StreamReaderBuilder skipCorrupt(boolean skipCorrupt) {
-        this.skipCorrupt = skipCorrupt;
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
         return this;
       }
 
@@ -175,7 +170,7 @@ public TimestampStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new TimestampStreamReader(columnIndex, present, data, nanos,
-            isFileCompressed, columnEncoding, skipCorrupt, writerTimezone);
+            isFileCompressed, columnEncoding, context);
       }
     }
 
@@ -196,8 +191,9 @@ protected static class StringStreamReader extends StringTreeReader
     private StringStreamReader(int columnId, SettableUncompressedStream present,
         SettableUncompressedStream data, SettableUncompressedStream length,
         SettableUncompressedStream dictionary,
-        boolean isFileCompressed, OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, length, dictionary, encoding);
+        boolean isFileCompressed, OrcProto.ColumnEncoding encoding,
+        TreeReaderFactory.Context context) throws IOException {
+      super(columnId, present, data, length, dictionary, encoding, context);
       this._isDictionaryEncoding = dictionary != null;
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
@@ -288,6 +284,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData lengthStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReaderFactory.Context context;
 
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
@@ -325,6 +322,11 @@ public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public StringStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -343,7 +345,7 @@ public StringStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new StringStreamReader(columnIndex, present, data, length, dictionary,
-            isFileCompressed, columnEncoding);
+            isFileCompressed, columnEncoding, context);
       }
     }
 
@@ -360,8 +362,9 @@ protected static class ShortStreamReader extends ShortTreeReader implements Sett
 
     private ShortStreamReader(int columnId, SettableUncompressedStream present,
         SettableUncompressedStream data, boolean isFileCompressed,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, encoding);
+        OrcProto.ColumnEncoding encoding,
+        TreeReaderFactory.Context context) throws IOException {
+      super(columnId, present, data, encoding, context);
       this.isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._dataStream = data;
@@ -404,7 +407,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData dataStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
-
+      private TreeReaderFactory.Context context;
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -431,6 +434,11 @@ public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public ShortStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -442,7 +450,7 @@ public ShortStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new ShortStreamReader(columnIndex, present, data, isFileCompressed,
-            columnEncoding);
+            columnEncoding, context);
       }
     }
 
@@ -458,8 +466,9 @@ protected static class LongStreamReader extends LongTreeReader implements Settab
 
     private LongStreamReader(int columnId, SettableUncompressedStream present,
         SettableUncompressedStream data, boolean isFileCompressed,
-        OrcProto.ColumnEncoding encoding, boolean skipCorrupt) throws IOException {
-      super(columnId, present, data, encoding, skipCorrupt);
+        OrcProto.ColumnEncoding encoding, TreeReaderFactory.Context context
+        ) throws IOException {
+      super(columnId, present, data, encoding, context);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._dataStream = data;
@@ -502,7 +511,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData dataStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
-      private boolean skipCorrupt;
+      private TreeReaderFactory.Context context;
 
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
@@ -530,8 +539,8 @@ public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         return this;
       }
 
-      public StreamReaderBuilder skipCorrupt(boolean skipCorrupt) {
-        this.skipCorrupt = skipCorrupt;
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
         return this;
       }
 
@@ -546,7 +555,7 @@ public LongStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new LongStreamReader(columnIndex, present, data, isFileCompressed,
-            columnEncoding, skipCorrupt);
+            columnEncoding, context);
       }
     }
 
@@ -562,8 +571,9 @@ protected static class IntStreamReader extends IntTreeReader implements Settable
 
     private IntStreamReader(int columnId, SettableUncompressedStream present,
         SettableUncompressedStream data, boolean isFileCompressed,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, encoding);
+        OrcProto.ColumnEncoding encoding, TreeReaderFactory.Context context
+        ) throws IOException {
+      super(columnId, present, data, encoding, context);
       this._isFileCompressed = isFileCompressed;
       this._dataStream = data;
       this._presentStream = present;
@@ -606,7 +616,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData dataStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
-
+      private TreeReaderFactory.Context context;
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -633,6 +643,11 @@ public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public IntStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -644,7 +659,7 @@ public IntStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new IntStreamReader(columnIndex, present, data, isFileCompressed,
-            columnEncoding);
+            columnEncoding, context);
       }
     }
 
@@ -794,7 +809,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData presentStream;
       private ColumnStreamData dataStream;
       private CompressionCodec compressionCodec;
-
+      private TreeReaderFactory.Context context;
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -816,6 +831,11 @@ public StreamReaderBuilder setCompressionCodec(CompressionCodec compressionCodec
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public DoubleStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -845,8 +865,9 @@ private DecimalStreamReader(int columnId, int precision, int scale,
         SettableUncompressedStream presentStream,
         SettableUncompressedStream valueStream, SettableUncompressedStream scaleStream,
         boolean isFileCompressed,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, precision, scale, presentStream, valueStream, scaleStream, encoding);
+        OrcProto.ColumnEncoding encoding, TreeReaderFactory.Context context
+        ) throws IOException {
+      super(columnId, presentStream, valueStream, scaleStream, encoding, context);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = presentStream;
       this._valueStream = valueStream;
@@ -903,6 +924,7 @@ public static class StreamReaderBuilder {
       private int precision;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReaderFactory.Context context;
 
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
@@ -920,6 +942,11 @@ public StreamReaderBuilder setScale(int scale) {
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public StreamReaderBuilder setPresentStream(ColumnStreamData presentStream) {
         this.presentStream = presentStream;
         return this;
@@ -958,7 +985,7 @@ public DecimalStreamReader build() throws IOException {
         boolean isFileCompressed = compressionCodec != null;
         return new DecimalStreamReader(columnIndex, precision, scale, presentInStream,
             valueInStream,
-            scaleInStream, isFileCompressed, columnEncoding);
+            scaleInStream, isFileCompressed, columnEncoding, context);
       }
     }
 
@@ -974,8 +1001,9 @@ protected static class DateStreamReader extends DateTreeReader implements Settab
 
     private DateStreamReader(int columnId, SettableUncompressedStream present,
         SettableUncompressedStream data, boolean isFileCompressed,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, encoding);
+        OrcProto.ColumnEncoding encoding, TreeReaderFactory.Context context
+        ) throws IOException {
+      super(columnId, present, data, encoding, context);
       this.isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._dataStream = data;
@@ -1018,6 +1046,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData dataStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
+      private TreeReaderFactory.Context context;
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -1039,6 +1068,11 @@ public StreamReaderBuilder setCompressionCodec(CompressionCodec compressionCodec
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         this.columnEncoding = encoding;
         return this;
@@ -1056,7 +1090,7 @@ public DateStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new DateStreamReader(columnIndex, present, data, isFileCompressed,
-            columnEncoding);
+            columnEncoding, context);
       }
     }
 
@@ -1514,8 +1548,8 @@ protected static class BinaryStreamReader extends BinaryTreeReader implements Se
     private BinaryStreamReader(int columnId, SettableUncompressedStream present,
         SettableUncompressedStream data, SettableUncompressedStream length,
         boolean isFileCompressed,
-        OrcProto.ColumnEncoding encoding) throws IOException {
-      super(columnId, present, data, length, encoding);
+        OrcProto.ColumnEncoding encoding, TreeReaderFactory.Context context) throws IOException {
+      super(columnId, present, data, length, encoding, context);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._dataStream = data;
@@ -1570,7 +1604,7 @@ public static class StreamReaderBuilder {
       private ColumnStreamData lengthStream;
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
-
+      private TreeReaderFactory.Context context;
 
       public StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -1602,6 +1636,11 @@ public StreamReaderBuilder setColumnEncoding(OrcProto.ColumnEncoding encoding) {
         return this;
       }
 
+      public StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public BinaryStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils.createSettableUncompressedStream(
             OrcProto.Stream.Kind.PRESENT.name(), presentStream);
@@ -1614,7 +1653,7 @@ public BinaryStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new BinaryStreamReader(columnIndex, present, data, length, isFileCompressed,
-            columnEncoding);
+            columnEncoding, context);
       }
     }
 
@@ -1715,7 +1754,7 @@ public static StreamReaderBuilder builder() {
 
   public static StructTreeReader createRootTreeReader(TypeDescription schema,
        List<OrcProto.ColumnEncoding> encodings, EncodedColumnBatch<OrcBatchKey> batch,
-       CompressionCodec codec, boolean skipCorrupt, String tz, int[] columnMapping)
+       CompressionCodec codec, TreeReaderFactory.Context context, int[] columnMapping)
            throws IOException {
     if (schema.getCategory() != Category.STRUCT) {
       throw new AssertionError("Schema is not a struct: " + schema);
@@ -1737,7 +1776,7 @@ public static StructTreeReader createRootTreeReader(TypeDescription schema,
     for (int schemaChildIx = 0, inclChildIx = -1; schemaChildIx < childCount; ++schemaChildIx) {
       if (!batch.hasData(children.get(schemaChildIx).getId())) continue;
       childReaders[++inclChildIx] = createEncodedTreeReader(
-          schema.getChildren().get(schemaChildIx), encodings, batch, codec, skipCorrupt, tz);
+          schema.getChildren().get(schemaChildIx), encodings, batch, codec, context);
       columnMapping[inclChildIx] = schemaChildIx;
     }
     return StructStreamReader.builder()
@@ -1745,13 +1784,14 @@ public static StructTreeReader createRootTreeReader(TypeDescription schema,
         .setCompressionCodec(codec)
         .setColumnEncoding(encodings.get(0))
         .setChildReaders(childReaders)
+        .setContext(context)
         .build();
   }
 
 
   private static TreeReader createEncodedTreeReader(TypeDescription schema,
       List<OrcProto.ColumnEncoding> encodings, EncodedColumnBatch<OrcBatchKey> batch,
-      CompressionCodec codec, boolean skipCorrupt, String tz) throws IOException {
+      CompressionCodec codec, TreeReaderFactory.Context context) throws IOException {
       int columnIndex = schema.getId();
     ColumnStreamData[] streamBuffers = batch.getColumnData(columnIndex);
 
@@ -1775,7 +1815,8 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
       LOG.debug("columnIndex: {} columnType: {} streamBuffers.length: {} columnEncoding: {}" +
           " present: {} data: {} dictionary: {} lengths: {} secondary: {} tz: {}",
           columnIndex, schema, streamBuffers.length, columnEncoding, present != null,
-          data, dictionary != null, lengths != null, secondary != null, tz);
+          data, dictionary != null, lengths != null, secondary != null,
+          context.getWriterTimezone());
     }
     switch (schema.getCategory()) {
       case BINARY:
@@ -1793,11 +1834,11 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
       case TIMESTAMP:
       case DATE:
         return getPrimitiveTreeReaders(columnIndex, schema, codec, columnEncoding,
-            present, data, dictionary, lengths, secondary, skipCorrupt, tz);
+            present, data, dictionary, lengths, secondary, context);
       case LIST:
         TypeDescription elementType = schema.getChildren().get(0);
         TreeReader elementReader = createEncodedTreeReader(
-            elementType, encodings, batch, codec, skipCorrupt, tz);
+            elementType, encodings, batch, codec, context);
         return ListStreamReader.builder()
             .setColumnIndex(columnIndex)
             .setColumnEncoding(columnEncoding)
@@ -1805,14 +1846,15 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
             .setPresentStream(present)
             .setLengthStream(lengths)
             .setElementReader(elementReader)
+            .setContext(context)
             .build();
       case MAP:
         TypeDescription keyType = schema.getChildren().get(0);
         TypeDescription valueType = schema.getChildren().get(1);
         TreeReader keyReader = createEncodedTreeReader(
-            keyType, encodings, batch, codec, skipCorrupt, tz);
+            keyType, encodings, batch, codec, context);
         TreeReader valueReader = createEncodedTreeReader(
-            valueType, encodings, batch, codec, skipCorrupt, tz);
+            valueType, encodings, batch, codec, context);
         return MapStreamReader.builder()
             .setColumnIndex(columnIndex)
             .setColumnEncoding(columnEncoding)
@@ -1821,6 +1863,7 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
             .setLengthStream(lengths)
             .setKeyReader(keyReader)
             .setValueReader(valueReader)
+            .setContext(context)
             .build();
       case STRUCT: {
         int childCount = schema.getChildren().size();
@@ -1828,7 +1871,7 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
         for (int i = 0; i < childCount; i++) {
           TypeDescription childType = schema.getChildren().get(i);
           childReaders[i] = createEncodedTreeReader(
-              childType, encodings, batch, codec, skipCorrupt, tz);
+              childType, encodings, batch, codec, context);
         }
         return StructStreamReader.builder()
             .setColumnIndex(columnIndex)
@@ -1836,6 +1879,7 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
             .setColumnEncoding(columnEncoding)
             .setPresentStream(present)
             .setChildReaders(childReaders)
+            .setContext(context)
             .build();
       }
       case UNION: {
@@ -1844,7 +1888,7 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
         for (int i = 0; i < childCount; i++) {
           TypeDescription childType = schema.getChildren().get(i);
           childReaders[i] = createEncodedTreeReader(
-              childType, encodings, batch, codec, skipCorrupt, tz);
+              childType, encodings, batch, codec, context);
         }
         return UnionStreamReader.builder()
               .setColumnIndex(columnIndex)
@@ -1853,6 +1897,7 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
               .setPresentStream(present)
               .setDataStream(data)
               .setChildReaders(childReaders)
+              .setContext(context)
               .build();
       }
       default:
@@ -1863,7 +1908,7 @@ private static TreeReader createEncodedTreeReader(TypeDescription schema,
   private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
       TypeDescription columnType, CompressionCodec codec, OrcProto.ColumnEncoding columnEncoding,
       ColumnStreamData present, ColumnStreamData data, ColumnStreamData dictionary,
-      ColumnStreamData lengths, ColumnStreamData secondary, boolean skipCorrupt, String tz)
+      ColumnStreamData lengths, ColumnStreamData secondary, TreeReaderFactory.Context context)
           throws IOException {
     switch (columnType.getCategory()) {
       case BINARY:
@@ -1874,6 +1919,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setLengthStream(lengths)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
+            .setContext(context)
             .build();
       case BOOLEAN:
         return BooleanStreamReader.builder()
@@ -1896,6 +1942,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setDataStream(data)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
+            .setContext(context)
             .build();
       case INT:
         return IntStreamReader.builder()
@@ -1904,6 +1951,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setDataStream(data)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
+            .setContext(context)
             .build();
       case LONG:
         return LongStreamReader.builder()
@@ -1912,7 +1960,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setDataStream(data)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
-            .skipCorrupt(skipCorrupt)
+            .setContext(context)
             .build();
       case FLOAT:
         return FloatStreamReader.builder()
@@ -1970,6 +2018,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setScaleStream(secondary)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
+            .setContext(context)
             .build();
       case TIMESTAMP:
         return TimestampStreamReader.builder()
@@ -1979,8 +2028,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setNanosStream(secondary)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
-            .setWriterTimezone(tz)
-            .skipCorrupt(skipCorrupt)
+            .setContext(context)
             .build();
       case DATE:
         return DateStreamReader.builder()
@@ -1989,6 +2037,7 @@ private static TreeReader getPrimitiveTreeReaders(final int columnIndex,
             .setDataStream(data)
             .setCompressionCodec(codec)
             .setColumnEncoding(columnEncoding)
+            .setContext(context)
             .build();
     default:
       throw new AssertionError("Not a primitive category: " + columnType.getCategory());
@@ -2003,8 +2052,9 @@ protected static class ListStreamReader extends ListTreeReader implements Settab
     public ListStreamReader(final int columnIndex,
         final SettableUncompressedStream present, final SettableUncompressedStream lengthStream,
         final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
-        final TreeReader elementReader) throws IOException {
-      super(columnIndex, present, lengthStream, columnEncoding, elementReader);
+        final TreeReader elementReader,
+                            TreeReaderFactory.Context context) throws IOException {
+      super(columnIndex, present, context, lengthStream, columnEncoding, elementReader);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._lengthStream = lengthStream;
@@ -2062,7 +2112,7 @@ public static class StreamReaderBuilder {
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
       private TreeReader elementReader;
-
+      private TreeReaderFactory.Context context;
 
       public ListStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -2094,6 +2144,11 @@ public ListStreamReader.StreamReaderBuilder setElementReader(TreeReader elementR
         return this;
       }
 
+      public ListStreamReader.StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public ListStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -2105,7 +2160,7 @@ public ListStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new ListStreamReader(columnIndex, present, length, columnEncoding, isFileCompressed,
-            elementReader);
+            elementReader, context);
       }
     }
 
@@ -2122,8 +2177,9 @@ protected static class MapStreamReader extends MapTreeReader implements Settable
     public MapStreamReader(final int columnIndex,
         final SettableUncompressedStream present, final SettableUncompressedStream lengthStream,
         final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
-        final TreeReader keyReader, final TreeReader valueReader) throws IOException {
-      super(columnIndex, present, lengthStream, columnEncoding, keyReader, valueReader);
+        final TreeReader keyReader, final TreeReader valueReader,
+                           TreeReaderFactory.Context context) throws IOException {
+      super(columnIndex, present, context, lengthStream, columnEncoding, keyReader, valueReader);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._lengthStream = lengthStream;
@@ -2188,7 +2244,7 @@ public static class StreamReaderBuilder {
       private OrcProto.ColumnEncoding columnEncoding;
       private TreeReader keyReader;
       private TreeReader valueReader;
-
+      private TreeReaderFactory.Context context;
 
       public MapStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -2225,6 +2281,11 @@ public MapStreamReader.StreamReaderBuilder setValueReader(TreeReader valueReader
         return this;
       }
 
+      public MapStreamReader.StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public MapStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -2236,7 +2297,7 @@ public MapStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new MapStreamReader(columnIndex, present, length, columnEncoding, isFileCompressed,
-            keyReader, valueReader);
+            keyReader, valueReader, context);
       }
     }
 
@@ -2253,8 +2314,8 @@ protected static class StructStreamReader extends StructTreeReader
     public StructStreamReader(final int columnIndex,
         final SettableUncompressedStream present,
         final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
-        final TreeReader[] childReaders) throws IOException {
-      super(columnIndex, present, columnEncoding, childReaders);
+        final TreeReader[] childReaders, TreeReaderFactory.Context context) throws IOException {
+      super(columnIndex, present, context, columnEncoding, childReaders);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
     }
@@ -2303,7 +2364,7 @@ public static class StreamReaderBuilder {
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
       private TreeReader[] childReaders;
-
+      private TreeReaderFactory.Context context;
 
       public StructStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -2330,6 +2391,11 @@ public StructStreamReader.StreamReaderBuilder setChildReaders(TreeReader[] child
         return this;
       }
 
+      public StructStreamReader.StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public StructStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils
             .createSettableUncompressedStream(OrcProto.Stream.Kind.PRESENT.name(),
@@ -2337,7 +2403,7 @@ public StructStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new StructStreamReader(columnIndex, present, columnEncoding, isFileCompressed,
-            childReaders);
+            childReaders, context);
       }
     }
 
@@ -2354,8 +2420,8 @@ protected static class UnionStreamReader extends UnionTreeReader implements Sett
     public UnionStreamReader(final int columnIndex,
         final SettableUncompressedStream present, final SettableUncompressedStream dataStream,
         final OrcProto.ColumnEncoding columnEncoding, final boolean isFileCompressed,
-        final TreeReader[] childReaders) throws IOException {
-      super(columnIndex, present, columnEncoding, childReaders);
+        final TreeReader[] childReaders, TreeReaderFactory.Context context) throws IOException {
+      super(columnIndex, present, context, columnEncoding, childReaders);
       this._isFileCompressed = isFileCompressed;
       this._presentStream = present;
       this._dataStream = dataStream;
@@ -2420,7 +2486,7 @@ public static class StreamReaderBuilder {
       private CompressionCodec compressionCodec;
       private OrcProto.ColumnEncoding columnEncoding;
       private TreeReader[] childReaders;
-
+      private TreeReaderFactory.Context context;
 
       public UnionStreamReader.StreamReaderBuilder setColumnIndex(int columnIndex) {
         this.columnIndex = columnIndex;
@@ -2452,6 +2518,11 @@ public UnionStreamReader.StreamReaderBuilder setChildReaders(TreeReader[] childR
         return this;
       }
 
+      public UnionStreamReader.StreamReaderBuilder setContext(TreeReaderFactory.Context context) {
+        this.context = context;
+        return this;
+      }
+
       public UnionStreamReader build() throws IOException {
         SettableUncompressedStream present = StreamUtils.createSettableUncompressedStream(
             OrcProto.Stream.Kind.PRESENT.name(), presentStream);
@@ -2461,7 +2532,7 @@ public UnionStreamReader build() throws IOException {
 
         boolean isFileCompressed = compressionCodec != null;
         return new UnionStreamReader(columnIndex, present, data,
-            columnEncoding, isFileCompressed, childReaders);
+            columnEncoding, isFileCompressed, childReaders, context);
       }
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java
index eab18865d9..0ffa1827a7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.ql.metadata.Hive;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.orc.OrcConf;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -52,7 +53,19 @@ public class SetProcessor implements CommandProcessor {
   private static final Logger LOG = LoggerFactory.getLogger(SetProcessor.class);
 
   private static final String prefix = "set: ";
-  private static final Set<String> removedConfigs = Sets.newHashSet("hive.mapred.supports.subdirectories","hive.enforce.sorting","hive.enforce.bucketing", "hive.outerjoin.supports.filters");
+  private static final Set<String> removedConfigs =
+      Sets.newHashSet("hive.mapred.supports.subdirectories",
+          "hive.enforce.sorting","hive.enforce.bucketing",
+          "hive.outerjoin.supports.filters");
+  // Allow the user to set the ORC properties without getting an error.
+  static {
+    for(OrcConf var: OrcConf.values()) {
+      String name = var.getHiveConfName();
+      if (name != null && name.startsWith("hive.")) {
+        removedConfigs.add(name);
+      }
+    }
+  }
 
   private static final String[] PASSWORD_STRINGS = new String[] {"password", "paswd", "pswd"};
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
index aa23df8ef7..4fa0651da0 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
@@ -121,8 +121,9 @@
 import org.apache.hadoop.mapred.Reporter;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.hadoop.util.Progressable;
-import org.apache.orc.*;
-import org.apache.orc.impl.PhysicalFsWriter;
+import org.apache.orc.OrcConf;
+import org.apache.orc.OrcProto;
+import org.apache.orc.TypeDescription;
 import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
@@ -2115,6 +2116,26 @@ JobConf createMockExecutionEnvironment(Path workDir,
     return conf;
   }
 
+  /**
+   * Set the mockblocks for a file after it has been written
+   * @param path the path to modify
+   * @param conf the configuration
+   * @param blocks the blocks to uses
+   * @throws IOException
+   */
+  static void setBlocks(Path path, Configuration conf,
+                        MockBlock... blocks) throws IOException {
+    FileSystem mockFs = path.getFileSystem(conf);
+    MockOutputStream stream = (MockOutputStream) mockFs.create(path);
+    stream.setBlocks(blocks);
+  }
+
+  static int getLength(Path path, Configuration conf) throws IOException {
+    FileSystem mockFs = path.getFileSystem(conf);
+    FileStatus stat = mockFs.getFileStatus(path);
+    return (int) stat.getLen();
+  }
+
   /**
    * Test vectorization, non-acid, non-combine.
    * @throws Exception
@@ -2132,15 +2153,16 @@ public void testVectorization() throws Exception {
         "vectorization", inspector, true, 1);
 
     // write the orc file to the mock file system
+    Path path = new Path(conf.get("mapred.input.dir") + "/0_0");
     Writer writer =
-        OrcFile.createWriter(new Path(conf.get("mapred.input.dir") + "/0_0"),
+        OrcFile.createWriter(path,
            OrcFile.writerOptions(conf).blockPadding(false)
                   .bufferSize(1024).inspector(inspector));
     for(int i=0; i < 10; ++i) {
       writer.addRow(new MyRow(i, 2*i));
     }
     writer.close();
-    getStreamFromWriter(writer).setBlocks(new MockBlock("host0", "host1"));
+    setBlocks(path, conf, new MockBlock("host0", "host1"));
 
     // call getsplits
     HiveInputFormat<?,?> inputFormat =
@@ -2161,11 +2183,6 @@ public void testVectorization() throws Exception {
     assertEquals(false, reader.next(key, value));
   }
 
-  private MockOutputStream getStreamFromWriter(Writer writer) throws IOException {
-    PhysicalFsWriter pfr = (PhysicalFsWriter)((WriterImpl) writer).getPhysicalWriter();
-    return (MockOutputStream)pfr.getStream();
-  }
-
   /**
    * Test vectorization, non-acid, non-combine.
    * @throws Exception
@@ -2183,15 +2200,16 @@ public void testVectorizationWithBuckets() throws Exception {
         "vectorBuckets", inspector, true, 1);
 
     // write the orc file to the mock file system
+    Path path = new Path(conf.get("mapred.input.dir") + "/0_0");
     Writer writer =
-        OrcFile.createWriter(new Path(conf.get("mapred.input.dir") + "/0_0"),
+        OrcFile.createWriter(path,
             OrcFile.writerOptions(conf).blockPadding(false)
                 .bufferSize(1024).inspector(inspector));
     for(int i=0; i < 10; ++i) {
       writer.addRow(new MyRow(i, 2*i));
     }
     writer.close();
-    getStreamFromWriter(writer).setBlocks(new MockBlock("host0", "host1"));
+    setBlocks(path, conf, new MockBlock("host0", "host1"));
 
     // call getsplits
     conf.setInt(hive_metastoreConstants.BUCKET_COUNT, 3);
@@ -2229,9 +2247,9 @@ public void testVectorizationWithAcid() throws Exception {
       BigRow row = new BigRow(i);
       writer.insert(10, row);
     }
-    WriterImpl baseWriter = (WriterImpl) writer.getWriter();
     writer.close(false);
-    getStreamFromWriter(baseWriter).setBlocks(new MockBlock("host0", "host1"));
+    Path path = new Path("mock:/vectorizationAcid/p=0/base_0000010/bucket_00000");
+    setBlocks(path, conf, new MockBlock("host0", "host1"));
 
     // call getsplits
     HiveInputFormat<?, ?> inputFormat =
@@ -2309,9 +2327,10 @@ public void testCombinationInputFormat() throws Exception {
       writer.addRow(new MyRow(i, 2*i));
     }
     writer.close();
-    MockOutputStream outputStream = getStreamFromWriter(writer);
-    outputStream.setBlocks(new MockBlock("host0", "host1"));
-    int length0 = outputStream.file.length;
+    Path path = new Path("mock:/combination/p=0/0_0");
+    setBlocks(path, conf, new MockBlock("host0", "host1"));
+    MockFileSystem mockFs = (MockFileSystem) partDir.getFileSystem(conf);
+    int length0 = getLength(path, conf);
     writer =
         OrcFile.createWriter(new Path(partDir, "1_0"),
             OrcFile.writerOptions(conf).blockPadding(false)
@@ -2320,8 +2339,8 @@ public void testCombinationInputFormat() throws Exception {
       writer.addRow(new MyRow(i, 2*i));
     }
     writer.close();
-    outputStream = getStreamFromWriter(writer);
-    outputStream.setBlocks(new MockBlock("host1", "host2"));
+    Path path1 = new Path("mock:/combination/p=0/1_0");
+    setBlocks(path1, conf, new MockBlock("host1", "host2"));
 
     // call getsplits
     HiveInputFormat<?,?> inputFormat =
@@ -2336,7 +2355,7 @@ public void testCombinationInputFormat() throws Exception {
     assertEquals(partDir.toString() + "/0_0", split.getPath(0).toString());
     assertEquals(partDir.toString() + "/1_0", split.getPath(1).toString());
     assertEquals(length0, split.getLength(0));
-    assertEquals(outputStream.file.length, split.getLength(1));
+    assertEquals(getLength(path1, conf), split.getLength(1));
     assertEquals(0, split.getOffset(0));
     assertEquals(0, split.getOffset(1));
     // hadoop-1 gets 3 and hadoop-2 gets 0. *sigh*
@@ -2384,11 +2403,11 @@ public void testCombinationInputFormatWithAcid() throws Exception {
     for(int i=0; i < 10; ++i) {
       writer.insert(10, new MyRow(i, 2 * i));
     }
-    WriterImpl baseWriter = (WriterImpl) writer.getWriter();
     writer.close(false);
 
-    MockOutputStream outputStream = getStreamFromWriter(baseWriter);
-    outputStream.setBlocks(new MockBlock("host1", "host2"));
+    // base file
+    Path base0 = new Path("mock:/combinationAcid/p=0/base_0000010/bucket_00000");
+    setBlocks(base0, conf, new MockBlock("host1", "host2"));
 
     // write a delta file in partition 0
     writer = new OrcRecordUpdater(partDir[0],
@@ -2397,23 +2416,22 @@ public void testCombinationInputFormatWithAcid() throws Exception {
     for(int i=10; i < 20; ++i) {
       writer.insert(10, new MyRow(i, 2*i));
     }
-    WriterImpl deltaWriter = (WriterImpl) writer.getWriter();
-    outputStream = getStreamFromWriter(deltaWriter);
     writer.close(false);
-    outputStream.setBlocks(new MockBlock("host1", "host2"));
+    Path base1 = new Path("mock:/combinationAcid/p=0/base_0000010/bucket_00001");
+    setBlocks(base1, conf, new MockBlock("host1", "host2"));
 
     // write three files in partition 1
     for(int bucket=0; bucket < BUCKETS; ++bucket) {
+      Path path = new Path(partDir[1], "00000" + bucket + "_0");
       Writer orc = OrcFile.createWriter(
-          new Path(partDir[1], "00000" + bucket + "_0"),
+          path,
           OrcFile.writerOptions(conf)
               .blockPadding(false)
               .bufferSize(1024)
               .inspector(inspector));
       orc.addRow(new MyRow(1, 2));
-      outputStream = getStreamFromWriter(orc);
       orc.close();
-      outputStream.setBlocks(new MockBlock("host3", "host4"));
+      setBlocks(path, conf, new MockBlock("host3", "host4"));
     }
 
     // call getsplits
@@ -3633,13 +3651,13 @@ public void testRowNumberUniquenessInDifferentSplits() throws Exception {
     }
 
     // Save the conf variable values so that they can be restored later.
-    long oldDefaultStripeSize = conf.getLong(HiveConf.ConfVars.HIVE_ORC_DEFAULT_STRIPE_SIZE.varname, -1L);
+    long oldDefaultStripeSize = conf.getLong(OrcConf.STRIPE_SIZE.getHiveConfName(), -1L);
     long oldMaxSplitSize = conf.getLong(HiveConf.ConfVars.MAPREDMAXSPLITSIZE.varname, -1L);
 
     // Set the conf variable values for this test.
     long newStripeSize = 10000L; // 10000 bytes per stripe
     long newMaxSplitSize = 100L; // 1024 bytes per split
-    conf.setLong(HiveConf.ConfVars.HIVE_ORC_DEFAULT_STRIPE_SIZE.varname, newStripeSize);
+    conf.setLong(OrcConf.STRIPE_SIZE.getHiveConfName(), newStripeSize);
     conf.setLong(HiveConf.ConfVars.MAPREDMAXSPLITSIZE.varname, newMaxSplitSize);
 
     AbstractSerDe serde = new OrcSerde();
@@ -3681,10 +3699,10 @@ public void testRowNumberUniquenessInDifferentSplits() throws Exception {
 
     // Reset the conf variable values that we changed for this test.
     if (oldDefaultStripeSize != -1L) {
-      conf.setLong(HiveConf.ConfVars.HIVE_ORC_DEFAULT_STRIPE_SIZE.varname, oldDefaultStripeSize);
+      conf.setLong(OrcConf.STRIPE_SIZE.getHiveConfName(), oldDefaultStripeSize);
     } else {
       // this means that nothing was set for default stripe size previously, so we should unset it.
-      conf.unset(HiveConf.ConfVars.HIVE_ORC_DEFAULT_STRIPE_SIZE.varname);
+      conf.unset(OrcConf.STRIPE_SIZE.getHiveConfName());
     }
     if (oldMaxSplitSize != -1L) {
       conf.setLong(HiveConf.ConfVars.MAPREDMAXSPLITSIZE.varname, oldMaxSplitSize);
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
index c7c2c9d8e1..84e83df22d 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
@@ -85,6 +85,7 @@
 import org.apache.orc.DecimalColumnStatistics;
 import org.apache.orc.DoubleColumnStatistics;
 import org.apache.orc.IntegerColumnStatistics;
+import org.apache.orc.OrcConf;
 import org.apache.orc.impl.MemoryManager;
 import org.apache.orc.OrcProto;
 
@@ -247,7 +248,7 @@ public TestOrcFile(Boolean zcr) {
   public void openFileSystem () throws Exception {
     conf = new Configuration();
     if(zeroCopy) {
-      conf.setBoolean(HiveConf.ConfVars.HIVE_ORC_ZEROCOPY.varname, zeroCopy);
+      conf.setBoolean(OrcConf.USE_ZEROCOPY.getHiveConfName(), zeroCopy);
     }
     fs = FileSystem.getLocal(conf);
     testFilePath = new Path(workDir, "TestOrcFile." +
@@ -1817,7 +1818,7 @@ public void testZeroCopySeek() throws Exception {
     assertEquals(COUNT, reader.getNumberOfRows());
     /* enable zero copy record reader */
     Configuration conf = new Configuration();
-    HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_ORC_ZEROCOPY, true);
+    conf.setBoolean(OrcConf.USE_ZEROCOPY.getHiveConfName(), true);
     RecordReader rows = reader.rows();
     /* all tests are identical to the other seek() tests */
     OrcStruct row = null;
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorizedOrcAcidRowBatchReader.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorizedOrcAcidRowBatchReader.java
index 4656ab2f8f..6bf13129b8 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorizedOrcAcidRowBatchReader.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorizedOrcAcidRowBatchReader.java
@@ -76,7 +76,7 @@ static class DummyRow {
     }
 
     static String getColumnNamesProperty() {
-      return "x";
+      return "field";
     }
     static String getColumnTypesProperty() {
       return "bigint";
diff --git a/ql/src/test/queries/clientpositive/orc_remove_cols.q b/ql/src/test/queries/clientpositive/orc_remove_cols.q
index fdae064146..c3c95f36dd 100644
--- a/ql/src/test/queries/clientpositive/orc_remove_cols.q
+++ b/ql/src/test/queries/clientpositive/orc_remove_cols.q
@@ -9,7 +9,7 @@ insert into table orc_partitioned partition (ds = 'tomorrow') select cint, cstri
 
 -- Use the old change the SERDE trick to avoid ORC DDL checks... and remove a column on the end.
 ALTER TABLE orc_partitioned SET SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
-ALTER TABLE orc_partitioned REPLACE COLUMNS (cint int);
+ALTER TABLE orc_partitioned REPLACE COLUMNS (a int);
 ALTER TABLE orc_partitioned SET SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';
 
 SELECT * FROM orc_partitioned WHERE ds = 'today';
diff --git a/ql/src/test/queries/clientpositive/orc_schema_evolution.q b/ql/src/test/queries/clientpositive/orc_schema_evolution.q
index c78cfe8405..aa6fc58716 100644
--- a/ql/src/test/queries/clientpositive/orc_schema_evolution.q
+++ b/ql/src/test/queries/clientpositive/orc_schema_evolution.q
@@ -21,21 +21,21 @@ select sum(hash(*)) from src_orc;
 insert overwrite table src_orc2 select * from src;
 select sum(hash(*)) from src_orc2;
 
-alter table src_orc2 replace columns (k smallint, v string);
+alter table src_orc2 replace columns (key smallint, val string);
 select sum(hash(*)) from src_orc2;
 
-alter table src_orc2 replace columns (k int, v string);
+alter table src_orc2 replace columns (key int, val string);
 select sum(hash(*)) from src_orc2;
 
-alter table src_orc2 replace columns (k bigint, v string);
+alter table src_orc2 replace columns (key bigint, val string);
 select sum(hash(*)) from src_orc2;
 
-alter table src_orc2 replace columns (k bigint, v string, z int);
+alter table src_orc2 replace columns (key bigint, val string, z int);
 select sum(hash(*)) from src_orc2;
 
-alter table src_orc2 replace columns (k bigint, v string, z bigint);
+alter table src_orc2 replace columns (key bigint, val string, z bigint);
 select sum(hash(*)) from src_orc2;
 
-alter table src_orc2 replace columns (k bigint, v string, z bigint, y float);
+alter table src_orc2 replace columns (key bigint, val string, z bigint, y float);
 select sum(hash(*)) from src_orc2;
 
diff --git a/ql/src/test/results/clientpositive/llap/llap_nullscan.q.out b/ql/src/test/results/clientpositive/llap/llap_nullscan.q.out
index b798e82950..7d01c695d0 100644
--- a/ql/src/test/results/clientpositive/llap/llap_nullscan.q.out
+++ b/ql/src/test/results/clientpositive/llap/llap_nullscan.q.out
@@ -99,7 +99,7 @@ STAGE PLANS:
                     serialization.ddl struct src_orc { string key, string value, string ds, string hr}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.NullStructSerDe
-                    totalSize 633
+                    totalSize 626
 #### A masked pattern was here ####
                   serde: org.apache.hadoop.hive.serde2.NullStructSerDe
                 
@@ -120,7 +120,7 @@ STAGE PLANS:
                       serialization.ddl struct src_orc { string key, string value, string ds, string hr}
                       serialization.format 1
                       serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-                      totalSize 633
+                      totalSize 626
 #### A masked pattern was here ####
                     serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
                     name: default.src_orc
diff --git a/ql/src/test/results/clientpositive/llap/orc_analyze.q.out b/ql/src/test/results/clientpositive/llap/orc_analyze.q.out
index 91f363bb87..fdfb74ac2c 100644
--- a/ql/src/test/results/clientpositive/llap/orc_analyze.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_analyze.q.out
@@ -102,7 +102,7 @@ Table Parameters:
 	numFiles            	1                   
 	numRows             	100                 
 	rawDataSize         	52600               
-	totalSize           	3202                
+	totalSize           	3197                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -150,7 +150,7 @@ Table Parameters:
 	numFiles            	1                   
 	numRows             	100                 
 	rawDataSize         	52600               
-	totalSize           	3202                
+	totalSize           	3197                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -198,7 +198,7 @@ Table Parameters:
 	numFiles            	1                   
 	numRows             	100                 
 	rawDataSize         	52600               
-	totalSize           	3202                
+	totalSize           	3197                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -285,7 +285,7 @@ Table Parameters:
 	numFiles            	1                   
 	numRows             	100                 
 	rawDataSize         	52600               
-	totalSize           	3202                
+	totalSize           	3197                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -395,7 +395,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -438,7 +438,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	22050               
-	totalSize           	2118                
+	totalSize           	2113                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -493,7 +493,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -536,7 +536,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	22050               
-	totalSize           	2118                
+	totalSize           	2113                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -591,7 +591,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -634,7 +634,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	22050               
-	totalSize           	2118                
+	totalSize           	2113                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -732,7 +732,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -775,7 +775,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	22050               
-	totalSize           	2118                
+	totalSize           	2113                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -891,7 +891,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	21975               
-	totalSize           	5263                
+	totalSize           	5259                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -934,7 +934,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	22043               
-	totalSize           	5336                
+	totalSize           	5326                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -989,7 +989,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	21975               
-	totalSize           	5263                
+	totalSize           	5259                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1032,7 +1032,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	22043               
-	totalSize           	5336                
+	totalSize           	5326                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1087,7 +1087,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	21975               
-	totalSize           	5263                
+	totalSize           	5259                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1130,7 +1130,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	22043               
-	totalSize           	5336                
+	totalSize           	5326                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1234,7 +1234,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	21975               
-	totalSize           	5263                
+	totalSize           	5259                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1277,7 +1277,7 @@ Partition Parameters:
 	numFiles            	4                   
 	numRows             	50                  
 	rawDataSize         	22043               
-	totalSize           	5336                
+	totalSize           	5326                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1387,7 +1387,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1442,7 +1442,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -1497,7 +1497,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	50                  
 	rawDataSize         	21950               
-	totalSize           	2102                
+	totalSize           	2099                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
diff --git a/ql/src/test/results/clientpositive/llap/orc_llap_counters.q.out b/ql/src/test/results/clientpositive/llap/orc_llap_counters.q.out
index bc3915aece..318858b3a8 100644
--- a/ql/src/test/results/clientpositive/llap/orc_llap_counters.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_llap_counters.q.out
@@ -233,7 +233,7 @@ Table Parameters:
 	orc.bloom.filter.columns	*                   
 	orc.row.index.stride	1000                
 	rawDataSize         	1139514             
-	totalSize           	57100               
+	totalSize           	55376               
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -251,7 +251,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16677
+   HDFS_BYTES_READ: 16673
    HDFS_BYTES_WRITTEN: 104
    HDFS_READ_OPS: 5
    HDFS_LARGE_READ_OPS: 0
@@ -288,7 +288,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 1467
+   HDFS_BYTES_READ: 1344
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -697,7 +697,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 6132
+   HDFS_BYTES_READ: 5980
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
diff --git a/ql/src/test/results/clientpositive/llap/orc_llap_counters1.q.out b/ql/src/test/results/clientpositive/llap/orc_llap_counters1.q.out
index 5c54622b42..2fb204e81b 100644
--- a/ql/src/test/results/clientpositive/llap/orc_llap_counters1.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_llap_counters1.q.out
@@ -233,7 +233,7 @@ Table Parameters:
 	orc.bloom.filter.columns	*                   
 	orc.row.index.stride	1000                
 	rawDataSize         	1139514             
-	totalSize           	57100               
+	totalSize           	55376               
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -251,7 +251,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 104
    HDFS_READ_OPS: 6
    HDFS_LARGE_READ_OPS: 0
diff --git a/ql/src/test/results/clientpositive/llap/orc_merge10.q.out b/ql/src/test/results/clientpositive/llap/orc_merge10.q.out
index bac45eebb4..01975920b8 100644
--- a/ql/src/test/results/clientpositive/llap/orc_merge10.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_merge10.q.out
@@ -542,7 +542,7 @@ PREHOOK: Input: default@orcfile_merge1@ds=1/part=0
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 242
 Compression: SNAPPY
 Compression size: 4096
@@ -568,13 +568,13 @@ File Statistics:
   Column 2: count: 242 hasNull: false min: val_0 max: val_97 sum: 1646
 
 Stripes:
-  Stripe: offset: 3 data: 616 rows: 90 tail: 61 index: 76
+  Stripe: offset: 3 data: 613 rows: 90 tail: 61 index: 76
     Stream: column 0 section ROW_INDEX start: 3 length 11
     Stream: column 1 section ROW_INDEX start: 14 length 27
     Stream: column 2 section ROW_INDEX start: 41 length 38
     Stream: column 1 section DATA start: 79 length 185
     Stream: column 2 section DATA start: 264 length 377
-    Stream: column 2 section LENGTH start: 641 length 54
+    Stream: column 2 section LENGTH start: 641 length 51
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -584,13 +584,13 @@ Stripes:
       Entry 0: count: 90 hasNull: false min: 0 max: 495 sum: 22736 positions: 0,0,0
     Row group indices for column 2:
       Entry 0: count: 90 hasNull: false min: val_0 max: val_86 sum: 612 positions: 0,0,0,0,0
-  Stripe: offset: 756 data: 544 rows: 78 tail: 61 index: 76
-    Stream: column 0 section ROW_INDEX start: 756 length 11
-    Stream: column 1 section ROW_INDEX start: 767 length 27
-    Stream: column 2 section ROW_INDEX start: 794 length 38
-    Stream: column 1 section DATA start: 832 length 161
-    Stream: column 2 section DATA start: 993 length 332
-    Stream: column 2 section LENGTH start: 1325 length 51
+  Stripe: offset: 753 data: 541 rows: 78 tail: 61 index: 76
+    Stream: column 0 section ROW_INDEX start: 753 length 11
+    Stream: column 1 section ROW_INDEX start: 764 length 27
+    Stream: column 2 section ROW_INDEX start: 791 length 38
+    Stream: column 1 section DATA start: 829 length 161
+    Stream: column 2 section DATA start: 990 length 332
+    Stream: column 2 section LENGTH start: 1322 length 48
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -600,13 +600,13 @@ Stripes:
       Entry 0: count: 78 hasNull: false min: 0 max: 497 sum: 18371 positions: 0,0,0
     Row group indices for column 2:
       Entry 0: count: 78 hasNull: false min: val_0 max: val_95 sum: 529 positions: 0,0,0,0,0
-  Stripe: offset: 1437 data: 519 rows: 74 tail: 61 index: 78
-    Stream: column 0 section ROW_INDEX start: 1437 length 11
-    Stream: column 1 section ROW_INDEX start: 1448 length 27
-    Stream: column 2 section ROW_INDEX start: 1475 length 40
-    Stream: column 1 section DATA start: 1515 length 153
-    Stream: column 2 section DATA start: 1668 length 331
-    Stream: column 2 section LENGTH start: 1999 length 35
+  Stripe: offset: 1431 data: 516 rows: 74 tail: 61 index: 78
+    Stream: column 0 section ROW_INDEX start: 1431 length 11
+    Stream: column 1 section ROW_INDEX start: 1442 length 27
+    Stream: column 2 section ROW_INDEX start: 1469 length 40
+    Stream: column 1 section DATA start: 1509 length 153
+    Stream: column 2 section DATA start: 1662 length 331
+    Stream: column 2 section LENGTH start: 1993 length 32
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -617,7 +617,7 @@ Stripes:
     Row group indices for column 2:
       Entry 0: count: 74 hasNull: false min: val_105 max: val_97 sum: 505 positions: 0,0,0,0,0
 
-File length: 2393 bytes
+File length: 2384 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
@@ -631,7 +631,7 @@ PREHOOK: Input: default@orcfile_merge1c@ds=1/part=0
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 242
 Compression: SNAPPY
 Compression size: 4096
@@ -657,13 +657,13 @@ File Statistics:
   Column 2: count: 242 hasNull: false min: val_0 max: val_97 sum: 1646
 
 Stripes:
-  Stripe: offset: 3 data: 616 rows: 90 tail: 61 index: 76
+  Stripe: offset: 3 data: 613 rows: 90 tail: 61 index: 76
     Stream: column 0 section ROW_INDEX start: 3 length 11
     Stream: column 1 section ROW_INDEX start: 14 length 27
     Stream: column 2 section ROW_INDEX start: 41 length 38
     Stream: column 1 section DATA start: 79 length 185
     Stream: column 2 section DATA start: 264 length 377
-    Stream: column 2 section LENGTH start: 641 length 54
+    Stream: column 2 section LENGTH start: 641 length 51
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -673,13 +673,13 @@ Stripes:
       Entry 0: count: 90 hasNull: false min: 0 max: 495 sum: 22736 positions: 0,0,0
     Row group indices for column 2:
       Entry 0: count: 90 hasNull: false min: val_0 max: val_86 sum: 612 positions: 0,0,0,0,0
-  Stripe: offset: 756 data: 544 rows: 78 tail: 61 index: 76
-    Stream: column 0 section ROW_INDEX start: 756 length 11
-    Stream: column 1 section ROW_INDEX start: 767 length 27
-    Stream: column 2 section ROW_INDEX start: 794 length 38
-    Stream: column 1 section DATA start: 832 length 161
-    Stream: column 2 section DATA start: 993 length 332
-    Stream: column 2 section LENGTH start: 1325 length 51
+  Stripe: offset: 753 data: 541 rows: 78 tail: 61 index: 76
+    Stream: column 0 section ROW_INDEX start: 753 length 11
+    Stream: column 1 section ROW_INDEX start: 764 length 27
+    Stream: column 2 section ROW_INDEX start: 791 length 38
+    Stream: column 1 section DATA start: 829 length 161
+    Stream: column 2 section DATA start: 990 length 332
+    Stream: column 2 section LENGTH start: 1322 length 48
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -689,13 +689,13 @@ Stripes:
       Entry 0: count: 78 hasNull: false min: 0 max: 497 sum: 18371 positions: 0,0,0
     Row group indices for column 2:
       Entry 0: count: 78 hasNull: false min: val_0 max: val_95 sum: 529 positions: 0,0,0,0,0
-  Stripe: offset: 1437 data: 519 rows: 74 tail: 61 index: 78
-    Stream: column 0 section ROW_INDEX start: 1437 length 11
-    Stream: column 1 section ROW_INDEX start: 1448 length 27
-    Stream: column 2 section ROW_INDEX start: 1475 length 40
-    Stream: column 1 section DATA start: 1515 length 153
-    Stream: column 2 section DATA start: 1668 length 331
-    Stream: column 2 section LENGTH start: 1999 length 35
+  Stripe: offset: 1431 data: 516 rows: 74 tail: 61 index: 78
+    Stream: column 0 section ROW_INDEX start: 1431 length 11
+    Stream: column 1 section ROW_INDEX start: 1442 length 27
+    Stream: column 2 section ROW_INDEX start: 1469 length 40
+    Stream: column 1 section DATA start: 1509 length 153
+    Stream: column 2 section DATA start: 1662 length 331
+    Stream: column 2 section LENGTH start: 1993 length 32
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -706,7 +706,7 @@ Stripes:
     Row group indices for column 2:
       Entry 0: count: 74 hasNull: false min: val_105 max: val_97 sum: 505 positions: 0,0,0,0,0
 
-File length: 2393 bytes
+File length: 2384 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
diff --git a/ql/src/test/results/clientpositive/llap/orc_merge11.q.out b/ql/src/test/results/clientpositive/llap/orc_merge11.q.out
index a8ab854646..9c50f6dec9 100644
--- a/ql/src/test/results/clientpositive/llap/orc_merge11.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_merge11.q.out
@@ -72,7 +72,7 @@ PREHOOK: Input: default@orcfile_merge1
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 50000
 Compression: ZLIB
 Compression size: 4096
@@ -163,7 +163,7 @@ ________________________________________________________________________________
 -- END ORC FILE DUMP --
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 50000
 Compression: ZLIB
 Compression size: 4096
@@ -275,7 +275,7 @@ PREHOOK: Input: default@orcfile_merge1
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 100000
 Compression: ZLIB
 Compression size: 4096
diff --git a/ql/src/test/results/clientpositive/llap/orc_merge12.q.out b/ql/src/test/results/clientpositive/llap/orc_merge12.q.out
index 6a86fcf7d7..2c49e0f865 100644
--- a/ql/src/test/results/clientpositive/llap/orc_merge12.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_merge12.q.out
@@ -144,7 +144,7 @@ PREHOOK: Input: default@alltypesorc3xcols
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 24576
 Compression: ZLIB
 Compression size: 262144
diff --git a/ql/src/test/results/clientpositive/llap/orc_ppd_basic.q.out b/ql/src/test/results/clientpositive/llap/orc_ppd_basic.q.out
index e28ed5de46..574865963b 100644
--- a/ql/src/test/results/clientpositive/llap/orc_ppd_basic.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_ppd_basic.q.out
@@ -203,7 +203,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16677
+   HDFS_BYTES_READ: 16673
    HDFS_BYTES_WRITTEN: 104
    HDFS_READ_OPS: 5
    HDFS_LARGE_READ_OPS: 0
@@ -240,7 +240,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 1467
+   HDFS_BYTES_READ: 1344
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -649,7 +649,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 6132
+   HDFS_BYTES_READ: 5980
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1171,7 +1171,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 5181
+   HDFS_BYTES_READ: 5201
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
diff --git a/ql/src/test/results/clientpositive/llap/orc_ppd_schema_evol_3a.q.out b/ql/src/test/results/clientpositive/llap/orc_ppd_schema_evol_3a.q.out
index 4cc8984778..c90dcb5377 100644
--- a/ql/src/test/results/clientpositive/llap/orc_ppd_schema_evol_3a.q.out
+++ b/ql/src/test/results/clientpositive/llap/orc_ppd_schema_evol_3a.q.out
@@ -203,7 +203,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17012
+   HDFS_BYTES_READ: 17008
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 6
    HDFS_LARGE_READ_OPS: 0
@@ -347,7 +347,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -380,7 +380,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -399,7 +399,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -418,7 +418,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -437,7 +437,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -460,7 +460,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -493,7 +493,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -512,7 +512,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -531,7 +531,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -550,7 +550,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -573,7 +573,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -606,7 +606,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -625,7 +625,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -644,7 +644,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -663,7 +663,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -686,7 +686,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 104
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -705,7 +705,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 104
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -724,7 +724,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -743,7 +743,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -762,7 +762,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 16902
+   HDFS_BYTES_READ: 16898
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -781,7 +781,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 17875
+   HDFS_BYTES_READ: 17728
    HDFS_BYTES_WRITTEN: 102
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -800,7 +800,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 5181
+   HDFS_BYTES_READ: 5201
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -857,7 +857,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 21462
+   HDFS_BYTES_READ: 21458
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -876,7 +876,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 23522
+   HDFS_BYTES_READ: 23336
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -899,7 +899,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 21462
+   HDFS_BYTES_READ: 21458
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -918,7 +918,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 23522
+   HDFS_BYTES_READ: 23336
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -937,7 +937,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 4368
+   HDFS_BYTES_READ: 4388
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -994,7 +994,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 20633
+   HDFS_BYTES_READ: 20629
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1013,7 +1013,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 22540
+   HDFS_BYTES_READ: 22364
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1036,7 +1036,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 20633
+   HDFS_BYTES_READ: 20629
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1055,7 +1055,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 22540
+   HDFS_BYTES_READ: 22364
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1078,7 +1078,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 20633
+   HDFS_BYTES_READ: 20629
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1097,7 +1097,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 22540
+   HDFS_BYTES_READ: 22364
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1174,7 +1174,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 18751
+   HDFS_BYTES_READ: 18747
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1193,7 +1193,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 18751
+   HDFS_BYTES_READ: 18747
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1212,7 +1212,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 20222
+   HDFS_BYTES_READ: 20073
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
@@ -1231,7 +1231,7 @@ PREHOOK: type: QUERY
 PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 Stage-1 FILE SYSTEM COUNTERS:
-   HDFS_BYTES_READ: 20222
+   HDFS_BYTES_READ: 20073
    HDFS_BYTES_WRITTEN: 101
    HDFS_READ_OPS: 4
    HDFS_LARGE_READ_OPS: 0
diff --git a/ql/src/test/results/clientpositive/materialized_view_drop.q.out b/ql/src/test/results/clientpositive/materialized_view_drop.q.out
index 3cf13d0d0b..ceb34f7b13 100644
--- a/ql/src/test/results/clientpositive/materialized_view_drop.q.out
+++ b/ql/src/test/results/clientpositive/materialized_view_drop.q.out
@@ -20,9 +20,9 @@ columns:struct columns { i32 cint, string cstring1}
 partitioned:false
 partitionColumns:
 totalNumberFiles:1
-totalFileSize:47140
-maxFileSize:47140
-minFileSize:47140
+totalFileSize:47120
+maxFileSize:47120
+minFileSize:47120
 #### A masked pattern was here ####
 
 PREHOOK: query: drop materialized view dmv_mat_view
diff --git a/ql/src/test/results/clientpositive/orc_file_dump.q.out b/ql/src/test/results/clientpositive/orc_file_dump.q.out
index 4cc8e3ba17..67c75d06f9 100644
--- a/ql/src/test/results/clientpositive/orc_file_dump.q.out
+++ b/ql/src/test/results/clientpositive/orc_file_dump.q.out
@@ -93,7 +93,7 @@ PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 1049
 Compression: ZLIB
 Compression size: 262144
@@ -129,49 +129,49 @@ File Statistics:
   Column 11: count: 1049 hasNull: false sum: 13278
 
 Stripes:
-  Stripe: offset: 3 data: 22593 rows: 1049 tail: 250 index: 9943
+  Stripe: offset: 3 data: 22593 rows: 1049 tail: 250 index: 8963
     Stream: column 0 section ROW_INDEX start: 3 length 20
-    Stream: column 0 section BLOOM_FILTER start: 23 length 45
-    Stream: column 1 section ROW_INDEX start: 68 length 58
-    Stream: column 1 section BLOOM_FILTER start: 126 length 799
-    Stream: column 2 section ROW_INDEX start: 925 length 58
-    Stream: column 2 section BLOOM_FILTER start: 983 length 978
-    Stream: column 3 section ROW_INDEX start: 1961 length 61
-    Stream: column 3 section BLOOM_FILTER start: 2022 length 983
-    Stream: column 4 section ROW_INDEX start: 3005 length 69
-    Stream: column 4 section BLOOM_FILTER start: 3074 length 963
-    Stream: column 5 section ROW_INDEX start: 4037 length 78
-    Stream: column 5 section BLOOM_FILTER start: 4115 length 1291
-    Stream: column 6 section ROW_INDEX start: 5406 length 85
-    Stream: column 6 section BLOOM_FILTER start: 5491 length 1280
-    Stream: column 7 section ROW_INDEX start: 6771 length 41
-    Stream: column 7 section BLOOM_FILTER start: 6812 length 45
-    Stream: column 8 section ROW_INDEX start: 6857 length 86
-    Stream: column 8 section BLOOM_FILTER start: 6943 length 1157
-    Stream: column 9 section ROW_INDEX start: 8100 length 50
-    Stream: column 9 section BLOOM_FILTER start: 8150 length 62
-    Stream: column 10 section ROW_INDEX start: 8212 length 82
-    Stream: column 10 section BLOOM_FILTER start: 8294 length 1297
-    Stream: column 11 section ROW_INDEX start: 9591 length 47
-    Stream: column 11 section BLOOM_FILTER start: 9638 length 308
-    Stream: column 1 section PRESENT start: 9946 length 17
-    Stream: column 1 section DATA start: 9963 length 962
-    Stream: column 2 section PRESENT start: 10925 length 17
-    Stream: column 2 section DATA start: 10942 length 1441
-    Stream: column 3 section DATA start: 12383 length 1704
-    Stream: column 4 section DATA start: 14087 length 1998
-    Stream: column 5 section DATA start: 16085 length 2925
-    Stream: column 6 section DATA start: 19010 length 3323
-    Stream: column 7 section DATA start: 22333 length 137
-    Stream: column 8 section DATA start: 22470 length 1572
-    Stream: column 8 section LENGTH start: 24042 length 310
-    Stream: column 8 section DICTIONARY_DATA start: 24352 length 1548
-    Stream: column 9 section DATA start: 25900 length 19
-    Stream: column 9 section SECONDARY start: 25919 length 1783
-    Stream: column 10 section DATA start: 27702 length 2138
-    Stream: column 10 section SECONDARY start: 29840 length 231
-    Stream: column 11 section DATA start: 30071 length 1877
-    Stream: column 11 section LENGTH start: 31948 length 591
+    Stream: column 0 section BLOOM_FILTER_UTF8 start: 23 length 34
+    Stream: column 1 section ROW_INDEX start: 57 length 58
+    Stream: column 1 section BLOOM_FILTER_UTF8 start: 115 length 696
+    Stream: column 2 section ROW_INDEX start: 811 length 58
+    Stream: column 2 section BLOOM_FILTER_UTF8 start: 869 length 867
+    Stream: column 3 section ROW_INDEX start: 1736 length 61
+    Stream: column 3 section BLOOM_FILTER_UTF8 start: 1797 length 861
+    Stream: column 4 section ROW_INDEX start: 2658 length 69
+    Stream: column 4 section BLOOM_FILTER_UTF8 start: 2727 length 850
+    Stream: column 5 section ROW_INDEX start: 3577 length 78
+    Stream: column 5 section BLOOM_FILTER_UTF8 start: 3655 length 1172
+    Stream: column 6 section ROW_INDEX start: 4827 length 85
+    Stream: column 6 section BLOOM_FILTER_UTF8 start: 4912 length 1167
+    Stream: column 7 section ROW_INDEX start: 6079 length 41
+    Stream: column 7 section BLOOM_FILTER_UTF8 start: 6120 length 34
+    Stream: column 8 section ROW_INDEX start: 6154 length 86
+    Stream: column 8 section BLOOM_FILTER_UTF8 start: 6240 length 1051
+    Stream: column 9 section ROW_INDEX start: 7291 length 50
+    Stream: column 9 section BLOOM_FILTER_UTF8 start: 7341 length 53
+    Stream: column 10 section ROW_INDEX start: 7394 length 82
+    Stream: column 10 section BLOOM_FILTER_UTF8 start: 7476 length 1189
+    Stream: column 11 section ROW_INDEX start: 8665 length 47
+    Stream: column 11 section BLOOM_FILTER_UTF8 start: 8712 length 254
+    Stream: column 1 section PRESENT start: 8966 length 17
+    Stream: column 1 section DATA start: 8983 length 962
+    Stream: column 2 section PRESENT start: 9945 length 17
+    Stream: column 2 section DATA start: 9962 length 1441
+    Stream: column 3 section DATA start: 11403 length 1704
+    Stream: column 4 section DATA start: 13107 length 1998
+    Stream: column 5 section DATA start: 15105 length 2925
+    Stream: column 6 section DATA start: 18030 length 3323
+    Stream: column 7 section DATA start: 21353 length 137
+    Stream: column 8 section DATA start: 21490 length 1572
+    Stream: column 8 section LENGTH start: 23062 length 310
+    Stream: column 8 section DICTIONARY_DATA start: 23372 length 1548
+    Stream: column 9 section DATA start: 24920 length 19
+    Stream: column 9 section SECONDARY start: 24939 length 1783
+    Stream: column 10 section DATA start: 26722 length 2138
+    Stream: column 10 section SECONDARY start: 28860 length 231
+    Stream: column 11 section DATA start: 29091 length 1877
+    Stream: column 11 section LENGTH start: 30968 length 591
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT
     Encoding column 2: DIRECT_V2
@@ -269,7 +269,7 @@ Stripes:
       Entry 1: numHashFunctions: 4 bitCount: 6272 popCount: 98 loadFactor: 0.0156 expectedFpp: 5.9604645E-8
       Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 102 loadFactor: 0.0163 expectedFpp: 6.9948186E-8
 
-File length: 33416 bytes
+File length: 32435 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
@@ -290,7 +290,7 @@ PREHOOK: Input: default@orc_ppd
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 1049
 Compression: ZLIB
 Compression size: 262144
@@ -326,49 +326,49 @@ File Statistics:
   Column 11: count: 1049 hasNull: false sum: 13278
 
 Stripes:
-  Stripe: offset: 3 data: 22593 rows: 1049 tail: 250 index: 15095
+  Stripe: offset: 3 data: 22593 rows: 1049 tail: 246 index: 13609
     Stream: column 0 section ROW_INDEX start: 3 length 20
-    Stream: column 0 section BLOOM_FILTER start: 23 length 56
-    Stream: column 1 section ROW_INDEX start: 79 length 58
-    Stream: column 1 section BLOOM_FILTER start: 137 length 1258
-    Stream: column 2 section ROW_INDEX start: 1395 length 58
-    Stream: column 2 section BLOOM_FILTER start: 1453 length 1544
-    Stream: column 3 section ROW_INDEX start: 2997 length 61
-    Stream: column 3 section BLOOM_FILTER start: 3058 length 1543
-    Stream: column 4 section ROW_INDEX start: 4601 length 69
-    Stream: column 4 section BLOOM_FILTER start: 4670 length 1556
-    Stream: column 5 section ROW_INDEX start: 6226 length 78
-    Stream: column 5 section BLOOM_FILTER start: 6304 length 1991
-    Stream: column 6 section ROW_INDEX start: 8295 length 85
-    Stream: column 6 section BLOOM_FILTER start: 8380 length 1964
-    Stream: column 7 section ROW_INDEX start: 10344 length 41
-    Stream: column 7 section BLOOM_FILTER start: 10385 length 56
-    Stream: column 8 section ROW_INDEX start: 10441 length 86
-    Stream: column 8 section BLOOM_FILTER start: 10527 length 1829
-    Stream: column 9 section ROW_INDEX start: 12356 length 50
-    Stream: column 9 section BLOOM_FILTER start: 12406 length 95
-    Stream: column 10 section ROW_INDEX start: 12501 length 82
-    Stream: column 10 section BLOOM_FILTER start: 12583 length 1994
-    Stream: column 11 section ROW_INDEX start: 14577 length 47
-    Stream: column 11 section BLOOM_FILTER start: 14624 length 474
-    Stream: column 1 section PRESENT start: 15098 length 17
-    Stream: column 1 section DATA start: 15115 length 962
-    Stream: column 2 section PRESENT start: 16077 length 17
-    Stream: column 2 section DATA start: 16094 length 1441
-    Stream: column 3 section DATA start: 17535 length 1704
-    Stream: column 4 section DATA start: 19239 length 1998
-    Stream: column 5 section DATA start: 21237 length 2925
-    Stream: column 6 section DATA start: 24162 length 3323
-    Stream: column 7 section DATA start: 27485 length 137
-    Stream: column 8 section DATA start: 27622 length 1572
-    Stream: column 8 section LENGTH start: 29194 length 310
-    Stream: column 8 section DICTIONARY_DATA start: 29504 length 1548
-    Stream: column 9 section DATA start: 31052 length 19
-    Stream: column 9 section SECONDARY start: 31071 length 1783
-    Stream: column 10 section DATA start: 32854 length 2138
-    Stream: column 10 section SECONDARY start: 34992 length 231
-    Stream: column 11 section DATA start: 35223 length 1877
-    Stream: column 11 section LENGTH start: 37100 length 591
+    Stream: column 0 section BLOOM_FILTER_UTF8 start: 23 length 43
+    Stream: column 1 section ROW_INDEX start: 66 length 58
+    Stream: column 1 section BLOOM_FILTER_UTF8 start: 124 length 1091
+    Stream: column 2 section ROW_INDEX start: 1215 length 58
+    Stream: column 2 section BLOOM_FILTER_UTF8 start: 1273 length 1372
+    Stream: column 3 section ROW_INDEX start: 2645 length 61
+    Stream: column 3 section BLOOM_FILTER_UTF8 start: 2706 length 1374
+    Stream: column 4 section ROW_INDEX start: 4080 length 69
+    Stream: column 4 section BLOOM_FILTER_UTF8 start: 4149 length 1375
+    Stream: column 5 section ROW_INDEX start: 5524 length 78
+    Stream: column 5 section BLOOM_FILTER_UTF8 start: 5602 length 1824
+    Stream: column 6 section ROW_INDEX start: 7426 length 85
+    Stream: column 6 section BLOOM_FILTER_UTF8 start: 7511 length 1795
+    Stream: column 7 section ROW_INDEX start: 9306 length 41
+    Stream: column 7 section BLOOM_FILTER_UTF8 start: 9347 length 43
+    Stream: column 8 section ROW_INDEX start: 9390 length 86
+    Stream: column 8 section BLOOM_FILTER_UTF8 start: 9476 length 1653
+    Stream: column 9 section ROW_INDEX start: 11129 length 50
+    Stream: column 9 section BLOOM_FILTER_UTF8 start: 11179 length 80
+    Stream: column 10 section ROW_INDEX start: 11259 length 82
+    Stream: column 10 section BLOOM_FILTER_UTF8 start: 11341 length 1802
+    Stream: column 11 section ROW_INDEX start: 13143 length 47
+    Stream: column 11 section BLOOM_FILTER_UTF8 start: 13190 length 422
+    Stream: column 1 section PRESENT start: 13612 length 17
+    Stream: column 1 section DATA start: 13629 length 962
+    Stream: column 2 section PRESENT start: 14591 length 17
+    Stream: column 2 section DATA start: 14608 length 1441
+    Stream: column 3 section DATA start: 16049 length 1704
+    Stream: column 4 section DATA start: 17753 length 1998
+    Stream: column 5 section DATA start: 19751 length 2925
+    Stream: column 6 section DATA start: 22676 length 3323
+    Stream: column 7 section DATA start: 25999 length 137
+    Stream: column 8 section DATA start: 26136 length 1572
+    Stream: column 8 section LENGTH start: 27708 length 310
+    Stream: column 8 section DICTIONARY_DATA start: 28018 length 1548
+    Stream: column 9 section DATA start: 29566 length 19
+    Stream: column 9 section SECONDARY start: 29585 length 1783
+    Stream: column 10 section DATA start: 31368 length 2138
+    Stream: column 10 section SECONDARY start: 33506 length 231
+    Stream: column 11 section DATA start: 33737 length 1877
+    Stream: column 11 section LENGTH start: 35614 length 591
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT
     Encoding column 2: DIRECT_V2
@@ -466,7 +466,7 @@ Stripes:
       Entry 1: numHashFunctions: 7 bitCount: 9600 popCount: 174 loadFactor: 0.0181 expectedFpp: 6.426078E-13
       Stripe level merge: numHashFunctions: 7 bitCount: 9600 popCount: 181 loadFactor: 0.0189 expectedFpp: 8.4693775E-13
 
-File length: 38568 bytes
+File length: 37078 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
@@ -499,7 +499,7 @@ PREHOOK: Input: default@orc_ppd_part@ds=2015/hr=10
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 1049
 Compression: ZLIB
 Compression size: 262144
@@ -535,49 +535,49 @@ File Statistics:
   Column 11: count: 1049 hasNull: false sum: 13278
 
 Stripes:
-  Stripe: offset: 3 data: 22593 rows: 1049 tail: 250 index: 9943
+  Stripe: offset: 3 data: 22593 rows: 1049 tail: 250 index: 8963
     Stream: column 0 section ROW_INDEX start: 3 length 20
-    Stream: column 0 section BLOOM_FILTER start: 23 length 45
-    Stream: column 1 section ROW_INDEX start: 68 length 58
-    Stream: column 1 section BLOOM_FILTER start: 126 length 799
-    Stream: column 2 section ROW_INDEX start: 925 length 58
-    Stream: column 2 section BLOOM_FILTER start: 983 length 978
-    Stream: column 3 section ROW_INDEX start: 1961 length 61
-    Stream: column 3 section BLOOM_FILTER start: 2022 length 983
-    Stream: column 4 section ROW_INDEX start: 3005 length 69
-    Stream: column 4 section BLOOM_FILTER start: 3074 length 963
-    Stream: column 5 section ROW_INDEX start: 4037 length 78
-    Stream: column 5 section BLOOM_FILTER start: 4115 length 1291
-    Stream: column 6 section ROW_INDEX start: 5406 length 85
-    Stream: column 6 section BLOOM_FILTER start: 5491 length 1280
-    Stream: column 7 section ROW_INDEX start: 6771 length 41
-    Stream: column 7 section BLOOM_FILTER start: 6812 length 45
-    Stream: column 8 section ROW_INDEX start: 6857 length 86
-    Stream: column 8 section BLOOM_FILTER start: 6943 length 1157
-    Stream: column 9 section ROW_INDEX start: 8100 length 50
-    Stream: column 9 section BLOOM_FILTER start: 8150 length 62
-    Stream: column 10 section ROW_INDEX start: 8212 length 82
-    Stream: column 10 section BLOOM_FILTER start: 8294 length 1297
-    Stream: column 11 section ROW_INDEX start: 9591 length 47
-    Stream: column 11 section BLOOM_FILTER start: 9638 length 308
-    Stream: column 1 section PRESENT start: 9946 length 17
-    Stream: column 1 section DATA start: 9963 length 962
-    Stream: column 2 section PRESENT start: 10925 length 17
-    Stream: column 2 section DATA start: 10942 length 1441
-    Stream: column 3 section DATA start: 12383 length 1704
-    Stream: column 4 section DATA start: 14087 length 1998
-    Stream: column 5 section DATA start: 16085 length 2925
-    Stream: column 6 section DATA start: 19010 length 3323
-    Stream: column 7 section DATA start: 22333 length 137
-    Stream: column 8 section DATA start: 22470 length 1572
-    Stream: column 8 section LENGTH start: 24042 length 310
-    Stream: column 8 section DICTIONARY_DATA start: 24352 length 1548
-    Stream: column 9 section DATA start: 25900 length 19
-    Stream: column 9 section SECONDARY start: 25919 length 1783
-    Stream: column 10 section DATA start: 27702 length 2138
-    Stream: column 10 section SECONDARY start: 29840 length 231
-    Stream: column 11 section DATA start: 30071 length 1877
-    Stream: column 11 section LENGTH start: 31948 length 591
+    Stream: column 0 section BLOOM_FILTER_UTF8 start: 23 length 34
+    Stream: column 1 section ROW_INDEX start: 57 length 58
+    Stream: column 1 section BLOOM_FILTER_UTF8 start: 115 length 696
+    Stream: column 2 section ROW_INDEX start: 811 length 58
+    Stream: column 2 section BLOOM_FILTER_UTF8 start: 869 length 867
+    Stream: column 3 section ROW_INDEX start: 1736 length 61
+    Stream: column 3 section BLOOM_FILTER_UTF8 start: 1797 length 861
+    Stream: column 4 section ROW_INDEX start: 2658 length 69
+    Stream: column 4 section BLOOM_FILTER_UTF8 start: 2727 length 850
+    Stream: column 5 section ROW_INDEX start: 3577 length 78
+    Stream: column 5 section BLOOM_FILTER_UTF8 start: 3655 length 1172
+    Stream: column 6 section ROW_INDEX start: 4827 length 85
+    Stream: column 6 section BLOOM_FILTER_UTF8 start: 4912 length 1167
+    Stream: column 7 section ROW_INDEX start: 6079 length 41
+    Stream: column 7 section BLOOM_FILTER_UTF8 start: 6120 length 34
+    Stream: column 8 section ROW_INDEX start: 6154 length 86
+    Stream: column 8 section BLOOM_FILTER_UTF8 start: 6240 length 1051
+    Stream: column 9 section ROW_INDEX start: 7291 length 50
+    Stream: column 9 section BLOOM_FILTER_UTF8 start: 7341 length 53
+    Stream: column 10 section ROW_INDEX start: 7394 length 82
+    Stream: column 10 section BLOOM_FILTER_UTF8 start: 7476 length 1189
+    Stream: column 11 section ROW_INDEX start: 8665 length 47
+    Stream: column 11 section BLOOM_FILTER_UTF8 start: 8712 length 254
+    Stream: column 1 section PRESENT start: 8966 length 17
+    Stream: column 1 section DATA start: 8983 length 962
+    Stream: column 2 section PRESENT start: 9945 length 17
+    Stream: column 2 section DATA start: 9962 length 1441
+    Stream: column 3 section DATA start: 11403 length 1704
+    Stream: column 4 section DATA start: 13107 length 1998
+    Stream: column 5 section DATA start: 15105 length 2925
+    Stream: column 6 section DATA start: 18030 length 3323
+    Stream: column 7 section DATA start: 21353 length 137
+    Stream: column 8 section DATA start: 21490 length 1572
+    Stream: column 8 section LENGTH start: 23062 length 310
+    Stream: column 8 section DICTIONARY_DATA start: 23372 length 1548
+    Stream: column 9 section DATA start: 24920 length 19
+    Stream: column 9 section SECONDARY start: 24939 length 1783
+    Stream: column 10 section DATA start: 26722 length 2138
+    Stream: column 10 section SECONDARY start: 28860 length 231
+    Stream: column 11 section DATA start: 29091 length 1877
+    Stream: column 11 section LENGTH start: 30968 length 591
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT
     Encoding column 2: DIRECT_V2
@@ -675,7 +675,7 @@ Stripes:
       Entry 1: numHashFunctions: 4 bitCount: 6272 popCount: 98 loadFactor: 0.0156 expectedFpp: 5.9604645E-8
       Stripe level merge: numHashFunctions: 4 bitCount: 6272 popCount: 102 loadFactor: 0.0163 expectedFpp: 6.9948186E-8
 
-File length: 33416 bytes
+File length: 32435 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
diff --git a/ql/src/test/results/clientpositive/orc_merge10.q.out b/ql/src/test/results/clientpositive/orc_merge10.q.out
index 8d4cb0d6f2..61c6cdc1d9 100644
--- a/ql/src/test/results/clientpositive/orc_merge10.q.out
+++ b/ql/src/test/results/clientpositive/orc_merge10.q.out
@@ -501,7 +501,7 @@ PREHOOK: Input: default@orcfile_merge1@ds=1/part=0
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 242
 Compression: SNAPPY
 Compression size: 4096
@@ -540,13 +540,13 @@ Stripes:
       Entry 0: count: 152 hasNull: false min: 0 max: 497 sum: 38034 positions: 0,0,0
     Row group indices for column 2:
       Entry 0: count: 152 hasNull: false min: val_0 max: val_97 sum: 1034 positions: 0,0,0
-  Stripe: offset: 1140 data: 616 rows: 90 tail: 61 index: 76
+  Stripe: offset: 1140 data: 613 rows: 90 tail: 61 index: 76
     Stream: column 0 section ROW_INDEX start: 1140 length 11
     Stream: column 1 section ROW_INDEX start: 1151 length 27
     Stream: column 2 section ROW_INDEX start: 1178 length 38
     Stream: column 1 section DATA start: 1216 length 185
     Stream: column 2 section DATA start: 1401 length 377
-    Stream: column 2 section LENGTH start: 1778 length 54
+    Stream: column 2 section LENGTH start: 1778 length 51
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -557,7 +557,7 @@ Stripes:
     Row group indices for column 2:
       Entry 0: count: 90 hasNull: false min: val_0 max: val_86 sum: 612 positions: 0,0,0,0,0
 
-File length: 2137 bytes
+File length: 2134 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
@@ -571,7 +571,7 @@ PREHOOK: Input: default@orcfile_merge1c@ds=1/part=0
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 242
 Compression: SNAPPY
 Compression size: 4096
@@ -610,13 +610,13 @@ Stripes:
       Entry 0: count: 152 hasNull: false min: 0 max: 497 sum: 38034 positions: 0,0,0
     Row group indices for column 2:
       Entry 0: count: 152 hasNull: false min: val_0 max: val_97 sum: 1034 positions: 0,0,0
-  Stripe: offset: 1140 data: 616 rows: 90 tail: 61 index: 76
+  Stripe: offset: 1140 data: 613 rows: 90 tail: 61 index: 76
     Stream: column 0 section ROW_INDEX start: 1140 length 11
     Stream: column 1 section ROW_INDEX start: 1151 length 27
     Stream: column 2 section ROW_INDEX start: 1178 length 38
     Stream: column 1 section DATA start: 1216 length 185
     Stream: column 2 section DATA start: 1401 length 377
-    Stream: column 2 section LENGTH start: 1778 length 54
+    Stream: column 2 section LENGTH start: 1778 length 51
     Encoding column 0: DIRECT
     Encoding column 1: DIRECT_V2
     Encoding column 2: DIRECT_V2
@@ -627,7 +627,7 @@ Stripes:
     Row group indices for column 2:
       Entry 0: count: 90 hasNull: false min: val_0 max: val_86 sum: 612 positions: 0,0,0,0,0
 
-File length: 2137 bytes
+File length: 2134 bytes
 Padding length: 0 bytes
 Padding ratio: 0%
 ________________________________________________________________________________________________________________________
diff --git a/ql/src/test/results/clientpositive/orc_merge11.q.out b/ql/src/test/results/clientpositive/orc_merge11.q.out
index a8ab854646..9c50f6dec9 100644
--- a/ql/src/test/results/clientpositive/orc_merge11.q.out
+++ b/ql/src/test/results/clientpositive/orc_merge11.q.out
@@ -72,7 +72,7 @@ PREHOOK: Input: default@orcfile_merge1
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 50000
 Compression: ZLIB
 Compression size: 4096
@@ -163,7 +163,7 @@ ________________________________________________________________________________
 -- END ORC FILE DUMP --
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 50000
 Compression: ZLIB
 Compression size: 4096
@@ -275,7 +275,7 @@ PREHOOK: Input: default@orcfile_merge1
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 100000
 Compression: ZLIB
 Compression size: 4096
diff --git a/ql/src/test/results/clientpositive/orc_merge12.q.out b/ql/src/test/results/clientpositive/orc_merge12.q.out
index 6a86fcf7d7..2c49e0f865 100644
--- a/ql/src/test/results/clientpositive/orc_merge12.q.out
+++ b/ql/src/test/results/clientpositive/orc_merge12.q.out
@@ -144,7 +144,7 @@ PREHOOK: Input: default@alltypesorc3xcols
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 24576
 Compression: ZLIB
 Compression size: 262144
diff --git a/ql/src/test/results/clientpositive/orc_remove_cols.q.out b/ql/src/test/results/clientpositive/orc_remove_cols.q.out
index 178a9747fc..c09046686e 100644
--- a/ql/src/test/results/clientpositive/orc_remove_cols.q.out
+++ b/ql/src/test/results/clientpositive/orc_remove_cols.q.out
@@ -34,11 +34,11 @@ POSTHOOK: query: ALTER TABLE orc_partitioned SET SERDE 'org.apache.hadoop.hive.s
 POSTHOOK: type: ALTERTABLE_SERIALIZER
 POSTHOOK: Input: default@orc_partitioned
 POSTHOOK: Output: default@orc_partitioned
-PREHOOK: query: ALTER TABLE orc_partitioned REPLACE COLUMNS (cint int)
+PREHOOK: query: ALTER TABLE orc_partitioned REPLACE COLUMNS (a int)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@orc_partitioned
 PREHOOK: Output: default@orc_partitioned
-POSTHOOK: query: ALTER TABLE orc_partitioned REPLACE COLUMNS (cint int)
+POSTHOOK: query: ALTER TABLE orc_partitioned REPLACE COLUMNS (a int)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@orc_partitioned
 POSTHOOK: Output: default@orc_partitioned
diff --git a/ql/src/test/results/clientpositive/orc_schema_evolution.q.out b/ql/src/test/results/clientpositive/orc_schema_evolution.q.out
index b536a754dc..b2f4d9df34 100644
--- a/ql/src/test/results/clientpositive/orc_schema_evolution.q.out
+++ b/ql/src/test/results/clientpositive/orc_schema_evolution.q.out
@@ -103,11 +103,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc2
 #### A masked pattern was here ####
 36214430891
-PREHOOK: query: alter table src_orc2 replace columns (k smallint, v string)
+PREHOOK: query: alter table src_orc2 replace columns (key smallint, val string)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@src_orc2
 PREHOOK: Output: default@src_orc2
-POSTHOOK: query: alter table src_orc2 replace columns (k smallint, v string)
+POSTHOOK: query: alter table src_orc2 replace columns (key smallint, val string)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@src_orc2
 POSTHOOK: Output: default@src_orc2
@@ -120,11 +120,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc2
 #### A masked pattern was here ####
 36214430891
-PREHOOK: query: alter table src_orc2 replace columns (k int, v string)
+PREHOOK: query: alter table src_orc2 replace columns (key int, val string)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@src_orc2
 PREHOOK: Output: default@src_orc2
-POSTHOOK: query: alter table src_orc2 replace columns (k int, v string)
+POSTHOOK: query: alter table src_orc2 replace columns (key int, val string)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@src_orc2
 POSTHOOK: Output: default@src_orc2
@@ -137,11 +137,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc2
 #### A masked pattern was here ####
 36214430891
-PREHOOK: query: alter table src_orc2 replace columns (k bigint, v string)
+PREHOOK: query: alter table src_orc2 replace columns (key bigint, val string)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@src_orc2
 PREHOOK: Output: default@src_orc2
-POSTHOOK: query: alter table src_orc2 replace columns (k bigint, v string)
+POSTHOOK: query: alter table src_orc2 replace columns (key bigint, val string)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@src_orc2
 POSTHOOK: Output: default@src_orc2
@@ -154,11 +154,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc2
 #### A masked pattern was here ####
 36214430891
-PREHOOK: query: alter table src_orc2 replace columns (k bigint, v string, z int)
+PREHOOK: query: alter table src_orc2 replace columns (key bigint, val string, z int)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@src_orc2
 PREHOOK: Output: default@src_orc2
-POSTHOOK: query: alter table src_orc2 replace columns (k bigint, v string, z int)
+POSTHOOK: query: alter table src_orc2 replace columns (key bigint, val string, z int)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@src_orc2
 POSTHOOK: Output: default@src_orc2
@@ -171,11 +171,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc2
 #### A masked pattern was here ####
 -586749626187
-PREHOOK: query: alter table src_orc2 replace columns (k bigint, v string, z bigint)
+PREHOOK: query: alter table src_orc2 replace columns (key bigint, val string, z bigint)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@src_orc2
 PREHOOK: Output: default@src_orc2
-POSTHOOK: query: alter table src_orc2 replace columns (k bigint, v string, z bigint)
+POSTHOOK: query: alter table src_orc2 replace columns (key bigint, val string, z bigint)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@src_orc2
 POSTHOOK: Output: default@src_orc2
@@ -188,11 +188,11 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@src_orc2
 #### A masked pattern was here ####
 -586749626187
-PREHOOK: query: alter table src_orc2 replace columns (k bigint, v string, z bigint, y float)
+PREHOOK: query: alter table src_orc2 replace columns (key bigint, val string, z bigint, y float)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@src_orc2
 PREHOOK: Output: default@src_orc2
-POSTHOOK: query: alter table src_orc2 replace columns (k bigint, v string, z bigint, y float)
+POSTHOOK: query: alter table src_orc2 replace columns (key bigint, val string, z bigint, y float)
 POSTHOOK: type: ALTERTABLE_REPLACECOLS
 POSTHOOK: Input: default@src_orc2
 POSTHOOK: Output: default@src_orc2
diff --git a/ql/src/test/results/clientpositive/tez/orc_merge12.q.out b/ql/src/test/results/clientpositive/tez/orc_merge12.q.out
index 6a86fcf7d7..2c49e0f865 100644
--- a/ql/src/test/results/clientpositive/tez/orc_merge12.q.out
+++ b/ql/src/test/results/clientpositive/tez/orc_merge12.q.out
@@ -144,7 +144,7 @@ PREHOOK: Input: default@alltypesorc3xcols
 #### A masked pattern was here ####
 -- BEGIN ORC FILE DUMP --
 #### A masked pattern was here ####
-File Version: 0.12 with HIVE_13083
+File Version: 0.12 with ORC_101
 Rows: 24576
 Compression: ZLIB
 Compression size: 262144
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java b/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java
index db0a582f45..6d8c83bff7 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java
@@ -55,6 +55,14 @@ public static final class PredicateLeafImpl implements PredicateLeaf {
       literalList = null;
     }
 
+    public PredicateLeafImpl(Operator operator,
+                             Type type,
+                             String columnName,
+                             Object literal,
+                             List<Object> literalList) {
+      this(operator, type, columnName, literal, literalList, null);
+    }
+
     public PredicateLeafImpl(Operator operator,
                              Type type,
                              String columnName,
diff --git a/testutils/ptest2/conf/deployed/master-mr2.properties b/testutils/ptest2/conf/deployed/master-mr2.properties
index 375d50dac2..05c0405991 100644
--- a/testutils/ptest2/conf/deployed/master-mr2.properties
+++ b/testutils/ptest2/conf/deployed/master-mr2.properties
@@ -39,9 +39,6 @@ ut.hbase-handler.batchSize=0
 unitTests.module.metastore=metastore
 ut.metastore.batchSize=12
 
-unitTests.module.orc=orc
-ut.orc.batchSize=10
-
 unitTests.module.ql=ql
 ut.ql.batchSize=10
 ut.ql.skipBatching=TestDbTxnManager2 TestTxnCommands2WithSplitUpdateAndVectorization TestTxnCommands2WithSplitUpdate TestOrcFile TestTxnCommands2 TestCompactor TestInitiator TestWorker TestWorker2 TestExecDriver
diff --git a/testutils/ptest2/src/test/resources/test-configuration2.properties b/testutils/ptest2/src/test/resources/test-configuration2.properties
index 0ba2f3bfbf..5f15a59fc7 100644
--- a/testutils/ptest2/src/test/resources/test-configuration2.properties
+++ b/testutils/ptest2/src/test/resources/test-configuration2.properties
@@ -69,9 +69,6 @@ ut.hbase-handler.batchSize=0
 unitTests.module.metastore=metastore
 ut.metastore.batchSize=12
 
-unitTests.module.orc=orc
-ut.orc.batchSize=0
-
 unitTests.module.service=service
 ut.service.batchSize=8
 
