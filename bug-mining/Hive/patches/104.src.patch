diff --git a/CHANGES.txt b/CHANGES.txt
index c47d4800e0..1ce054ba2a 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -117,6 +117,9 @@ Release 0.3.0 - Unreleased
     HIVE-375. LazySimpleSerDe directly creates a UTF8 buffer for primitive types.
     (Zheng Shao via namit)
 
+    HIVE-382 Fix for hash aggr, remove elements from hash table in the loop
+    during close, rather than waiting till the end. (Namit Jain via rmurthy)
+
 Release 0.2.0 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/data/scripts/dumpdata_script.py b/data/scripts/dumpdata_script.py
new file mode 100644
index 0000000000..19be0cbfbf
--- /dev/null
+++ b/data/scripts/dumpdata_script.py
@@ -0,0 +1,11 @@
+for i in xrange(100):
+   for j in xrange(10):
+      for k in xrange(42022):      
+         print 42000 * i + k
+
+
+for i in xrange(100):
+   for j in xrange(10):
+      for k in xrange(42022):      
+         print 5000000 + (42000 * i) + k
+
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index a5ee515d6b..40ca77e60a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -240,7 +240,7 @@ private void computeMaxEntriesHashAggr(Configuration hconf) {
     estimateRowSize();
   }
 
-  private static final int javaObjectOverHead    = 16;
+  private static final int javaObjectOverHead    = 64;
   private static final int javaHashEntryOverHead = 64;
   private static final int javaSizePrimitiveType = 16;
   private static final int javaSizeUnknownType   = 256;
@@ -579,11 +579,12 @@ private void flush(boolean complete) throws HiveException {
       }
       hashAggregations.clear();
       hashAggregations = null;
+      LOG.warn("Hash Table completed flushed");
       return;
     }
 
     int oldSize = hashAggregations.size();
-    LOG.trace("Hash Tbl flush: #hash table = " + oldSize);
+    LOG.warn("Hash Tbl flush: #hash table = " + oldSize);
     Iterator iter = hashAggregations.entrySet().iterator();
     int numDel = 0;
     while (iter.hasNext()) {
@@ -591,8 +592,10 @@ private void flush(boolean complete) throws HiveException {
       forward(m.getKey(), m.getValue());
       iter.remove();
       numDel++;
-      if (numDel * 10 >= oldSize)
+      if (numDel * 10 >= oldSize) {
+        LOG.warn("Hash Table flushed: new size = " + hashAggregations.size());
         return;
+      }
     }
   }
 
@@ -646,9 +649,12 @@ public void close(boolean abort) throws HiveException {
         }
         else {
           if (hashAggregations != null) {
-            // hash-based aggregations
-            for (ArrayList<Object> key: hashAggregations.keySet()) {
-              forward(key, hashAggregations.get(key));
+            LOG.warn("Begin Hash Table flush at close: size = " + hashAggregations.size());
+            Iterator iter = hashAggregations.entrySet().iterator();
+            while (iter.hasNext()) {
+              Map.Entry<ArrayList<Object>, UDAFEvaluator[]> m = (Map.Entry)iter.next();
+              forward(m.getKey(), m.getValue());
+              iter.remove();
             }
             hashAggregations.clear();
           }
diff --git a/ql/src/test/queries/clientpositive/groupby_bigdata.q b/ql/src/test/queries/clientpositive/groupby_bigdata.q
new file mode 100644
index 0000000000..d45510707e
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/groupby_bigdata.q
@@ -0,0 +1,4 @@
+set hive.map.aggr.hash.percentmemory = 0.4;
+
+select count(distinct subq.key) from
+(FROM src MAP src.key USING 'python ../data/scripts/dumpdata_script.py' AS key WHERE src.key = 10) subq;
diff --git a/ql/src/test/results/clientpositive/groupby_bigdata.q.out b/ql/src/test/results/clientpositive/groupby_bigdata.q.out
new file mode 100644
index 0000000000..47d71862de
--- /dev/null
+++ b/ql/src/test/results/clientpositive/groupby_bigdata.q.out
@@ -0,0 +1 @@
+8400044
