diff --git a/ant/src/org/apache/hadoop/hive/ant/GenVectorCode.java b/ant/src/org/apache/hadoop/hive/ant/GenVectorCode.java
index 2e369ec1be..6c6cc63394 100644
--- a/ant/src/org/apache/hadoop/hive/ant/GenVectorCode.java
+++ b/ant/src/org/apache/hadoop/hive/ant/GenVectorCode.java
@@ -77,15 +77,6 @@ public class GenVectorCode extends Task {
       {"DTIScalarArithmeticDTIColumnNoConvert", "Subtract", "interval_year_month", "interval_year_month", "-"},
       {"DTIColumnArithmeticDTIColumnNoConvert", "Subtract", "interval_year_month", "interval_year_month", "-"},
 
-      // Arithmetic on two TimestampColumnVector base classes.
-      {"TimestampArithmeticTimestampBase", "Add", "Col", "Column"},
-      {"TimestampArithmeticTimestampBase", "Add", "Scalar", "Column"},
-      {"TimestampArithmeticTimestampBase", "Add", "Col", "Scalar"},
-
-      {"TimestampArithmeticTimestampBase", "Subtract", "Col", "Column"},
-      {"TimestampArithmeticTimestampBase", "Subtract", "Scalar", "Column"},
-      {"TimestampArithmeticTimestampBase", "Subtract", "Col", "Scalar"},
-
       // Arithmetic on two type interval_day_time (TimestampColumnVector storing nanosecond interval
       // in 2 longs) produces a interval_day_time.
       {"TimestampArithmeticTimestamp", "Add", "interval_day_time", "Col", "interval_day_time", "Scalar"},
@@ -111,30 +102,13 @@ public class GenVectorCode extends Task {
       {"TimestampArithmeticTimestamp", "Subtract", "timestamp", "Col", "interval_day_time", "Column"},
 
       // A type timestamp (TimestampColumnVector) minus a type timestamp produces a
-      // type interval_day_time (TimestampColumnVector storing nanosecond interval in 2 longs).
+      // type interval_day_time (IntervalDayTimeColumnVector storing nanosecond interval in 2 primitives).
       {"TimestampArithmeticTimestamp", "Subtract", "timestamp", "Col", "timestamp", "Scalar"},
       {"TimestampArithmeticTimestamp", "Subtract", "timestamp", "Scalar", "timestamp", "Column"},
       {"TimestampArithmeticTimestamp", "Subtract", "timestamp", "Col", "timestamp", "Column"},
 
-      // Arithmetic on a TimestampColumnVector and date base classes.
-      {"DateArithmeticTimestampBase", "Add", "Col", "Column"},
-      {"DateArithmeticTimestampBase", "Add", "Scalar", "Column"},
-      {"DateArithmeticTimestampBase", "Add", "Col", "Scalar"},
-
-      {"DateArithmeticTimestampBase", "Subtract", "Col", "Column"},
-      {"DateArithmeticTimestampBase", "Subtract", "Scalar", "Column"},
-      {"DateArithmeticTimestampBase", "Subtract", "Col", "Scalar"},
-
-      {"TimestampArithmeticDateBase", "Add", "Col", "Column"},
-      {"TimestampArithmeticDateBase", "Add", "Scalar", "Column"},
-      {"TimestampArithmeticDateBase", "Add", "Col", "Scalar"},
-
-      {"TimestampArithmeticDateBase", "Subtract", "Col", "Column"},
-      {"TimestampArithmeticDateBase", "Subtract", "Scalar", "Column"},
-      {"TimestampArithmeticDateBase", "Subtract", "Col", "Scalar"},
-
-      // Arithmetic with a type date (LongColumnVector storing epoch days) and type interval_day_time (TimestampColumnVector storing
-      // nanosecond interval in 2 longs) produces a type timestamp (TimestampColumnVector).
+      // Arithmetic with a type date (LongColumnVector storing epoch days) and type interval_day_time (IntervalDayTimeColumnVector storing
+      // nanosecond interval in 2 primitives) produces a type timestamp (TimestampColumnVector).
       {"DateArithmeticTimestamp", "Add", "date", "Col", "interval_day_time", "Column"},
       {"DateArithmeticTimestamp", "Add", "date", "Scalar", "interval_day_time", "Column"},
       {"DateArithmeticTimestamp", "Add", "date", "Col", "interval_day_time", "Scalar"},
@@ -147,7 +121,8 @@ public class GenVectorCode extends Task {
       {"TimestampArithmeticDate", "Add", "interval_day_time", "Scalar", "date", "Column"},
       {"TimestampArithmeticDate", "Add", "interval_day_time", "Col", "date", "Scalar"},
 
-      // Subtraction with a type date (LongColumnVector storing epoch days) and type timestamp produces a type timestamp (TimestampColumnVector).
+      // Subtraction with a type date (LongColumnVector storing days) and type timestamp produces a
+      // type interval_day_time (IntervalDayTimeColumnVector).
       {"DateArithmeticTimestamp", "Subtract", "date", "Col", "timestamp", "Column"},
       {"DateArithmeticTimestamp", "Subtract", "date", "Scalar", "timestamp", "Column"},
       {"DateArithmeticTimestamp", "Subtract", "date", "Col", "timestamp", "Scalar"},
@@ -318,70 +293,48 @@ public class GenVectorCode extends Task {
       {"ScalarCompareColumn", "Greater", "double", "long", ">"},
       {"ScalarCompareColumn", "GreaterEqual", "double", "long", ">="},
 
-      // Base compare timestamp to timestamp used by Timestamp and IntervalDayTime.
-      {"TimestampCompareTimestampBase", "Equal", "==", "Col", "Column"},
-      {"TimestampCompareTimestampBase", "NotEqual", "!=", "Col", "Column"},
-      {"TimestampCompareTimestampBase", "Less", "<", "Col", "Column"},
-      {"TimestampCompareTimestampBase", "LessEqual", "<=", "Col", "Column"},
-      {"TimestampCompareTimestampBase", "Greater", ">", "Col", "Column"},
-      {"TimestampCompareTimestampBase", "GreaterEqual", ">=", "Col", "Column"},
-
-      {"TimestampCompareTimestampBase", "Equal", "==", "Col", "Scalar"},
-      {"TimestampCompareTimestampBase", "NotEqual", "!=", "Col", "Scalar"},
-      {"TimestampCompareTimestampBase", "Less", "<", "Col", "Scalar"},
-      {"TimestampCompareTimestampBase", "LessEqual", "<=", "Col", "Scalar"},
-      {"TimestampCompareTimestampBase", "Greater", ">", "Col", "Scalar"},
-      {"TimestampCompareTimestampBase", "GreaterEqual", ">=", "Col", "Scalar"},
-
-      {"TimestampCompareTimestampBase", "Equal", "==", "Scalar", "Column"},
-      {"TimestampCompareTimestampBase", "NotEqual", "!=", "Scalar", "Column"},
-      {"TimestampCompareTimestampBase", "Less", "<", "Scalar", "Column"},
-      {"TimestampCompareTimestampBase", "LessEqual", "<=", "Scalar", "Column"},
-      {"TimestampCompareTimestampBase", "Greater", ">", "Scalar", "Column"},
-      {"TimestampCompareTimestampBase", "GreaterEqual", ">=", "Scalar", "Column"},
-
       // Compare timestamp to timestamp.
-      {"TimestampCompareTimestamp", "Equal", "timestamp", "Col", "Column"},
-      {"TimestampCompareTimestamp", "NotEqual", "timestamp", "Col", "Column"},
-      {"TimestampCompareTimestamp", "Less", "timestamp", "Col", "Column"},
-      {"TimestampCompareTimestamp", "LessEqual", "timestamp", "Col", "Column"},
-      {"TimestampCompareTimestamp", "Greater", "timestamp", "Col", "Column"},
-      {"TimestampCompareTimestamp", "GreaterEqual", "timestamp", "Col", "Column"},
-
-      {"TimestampCompareTimestamp", "Equal", "timestamp", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "NotEqual", "timestamp", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "Less", "timestamp", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "LessEqual", "timestamp", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "Greater", "timestamp", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "GreaterEqual", "timestamp", "Col", "Scalar"},
-
-      {"TimestampCompareTimestamp", "Equal", "timestamp", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "NotEqual", "timestamp", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "Less", "timestamp", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "LessEqual", "timestamp", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "Greater", "timestamp", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "GreaterEqual", "timestamp", "Scalar", "Column"},
-
-      {"TimestampCompareTimestamp", "Equal", "interval_day_time", "Col", "Column"},
-      {"TimestampCompareTimestamp", "NotEqual", "interval_day_time", "Col", "Column"},
-      {"TimestampCompareTimestamp", "Less", "interval_day_time", "Col", "Column"},
-      {"TimestampCompareTimestamp", "LessEqual", "interval_day_time", "Col", "Column"},
-      {"TimestampCompareTimestamp", "Greater", "interval_day_time", "Col", "Column"},
-      {"TimestampCompareTimestamp", "GreaterEqual", "interval_day_time", "Col", "Column"},
-
-      {"TimestampCompareTimestamp", "Equal", "interval_day_time", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "NotEqual", "interval_day_time", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "Less", "interval_day_time", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "LessEqual", "interval_day_time", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "Greater", "interval_day_time", "Col", "Scalar"},
-      {"TimestampCompareTimestamp", "GreaterEqual", "interval_day_time", "Col", "Scalar"},
-
-      {"TimestampCompareTimestamp", "Equal", "interval_day_time", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "NotEqual", "interval_day_time", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "Less", "interval_day_time", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "LessEqual", "interval_day_time", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "Greater", "interval_day_time", "Scalar", "Column"},
-      {"TimestampCompareTimestamp", "GreaterEqual", "interval_day_time", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "Equal", "==", "timestamp", "Col", "Column"},
+      {"TimestampCompareTimestamp", "NotEqual", "!=", "timestamp", "Col", "Column"},
+      {"TimestampCompareTimestamp", "Less", "<", "timestamp", "Col", "Column"},
+      {"TimestampCompareTimestamp", "LessEqual", "<=", "timestamp", "Col", "Column"},
+      {"TimestampCompareTimestamp", "Greater", ">", "timestamp", "Col", "Column"},
+      {"TimestampCompareTimestamp", "GreaterEqual", ">=", "timestamp", "Col", "Column"},
+
+      {"TimestampCompareTimestamp", "Equal", "==", "timestamp", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "NotEqual", "!=", "timestamp", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "Less", "<", "timestamp", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "LessEqual", "<=", "timestamp", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "Greater", ">", "timestamp", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "GreaterEqual", ">=", "timestamp", "Col", "Scalar"},
+
+      {"TimestampCompareTimestamp", "Equal", "==", "timestamp", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "NotEqual", "!=", "timestamp", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "Less", "<", "timestamp", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "LessEqual", "<=", "timestamp", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "Greater", ">", "timestamp", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "GreaterEqual", ">=", "timestamp", "Scalar", "Column"},
+
+      {"TimestampCompareTimestamp", "Equal", "==", "interval_day_time", "Col", "Column"},
+      {"TimestampCompareTimestamp", "NotEqual", "!=", "interval_day_time", "Col", "Column"},
+      {"TimestampCompareTimestamp", "Less", "<", "interval_day_time", "Col", "Column"},
+      {"TimestampCompareTimestamp", "LessEqual", "<=", "interval_day_time", "Col", "Column"},
+      {"TimestampCompareTimestamp", "Greater", ">", "interval_day_time", "Col", "Column"},
+      {"TimestampCompareTimestamp", "GreaterEqual", ">=", "interval_day_time", "Col", "Column"},
+
+      {"TimestampCompareTimestamp", "Equal", "==", "interval_day_time", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "NotEqual", "!=", "interval_day_time", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "Less", "<", "interval_day_time", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "LessEqual", "<=", "interval_day_time", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "Greater", ">", "interval_day_time", "Col", "Scalar"},
+      {"TimestampCompareTimestamp", "GreaterEqual", ">=", "interval_day_time", "Col", "Scalar"},
+
+      {"TimestampCompareTimestamp", "Equal", "==", "interval_day_time", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "NotEqual", "!=", "interval_day_time", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "Less", "<", "interval_day_time", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "LessEqual", "<=", "interval_day_time", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "Greater", ">", "interval_day_time", "Scalar", "Column"},
+      {"TimestampCompareTimestamp", "GreaterEqual", ">=", "interval_day_time", "Scalar", "Column"},
 
       // Compare timestamp to integer seconds or double seconds with fractional nanoseonds.
       {"TimestampCompareLongDouble", "Equal", "long", "==", "Col", "Column"},
@@ -515,71 +468,49 @@ public class GenVectorCode extends Task {
       {"FilterScalarCompareColumn", "GreaterEqual", "long", "long", ">="},
       {"FilterScalarCompareColumn", "GreaterEqual", "double", "long", ">="},
 
-      // Base filter timestamp against timestamp used by Timestamp and IntervalDayTime.
-      {"FilterTimestampCompareTimestampBase", "Equal", "==", "Col", "Column"},
-      {"FilterTimestampCompareTimestampBase", "NotEqual", "!=", "Col", "Column"},
-      {"FilterTimestampCompareTimestampBase", "Less", "<", "Col", "Column"},
-      {"FilterTimestampCompareTimestampBase", "LessEqual", "<=", "Col", "Column"},
-      {"FilterTimestampCompareTimestampBase", "Greater", ">", "Col", "Column"},
-      {"FilterTimestampCompareTimestampBase", "GreaterEqual", ">=", "Col", "Column"},
-
-      {"FilterTimestampCompareTimestampBase", "Equal", "==", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestampBase", "NotEqual", "!=", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestampBase", "Less", "<", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestampBase", "LessEqual", "<=", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestampBase", "Greater", ">", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestampBase", "GreaterEqual", ">=", "Col", "Scalar"},
-
-      {"FilterTimestampCompareTimestampBase", "Equal", "==", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestampBase", "NotEqual", "!=", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestampBase", "Less", "<", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestampBase", "LessEqual", "<=", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestampBase", "Greater", ">", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestampBase", "GreaterEqual", ">=", "Scalar", "Column"},
-
       // Filter timestamp against timestamp, or interval day time against interval day time.
 
-      {"FilterTimestampCompareTimestamp", "Equal", "timestamp", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "NotEqual", "timestamp", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "Less", "timestamp", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "LessEqual", "timestamp", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "Greater", "timestamp", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "GreaterEqual", "timestamp", "Col", "Column"},
-
-      {"FilterTimestampCompareTimestamp", "Equal", "timestamp", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "NotEqual", "timestamp", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "Less", "timestamp", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "LessEqual", "timestamp", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "Greater", "timestamp", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "GreaterEqual", "timestamp", "Col", "Scalar"},
-
-      {"FilterTimestampCompareTimestamp", "Equal", "timestamp", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "NotEqual", "timestamp", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "Less", "timestamp", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "LessEqual", "timestamp", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "Greater", "timestamp", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "GreaterEqual", "timestamp", "Scalar", "Column"},
-
-      {"FilterTimestampCompareTimestamp", "Equal", "interval_day_time", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "NotEqual", "interval_day_time", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "Less", "interval_day_time", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "LessEqual", "interval_day_time", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "Greater", "interval_day_time", "Col", "Column"},
-      {"FilterTimestampCompareTimestamp", "GreaterEqual", "interval_day_time", "Col", "Column"},
-
-      {"FilterTimestampCompareTimestamp", "Equal", "interval_day_time", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "NotEqual", "interval_day_time", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "Less", "interval_day_time", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "LessEqual", "interval_day_time", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "Greater", "interval_day_time", "Col", "Scalar"},
-      {"FilterTimestampCompareTimestamp", "GreaterEqual", "interval_day_time", "Col", "Scalar"},
-
-      {"FilterTimestampCompareTimestamp", "Equal", "interval_day_time", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "NotEqual", "interval_day_time", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "Less", "interval_day_time", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "LessEqual", "interval_day_time", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "Greater", "interval_day_time", "Scalar", "Column"},
-      {"FilterTimestampCompareTimestamp", "GreaterEqual", "interval_day_time", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "Equal", "==", "timestamp", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "NotEqual", "!=", "timestamp", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "Less", "<", "timestamp", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "LessEqual", "<=", "timestamp", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "Greater", ">", "timestamp", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "GreaterEqual", ">=", "timestamp", "Col", "Column"},
+
+      {"FilterTimestampCompareTimestamp", "Equal", "==", "timestamp", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "NotEqual", "!=", "timestamp", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "Less", "<", "timestamp", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "LessEqual", "<=", "timestamp", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "Greater", ">", "timestamp", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "GreaterEqual", ">=", "timestamp", "Col", "Scalar"},
+
+      {"FilterTimestampCompareTimestamp", "Equal", "==", "timestamp", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "NotEqual", "!=", "timestamp", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "Less", "<", "timestamp", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "LessEqual", "<=", "timestamp", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "Greater", ">", "timestamp", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "GreaterEqual", ">=", "timestamp", "Scalar", "Column"},
+
+      {"FilterTimestampCompareTimestamp", "Equal", "==", "interval_day_time", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "NotEqual", "!=", "interval_day_time", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "Less", "<", "interval_day_time", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "LessEqual", "<=", "interval_day_time", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "Greater", ">", "interval_day_time", "Col", "Column"},
+      {"FilterTimestampCompareTimestamp", "GreaterEqual", ">=", "interval_day_time", "Col", "Column"},
+
+      {"FilterTimestampCompareTimestamp", "Equal", "==", "interval_day_time", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "NotEqual", "!=", "interval_day_time", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "Less", "<", "interval_day_time", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "LessEqual", "<=", "interval_day_time", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "Greater", ">", "interval_day_time", "Col", "Scalar"},
+      {"FilterTimestampCompareTimestamp", "GreaterEqual", ">=", "interval_day_time", "Col", "Scalar"},
+
+      {"FilterTimestampCompareTimestamp", "Equal", "==", "interval_day_time", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "NotEqual", "!=", "interval_day_time", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "Less", "<", "interval_day_time", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "LessEqual", "<=", "interval_day_time", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "Greater", ">", "interval_day_time", "Scalar", "Column"},
+      {"FilterTimestampCompareTimestamp", "GreaterEqual", ">=", "interval_day_time", "Scalar", "Column"},
 
       // Filter timestamp against long (seconds) or double (seconds with fractional
       // nanoseconds).
@@ -1057,6 +988,11 @@ public class GenVectorCode extends Task {
       {"VectorUDAFMinMaxTimestamp", "VectorUDAFMinTimestamp", ">", "min",
           "_FUNC_(expr) - Returns the minimum value of expr (vectorized, type: timestamp)"},
 
+      {"VectorUDAFMinMaxIntervalDayTime", "VectorUDAFMaxIntervalDayTime", "<", "max",
+          "_FUNC_(expr) - Returns the maximum value of expr (vectorized, type: interval_day_time)"},
+      {"VectorUDAFMinMaxIntervalDayTime", "VectorUDAFMinIntervalDayTime", ">", "min",
+          "_FUNC_(expr) - Returns the minimum value of expr (vectorized, type: interval_day_time)"},
+
         //template, <ClassName>, <ValueType>
         {"VectorUDAFSum", "VectorUDAFSumLong", "long"},
         {"VectorUDAFSum", "VectorUDAFSumDouble", "double"},
@@ -1202,9 +1138,6 @@ private void generate() throws Exception {
       } else if (tdesc[0].equals("ScalarCompareColumn")) {
         generateScalarCompareColumn(tdesc);
 
-      } else if (tdesc[0].equals("TimestampCompareTimestampBase")) {
-        generateTimestampCompareTimestampBase(tdesc);
-
       } else if (tdesc[0].equals("TimestampCompareTimestamp")) {
         generateTimestampCompareTimestamp(tdesc);
 
@@ -1219,9 +1152,6 @@ private void generate() throws Exception {
       } else if (tdesc[0].equals("FilterScalarCompareColumn")) {
         generateFilterScalarCompareColumn(tdesc);
 
-      } else if (tdesc[0].equals("FilterTimestampCompareTimestampBase")) {
-        generateFilterTimestampCompareTimestampBase(tdesc);
-
       } else if (tdesc[0].equals("FilterTimestampCompareTimestamp")) {
         generateFilterTimestampCompareTimestamp(tdesc);
 
@@ -1255,6 +1185,8 @@ private void generate() throws Exception {
         generateVectorUDAFMinMaxObject(tdesc);
       } else if (tdesc[0].equals("VectorUDAFMinMaxTimestamp")) {
         generateVectorUDAFMinMaxObject(tdesc);
+      } else if (tdesc[0].equals("VectorUDAFMinMaxIntervalDayTime")) {
+        generateVectorUDAFMinMaxObject(tdesc);
       } else if (tdesc[0].equals("VectorUDAFSum")) {
         generateVectorUDAFSum(tdesc);
       } else if (tdesc[0].equals("VectorUDAFAvg")) {
@@ -1338,21 +1270,12 @@ private void generate() throws Exception {
       } else if (tdesc[0].equals("IntervalYearMonthArithmeticTimestamp")) {
         generateDateTimeArithmeticIntervalYearMonth(tdesc);
 
-      } else if (tdesc[0].equals("TimestampArithmeticTimestampBase")) {
-        generateTimestampArithmeticTimestampBase(tdesc);
-
       } else if (tdesc[0].equals("TimestampArithmeticTimestamp")) {
         generateTimestampArithmeticTimestamp(tdesc);
 
-      } else if (tdesc[0].equals("DateArithmeticTimestampBase")) {
-        generateDateArithmeticTimestampBase(tdesc);
-
       } else if (tdesc[0].equals("DateArithmeticTimestamp")) {
         generateDateArithmeticTimestamp(tdesc);
 
-      } else if (tdesc[0].equals("TimestampArithmeticDateBase")) {
-        generateTimestampArithmeticDateBase(tdesc);
-
       } else if (tdesc[0].equals("TimestampArithmeticDate")) {
         generateTimestampArithmeticDate(tdesc);
 
@@ -2182,35 +2105,28 @@ private void generateColumnCompareOperatorColumn(String[] tdesc, boolean filter,
   //
   // -----------------------------------------------------------------------------------------------
 
-  private void generateFilterTimestampCompareTimestampBase(String[] tdesc) throws Exception {
+  private void generateFilterTimestampCompareTimestamp(String[] tdesc) throws Exception {
     String operatorName = tdesc[1];
     String operatorSymbol = tdesc[2];
-    String className = "FilterTimestamp" + tdesc[3] + operatorName + "Timestamp" + tdesc[4] + "Base";
+    String operandType = tdesc[3];
+    String camelOperandType = getCamelCaseType(operandType);
 
+    String className = "Filter" + camelOperandType + tdesc[4] + operatorName + camelOperandType + tdesc[5];
+    String baseClassName = "FilterTimestamp" + tdesc[4] + operatorName + "Timestamp" + tdesc[5] + "Base";
     //Read the template into a string;
-    String fileName = "FilterTimestamp" + (tdesc[3].equals("Col") ? "Column" : tdesc[3]) + "CompareTimestamp" +
-        tdesc[4] + "Base";
+    String fileName = "FilterTimestamp" + (tdesc[4].equals("Col") ? "Column" : tdesc[4]) + "CompareTimestamp" +
+        tdesc[5];
     File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
     templateString = templateString.replaceAll("<OperatorSymbol>", operatorSymbol);
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        className, templateString);
-  }
+    templateString = templateString.replaceAll("<OperandType>", operandType);
+    templateString = templateString.replaceAll("<CamelOperandType>", camelOperandType);
+    templateString = templateString.replaceAll("<HiveOperandType>", getTimestampHiveType(operandType));
+
+    String inputColumnVectorType = this.getColumnVectorType(operandType);
+    templateString = templateString.replaceAll("<InputColumnVectorType>", inputColumnVectorType);
 
-  private void generateFilterTimestampCompareTimestamp(String[] tdesc) throws Exception {
-    String operatorName = tdesc[1];
-    String operandType = tdesc[2];
-    String camelCaseOperandType = getCamelCaseType(operandType);
-    String className = "Filter" + camelCaseOperandType + tdesc[3] + operatorName + camelCaseOperandType + tdesc[4];
-    String baseClassName = "FilterTimestamp" + tdesc[3] + operatorName + "Timestamp" + tdesc[4] + "Base";
-    //Read the template into a string;
-    String fileName = "Filter" + camelCaseOperandType + (tdesc[3].equals("Col") ? "Column" : tdesc[3]) + "Compare" + camelCaseOperandType +
-        tdesc[4];
-    File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
-    String templateString = readFile(templateFile);
-    templateString = templateString.replaceAll("<ClassName>", className);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
     writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
         className, templateString);
   }
@@ -2287,9 +2203,9 @@ private void generateFilterLongDoubleCompareTimestamp(String[] tdesc) throws Exc
 
   private String timestampLongDoubleMethod(String operandType) {
     if (operandType.equals("long")) {
-      return "getTimestampSeconds";
+      return "getTimestampAsLong";
     } else if (operandType.equals("double")) {
-      return "getTimestampSecondsWithFractionalNanos";
+      return "getDouble";
     } else {
       return "unknown";
     }
@@ -2314,35 +2230,26 @@ private String timestampLongDoubleMethod(String operandType) {
   //
   // -----------------------------------------------------------------------------------------------
 
-  private void generateTimestampCompareTimestampBase(String[] tdesc) throws Exception {
+
+  private void generateTimestampCompareTimestamp(String[] tdesc) throws Exception {
     String operatorName = tdesc[1];
     String operatorSymbol = tdesc[2];
-    String className = "Timestamp" + tdesc[3] + operatorName + "Timestamp" + tdesc[4] + "Base";
+    String operandType = tdesc[3];
+    String camelOperandType = getCamelCaseType(operandType);
+    String className = camelOperandType + tdesc[4] + operatorName + camelOperandType + tdesc[5];
 
     //Read the template into a string;
-    String fileName = "Timestamp" + (tdesc[3].equals("Col") ? "Column" : tdesc[3]) + "CompareTimestamp" +
-        tdesc[4] + "Base";
+    String fileName = "Timestamp" + (tdesc[4].equals("Col") ? "Column" : tdesc[4]) + "CompareTimestamp" +
+        (tdesc[5].equals("Col") ? "Column" : tdesc[5]);
     File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
     templateString = templateString.replaceAll("<OperatorSymbol>", operatorSymbol);
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        className, templateString);
-  }
+    templateString = templateString.replaceAll("<OperandType>", operandType);
+    templateString = templateString.replaceAll("<CamelOperandType>", camelOperandType);
+    templateString = templateString.replaceAll("<HiveOperandType>", getTimestampHiveType(operandType));
+    templateString = templateString.replaceAll("<InputColumnVectorType>", getColumnVectorType(operandType));
 
-  private void generateTimestampCompareTimestamp(String[] tdesc) throws Exception {
-    String operatorName = tdesc[1];
-    String operandType = tdesc[2];
-    String camelCaseOperandType = getCamelCaseType(operandType);
-    String className = camelCaseOperandType + tdesc[3] + operatorName + camelCaseOperandType + tdesc[4];
-    String baseClassName = "Timestamp" + tdesc[3] + operatorName + "Timestamp" + tdesc[4] + "Base";
-    //Read the template into a string;
-    String fileName = camelCaseOperandType + (tdesc[3].equals("Col") ? "Column" : tdesc[3]) + "Compare" + camelCaseOperandType +
-        tdesc[4];
-    File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
-    String templateString = readFile(templateFile);
-    templateString = templateString.replaceAll("<ClassName>", className);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
     writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
         className, templateString);
   }
@@ -2851,6 +2758,7 @@ private void generateDateTimeArithmeticIntervalYearMonth(String[] tdesc) throws
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
     templateString = templateString.replaceAll("<OperatorSymbol>", operatorSymbol);
+    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
     writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
         className, templateString);
 
@@ -2924,30 +2832,6 @@ private String replaceTimestampScalar(String templateString, int argNum, String
     return templateString;
   }
 
-  // TimestampColumnArithmeticTimestampColumnBase.txt
-  // TimestampScalarArithmeticTimestampColumnBase.txt
-  // TimestampColumnArithmeticTimestampScalarBase.txt
-  //
-  private void generateTimestampArithmeticTimestampBase(String[] tdesc) throws Exception {
-    String operatorName = tdesc[1];
-    String colOrScalar1 = tdesc[2];
-    String colOrScalar2 = tdesc[3];
-
-    String baseClassName = "Timestamp" + colOrScalar1 + operatorName +
-        "Timestamp" + colOrScalar2 + "Base";
-
-    //Read the template into a string;
-    String fileName = "Timestamp" + (colOrScalar1.equals("Col") ? "Column" : colOrScalar1) + "Arithmetic" +
-        "Timestamp" + colOrScalar2 + "Base";
-    File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
-    String templateString = readFile(templateFile);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
-    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
-
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        baseClassName, templateString);
-  }
-
   // TimestampColumnArithmeticTimestampColumn.txt
   // TimestampScalarArithmeticTimestampColumn.txt
   // TimestampColumnArithmeticTimestampScalar.txt
@@ -2955,10 +2839,23 @@ private void generateTimestampArithmeticTimestampBase(String[] tdesc) throws Exc
   private void generateTimestampArithmeticTimestamp(String[] tdesc) throws Exception {
     String operatorName = tdesc[1];
     String operandType1 = tdesc[2];
+    String camelOperandType1 = getCamelCaseType(operandType1);
     String colOrScalar1 = tdesc[3];
     String operandType2 = tdesc[4];
+    String camelOperandType2 = getCamelCaseType(operandType2);
     String colOrScalar2 = tdesc[5];
 
+    String returnType;
+    if (operandType1.equals(operandType2)) {
+      // timestamp - timestamp
+      // interval_day_time +/- interval_day_time
+      returnType = "interval_day_time";
+    } else {
+      // timestamp +/- interval_day_time
+      // interval_day_time + timestamp
+      returnType = "timestamp";
+    }
+
     String className = getCamelCaseType(operandType1) + colOrScalar1 + operatorName +
         getCamelCaseType(operandType2) + colOrScalar2;
     String baseClassName = "Timestamp" + colOrScalar1 + operatorName +
@@ -2971,20 +2868,26 @@ private void generateTimestampArithmeticTimestamp(String[] tdesc) throws Excepti
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
     templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
+    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
     templateString = templateString.replaceAll("<OperandType1>", operandType1);
     templateString = templateString.replaceAll("<OperandType2>", operandType2);
-    if (colOrScalar1.equals("Scalar")) {
-      templateString = replaceTimestampScalar(templateString, 1, operandType1);
-    }
-    if (colOrScalar2.equals("Scalar")) {
-      templateString = replaceTimestampScalar(templateString, 2, operandType2);
-    }
-
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        className, templateString);
+    templateString = templateString.replaceAll("<CamelOperandType1>", camelOperandType1);
+    templateString = templateString.replaceAll("<CamelOperandType2>", camelOperandType2);
+    templateString = templateString.replaceAll("<HiveOperandType1>", getTimestampHiveType(operandType1));
+    templateString = templateString.replaceAll("<HiveOperandType2>", getTimestampHiveType(operandType2));
 
     String inputColumnVectorType1 = this.getColumnVectorType(operandType1);
+    templateString = templateString.replaceAll("<InputColumnVectorType1>", inputColumnVectorType1);
     String inputColumnVectorType2 = this.getColumnVectorType(operandType2);
+    templateString = templateString.replaceAll("<InputColumnVectorType2>", inputColumnVectorType2);
+
+    String outputColumnVectorType = this.getColumnVectorType(returnType);
+    templateString = templateString.replaceAll("<OutputColumnVectorType>", outputColumnVectorType);
+    templateString = templateString.replaceAll("<CamelReturnType>", getCamelCaseType(returnType));
+    templateString = templateString.replaceAll("<ReturnType>", returnType);
+
+    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
+        className, templateString);
 
     /* UNDONE: Col Col, vs Scalar Col vs Col Scalar
     testCodeGen.addColumnColumnOperationTestCases(
@@ -2995,30 +2898,6 @@ private void generateTimestampArithmeticTimestamp(String[] tdesc) throws Excepti
     */
   }
 
-  // DateColumnArithmeticTimestampColumnBase.txt
-  // DateScalarArithmeticTimestampColumnBase.txt
-  // DateColumnArithmeticTimestampScalarBase.txt
-  //
-  private void generateDateArithmeticTimestampBase(String[] tdesc) throws Exception {
-    String operatorName = tdesc[1];
-    String colOrScalar1 = tdesc[2];
-    String colOrScalar2 = tdesc[3];
-
-    String baseClassName = "Date" + colOrScalar1 + operatorName +
-        "Timestamp" + colOrScalar2 + "Base";
-
-    //Read the template into a string;
-    String fileName = "Date" + (colOrScalar1.equals("Col") ? "Column" : colOrScalar1) + "Arithmetic" +
-        "Timestamp" + colOrScalar2 + "Base";
-    File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
-    String templateString = readFile(templateFile);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
-    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
-
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        baseClassName, templateString);
-  }
-
   // DateColumnArithmeticTimestampColumn.txt
   // DateScalarArithmeticTimestampColumn.txt
   // DateColumnArithmeticTimestampScalar.txt
@@ -3026,14 +2905,23 @@ private void generateDateArithmeticTimestampBase(String[] tdesc) throws Exceptio
   private void generateDateArithmeticTimestamp(String[] tdesc) throws Exception {
     String operatorName = tdesc[1];
     String operandType1 = tdesc[2];
+    String camelOperandType1 = getCamelCaseType(operandType1);
     String colOrScalar1 = tdesc[3];
     String operandType2 = tdesc[4];
+    String camelOperandType2 = getCamelCaseType(operandType2);
     String colOrScalar2 = tdesc[5];
 
-    String className = getCamelCaseType(operandType1) + colOrScalar1 + operatorName +
-        getCamelCaseType(operandType2) + colOrScalar2;
-    String baseClassName = "Date" + colOrScalar1 + operatorName +
-        "Timestamp" + colOrScalar2 + "Base";
+    String returnType;
+    if (operandType1.equals("interval_day_time") || operandType2.equals("interval_day_time")) {
+      returnType = "timestamp";
+    } else if (operandType1.equals("timestamp") || operandType2.equals("timestamp")) {
+      returnType = "interval_day_time";
+    } else {
+      returnType = "unknown";
+    }
+
+    String className = camelOperandType1 + colOrScalar1 + operatorName +
+        camelOperandType2 + colOrScalar2;
 
     //Read the template into a string;
     String fileName = "Date" + (colOrScalar1.equals("Col") ? "Column" : colOrScalar1) + "Arithmetic" +
@@ -3041,21 +2929,26 @@ private void generateDateArithmeticTimestamp(String[] tdesc) throws Exception {
     File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
+    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
     templateString = templateString.replaceAll("<OperandType1>", operandType1);
     templateString = templateString.replaceAll("<OperandType2>", operandType2);
-    if (colOrScalar1.equals("Scalar")) {
-      templateString = replaceTimestampScalar(templateString, 1, operandType1);
-    }
-    if (colOrScalar2.equals("Scalar")) {
-      templateString = replaceTimestampScalar(templateString, 2, operandType2);
-    }
-
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        className, templateString);
+    templateString = templateString.replaceAll("<CamelOperandType1>", camelOperandType1);
+    templateString = templateString.replaceAll("<CamelOperandType2>", camelOperandType2);
+    templateString = templateString.replaceAll("<HiveOperandType1>", getTimestampHiveType(operandType1));
+    templateString = templateString.replaceAll("<HiveOperandType2>", getTimestampHiveType(operandType2));
 
     String inputColumnVectorType1 = this.getColumnVectorType(operandType1);
+    templateString = templateString.replaceAll("<InputColumnVectorType1>", inputColumnVectorType1);
     String inputColumnVectorType2 = this.getColumnVectorType(operandType2);
+    templateString = templateString.replaceAll("<InputColumnVectorType2>", inputColumnVectorType2);
+
+    String outputColumnVectorType = this.getColumnVectorType(returnType);
+    templateString = templateString.replaceAll("<OutputColumnVectorType>", outputColumnVectorType);
+    templateString = templateString.replaceAll("<CamelReturnType>", getCamelCaseType(returnType));
+    templateString = templateString.replaceAll("<ReturnType>", returnType);
+
+    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
+        className, templateString);
 
     /* UNDONE: Col Col, vs Scalar Col vs Col Scalar
     testCodeGen.addColumnColumnOperationTestCases(
@@ -3066,30 +2959,6 @@ private void generateDateArithmeticTimestamp(String[] tdesc) throws Exception {
     */
   }
 
-  // TimestampColumnArithmeticDateColumnBase.txt
-  // TimestampScalarArithmeticDateColumnBase.txt
-  // TimestampColumnArithmeticDateScalarBase.txt
-  //
-  private void generateTimestampArithmeticDateBase(String[] tdesc) throws Exception {
-    String operatorName = tdesc[1];
-    String colOrScalar1 = tdesc[2];
-    String colOrScalar2 = tdesc[3];
-
-    String baseClassName = "Timestamp" + colOrScalar1 + operatorName +
-        "Date" + colOrScalar2 + "Base";
-
-    //Read the template into a string;
-    String fileName = "Timestamp" + (colOrScalar1.equals("Col") ? "Column" : colOrScalar1) + "Arithmetic" +
-        "Date" + colOrScalar2 + "Base";
-    File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
-    String templateString = readFile(templateFile);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
-    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
-
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        baseClassName, templateString);
-  }
-
   // TimestampColumnArithmeticDateColumn.txt
   // TimestampScalarArithmeticDateColumn.txt
   // TimestampColumnArithmeticDateScalar.txt
@@ -3097,14 +2966,23 @@ private void generateTimestampArithmeticDateBase(String[] tdesc) throws Exceptio
   private void generateTimestampArithmeticDate(String[] tdesc) throws Exception {
     String operatorName = tdesc[1];
     String operandType1 = tdesc[2];
+    String camelOperandType1 = getCamelCaseType(operandType1);
     String colOrScalar1 = tdesc[3];
     String operandType2 = tdesc[4];
+    String camelOperandType2 = getCamelCaseType(operandType2);
     String colOrScalar2 = tdesc[5];
 
-    String className = getCamelCaseType(operandType1) + colOrScalar1 + operatorName +
-        getCamelCaseType(operandType2) + colOrScalar2;
-    String baseClassName = "Timestamp" + colOrScalar1 + operatorName +
-        "Date" + colOrScalar2 + "Base";
+    String returnType;
+    if (operandType1.equals("interval_day_time") || operandType2.equals("interval_day_time")) {
+      returnType = "timestamp";
+    } else if (operandType1.equals("timestamp") || operandType2.equals("timestamp")) {
+      returnType = "interval_day_time";
+    } else {
+      returnType = "unknown";
+    }
+
+    String className = camelOperandType1 + colOrScalar1 + operatorName +
+        camelOperandType2 + colOrScalar2;
 
     //Read the template into a string;
     String fileName = "Timestamp" + (colOrScalar1.equals("Col") ? "Column" : colOrScalar1) + "Arithmetic" +
@@ -3112,21 +2990,26 @@ private void generateTimestampArithmeticDate(String[] tdesc) throws Exception {
     File templateFile = new File(joinPath(this.expressionTemplateDirectory, fileName + ".txt"));
     String templateString = readFile(templateFile);
     templateString = templateString.replaceAll("<ClassName>", className);
-    templateString = templateString.replaceAll("<BaseClassName>", baseClassName);
+    templateString = templateString.replaceAll("<OperatorMethod>", operatorName.toLowerCase());
     templateString = templateString.replaceAll("<OperandType1>", operandType1);
     templateString = templateString.replaceAll("<OperandType2>", operandType2);
-    if (colOrScalar1.equals("Scalar")) {
-      templateString = replaceTimestampScalar(templateString, 1, operandType1);
-    }
-    if (colOrScalar2.equals("Scalar")) {
-      templateString = replaceTimestampScalar(templateString, 2, operandType2);
-    }
-
-    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
-        className, templateString);
+    templateString = templateString.replaceAll("<CamelOperandType1>", camelOperandType1);
+    templateString = templateString.replaceAll("<CamelOperandType2>", camelOperandType2);
+    templateString = templateString.replaceAll("<HiveOperandType1>", getTimestampHiveType(operandType1));
+    templateString = templateString.replaceAll("<HiveOperandType2>", getTimestampHiveType(operandType2));
 
     String inputColumnVectorType1 = this.getColumnVectorType(operandType1);
+    templateString = templateString.replaceAll("<InputColumnVectorType1>", inputColumnVectorType1);
     String inputColumnVectorType2 = this.getColumnVectorType(operandType2);
+    templateString = templateString.replaceAll("<InputColumnVectorType2>", inputColumnVectorType2);
+
+    String outputColumnVectorType = this.getColumnVectorType(returnType);
+    templateString = templateString.replaceAll("<OutputColumnVectorType>", outputColumnVectorType);
+    templateString = templateString.replaceAll("<CamelReturnType>", getCamelCaseType(returnType));
+    templateString = templateString.replaceAll("<ReturnType>", returnType);
+
+    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,
+        className, templateString);
 
     /* UNDONE: Col Col, vs Scalar Col vs Col Scalar
     testCodeGen.addColumnColumnOperationTestCases(
@@ -3272,8 +3155,10 @@ private String getColumnVectorType(String primitiveType) throws Exception {
         return "DecimalColumnVector";
     } else if (primitiveType.equals("string")) {
       return "BytesColumnVector";
-    } else if (isTimestampIntervalType(primitiveType)) {
+    } else if (primitiveType.equals("timestamp")) {
       return "TimestampColumnVector";
+    } else if (primitiveType.equals("interval_day_time")) {
+      return "IntervalDayTimeColumnVector";
     }
     throw new Exception("Unimplemented primitive column vector type: " + primitiveType);
   }
diff --git a/common/src/java/org/apache/hive/common/util/DateUtils.java b/common/src/java/org/apache/hive/common/util/DateUtils.java
index c749bcb5e6..959a54247b 100644
--- a/common/src/java/org/apache/hive/common/util/DateUtils.java
+++ b/common/src/java/org/apache/hive/common/util/DateUtils.java
@@ -21,8 +21,6 @@
 import java.math.BigDecimal;
 import java.text.SimpleDateFormat;
 
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-
 /**
  * DateUtils. Thread-safe class
  *
@@ -56,21 +54,4 @@ public static int parseNumericValueWithRange(String fieldName,
     }
     return result;
   }
-
-  public static long getIntervalDayTimeTotalNanos(HiveIntervalDayTime intervalDayTime) {
-    return intervalDayTime.getTotalSeconds() * NANOS_PER_SEC + intervalDayTime.getNanos();
-  }
-
-  public static void setIntervalDayTimeTotalNanos(HiveIntervalDayTime intervalDayTime,
-      long totalNanos) {
-    intervalDayTime.set(totalNanos / NANOS_PER_SEC, (int) (totalNanos % NANOS_PER_SEC));
-  }
-
-  public static long getIntervalDayTimeTotalSecondsFromTotalNanos(long totalNanos) {
-    return totalNanos / NANOS_PER_SEC;
-  }
-
-  public static int getIntervalDayTimeNanosFromTotalNanos(long totalNanos) {
-    return (int) (totalNanos % NANOS_PER_SEC);
-  }
 }
diff --git a/data/files/timestamps.txt b/data/files/timestamps.txt
new file mode 100644
index 0000000000..36ffd23582
--- /dev/null
+++ b/data/files/timestamps.txt
@@ -0,0 +1,50 @@
+6631-11-13 16:31:29.702202248
+6731-02-12 08:12:48.287783702
+6705-09-28 18:27:28.000845672
+5397-07-13 07:12:32.000896438
+9209-11-11 04:08:58.223768453
+9403-01-09 18:12:33.547
+6482-04-27 12:07:38.073915413
+7503-06-23 23:14:17.486
+1883-04-17 04:14:34.647766229
+0004-09-22 18:26:29.519542222
+7160-12-02 06:00:24.81200852
+8422-07-22 03:21:45.745036084
+4143-07-08 10:53:27.252802259
+5344-10-04 18:40:08.165
+5966-07-09 03:30:50.597
+9075-06-13 16:20:09.218517797
+1815-05-06 00:12:37.543584705
+7409-09-07 23:33:32.459349602
+5339-02-01 14:10:01.085678691
+4966-12-04 09:30:55.202
+1319-02-02 16:31:57.778
+1404-07-23 15:32:16.059185026
+6229-06-28 02:54:28.970117179
+0528-10-27 08:15:18.941718273
+8521-01-16 20:42:05.668832388
+1976-05-06 00:42:30.910786948
+2003-09-23 22:33:17.00003252
+2007-02-09 05:17:29.368756876
+1998-10-16 20:05:29.397591987
+1976-03-03 04:54:33.000895162
+1985-07-20 09:30:11.0
+2021-09-24 03:18:32.413655165
+2013-04-07 02:44:43.00086821
+2002-05-10 05:29:48.990818073
+1973-04-17 06:30:38.596784156
+1987-02-21 19:48:29.0
+1981-11-15 23:03:10.999338387
+2000-12-18 08:42:30.000595596
+1999-10-03 16:59:10.396903939
+2024-11-11 16:42:41.101
+2013-04-10 00:43:46.854731546
+2010-04-08 02:43:35.861742727
+2004-03-07 20:14:13.0
+1987-05-28 13:52:07.900916635
+1978-08-05 14:41:05.501
+1966-08-16 13:36:50.183618031
+2009-01-21 10:49:07.108
+1981-04-25 09:01:12.077192689
+1985-11-18 16:37:54.0
+1974-10-04 17:21:03.989
diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index f8e8bda550..0672e0e289 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -289,6 +289,7 @@ minitez.query.files.shared=acid_globallimit.q,\
   vector_inner_join.q,\
   vector_interval_1.q,\
   vector_interval_2.q,\
+  vector_interval_arithmetic.q,\
   vector_interval_mapjoin.q,\
   vector_join30.q,\
   vector_join_filters.q,\
@@ -319,6 +320,7 @@ minitez.query.files.shared=acid_globallimit.q,\
   vector_reduce3.q,\
   vector_string_concat.q,\
   vector_struct_in.q,\
+  vectorized_timestamp.q,\
   vector_varchar_4.q,\
   vector_varchar_mapjoin1.q,\
   vector_varchar_simple.q,\
diff --git a/orc/src/java/org/apache/orc/impl/WriterImpl.java b/orc/src/java/org/apache/orc/impl/WriterImpl.java
index 6497ecfa02..d4b9a14be6 100644
--- a/orc/src/java/org/apache/orc/impl/WriterImpl.java
+++ b/orc/src/java/org/apache/orc/impl/WriterImpl.java
@@ -1737,19 +1737,17 @@ void writeBatch(ColumnVector vector, int offset,
                     int length) throws IOException {
       super.writeBatch(vector, offset, length);
       TimestampColumnVector vec = (TimestampColumnVector) vector;
+      Timestamp val;
       if (vector.isRepeating) {
         if (vector.noNulls || !vector.isNull[0]) {
-          long millis = vec.getEpochMilliseconds(0);
-          int adjustedNanos = vec.getSignedNanos(0);
-          if (adjustedNanos < 0) {
-            adjustedNanos += NANOS_PER_SECOND;
-          }
+          val = vec.asScratchTimestamp(0);
+          long millis = val.getTime();
           indexStatistics.updateTimestamp(millis);
           if (createBloomFilter) {
             bloomFilter.addLong(millis);
           }
-          final long secs = vec.getEpochSeconds(0) - base_timestamp;
-          final long nano = formatNanos(adjustedNanos);
+          final long secs = millis / MILLIS_PER_SECOND - base_timestamp;
+          final long nano = formatNanos(val.getNanos());
           for(int i=0; i < length; ++i) {
             seconds.write(secs);
             nanos.write(nano);
@@ -1758,14 +1756,11 @@ void writeBatch(ColumnVector vector, int offset,
       } else {
         for(int i=0; i < length; ++i) {
           if (vec.noNulls || !vec.isNull[i + offset]) {
-            long secs = vec.getEpochSeconds(i + offset) - base_timestamp;
-            long millis = vec.getEpochMilliseconds(i + offset);
-            int adjustedNanos = vec.getSignedNanos(i + offset);
-            if (adjustedNanos < 0) {
-              adjustedNanos += NANOS_PER_SECOND;
-            }
+            val = vec.asScratchTimestamp(i + offset);
+            long millis = val.getTime();
+            long secs = millis / MILLIS_PER_SECOND - base_timestamp;
             seconds.write(secs);
-            nanos.write(formatNanos(adjustedNanos));
+            nanos.write(formatNanos(val.getNanos()));
             indexStatistics.updateTimestamp(millis);
             if (createBloomFilter) {
               bloomFilter.addLong(millis);
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthColumn.txt
index 845bc5f7a8..c3d8d7eb21 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthColumn.txt
@@ -18,15 +18,18 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Date;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
- * Generated from template DateColumnArithmeticIntervalYearMonthColumn.txt, which covers binary arithmetic 
+ * Generated from template DateColumnArithmeticIntervalYearMonthColumn.txt, which covers binary arithmetic
  * expressions between date and interval year month columns.
  */
 public class <ClassName> extends VectorExpression {
@@ -36,12 +39,18 @@ public class <ClassName> extends VectorExpression {
   private int colNum1;
   private int colNum2;
   private int outputColumn;
+  private Date scratchDate1;
+  private HiveIntervalYearMonth scratchIntervalYearMonth2;
+  private Date outputDate;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
     this.colNum1 = colNum1;
     this.colNum2 = colNum2;
     this.outputColumn = outputColumn;
+    scratchDate1 = new Date(0);
+    scratchIntervalYearMonth2 = new HiveIntervalYearMonth();
+    outputDate = new Date(0);
   }
 
   public <ClassName>() {
@@ -54,10 +63,10 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type date (epochDays).
+    // Input #1 is type date.
     LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum1];
 
-    // Input #2 is type interval_year_month (months).
+    // Input #2 is type interval_year_month.
     LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum2];
 
     // Output is type date.
@@ -89,38 +98,65 @@ public class <ClassName> extends VectorExpression {
      * conditional checks in the inner loop.
      */
     if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputVector[0] = dtm.addMonthsToDays(vector1[0], <OperatorSymbol> (int) vector2[0]);
+      scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      scratchIntervalYearMonth2.set((int) vector2[0]);
+      dtm.<OperatorMethod>(
+          scratchDate1, scratchIntervalYearMonth2,  outputDate);
+      outputVector[0] = DateWritable.dateToDays(outputDate);
     } else if (inputColVector1.isRepeating) {
+      scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[0]));
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector1[0], <OperatorSymbol> (int) vector2[i]);
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              scratchDate1, scratchIntervalYearMonth2,  outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector1[0], <OperatorSymbol> (int) vector2[i]);
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              scratchDate1, scratchIntervalYearMonth2,  outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else if (inputColVector2.isRepeating) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector1[i], <OperatorSymbol> (int) vector2[0]);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              scratchDate1, scratchIntervalYearMonth2,  outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector1[i], <OperatorSymbol> (int) vector2[0]);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              scratchDate1, scratchIntervalYearMonth2,  outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector1[i], <OperatorSymbol> (int) vector2[i]);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              scratchDate1, scratchIntervalYearMonth2,  outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector1[i], <OperatorSymbol> (int) vector2[i]);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              scratchDate1, scratchIntervalYearMonth2,  outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthScalar.txt
index 86a95c98f7..d1474fb9e0 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticIntervalYearMonthScalar.txt
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Date;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -25,6 +27,7 @@ import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template DateColumnArithmeticIntervalYearMonthScalar.txt, which covers binary arithmetic
@@ -35,14 +38,18 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private long value;
+  private HiveIntervalYearMonth value;
   private int outputColumn;
+  private Date scratchDate1;
+  private Date outputDate;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum, long value, int outputColumn) {
     this.colNum = colNum;
-    this.value = value;
+    this.value = new HiveIntervalYearMonth((int) value);
     this.outputColumn = outputColumn;
+    scratchDate1 = new Date(0);
+    outputDate = new Date(0);
   }
 
   public <ClassName>() {
@@ -55,19 +62,19 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type date (epochDays).
-    LongColumnVector inputColVector = (LongColumnVector) batch.cols[colNum];
+    // Input #1 is type date.
+    LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum];
 
     // Output is type date.
     LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector.isNull;
+    boolean[] inputIsNull = inputColVector1.isNull;
     boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector.noNulls;
-    outputColVector.isRepeating = inputColVector.isRepeating;
+    outputColVector.noNulls = inputColVector1.noNulls;
+    outputColVector.isRepeating = inputColVector1.isRepeating;
     int n = batch.size;
-    long[] vector = inputColVector.vector;
+    long[] vector1 = inputColVector1.vector;
     long[] outputVector = outputColVector.vector;
 
     // return immediately if batch is empty
@@ -75,32 +82,46 @@ public class <ClassName> extends VectorExpression {
       return;
     }
 
-    if (inputColVector.isRepeating) {
-      outputVector[0] = dtm.addMonthsToDays(vector[0], <OperatorSymbol> (int) value);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+    if (inputColVector1.isRepeating) {
+      scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      dtm.<OperatorMethod>(
+          scratchDate1, value, outputDate);
+      outputVector[0] = DateWritable.dateToDays(outputDate);
+       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector.noNulls) {
+    } else if (inputColVector1.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchDate1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchDate1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else /* there are nulls */ {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchDate1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchDate1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumn.txt
index 6241ee24aa..63cebaf884 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumn.txt
@@ -18,28 +18,155 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
- * Generated from template DateColumnArithmeticTimestampColumn.txt, which covers binary arithmetic
- * expressions between a date column and a timestamp column.
+ * Generated from template DateColumnArithmeticTimestampColumn.txt, a class
+ * which covers binary arithmetic expressions between a date column and timestamp column.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int colNum1;
+  private int colNum2;
+  private int outputColumn;
+  private Timestamp scratchTimestamp1;
+  private DateTimeMath dtm = new DateTimeMath();
+
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
-    super(colNum1, colNum2, outputColumn);
+    this.colNum1 = colNum1;
+    this.colNum2 = colNum2;
+    this.outputColumn = outputColumn;
+    scratchTimestamp1 = new Timestamp(0);
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #1 is type Date (days).  For the math we convert it to a timestamp.
+    LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum1];
+
+    // Input #2 is type <OperandType2>.
+    <InputColumnVectorType2> inputColVector2 = (<InputColumnVectorType2>) batch.cols[colNum2];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    int n = batch.size;
+    long[] vector1 = inputColVector1.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    outputColVector.isRepeating =
+         inputColVector1.isRepeating && inputColVector2.isRepeating
+      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
+      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
+
+    // Handle nulls first
+    NullUtil.propagateNullsColCol(
+      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
+
+    /* Disregard nulls for processing. In other words,
+     * the arithmetic operation is performed even if one or
+     * more inputs are null. This is to improve speed by avoiding
+     * conditional checks in the inner loop.
+     */
+    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+      scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      dtm.<OperatorMethod>(
+          scratchTimestamp1, inputColVector2.asScratch<CamelOperandType2>(0), outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+    } else if (inputColVector1.isRepeating) {
+      scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              scratchTimestamp1, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              scratchTimestamp1, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else if (inputColVector2.isRepeating) {
+      <HiveOperandType2> value2 = inputColVector2.asScratch<CamelOperandType2>(0);
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchTimestamp1, value2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+         }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchTimestamp1, value2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+         dtm.<OperatorMethod>(
+              scratchTimestamp1, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+              scratchTimestamp1, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    }
+
+    /* For the case when the output can have null values, follow
+     * the convention that the data values must be 1 for long and
+     * NaN for double. This is to prevent possible later zero-divide errors
+     * in complex arithmetic expressions like col2 / (col1 - 1)
+     * in the case when some col1 entries are null.
+     */
+    NullUtil.setNullDataEntries<CamelReturnType>(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
   }
 
   @Override
@@ -49,7 +176,7 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("<OperandType1>"),
+            VectorExpressionDescriptor.ArgumentType.getType("date"),
             VectorExpressionDescriptor.ArgumentType.getType("<OperandType2>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumnBase.txt
deleted file mode 100644
index a61b769c2f..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampColumnBase.txt
+++ /dev/null
@@ -1,171 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
- 
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-
-/**
- * Generated from template DateColumnArithmeticTimestampColumnBase.txt, a base class
- * which covers binary arithmetic expressions between a date column and timestamp column.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum1;
-  private int colNum2;
-  private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
-
-  public <BaseClassName>(int colNum1, int colNum2, int outputColumn) {
-    this.colNum1 = colNum1;
-    this.colNum2 = colNum2;
-    this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #1 is type Date (epochDays).
-    LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum1];
-
-    // Input #2 is type timestamp/interval_day_time.
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum2];
-
-    // Output is type timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    int n = batch.size;
-    long[] vector1 = inputColVector1.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    outputColVector.isRepeating =
-         inputColVector1.isRepeating && inputColVector2.isRepeating
-      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
-      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
-
-    // Handle nulls first  
-    NullUtil.propagateNullsColCol(
-      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
-
-    /* Disregard nulls for processing. In other words,
-     * the arithmetic operation is performed even if one or
-     * more inputs are null. This is to improve speed by avoiding
-     * conditional checks in the inner loop.
-     */
-    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputColVector.<OperatorMethod>(
-          scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-          inputColVector2.asScratchPisaTimestamp(0),
-          0);
-    } else if (inputColVector1.isRepeating) {
-        PisaTimestamp value1 =
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0]));
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-              value1,
-              inputColVector2.asScratchPisaTimestamp(i),
-              i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-              value1,
-              inputColVector2.asScratchPisaTimestamp(i),
-              i);
-        }
-      }
-    } else if (inputColVector2.isRepeating) {
-      PisaTimestamp value2 = inputColVector2.asScratchPisaTimestamp(0);
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              value2,
-              i);
-         }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              value2,
-              i);
-        }
-      }
-    } else {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              inputColVector2.asScratchPisaTimestamp(i),
-              i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              inputColVector2.asScratchPisaTimestamp(i),
-              i);
-        }
-      }
-    }
-
-    /* For the case when the output can have null values, follow
-     * the convention that the data values must be 1 for long and
-     * NaN for double. This is to prevent possible later zero-divide errors
-     * in complex arithmetic expressions like col2 / (col1 - 1)
-     * in the case when some col1 entries are null.
-     */
-    NullUtil.setNullDataEntriesTimestamp(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
-
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalar.txt
index b813d11fa7..7aee529662 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalar.txt
@@ -19,32 +19,123 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hive.common.util.DateUtils;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
- * Generated from template DateColumnArithmeticTimestampScalar.txt, which covers binary arithmetic
- * expressions between a date column and a timestamp scalar.
+ * Generated from template DateColumnArithmeticTimestampScalarBase.txt, a base class
+ * which covers binary arithmetic expressions between a date column and a timestamp scalar.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  public <ClassName>(int colNum, <ScalarHiveTimestampType2> value, int outputColumn) {
-    super(colNum, <PisaTimestampConversion2>, outputColumn);
+  private int colNum;
+  private <HiveOperandType2> value;
+  private int outputColumn;
+  private Timestamp scratchTimestamp1;
+  private DateTimeMath dtm = new DateTimeMath();
+
+  public <ClassName>(int colNum, <HiveOperandType2> value, int outputColumn) {
+    this.colNum = colNum;
+    this.value = value;
+    this.outputColumn = outputColumn;
+    scratchTimestamp1 = new Timestamp(0);
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #1 is type date (days).  For the math we convert it to a timestamp.
+    LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] inputIsNull = inputColVector1.isNull;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = inputColVector1.noNulls;
+    outputColVector.isRepeating = inputColVector1.isRepeating;
+    int n = batch.size;
+    long[] vector1 = inputColVector1.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector1.isRepeating) {
+      scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      dtm.<OperatorMethod>(
+          scratchTimestamp1, value, outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+      outputIsNull[0] = inputIsNull[0];
+    } else if (inputColVector1.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+             scratchTimestamp1, value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+             scratchTimestamp1, value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+             scratchTimestamp1, value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+          outputIsNull[i] = inputIsNull[i];
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.<OperatorMethod>(
+             scratchTimestamp1, value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
+      }
+    }
+
+    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
   }
 
   @Override
@@ -54,7 +145,7 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("<OperandType1>"),
+            VectorExpressionDescriptor.ArgumentType.getType("date"),
             VectorExpressionDescriptor.ArgumentType.getType("<OperandType2>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalarBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalarBase.txt
deleted file mode 100644
index d64fba0e5c..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateColumnArithmeticTimestampScalarBase.txt
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-
-/**
- * Generated from template DateColumnArithmeticTimestampScalarBase.txt, a base class
- * which covers binary arithmetic expressions between a date column and a timestamp scalar.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
-
-  public <BaseClassName>(int colNum, PisaTimestamp value, int outputColumn) {
-    this.colNum = colNum;
-    this.value = value;
-    this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #1 is type date (epochDays).
-    LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum];
-
-    // Output is type timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector1.isNull;
-    boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector1.noNulls;
-    outputColVector.isRepeating = inputColVector1.isRepeating;
-    int n = batch.size;
-    long[] vector1 = inputColVector1.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector1.isRepeating) {
-        outputColVector.<OperatorMethod>(
-          scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-          value,
-          0);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector1.noNulls) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-            value,
-            i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-            value,
-            i);
-        }
-      }
-    } else /* there are nulls */ {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-            value,
-            i);
-          outputIsNull[i] = inputIsNull[i];
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-            value,
-            i);
-        }
-        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
-      }
-    }
-
-    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticIntervalYearMonthColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticIntervalYearMonthColumn.txt
index 653565ee2a..c68ac34b82 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticIntervalYearMonthColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticIntervalYearMonthColumn.txt
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Date;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -33,6 +35,7 @@ import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template DateTimeScalarArithmeticIntervalYearMonthColumn.txt.
@@ -44,14 +47,18 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private long value;
+  private Date value;
   private int outputColumn;
+  private HiveIntervalYearMonth scratchIntervalYearMonth2;
+  private Date outputDate;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(long value, int colNum, int outputColumn) {
     this.colNum = colNum;
-    this.value = value;
+    this.value = new Date(DateWritable.daysToMillis((int) value));
     this.outputColumn = outputColumn;
+    scratchIntervalYearMonth2 = new HiveIntervalYearMonth();
+    outputDate = new Date(0);
   }
 
   public <ClassName>() {
@@ -70,18 +77,18 @@ public class <ClassName> extends VectorExpression {
     }
 
     // Input #2 is type Interval_Year_Month (months).
-    LongColumnVector inputColVector = (LongColumnVector) batch.cols[colNum];
+    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum];
 
     // Output is type Date.
     LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector.isNull;
+    boolean[] inputIsNull = inputColVector2.isNull;
     boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector.noNulls;
-    outputColVector.isRepeating = inputColVector.isRepeating;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
     int n = batch.size;
-    long[] vector = inputColVector.vector;
+    long[] vector2 = inputColVector2.vector;
     long[] outputVector = outputColVector.vector;
 
     // return immediately if batch is empty
@@ -89,32 +96,46 @@ public class <ClassName> extends VectorExpression {
       return;
     }
 
-    if (inputColVector.isRepeating) {
-      outputVector[0] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[0]);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+    if (inputColVector2.isRepeating) {
+      scratchIntervalYearMonth2.set((int) vector2[0]);
+      dtm.<OperatorMethod>(
+          value, scratchIntervalYearMonth2, outputDate);
+      outputVector[0] = DateWritable.dateToDays(outputDate);
+       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector.noNulls) {
+    } else if (inputColVector2.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              value, scratchIntervalYearMonth2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              value, scratchIntervalYearMonth2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else {                         /* there are nulls */
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              value, scratchIntervalYearMonth2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              value, scratchIntervalYearMonth2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumn.txt
index e93bed50dd..cb6b750bbd 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumn.txt
@@ -18,37 +18,133 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 /*
  * Because of the templatized nature of the code, either or both
  * of these ColumnVector imports may be needed. Listing both of them
  * rather than using ....vectorization.*;
  */
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
- * Generated from template DateScalarArithmeticTimestampColumn.txt.
+ * Generated from template DateTimeScalarArithmeticTimestampColumnBase.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int colNum;
+  private Timestamp value;
+  private int outputColumn;
+  private DateTimeMath dtm = new DateTimeMath();
+
   public <ClassName>(long value, int colNum, int outputColumn) {
-    super(value, colNum, outputColumn);
+    this.colNum = colNum;
+    // Scalar input #1 is type date (days).  For the math we convert it to a timestamp.
+    this.value = new Timestamp(0);
+    this.value.setTime(DateWritable.daysToMillis((int) value));
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
   }
 
+  @Override
+  /**
+   * Method to evaluate scalar-column operation in vectorized fashion.
+   *
+   * @batch a package of rows with each column stored in a vector
+   */
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #2 is type <OperandType2>.
+    <InputColumnVectorType2> inputColVector2 = (<InputColumnVectorType2>) batch.cols[colNum];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] inputIsNull = inputColVector2.isNull;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector2.isRepeating) {
+      dtm.<OperatorMethod>(
+          value, inputColVector2.asScratch<CamelOperandType2>(0), outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+      outputIsNull[0] = inputIsNull[0];
+    } else if (inputColVector2.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else {                         /* there are nulls */
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+          outputIsNull[i] = inputIsNull[i];
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
+      }
+    }
+
+    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
+  }
+
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder())
@@ -56,7 +152,7 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("<OperandType1>"),
+            VectorExpressionDescriptor.ArgumentType.getType("date"),
             VectorExpressionDescriptor.ArgumentType.getType("<OperandType2>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumnBase.txt
deleted file mode 100644
index a1f4e6fa7b..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/DateScalarArithmeticTimestampColumnBase.txt
+++ /dev/null
@@ -1,147 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
- 
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-/*
- * Because of the templatized nature of the code, either or both
- * of these ColumnVector imports may be needed. Listing both of them
- * rather than using ....vectorization.*;
- */
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-
-/**
- * Generated from template DateTimeScalarArithmeticTimestampColumnBase.txt.
- * Implements a vectorized arithmetic operator with a scalar on the left and a
- * column vector on the right. The result is output to an output column vector.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-
-  public <BaseClassName>(long value, int colNum, int outputColumn) {
-    this.colNum = colNum;
-    this.value = new PisaTimestamp().updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) value));
-    this.outputColumn = outputColumn;
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  /**
-   * Method to evaluate scalar-column operation in vectorized fashion.
-   *
-   * @batch a package of rows with each column stored in a vector
-   */
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #2 is type timestamp/interval_day_time.
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum];
-
-    // Output is type timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector2.isNull;
-    boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector2.noNulls;
-    outputColVector.isRepeating = inputColVector2.isRepeating;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector2.isRepeating) {
-      outputColVector.<OperatorMethod>(
-          value,
-          inputColVector2.asScratchPisaTimestamp(0),
-          0);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector2.noNulls) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value,
-            inputColVector2.asScratchPisaTimestamp(i),
-            i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value,
-            inputColVector2.asScratchPisaTimestamp(i),
-            i);
-        }
-      }
-    } else {                         /* there are nulls */
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value,
-            inputColVector2.asScratchPisaTimestamp(i),
-            i);
-          outputIsNull[i] = inputIsNull[i];
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value,
-            inputColVector2.asScratchPisaTimestamp(i),
-            i);
-        }
-        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
-      }
-    }
-
-    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeColumnCompareIntervalDayTimeColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeColumnCompareIntervalDayTimeColumn.txt
deleted file mode 100644
index 8d9bdf15f9..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeColumnCompareIntervalDayTimeColumn.txt
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template FilterIntervalDayTimeColumnCompareColumn.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type, however output is not
- * produced in a separate column.
- * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
- */
-public class <ClassName> extends <BaseClassName> {
-
-  public <ClassName>(int colNum1, int colNum2) {
-    super(colNum1, colNum2);
-  }
-
-  public <ClassName>() {
-    super();
-  }
-
-  @Override
-  public VectorExpressionDescriptor.Descriptor getDescriptor() {
-    return (new VectorExpressionDescriptor.Builder())
-        .setMode(
-            VectorExpressionDescriptor.Mode.FILTER)
-        .setNumArguments(2)
-        .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"),
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"))
-        .setInputExpressionTypes(
-            VectorExpressionDescriptor.InputExpressionType.COLUMN,
-            VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeColumnCompareIntervalDayTimeScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeColumnCompareIntervalDayTimeScalar.txt
deleted file mode 100644
index 7022b4f157..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeColumnCompareIntervalDayTimeScalar.txt
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template FilterIntervalDayTimeColumnCompareScalar.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type, however output is not
- * produced in a separate column.
- * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
- */
-public class <ClassName> extends <BaseClassName> {
-
-  public <ClassName>(int colNum, HiveIntervalDayTime value) {
-    super(colNum, value.pisaTimestampUpdate(new PisaTimestamp()));
-  }
-
-  public <ClassName>() {
-    super();
-  }
-
-  @Override
-  public VectorExpressionDescriptor.Descriptor getDescriptor() {
-    return (new VectorExpressionDescriptor.Builder())
-        .setMode(
-            VectorExpressionDescriptor.Mode.FILTER)
-        .setNumArguments(2)
-        .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"),
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"))
-        .setInputExpressionTypes(
-            VectorExpressionDescriptor.InputExpressionType.COLUMN,
-            VectorExpressionDescriptor.InputExpressionType.SCALAR).build();
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeScalarCompareIntervalDayTimeColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeScalarCompareIntervalDayTimeColumn.txt
deleted file mode 100644
index d227bf04eb..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterIntervalDayTimeScalarCompareIntervalDayTimeColumn.txt
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template FilterIntervalDayTimeScalarCompareColumn.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type, however output is not
- * produced in a separate column.
- * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
- */
-public class <ClassName> extends <BaseClassName> {
-
-  public <ClassName>(HiveIntervalDayTime value, int colNum) {
-    super(value.pisaTimestampUpdate(new PisaTimestamp()), colNum);
-  }
-
-  public <ClassName>() {
-    super();
-  }
-
-  @Override
-  public VectorExpressionDescriptor.Descriptor getDescriptor() {
-    return (new VectorExpressionDescriptor.Builder())
-        .setMode(
-            VectorExpressionDescriptor.Mode.FILTER)
-        .setNumArguments(2)
-        .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"),
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"))
-        .setInputExpressionTypes(
-            VectorExpressionDescriptor.InputExpressionType.SCALAR,
-            VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampColumn.txt
index 0c8321f6c7..57caf7eab4 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampColumn.txt
@@ -19,8 +19,8 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampScalar.txt
index 7e4d55e3fc..1b86691d56 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleColumnCompareTimestampScalar.txt
@@ -19,8 +19,8 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -36,7 +36,7 @@ public class <ClassName> extends <BaseClassName> {
   private static final long serialVersionUID = 1L;
 
   public <ClassName>(int colNum, Timestamp value) {
-    super(colNum, new PisaTimestamp(value).<GetTimestampLongDoubleMethod>());
+    super(colNum, TimestampColumnVector.<GetTimestampLongDoubleMethod>(value));
   }
 
   public <ClassName>() {
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleScalarCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleScalarCompareTimestampColumn.txt
index ba6ca66a9d..f5f59c26cf 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleScalarCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterLongDoubleScalarCompareTimestampColumn.txt
@@ -18,6 +18,10 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnBetween.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnBetween.txt
index 12f73da01c..4298d79603 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnBetween.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnBetween.txt
@@ -20,7 +20,6 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -39,14 +38,14 @@ public class <ClassName> extends VectorExpression {
   private int colNum;
 
   // The comparison is of the form "column BETWEEN leftValue AND rightValue"
-  private PisaTimestamp leftValue;
-  private PisaTimestamp rightValue;
-  private PisaTimestamp scratchValue;
+  private Timestamp leftValue;
+  private Timestamp rightValue;
+  private Timestamp scratchValue;
 
   public <ClassName>(int colNum, Timestamp leftValue, Timestamp rightValue) {
     this.colNum = colNum;
-    this.leftValue = new PisaTimestamp(leftValue);
-    this.rightValue = new PisaTimestamp(rightValue);
+    this.leftValue = leftValue;
+    this.rightValue = rightValue;
   }
 
   public <ClassName>() {
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumn.txt
index 746b2972f4..31dce1c2e4 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumn.txt
@@ -18,22 +18,421 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 
 /**
- * Generated from template FilterTimestampColumnCompareTimestampColumn.txt, which covers comparison 
- * expressions between a datetime/interval column and a scalar of the same type, however output is not
- * produced in a separate column.
+ * Generated from template FilterTimestampColumnCompareColumn.txt, which covers binary comparison
+ * filter expressions between two columns. Output is not produced in a separate column.
  * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
+
+  private static final long serialVersionUID = 1L;
 
-  public <ClassName>(int colNum1, int colNum2) { 
-    super(colNum1, colNum2);
+  private int colNum1;
+  private int colNum2;
+
+  public <ClassName>(int colNum1, int colNum2) {
+    this.colNum1 = colNum1;
+    this.colNum2 = colNum2;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #1 is type <OperandType>.
+    <InputColumnVectorType> inputColVector1 = (<InputColumnVectorType>) batch.cols[colNum1];
+
+     // Input #2 is type <OperandType>.
+    <InputColumnVectorType> inputColVector2 = (<InputColumnVectorType>) batch.cols[colNum2];
+
+    int[] sel = batch.selected;
+    boolean[] nullPos1 = inputColVector1.isNull;
+    boolean[] nullPos2 = inputColVector2.isNull;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    // handle case where neither input has nulls
+    if (inputColVector1.noNulls && inputColVector2.noNulls) {
+      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+
+        /* Either all must remain selected or all will be eliminated.
+         * Repeating property will not change.
+         */
+        if (!(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
+          batch.size = 0;
+        }
+      } else if (inputColVector1.isRepeating) {
+        if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else if (inputColVector2.isRepeating) {
+        if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else if (batch.selectedInUse) {
+        int newSize = 0;
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+            sel[newSize++] = i;
+          }
+        }
+        batch.size = newSize;
+      } else {
+        int newSize = 0;
+        for(int i = 0; i != n; i++) {
+          if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+            sel[newSize++] = i;
+          }
+        }
+        if (newSize < batch.size) {
+          batch.size = newSize;
+          batch.selectedInUse = true;
+        }
+      }
+
+    // handle case where only input 2 has nulls
+    } else if (inputColVector1.noNulls) {
+      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+        if (nullPos2[0] ||
+            !(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
+          batch.size = 0;
+        }
+      } else if (inputColVector1.isRepeating) {
+
+         // no need to check for nulls in input 1
+         if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos2[i]) {
+              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos2[i]) {
+              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else if (inputColVector2.isRepeating) {
+        if (nullPos2[0]) {
+
+          // no values will qualify because every comparison will be with NULL
+          batch.size = 0;
+          return;
+        }
+        if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else { // neither input is repeating
+        if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos2[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos2[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      }
+
+    // handle case where only input 1 has nulls
+    } else if (inputColVector2.noNulls) {
+      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+        if (nullPos1[0] ||
+            !(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
+          batch.size = 0;
+          return;
+        }
+      } else if (inputColVector1.isRepeating) {
+        if (nullPos1[0]) {
+
+          // if repeating value is null then every comparison will fail so nothing qualifies
+          batch.size = 0;
+          return;
+        }
+        if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else if (inputColVector2.isRepeating) {
+         if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos1[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos1[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else { // neither input is repeating
+         if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos1[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos1[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      }
+
+    // handle case where both inputs have nulls
+    } else {
+      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+        if (nullPos1[0] || nullPos2[0] ||
+            !(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
+          batch.size = 0;
+        }
+      } else if (inputColVector1.isRepeating) {
+         if (nullPos1[0]) {
+           batch.size = 0;
+           return;
+         }
+         if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos2[i]) {
+              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos2[i]) {
+              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else if (inputColVector2.isRepeating) {
+        if (nullPos2[0]) {
+          batch.size = 0;
+          return;
+        }
+        if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos1[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos1[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      } else { // neither input is repeating
+         if (batch.selectedInUse) {
+          int newSize = 0;
+          for(int j = 0; j != n; j++) {
+            int i = sel[j];
+            if (!nullPos1[i] && !nullPos2[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          batch.size = newSize;
+        } else {
+          int newSize = 0;
+          for(int i = 0; i != n; i++) {
+            if (!nullPos1[i] && !nullPos2[i]) {
+              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
+                sel[newSize++] = i;
+              }
+            }
+          }
+          if (newSize < batch.size) {
+            batch.size = newSize;
+            batch.selectedInUse = true;
+          }
+        }
+      }
+    }
+  }
+
+  @Override
+  public String getOutputType() {
+    return "boolean";
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return -1;
   }
 
   @Override
@@ -43,8 +442,8 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"),
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"),
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumnBase.txt
deleted file mode 100644
index b5a7a7afb0..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampColumnBase.txt
+++ /dev/null
@@ -1,429 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-
-/**
- * Generated from template FilterTimestampColumnCompareColumn.txt, which covers binary comparison
- * filter expressions between two columns. Output is not produced in a separate column.
- * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
- */
-public abstract class <ClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum1;
-  private int colNum2;
-
-  public <ClassName>(int colNum1, int colNum2) {
-    this.colNum1 = colNum1;
-    this.colNum2 = colNum2;
-  }
-
-  public <ClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum1];
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum2];
-    int[] sel = batch.selected;
-    boolean[] nullPos1 = inputColVector1.isNull;
-    boolean[] nullPos2 = inputColVector2.isNull;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    // handle case where neither input has nulls
-    if (inputColVector1.noNulls && inputColVector2.noNulls) {
-      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-
-        /* Either all must remain selected or all will be eliminated.
-         * Repeating property will not change.
-         */
-        if (!(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
-          batch.size = 0;
-        }
-      } else if (inputColVector1.isRepeating) {
-        if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else if (inputColVector2.isRepeating) {
-        if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else if (batch.selectedInUse) {
-        int newSize = 0;
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-            sel[newSize++] = i;
-          }
-        }
-        batch.size = newSize;
-      } else {
-        int newSize = 0;
-        for(int i = 0; i != n; i++) {
-          if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-            sel[newSize++] = i;
-          }
-        }
-        if (newSize < batch.size) {
-          batch.size = newSize;
-          batch.selectedInUse = true;
-        }
-      }
-
-    // handle case where only input 2 has nulls
-    } else if (inputColVector1.noNulls) {
-      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-        if (nullPos2[0] ||
-            !(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
-          batch.size = 0;
-        }
-      } else if (inputColVector1.isRepeating) {
-
-         // no need to check for nulls in input 1
-         if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos2[i]) {
-              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos2[i]) {
-              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else if (inputColVector2.isRepeating) {
-        if (nullPos2[0]) {
-
-          // no values will qualify because every comparison will be with NULL
-          batch.size = 0;
-          return;
-        }
-        if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else { // neither input is repeating
-        if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos2[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos2[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      }
-
-    // handle case where only input 1 has nulls
-    } else if (inputColVector2.noNulls) {
-      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-        if (nullPos1[0] ||
-            !(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
-          batch.size = 0;
-          return;
-        }
-      } else if (inputColVector1.isRepeating) {
-        if (nullPos1[0]) {
-
-          // if repeating value is null then every comparison will fail so nothing qualifies
-          batch.size = 0;
-          return;
-        }
-        if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else if (inputColVector2.isRepeating) {
-         if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos1[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos1[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else { // neither input is repeating
-         if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos1[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos1[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      }
-
-    // handle case where both inputs have nulls
-    } else {
-      if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-        if (nullPos1[0] || nullPos2[0] ||
-            !(inputColVector1.compareTo(0, inputColVector2, 0) <OperatorSymbol> 0)) {
-          batch.size = 0;
-        }
-      } else if (inputColVector1.isRepeating) {
-         if (nullPos1[0]) {
-           batch.size = 0;
-           return;
-         }
-         if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos2[i]) {
-              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos2[i]) {
-              if (inputColVector1.compareTo(0, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else if (inputColVector2.isRepeating) {
-        if (nullPos2[0]) {
-          batch.size = 0;
-          return;
-        }
-        if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos1[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos1[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, 0) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      } else { // neither input is repeating
-         if (batch.selectedInUse) {
-          int newSize = 0;
-          for(int j = 0; j != n; j++) {
-            int i = sel[j];
-            if (!nullPos1[i] && !nullPos2[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          batch.size = newSize;
-        } else {
-          int newSize = 0;
-          for(int i = 0; i != n; i++) {
-            if (!nullPos1[i] && !nullPos2[i]) {
-              if (inputColVector1.compareTo(i, inputColVector2, i) <OperatorSymbol> 0) {
-                sel[newSize++] = i;
-              }
-            }
-          }
-          if (newSize < batch.size) {
-            batch.size = newSize;
-            batch.selectedInUse = true;
-          }
-        }
-      }
-    }
-  }
-
-  @Override
-  public String getOutputType() {
-    return "boolean";
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return -1;
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalar.txt
index f744d9b77b..bab8508c83 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalar.txt
@@ -20,24 +20,130 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
- * Generated from template FilterTimestampColumnCompareScalar.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type, however output is not
- * produced in a separate column.
+ * Generated from template FilterColumnCompareScalar.txt, which covers binary comparison
+ * expressions between a column and a scalar, however output is not produced in a separate column.
  * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
+
+  private static final long serialVersionUID = 1L;
+
+  private int colNum;
+  private <HiveOperandType> value;
 
-  public <ClassName>(int colNum, Timestamp value) {
-    super(colNum, new PisaTimestamp(value));
+  public <ClassName>(int colNum, <HiveOperandType> value) {
+    this.colNum = colNum;
+    this.value = value;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #1 is type <OperandType>.
+    <InputColumnVectorType> inputColVector1 = (<InputColumnVectorType>) batch.cols[colNum];
+
+    int[] sel = batch.selected;
+    boolean[] nullPos = inputColVector1.isNull;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector1.noNulls) {
+      if (inputColVector1.isRepeating) {
+        //All must be selected otherwise size would be zero
+        //Repeating property will not change.
+        if (!(inputColVector1.compareTo(0, value) <OperatorSymbol> 0)) {
+          //Entire batch is filtered out.
+          batch.size = 0;
+        }
+      } else if (batch.selectedInUse) {
+        int newSize = 0;
+        for(int j=0; j != n; j++) {
+          int i = sel[j];
+          if (inputColVector1.compareTo(i, value) <OperatorSymbol> 0) {
+            sel[newSize++] = i;
+          }
+        }
+        batch.size = newSize;
+      } else {
+        int newSize = 0;
+        for(int i = 0; i != n; i++) {
+          if (inputColVector1.compareTo(i, value) <OperatorSymbol> 0) {
+            sel[newSize++] = i;
+          }
+        }
+        if (newSize < n) {
+          batch.size = newSize;
+          batch.selectedInUse = true;
+        }
+      }
+    } else {
+      if (inputColVector1.isRepeating) {
+        //All must be selected otherwise size would be zero
+        //Repeating property will not change.
+        if (!nullPos[0]) {
+          if (!(inputColVector1.compareTo(0, value) <OperatorSymbol> 0)) {
+            //Entire batch is filtered out.
+            batch.size = 0;
+          }
+        } else {
+          batch.size = 0;
+        }
+      } else if (batch.selectedInUse) {
+        int newSize = 0;
+        for(int j=0; j != n; j++) {
+          int i = sel[j];
+          if (!nullPos[i]) {
+           if (inputColVector1.compareTo(i, value) <OperatorSymbol> 0) {
+             sel[newSize++] = i;
+           }
+          }
+        }
+        //Change the selected vector
+        batch.size = newSize;
+      } else {
+        int newSize = 0;
+        for(int i = 0; i != n; i++) {
+          if (!nullPos[i]) {
+            if (inputColVector1.compareTo(i, value) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+        }
+        if (newSize < n) {
+          batch.size = newSize;
+          batch.selectedInUse = true;
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return -1;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "boolean";
   }
 
   @Override
@@ -47,8 +153,8 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"),
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"),
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalarBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalarBase.txt
deleted file mode 100644
index c84b4bf1c7..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampColumnCompareTimestampScalarBase.txt
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template FilterColumnCompareScalar.txt, which covers binary comparison
- * expressions between a column and a scalar, however output is not produced in a separate column.
- * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
- */
-public abstract class <ClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-
-  public <ClassName>(int colNum, PisaTimestamp value) {
-    this.colNum = colNum;
-    this.value = value;
-  }
-
-  public <ClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    TimestampColumnVector inputColVector = (TimestampColumnVector) batch.cols[colNum];
-    int[] sel = batch.selected;
-    boolean[] nullPos = inputColVector.isNull;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector.noNulls) {
-      if (inputColVector.isRepeating) {
-        //All must be selected otherwise size would be zero
-        //Repeating property will not change.
-        if (!(inputColVector.compareTo(0, value) <OperatorSymbol> 0)) {
-          //Entire batch is filtered out.
-          batch.size = 0;
-        }
-      } else if (batch.selectedInUse) {
-        int newSize = 0;
-        for(int j=0; j != n; j++) {
-          int i = sel[j];
-          if (inputColVector.compareTo(i, value) <OperatorSymbol> 0) {
-            sel[newSize++] = i;
-          }
-        }
-        batch.size = newSize;
-      } else {
-        int newSize = 0;
-        for(int i = 0; i != n; i++) {
-          if (inputColVector.compareTo(i, value) <OperatorSymbol> 0) {
-            sel[newSize++] = i;
-          }
-        }
-        if (newSize < n) {
-          batch.size = newSize;
-          batch.selectedInUse = true;
-        }
-      }
-    } else {
-      if (inputColVector.isRepeating) {
-        //All must be selected otherwise size would be zero
-        //Repeating property will not change.
-        if (!nullPos[0]) {
-          if (!(inputColVector.compareTo(0, value) <OperatorSymbol> 0)) {
-            //Entire batch is filtered out.
-            batch.size = 0;
-          }
-        } else {
-          batch.size = 0;
-        }
-      } else if (batch.selectedInUse) {
-        int newSize = 0;
-        for(int j=0; j != n; j++) {
-          int i = sel[j];
-          if (!nullPos[i]) {
-           if (inputColVector.compareTo(i, value) <OperatorSymbol> 0) {
-             sel[newSize++] = i;
-           }
-          }
-        }
-        //Change the selected vector
-        batch.size = newSize;
-      } else {
-        int newSize = 0;
-        for(int i = 0; i != n; i++) {
-          if (!nullPos[i]) {
-            if (inputColVector.compareTo(i, value) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-        }
-        if (newSize < n) {
-          batch.size = newSize;
-          batch.selectedInUse = true;
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return -1;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "boolean";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareLongDoubleColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareLongDoubleColumn.txt
index c3cd3b44c6..5e418de6b7 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareLongDoubleColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareLongDoubleColumn.txt
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -36,7 +35,7 @@ public class <ClassName> extends <BaseClassName> {
   private static final long serialVersionUID = 1L;
 
   public <ClassName>(Timestamp value, int colNum) {
-    super(new PisaTimestamp(value).<GetTimestampLongDoubleMethod>(), colNum);
+    super(TimestampColumnVector.<GetTimestampLongDoubleMethod>(value), colNum);
   }
 
   public <ClassName>() {
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumn.txt
index 05ab310250..ff5d11e52c 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumn.txt
@@ -20,24 +20,132 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.common.type.HiveDecimal;
 
 /**
- * Generated from template FilterTimestampScalarCompareTimestampColumn.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type, however output is not
- * produced in a separate column.
- * The selected vector of the input {@link VectorizedRowBatch} is updated for in-place filtering.
+ * This is a generated class to evaluate a <OperatorSymbol> comparison on a vector of timestamp
+ * values.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
+
+  private static final long serialVersionUID = 1L;
+
+  private int colNum;
+  private <HiveOperandType> value;
 
-  public <ClassName>(Timestamp value, int colNum) {
-    super(new PisaTimestamp(value), colNum);
+  public <ClassName>(<HiveOperandType> value, int colNum) {
+    this.colNum = colNum;
+    this.value = value;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+     // Input #2 is type <OperandType>.
+    <InputColumnVectorType> inputColVector2 = (<InputColumnVectorType>) batch.cols[colNum];
+
+    int[] sel = batch.selected;
+    boolean[] nullPos = inputColVector2.isNull;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector2.noNulls) {
+      if (inputColVector2.isRepeating) {
+
+        // All must be selected otherwise size would be zero. Repeating property will not change.
+        if (!(inputColVector2.compareTo(value, 0) <OperatorSymbol> 0)) {
+
+          // Entire batch is filtered out.
+          batch.size = 0;
+        }
+      } else if (batch.selectedInUse) {
+        int newSize = 0;
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          if (inputColVector2.compareTo(value, i) <OperatorSymbol> 0) {
+            sel[newSize++] = i;
+          }
+        }
+        batch.size = newSize;
+      } else {
+        int newSize = 0;
+        for(int i = 0; i != n; i++) {
+          if (inputColVector2.compareTo(value, i) <OperatorSymbol> 0) {
+            sel[newSize++] = i;
+          }
+        }
+        if (newSize < n) {
+          batch.size = newSize;
+          batch.selectedInUse = true;
+        }
+      }
+    } else {
+      if (inputColVector2.isRepeating) {
+
+        // All must be selected otherwise size would be zero. Repeating property will not change.
+        if (!nullPos[0]) {
+          if (!(inputColVector2.compareTo(value, 0) <OperatorSymbol> 0)) {
+
+            // Entire batch is filtered out.
+            batch.size = 0;
+          }
+        } else {
+          batch.size = 0;
+        }
+      } else if (batch.selectedInUse) {
+        int newSize = 0;
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          if (!nullPos[i]) {
+           if (inputColVector2.compareTo(value, i) <OperatorSymbol> 0) {
+             sel[newSize++] = i;
+           }
+          }
+        }
+
+        // Change the selected vector
+        batch.size = newSize;
+      } else {
+        int newSize = 0;
+        for(int i = 0; i != n; i++) {
+          if (!nullPos[i]) {
+            if (inputColVector2.compareTo(value, i) <OperatorSymbol> 0) {
+              sel[newSize++] = i;
+            }
+          }
+        }
+        if (newSize < n) {
+          batch.size = newSize;
+          batch.selectedInUse = true;
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return -1;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "boolean";
   }
 
   @Override
@@ -47,8 +155,8 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.FILTER)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"),
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"),
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumnBase.txt
deleted file mode 100644
index 608faefd77..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/FilterTimestampScalarCompareTimestampColumnBase.txt
+++ /dev/null
@@ -1,147 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.hive.common.type.HiveDecimal;
-
-/**
- * This is a generated class to evaluate a <OperatorSymbol> comparison on a vector of timestamp
- * values.
- */
-public abstract class <ClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-
-  public <ClassName>(PisaTimestamp value, int colNum) {
-    this.colNum = colNum;
-    this.value = value;
-  }
-
-  public <ClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-    TimestampColumnVector inputColVector = (TimestampColumnVector) batch.cols[colNum];
-    int[] sel = batch.selected;
-    boolean[] nullPos = inputColVector.isNull;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector.noNulls) {
-      if (inputColVector.isRepeating) {
-
-        // All must be selected otherwise size would be zero. Repeating property will not change.
-        if (!(inputColVector.compareTo(value, 0) <OperatorSymbol> 0)) {
-
-          // Entire batch is filtered out.
-          batch.size = 0;
-        }
-      } else if (batch.selectedInUse) {
-        int newSize = 0;
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          if (inputColVector.compareTo(value, i) <OperatorSymbol> 0) {
-            sel[newSize++] = i;
-          }
-        }
-        batch.size = newSize;
-      } else {
-        int newSize = 0;
-        for(int i = 0; i != n; i++) {
-          if (inputColVector.compareTo(value, i) <OperatorSymbol> 0) {
-            sel[newSize++] = i;
-          }
-        }
-        if (newSize < n) {
-          batch.size = newSize;
-          batch.selectedInUse = true;
-        }
-      }
-    } else {
-      if (inputColVector.isRepeating) {
-
-        // All must be selected otherwise size would be zero. Repeating property will not change.
-        if (!nullPos[0]) {
-          if (!(inputColVector.compareTo(value, 0) <OperatorSymbol> 0)) {
-
-            // Entire batch is filtered out.
-            batch.size = 0;
-          }
-        } else {
-          batch.size = 0;
-        }
-      } else if (batch.selectedInUse) {
-        int newSize = 0;
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          if (!nullPos[i]) {
-           if (inputColVector.compareTo(value, i) <OperatorSymbol> 0) {
-             sel[newSize++] = i;
-           }
-          }
-        }
-
-        // Change the selected vector
-        batch.size = newSize;
-      } else {
-        int newSize = 0;
-        for(int i = 0; i != n; i++) {
-          if (!nullPos[i]) {
-            if (inputColVector.compareTo(value, i) <OperatorSymbol> 0) {
-              sel[newSize++] = i;
-            }
-          }
-        }
-        if (newSize < n) {
-          batch.size = newSize;
-          batch.selectedInUse = true;
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return -1;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "boolean";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeColumnCompareIntervalDayTimeColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeColumnCompareIntervalDayTimeColumn.txt
deleted file mode 100644
index bf62b78e22..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeColumnCompareIntervalDayTimeColumn.txt
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-
-/**
- * Generated from template IntervalDayTimeColumnCompareColumn.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type. The boolean output
- * is stored in a separate boolean column.
- */
-public class <ClassName> extends <BaseClassName> {
-
-  private static final long serialVersionUID = 1L;
-
-  public <ClassName>(int colNum1, int colNum2, int outputColumn) {
-    super(colNum1, colNum2, outputColumn);
-  }
-
-  public <ClassName>() {
-    super();
-  }
-
-  @Override
-  public VectorExpressionDescriptor.Descriptor getDescriptor() {
-    return (new VectorExpressionDescriptor.Builder())
-        .setMode(
-            VectorExpressionDescriptor.Mode.PROJECTION)
-        .setNumArguments(2)
-        .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"),
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"))
-        .setInputExpressionTypes(
-            VectorExpressionDescriptor.InputExpressionType.COLUMN,
-            VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeColumnCompareIntervalDayTimeScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeColumnCompareIntervalDayTimeScalar.txt
deleted file mode 100644
index 1abb4a3d31..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeColumnCompareIntervalDayTimeScalar.txt
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-
-/**
- * Generated from template IntervalDayTimeColumnCompareScalar.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type. The boolean output
- * is stored in a separate boolean column.
- */
-public class <ClassName> extends <BaseClassName> {
-
-  private static final long serialVersionUID = 1L;
-
-  public <ClassName>(int colNum, HiveIntervalDayTime value, int outputColumn) {
-    super(colNum, value.pisaTimestampUpdate(new PisaTimestamp()), outputColumn);
-  }
-
-  public <ClassName>() {
-    super();
-  }
-
-  @Override
-  public VectorExpressionDescriptor.Descriptor getDescriptor() {
-    return (new VectorExpressionDescriptor.Builder())
-        .setMode(
-            VectorExpressionDescriptor.Mode.PROJECTION)
-        .setNumArguments(2)
-        .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"),
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"))
-        .setInputExpressionTypes(
-            VectorExpressionDescriptor.InputExpressionType.COLUMN,
-            VectorExpressionDescriptor.InputExpressionType.SCALAR).build();
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeScalarCompareIntervalDayTimeColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeScalarCompareIntervalDayTimeColumn.txt
deleted file mode 100644
index 26762ff9a0..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalDayTimeScalarCompareIntervalDayTimeColumn.txt
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-
-/**
- * Generated from template IntervalDayTimeColumnCompareScalar.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type. The boolean output
- * is stored in a separate boolean column.
- */
-public class <ClassName> extends <BaseClassName> {
-
-  private static final long serialVersionUID = 1L;
-
-  public <ClassName>(HiveIntervalDayTime value, int colNum, int outputColumn) {
-    super(value.pisaTimestampUpdate(new PisaTimestamp()), colNum, outputColumn);
-  }
-
-  public <ClassName>() {
-    super();
-  }
-
-  @Override
-  public VectorExpressionDescriptor.Descriptor getDescriptor() {
-    return (new VectorExpressionDescriptor.Builder())
-        .setMode(
-            VectorExpressionDescriptor.Mode.PROJECTION)
-        .setNumArguments(2)
-        .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"),
-            VectorExpressionDescriptor.ArgumentType.getType("interval_day_time"))
-        .setInputExpressionTypes(
-            VectorExpressionDescriptor.InputExpressionType.SCALAR,
-            VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateColumn.txt
index 7ae84b7e5e..8e3a419ed6 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateColumn.txt
@@ -18,12 +18,15 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Date;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template DateColumnArithmeticIntervalYearMonthColumn.txt, which covers binary arithmetic
@@ -36,12 +39,18 @@ public class <ClassName> extends VectorExpression {
   private int colNum1;
   private int colNum2;
   private int outputColumn;
+  private HiveIntervalYearMonth scratchIntervalYearMonth1;
+  private Date scratchDate2;
+  private Date outputDate;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
     this.colNum1 = colNum1;
     this.colNum2 = colNum2;
     this.outputColumn = outputColumn;
+    scratchIntervalYearMonth1 = new HiveIntervalYearMonth();
+    scratchDate2 = new Date(0);
+    outputDate = new Date(0);
   }
 
   public <ClassName>() {
@@ -54,10 +63,10 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type interval_year_month (months).
+    // Input #1 is type interval_year_month.
     LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum1];
 
-    // Input #2 is type date (epochDays).
+    // Input #2 is type date.
     LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum2];
 
     // Output is type date.
@@ -89,40 +98,64 @@ public class <ClassName> extends VectorExpression {
      * conditional checks in the inner loop.
      */
     if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputVector[0] = dtm.addMonthsToDays(vector2[0], <OperatorSymbol> (int) vector1[0]);
+      scratchIntervalYearMonth1.set((int) vector1[0]);
+      scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      dtm.<OperatorMethod>(
+          scratchIntervalYearMonth1, scratchDate2, outputDate);
+      outputVector[0] = DateWritable.dateToDays(outputDate);
     } else if (inputColVector1.isRepeating) {
-      long value1 = vector1[0];
+      scratchIntervalYearMonth1.set((int) vector1[0]);
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector2[i], <OperatorSymbol> (int) value1);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector2[i], <OperatorSymbol> (int) value1);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else if (inputColVector2.isRepeating) {
-      long value2 = vector2[0];
+      scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[0]));
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(value2, <OperatorSymbol> (int) vector1[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(value2, <OperatorSymbol> (int) vector1[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector2[i], <OperatorSymbol> (int) vector1[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector2[i], <OperatorSymbol> (int) vector1[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateScalar.txt
index 2f2522d385..ad65d520a9 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticDateScalar.txt
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Date;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -25,6 +27,7 @@ import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template DateColumnArithmeticIntervalYearMonthScalar.txt, which covers binary arithmetic
@@ -35,14 +38,18 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private long value;
+  private Date value;
   private int outputColumn;
+  private HiveIntervalYearMonth scratchIntervalYearMonth1;
+  private Date outputDate;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum, long value, int outputColumn) {
     this.colNum = colNum;
-    this.value = value;
+    this.value = new Date(DateWritable.daysToMillis((int) value));
     this.outputColumn = outputColumn;
+    scratchIntervalYearMonth1 = new HiveIntervalYearMonth();
+    outputDate = new Date(0);
   }
 
   public <ClassName>() {
@@ -56,18 +63,18 @@ public class <ClassName> extends VectorExpression {
     }
 
     // Input #1 is type interval_year_mont (epochMonths).
-    LongColumnVector inputColVector = (LongColumnVector) batch.cols[colNum];
+    LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum];
 
     // Output is type date.
     LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector.isNull;
+    boolean[] inputIsNull = inputColVector1.isNull;
     boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector.noNulls;
-    outputColVector.isRepeating = inputColVector.isRepeating;
+    outputColVector.noNulls = inputColVector1.noNulls;
+    outputColVector.isRepeating = inputColVector1.isRepeating;
     int n = batch.size;
-    long[] vector = inputColVector.vector;
+    long[] vector1 = inputColVector1.vector;
     long[] outputVector = outputColVector.vector;
 
     // return immediately if batch is empty
@@ -75,32 +82,46 @@ public class <ClassName> extends VectorExpression {
       return;
     }
 
-    if (inputColVector.isRepeating) {
-      outputVector[0] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[0]);
-
+    if (inputColVector1.isRepeating) {
+      scratchIntervalYearMonth1.set((int) vector1[0]);
+      dtm.<OperatorMethod>(
+          scratchIntervalYearMonth1, value, outputDate);
+      outputVector[0] = DateWritable.dateToDays(outputDate);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector.noNulls) {
+    } else if (inputColVector1.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else /* there are nulls */ {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(value, <OperatorSymbol> (int) vector[i]);
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampColumn.txt
index b3da89ffe4..858c3d725b 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampColumn.txt
@@ -18,7 +18,9 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -37,14 +39,14 @@ public class <ClassName> extends VectorExpression {
   private int colNum1;
   private int colNum2;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
+  private HiveIntervalYearMonth scratchIntervalYearMonth1;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
     this.colNum1 = colNum1;
     this.colNum2 = colNum2;
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchIntervalYearMonth1 = new HiveIntervalYearMonth();
   }
 
   public <ClassName>() {
@@ -57,10 +59,10 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type Interval_Year_Month (months).
+    // Input #1 is type Interval_Year_Month.
     LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum1];
 
-    // Input #2 is type Timestamp (PisaTimestamp).
+    // Input #2 is type Timestamp.
     TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum2];
 
     // Output is type Timestamp.
@@ -91,54 +93,59 @@ public class <ClassName> extends VectorExpression {
      * conditional checks in the inner loop.
      */
     if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputColVector.set(0,
-          dtm.addMonthsToPisaTimestamp(inputColVector2.asScratchPisaTimestamp(0), <OperatorSymbol> (int) vector1[0],
-              scratchPisaTimestamp));
+      scratchIntervalYearMonth1.set((int) vector1[0]);
+      dtm.<OperatorMethod>(
+          scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(0), outputColVector.getScratchTimestamp());
+      outputColVector.setFromScratchTimestamp(0);
     } else if (inputColVector1.isRepeating) {
-      long value1 = vector1[0];
+      scratchIntervalYearMonth1.set((int) vector1[0]);
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector2.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value1,
-                  scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector2.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value1,
-                  scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else if (inputColVector2.isRepeating) {
-      PisaTimestamp value2 = inputColVector2.asScratchPisaTimestamp(0);
+      Timestamp value2 = inputColVector2.asScratchTimestamp(0);
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(value2, <OperatorSymbol> (int) vector1[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(value2, <OperatorSymbol> (int) vector1[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector2.asScratchPisaTimestamp(i), <OperatorSymbol> (int) vector1[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(inputColVector2.asScratchPisaTimestamp(i), <OperatorSymbol> (int) vector1[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampScalar.txt
index 81f2a77823..66fffd25a2 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthColumnArithmeticTimestampScalar.txt
@@ -20,7 +20,7 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -38,16 +38,16 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private PisaTimestamp value;
+  private Timestamp value;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
+  private HiveIntervalYearMonth scratchIntervalYearMonth1;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum, Timestamp value, int outputColumn) {
     this.colNum = colNum;
-    this.value = new PisaTimestamp(value);
+    this.value = value;
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchIntervalYearMonth1 = new HiveIntervalYearMonth();
   }
 
   public <ClassName>() {
@@ -60,7 +60,7 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type interval_year_month (epochMonths).
+    // Input #1 is type interval_year_month.
     LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum];
 
     // Output is type Timestamp.
@@ -81,41 +81,45 @@ public class <ClassName> extends VectorExpression {
     }
 
     if (inputColVector1.isRepeating) {
-      outputColVector.set(0,
-          dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector1[0],
-              scratchPisaTimestamp));
-
+      scratchIntervalYearMonth1.set((int) vector1[0]);
+      dtm.<OperatorMethod>(
+          scratchIntervalYearMonth1, value, outputColVector.getScratchTimestamp());
+      outputColVector.setFromScratchTimestamp(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
     } else if (inputColVector1.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector1[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector1[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else /* there are nulls */ {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector1[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector1[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth1.set((int) vector1[i]);
+          dtm.<OperatorMethod>(
+              scratchIntervalYearMonth1, value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticDateColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticDateColumn.txt
index 3f4f05f697..ddde913b8f 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticDateColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticDateColumn.txt
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Date;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -33,6 +35,7 @@ import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template DateTimeScalarArithmeticIntervalYearMonthColumn.txt.
@@ -44,14 +47,18 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private long value;
+  private HiveIntervalYearMonth value;
   private int outputColumn;
+  private Date scratchDate2;
+  private Date outputDate;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(long value, int colNum, int outputColumn) {
     this.colNum = colNum;
-    this.value = value;
+    this.value = new HiveIntervalYearMonth((int) value);
     this.outputColumn = outputColumn;
+    scratchDate2 = new Date(0);
+    outputDate = new Date(0);
   }
 
   public <ClassName>() {
@@ -70,18 +77,18 @@ public class <ClassName> extends VectorExpression {
     }
 
     // Input #2 is type date.
-    LongColumnVector inputColVector = (LongColumnVector) batch.cols[colNum];
+    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum];
 
     // Output is type Date.
     LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector.isNull;
+    boolean[] inputIsNull = inputColVector2.isNull;
     boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector.noNulls;
-    outputColVector.isRepeating = inputColVector.isRepeating;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
     int n = batch.size;
-    long[] vector = inputColVector.vector;
+    long[] vector2 = inputColVector2.vector;
     long[] outputVector = outputColVector.vector;
 
     // return immediately if batch is empty
@@ -89,32 +96,46 @@ public class <ClassName> extends VectorExpression {
       return;
     }
 
-    if (inputColVector.isRepeating) {
-      outputVector[0] = dtm.addMonthsToDays(vector[0], <OperatorSymbol> (int) value);
-
+    if (inputColVector2.isRepeating) {
+      scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      dtm.<OperatorMethod>(
+          value, scratchDate2, outputDate);
+      outputVector[0] = DateWritable.dateToDays(outputDate);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector.noNulls) {
+    } else if (inputColVector2.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
       }
     } else {                         /* there are nulls */
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] = dtm.addMonthsToDays(vector[i], <OperatorSymbol> (int) value);
+          scratchDate2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchDate2, outputDate);
+          outputVector[i] = DateWritable.dateToDays(outputDate);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticTimestampColumn.txt
index 47d611eaa6..cbb7021f76 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/IntervalYearMonthScalarArithmeticTimestampColumn.txt
@@ -18,11 +18,13 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 /*
  * Because of the templatized nature of the code, either or both
  * of these ColumnVector imports may be needed. Listing both of them
@@ -44,16 +46,14 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private long value;
+  private HiveIntervalYearMonth value;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(long value, int colNum, int outputColumn) {
     this.colNum = colNum;
-    this.value = value;
+    this.value = new HiveIntervalYearMonth((int) value);
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
   }
 
   public <ClassName>() {
@@ -72,16 +72,16 @@ public class <ClassName> extends VectorExpression {
     }
 
     // Input #2 is type timestamp.
-    TimestampColumnVector inputColVector = (TimestampColumnVector) batch.cols[colNum];
+    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum];
 
-        // Output is type Timestamp.
+    // Output is type Timestamp.
     TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector.isNull;
+    boolean[] inputIsNull = inputColVector2.isNull;
     boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector.noNulls;
-    outputColVector.isRepeating = inputColVector.isRepeating;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
     int n = batch.size;
 
     // return immediately if batch is empty
@@ -89,42 +89,41 @@ public class <ClassName> extends VectorExpression {
       return;
     }
 
-    if (inputColVector.isRepeating) {
-      outputColVector.set(0,
-         dtm.addMonthsToPisaTimestamp(inputColVector.asScratchPisaTimestamp(0), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
-
+    if (inputColVector2.isRepeating) {
+      dtm.<OperatorMethod>(
+          value, inputColVector2.asScratchTimestamp(0), outputColVector.getScratchTimestamp());
+      outputColVector.setFromScratchTimestamp(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector.noNulls) {
+    } else if (inputColVector2.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(inputColVector.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else {                         /* there are nulls */
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratchTimestamp(i), outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampColumn.txt
index e804e2add8..9ccfaac8ba 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampColumn.txt
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampScalar.txt
index 90720ba44c..c7d8c65740 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleColumnCompareTimestampScalar.txt
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -41,7 +40,7 @@ public class <ClassName> extends VectorExpression {
 
   public <ClassName>(int colNum, Timestamp value, int outputColumn) {
     this.colNum = colNum;
-    this.value = new PisaTimestamp(value).<GetTimestampLongDoubleMethod>();
+    this.value = TimestampColumnVector.<GetTimestampLongDoubleMethod>(value);
     this.outputColumn = outputColumn;
   }
 
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleScalarCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleScalarCompareTimestampColumn.txt
index 7d3856ab48..d47bc10085 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleScalarCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/LongDoubleScalarCompareTimestampColumn.txt
@@ -21,6 +21,7 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumn.txt
index b086a88b5e..27e083d587 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumn.txt
@@ -18,28 +18,156 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template TimestampColumnArithmeticDateColumn.txt, which covers binary arithmetic
  * expressions between columns.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int colNum1;
+  private int colNum2;
+  private int outputColumn;
+  private Timestamp scratchTimestamp2;
+  private DateTimeMath dtm = new DateTimeMath();
+
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
-    super(colNum1, colNum2, outputColumn);
+    this.colNum1 = colNum1;
+    this.colNum2 = colNum2;
+    this.outputColumn = outputColumn;
+    scratchTimestamp2 = new Timestamp(0);
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #1 is type <OperandType1>.
+    <InputColumnVectorType1> inputColVector1 = (<InputColumnVectorType1>) batch.cols[colNum1];
+
+    // Input #2 is type date (days).  For the math we convert it to a timestamp.
+    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum2];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    int n = batch.size;
+
+    long[] vector2 = inputColVector2.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    outputColVector.isRepeating =
+         inputColVector1.isRepeating && inputColVector2.isRepeating
+      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
+      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
+
+    // Handle nulls first
+    NullUtil.propagateNullsColCol(
+      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
+
+    /* Disregard nulls for processing. In other words,
+     * the arithmetic operation is performed even if one or
+     * more inputs are null. This is to improve speed by avoiding
+     * conditional checks in the inner loop.
+     */
+    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+      scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      dtm.<OperatorMethod>(
+          inputColVector1.asScratch<CamelOperandType1>(0), scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+    } else if (inputColVector1.isRepeating) {
+      <HiveOperandType1> value1 = inputColVector1.asScratch<CamelOperandType1>(0);
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value1, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value1, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else if (inputColVector2.isRepeating) {
+      scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    }
+
+    /* For the case when the output can have null values, follow
+     * the convention that the data values must be 1 for long and
+     * NaN for double. This is to prevent possible later zero-divide errors
+     * in complex arithmetic expressions like col2 / (col1 - 1)
+     * in the case when some col1 entries are null.
+     */
+    NullUtil.setNullDataEntries<CamelReturnType>(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "interval_day_time";
   }
 
   @Override
@@ -50,7 +178,7 @@ public class <ClassName> extends <BaseClassName> {
         .setNumArguments(2)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.getType("<OperandType1>"),
-            VectorExpressionDescriptor.ArgumentType.getType("<OperandType2>"))
+            VectorExpressionDescriptor.ArgumentType.getType("date"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumnBase.txt
deleted file mode 100644
index 7f5496c85e..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateColumnBase.txt
+++ /dev/null
@@ -1,172 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-
-/**
- * Generated from template TimestampColumnArithmeticDateColumnBase.txt, which covers binary arithmetic
- * expressions between columns.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum1;
-  private int colNum2;
-  private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
-
-  public <BaseClassName>(int colNum1, int colNum2, int outputColumn) {
-    this.colNum1 = colNum1;
-    this.colNum2 = colNum2;
-    this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #1 is type timestamp (PisaTimestamp).
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum1];
-
-    // Input #2 is type date.
-    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum2];
-
-    // Output is type timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    int n = batch.size;
-
-    long[] vector2 = inputColVector2.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    outputColVector.isRepeating =
-         inputColVector1.isRepeating && inputColVector2.isRepeating
-      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
-      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
-
-    // Handle nulls first
-    NullUtil.propagateNullsColCol(
-      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
-
-    /* Disregard nulls for processing. In other words,
-     * the arithmetic operation is performed even if one or
-     * more inputs are null. This is to improve speed by avoiding
-     * conditional checks in the inner loop.
-     */
-    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-       outputColVector.<OperatorMethod>(
-          inputColVector1.asScratchPisaTimestamp(0),
-          scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0])),
-          0);
-    } else if (inputColVector1.isRepeating) {
-      PisaTimestamp value1 = inputColVector1.asScratchPisaTimestamp(0);
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value1,
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value1,
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-      }
-    } else if (inputColVector2.isRepeating) {
-      PisaTimestamp value2 =
-          scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0]));
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i),
-            value2,
-            i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i),
-            value2,
-            i);
-        }
-      }
-    } else {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i),
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i),
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-      }
-    }
-
-    /* For the case when the output can have null values, follow
-     * the convention that the data values must be 1 for long and
-     * NaN for double. This is to prevent possible later zero-divide errors
-     * in complex arithmetic expressions like col2 / (col1 - 1)
-     * in the case when some col1 entries are null.
-     */
-    NullUtil.setNullDataEntriesTimestamp(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
-
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalar.txt
index b8404db146..8b91a4a0d6 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalar.txt
@@ -18,7 +18,9 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -26,21 +28,107 @@ import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Generated from template TimestampColumnArithmeticDateScalar.txt, which covers binary arithmetic
  * expressions between a column and a scalar.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int colNum;
+  private Timestamp value;
+  private int outputColumn;
+  private DateTimeMath dtm = new DateTimeMath();
+
   public <ClassName>(int colNum, long value, int outputColumn) {
-    super(colNum, value, outputColumn);
+    this.colNum = colNum;
+    this.value = new Timestamp(0);
+    this.value.setTime(DateWritable.daysToMillis((int) value));
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #1 is type <OperandType1>.
+    <InputColumnVectorType1> inputColVector1 = (<InputColumnVectorType1>) batch.cols[colNum];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] inputIsNull = inputColVector1.isNull;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = inputColVector1.noNulls;
+    outputColVector.isRepeating = inputColVector1.isRepeating;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector1.isRepeating) {
+      dtm.<OperatorMethod>(
+          inputColVector1.asScratch<CamelOperandType1>(0), value, outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+      outputIsNull[0] = inputIsNull[0];
+    } else if (inputColVector1.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+          outputIsNull[i] = inputIsNull[i];
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
+      }
+    }
+
+    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
   }
 
   @Override
@@ -51,7 +139,7 @@ public class <ClassName> extends <BaseClassName> {
         .setNumArguments(2)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.getType("<OperandType1>"),
-            VectorExpressionDescriptor.ArgumentType.getType("<OperandType2>"))
+            VectorExpressionDescriptor.ArgumentType.getType("date"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalarBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalarBase.txt
deleted file mode 100644
index c2ddd6740b..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticDateScalarBase.txt
+++ /dev/null
@@ -1,126 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-
-/**
- * Generated from template TimestampColumnArithmeticDateScalarBase.txt, which covers binary arithmetic
- * expressions between a column and a scalar.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
-
-  public <BaseClassName>(int colNum, long value, int outputColumn) {
-    this.colNum = colNum;
-    this.value = new PisaTimestamp().updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) value));
-    this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #1 is type Timestamp (PisaTimestamp).
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum];
-
-    // Output is type Timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector1.isNull;
-    boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector1.noNulls;
-    outputColVector.isRepeating = inputColVector1.isRepeating;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector1.isRepeating) {
-      outputColVector.<OperatorMethod>(
-          inputColVector1.asScratchPisaTimestamp(0), value, 0);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector1.noNulls) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-        }
-      }
-    } else /* there are nulls */ {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-          outputIsNull[i] = inputIsNull[i];
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-        }
-        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
-      }
-    }
-
-    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthColumn.txt
index 2f33920677..4ac2174aa7 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthColumn.txt
@@ -18,7 +18,9 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -37,14 +39,14 @@ public class <ClassName> extends VectorExpression {
   private int colNum1;
   private int colNum2;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
+  private HiveIntervalYearMonth scratchIntervalYearMonth2;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
     this.colNum1 = colNum1;
     this.colNum2 = colNum2;
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchIntervalYearMonth2 = new HiveIntervalYearMonth();
   }
 
   public <ClassName>() {
@@ -57,7 +59,7 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type Timestamp (PisaTimestamp).
+    // Input #1 is type Timestamp.
     TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum1];
 
     // Input #2 is type Interval_Year_Month (months).
@@ -91,52 +93,59 @@ public class <ClassName> extends VectorExpression {
      * conditional checks in the inner loop.
      */
     if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputColVector.set(0,
-          dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(0), <OperatorSymbol> (int) vector2[0],
-              scratchPisaTimestamp));
+      scratchIntervalYearMonth2.set((int) vector2[0]);
+      dtm.<OperatorMethod>(
+          inputColVector1.asScratchTimestamp(0), scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+      outputColVector.setFromScratchTimestamp(0);
     } else if (inputColVector1.isRepeating) {
+      Timestamp value1 = inputColVector1.asScratchTimestamp(0);
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(0), <OperatorSymbol> (int) vector2[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              value1, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(0), <OperatorSymbol> (int) vector2[i],
-                  scratchPisaTimestamp));
+         scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              value1, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else if (inputColVector2.isRepeating) {
+      scratchIntervalYearMonth2.set((int) vector2[0]);
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) vector2[0],
-                  scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) vector2[0],
-                  scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) vector2[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) vector2[i],
-                  scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthScalar.txt
index 9f5c24ea16..9382aca912 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticIntervalYearMonthScalar.txt
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -36,16 +36,14 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private long value;
+  private HiveIntervalYearMonth value;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
   private DateTimeMath dtm = new DateTimeMath();
 
   public <ClassName>(int colNum, long value, int outputColumn) {
     this.colNum = colNum;
-    this.value = value;
+    this.value = new HiveIntervalYearMonth((int) value);
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
   }
 
   public <ClassName>() {
@@ -58,7 +56,7 @@ public class <ClassName> extends VectorExpression {
       super.evaluateChildren(batch);
     }
 
-    // Input #1 is type Timestamp (PisaTimestamp).
+    // Input #1 is type Timestamp.
     TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum];
 
     // Output is type Timestamp.
@@ -77,41 +75,40 @@ public class <ClassName> extends VectorExpression {
     }
 
     if (inputColVector1.isRepeating) {
-      outputColVector.set(0,
-          dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(0), <OperatorSymbol> (int) value,
-              scratchPisaTimestamp));
-      
+      dtm.<OperatorMethod>(
+          inputColVector1.asScratchTimestamp(0), value, outputColVector.getScratchTimestamp());
+      outputColVector.setFromScratchTimestamp(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
     } else if (inputColVector1.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else /* there are nulls */ {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                  scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(inputColVector1.asScratchPisaTimestamp(i), <OperatorSymbol> (int) value,
-                 scratchPisaTimestamp));
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratchTimestamp(i), value, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumn.txt
index dfd45abc00..5eaa4503d3 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumn.txt
@@ -18,7 +18,9 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -27,19 +29,135 @@ import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
 
 /**
- * Generated from template TimestampColumnArithmeticTimestampColumn.txt, which covers binary arithmetic
+ * Generated from template TimestampColumnArithmeticTimestampColumnBase.txt, which covers binary arithmetic
  * expressions between columns.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int colNum1;
+  private int colNum2;
+  private int outputColumn;
+  private DateTimeMath dtm = new DateTimeMath();
+
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
-    super(colNum1, colNum2, outputColumn);
+    this.colNum1 = colNum1;
+    this.colNum2 = colNum2;
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #1 is type <OperandType1>.
+    <InputColumnVectorType1> inputColVector1 = (<InputColumnVectorType1>) batch.cols[colNum1];
+
+    // Input #2 is type <OperandType2>.
+    <InputColumnVectorType2> inputColVector2 = (<InputColumnVectorType2>) batch.cols[colNum2];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    outputColVector.isRepeating =
+         inputColVector1.isRepeating && inputColVector2.isRepeating
+      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
+      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
+
+    // Handle nulls first
+    NullUtil.propagateNullsColCol(
+      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
+
+    /* Disregard nulls for processing. In other words,
+     * the arithmetic operation is performed even if one or
+     * more inputs are null. This is to improve speed by avoiding
+     * conditional checks in the inner loop.
+     */
+    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+      dtm.<OperatorMethod>(
+          inputColVector1.asScratch<CamelOperandType1>(0), inputColVector2.asScratch<CamelOperandType2>(0), outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+    } else if (inputColVector1.isRepeating) {
+      <HiveOperandType1> value1 = inputColVector1.asScratch<CamelOperandType1>(0);
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              value1, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              value1, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else if (inputColVector2.isRepeating) {
+      <HiveOperandType2> value2 = inputColVector2.asScratch<CamelOperandType2>(0);
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    }
+
+    /* For the case when the output can have null values, follow
+     * the convention that the data values must be 1 for long and
+     * NaN for double. This is to prevent possible later zero-divide errors
+     * in complex arithmetic expressions like col2 / (col1 - 1)
+     * in the case when some col1 entries are null.
+     */
+    NullUtil.setNullDataEntries<CamelReturnType>(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
   }
 
   @Override
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumnBase.txt
deleted file mode 100644
index 0e52f6cb0c..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampColumnBase.txt
+++ /dev/null
@@ -1,152 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.util.DateTimeMath;
-
-/**
- * Generated from template TimestampColumnArithmeticTimestampColumnBase.txt, which covers binary arithmetic
- * expressions between columns.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum1;
-  private int colNum2;
-  private int outputColumn;
-  private DateTimeMath dtm = new DateTimeMath();
-
-  public <BaseClassName>(int colNum1, int colNum2, int outputColumn) {
-    this.colNum1 = colNum1;
-    this.colNum2 = colNum2;
-    this.outputColumn = outputColumn;
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #1 is type timestamp/interval_day_time (PisaTimestamp).
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum1];
-
-    // Input #2 is type timestamp/interval_day_time (PisaTimestamp).
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum2];
-
-    // Output is type timestamp/interval_day_time (PisaTimestamp).
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    outputColVector.isRepeating =
-         inputColVector1.isRepeating && inputColVector2.isRepeating
-      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
-      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
-
-    // Handle nulls first
-    NullUtil.propagateNullsColCol(
-      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
-
-    /* Disregard nulls for processing. In other words,
-     * the arithmetic operation is performed even if one or
-     * more inputs are null. This is to improve speed by avoiding
-     * conditional checks in the inner loop.
-     */
-    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputColVector.<OperatorMethod>(
-          inputColVector1.asScratchPisaTimestamp(0), inputColVector2.asScratchPisaTimestamp(0), 0);
-    } else if (inputColVector1.isRepeating) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-             inputColVector1.asScratchPisaTimestamp(0), inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-             inputColVector1.asScratchPisaTimestamp(0), inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-      }
-    } else if (inputColVector2.isRepeating) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-             inputColVector1.asScratchPisaTimestamp(i), inputColVector2.asScratchPisaTimestamp(0), i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-           outputColVector.<OperatorMethod>(
-             inputColVector1.asScratchPisaTimestamp(i), inputColVector2.asScratchPisaTimestamp(0), i);
-        }
-      }
-    } else {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-             inputColVector1.asScratchPisaTimestamp(i), inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-             inputColVector1.asScratchPisaTimestamp(i), inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-      }
-    }
-
-    /* For the case when the output can have null values, follow
-     * the convention that the data values must be 1 for long and
-     * NaN for double. This is to prevent possible later zero-divide errors
-     * in complex arithmetic expressions like col2 / (col1 - 1)
-     * in the case when some col1 entries are null.
-     */
-    NullUtil.setNullDataEntriesTimestamp(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
-
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalar.txt
index f8004ff344..c6c872fbf7 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalar.txt
@@ -19,10 +19,8 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hive.common.util.DateUtils;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -35,16 +33,100 @@ import org.apache.hadoop.hive.ql.util.DateTimeMath;
  * Generated from template TimestampColumnArithmeticTimestampScalar.txt, which covers binary arithmetic
  * expressions between a column and a scalar.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  public <ClassName>(int colNum, <ScalarHiveTimestampType2> value, int outputColumn) {
-    super(colNum, <PisaTimestampConversion2>, outputColumn);
+  private int colNum;
+  private <HiveOperandType2> value;
+  private int outputColumn;
+  private DateTimeMath dtm = new DateTimeMath();
+
+  public <ClassName>(int colNum, <HiveOperandType2> value, int outputColumn) {
+    this.colNum = colNum;
+    this.value = value;
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #1 is type <OperandType1>.
+    <InputColumnVectorType1> inputColVector1 = (<InputColumnVectorType1>) batch.cols[colNum];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] inputIsNull = inputColVector1.isNull;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = inputColVector1.noNulls;
+    outputColVector.isRepeating = inputColVector1.isRepeating;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector1.isRepeating) {
+      dtm.<OperatorMethod>(
+          inputColVector1.asScratch<CamelOperandType1>(0), value, outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+      outputIsNull[0] = inputIsNull[0];
+    } else if (inputColVector1.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+          outputIsNull[i] = inputIsNull[i];
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              inputColVector1.asScratch<CamelOperandType1>(i), value, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
+      }
+    }
+
+    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
   }
 
   @Override
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalarBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalarBase.txt
deleted file mode 100644
index a0de1b3b08..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnArithmeticTimestampScalarBase.txt
+++ /dev/null
@@ -1,125 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.util.DateTimeMath;
-
-/**
- * Generated from template TimestampColumnArithmeticTimestampScalarBase.txt, which covers binary arithmetic
- * expressions between a column and a scalar.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-  private DateTimeMath dtm = new DateTimeMath();
-
-  public <BaseClassName>(int colNum, PisaTimestamp value, int outputColumn) {
-    this.colNum = colNum;
-    this.value = value;
-    this.outputColumn = outputColumn;
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #1 is type timestamp/interval_day_time (PisaTimestamp).
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum];
-
-    // Output is type timestamp/interval_day_time.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector1.isNull;
-    boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector1.noNulls;
-    outputColVector.isRepeating = inputColVector1.isRepeating;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector1.isRepeating) {
-      outputColVector.<OperatorMethod>(
-          inputColVector1.asScratchPisaTimestamp(0), value, 0);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector1.noNulls) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-        }
-      }
-    } else /* there are nulls */ {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-          outputIsNull[i] = inputIsNull[i];
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            inputColVector1.asScratchPisaTimestamp(i), value, i);
-        }
-        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
-      }
-    }
-
-    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareLongDoubleScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareLongDoubleScalar.txt
index 4332164486..e0ae206ce9 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareLongDoubleScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareLongDoubleScalar.txt
@@ -21,6 +21,7 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumn.txt
index fb82d5e121..f9fc4251e6 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumn.txt
@@ -18,24 +18,128 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import java.sql.Timestamp;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
+import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
- * Generated from template TimestampColumnCompareTimestampColumn.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type. The boolean output
- * is stored in a separate boolean column.
+ * Generated from template TimestampColumnCompareColumn.txt, which covers comparision
+ * expressions between timestamp columns.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int colNum1;
+  private int colNum2;
+  private int outputColumn;
+
   public <ClassName>(int colNum1, int colNum2, int outputColumn) {
-    super(colNum1, colNum2, outputColumn);
+    this.colNum1 = colNum1;
+    this.colNum2 = colNum2;
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #1 is type <OperandType>.
+    <InputColumnVectorType> inputColVector1 = (<InputColumnVectorType>) batch.cols[colNum1];
+
+     // Input #2 is type <OperandType>.
+    <InputColumnVectorType> inputColVector2 = (<InputColumnVectorType>) batch.cols[colNum2];
+
+    LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
+    int[] sel = batch.selected;
+    int n = batch.size;
+    long[] outputVector = outputColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    outputColVector.isRepeating =
+         inputColVector1.isRepeating && inputColVector2.isRepeating
+      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
+      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
+
+    // Handle nulls first
+    NullUtil.propagateNullsColCol(
+      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
+
+    /* Disregard nulls for processing. In other words,
+     * the arithmetic operation is performed even if one or
+     * more inputs are null. This is to improve speed by avoiding
+     * conditional checks in the inner loop.
+     */
+    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
+      outputVector[0] = inputColVector1.compareTo(0, inputColVector2.asScratch<CamelOperandType>(0)) <OperatorSymbol> 0 ? 1 : 0;
+    } else if (inputColVector1.isRepeating) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputVector[i] = inputColVector1.compareTo(0, inputColVector2.asScratch<CamelOperandType>(i)) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputVector[i] = inputColVector1.compareTo(0, inputColVector2.asScratch<CamelOperandType>(i)) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      }
+    } else if (inputColVector2.isRepeating) {
+      <HiveOperandType> value2 = inputColVector2.asScratch<CamelOperandType>(0);
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputVector[i] = inputColVector1.compareTo(i, value2) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputVector[i] = inputColVector1.compareTo(i, value2) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      }
+    } else {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputVector[i] = inputColVector1.compareTo(i, inputColVector2.asScratch<CamelOperandType>(i)) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputVector[i] = inputColVector1.compareTo(i, inputColVector2.asScratch<CamelOperandType>(i)) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      }
+    }
+
+    /* For the case when the output can have null values, follow
+     * the convention that the data values must be 1 for long and
+     * NaN for double. This is to prevent possible later zero-divide errors
+     * in complex arithmetic expressions like col2 / (col1 - 1)
+     * in the case when some col1 entries are null.
+     */
+    NullUtil.setNullDataEntriesLong(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "long";
   }
 
   @Override
@@ -45,8 +149,8 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"),
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"),
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumnBase.txt
deleted file mode 100644
index 302be41ce2..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampColumnBase.txt
+++ /dev/null
@@ -1,140 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template TimestampColumnCompareColumn.txt, which covers comparision
- * expressions between timestamp columns.
- */
-public abstract class <ClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum1;
-  private int colNum2;
-  private int outputColumn;
-
-  public <ClassName>(int colNum1, int colNum2, int outputColumn) {
-    this.colNum1 = colNum1;
-    this.colNum2 = colNum2;
-    this.outputColumn = outputColumn;
-  }
-
-  public <ClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum1];
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum2];
-    LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
-    int[] sel = batch.selected;
-    int n = batch.size;
-    long[] outputVector = outputColVector.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    outputColVector.isRepeating =
-         inputColVector1.isRepeating && inputColVector2.isRepeating
-      || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
-      || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
-
-    // Handle nulls first
-    NullUtil.propagateNullsColCol(
-      inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
-
-    /* Disregard nulls for processing. In other words,
-     * the arithmetic operation is performed even if one or
-     * more inputs are null. This is to improve speed by avoiding
-     * conditional checks in the inner loop.
-     */
-    if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputVector[0] = inputColVector1.compareTo(0, inputColVector2.asScratchPisaTimestamp(0)) <OperatorSymbol> 0 ? 1 : 0;
-    } else if (inputColVector1.isRepeating) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputVector[i] = inputColVector1.compareTo(0, inputColVector2.asScratchPisaTimestamp(i)) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputVector[i] = inputColVector1.compareTo(0, inputColVector2.asScratchPisaTimestamp(i)) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      }
-    } else if (inputColVector2.isRepeating) {
-      PisaTimestamp value2 = inputColVector2.asScratchPisaTimestamp(0);
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputVector[i] = inputColVector1.compareTo(i, value2) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputVector[i] = inputColVector1.compareTo(i, value2) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      }
-    } else {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputVector[i] = inputColVector1.compareTo(i, inputColVector2.asScratchPisaTimestamp(i)) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputVector[i] = inputColVector1.compareTo(i, inputColVector2.asScratchPisaTimestamp(i)) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      }
-    }
-
-    /* For the case when the output can have null values, follow
-     * the convention that the data values must be 1 for long and
-     * NaN for double. This is to prevent possible later zero-divide errors
-     * in complex arithmetic expressions like col2 / (col1 - 1)
-     * in the case when some col1 entries are null.
-     */
-    NullUtil.setNullDataEntriesLong(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "long";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalar.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalar.txt
index 58c3352b86..90701ecccc 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalar.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalar.txt
@@ -20,26 +20,116 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
-
 /**
- * Generated from template TimestampColumnCompareTimestampScalar.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type. The boolean output
- * is stored in a separate boolean column.
+ * Generated from template TimestampColumnCompareTimestampScalar.txt, which covers binary comparison
+ * expressions between a column and a scalar. The boolean output is stored in a
+ * separate boolean column.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  public <ClassName>(int colNum, Timestamp value, int outputColumn) {
-    super(colNum, new PisaTimestamp(value), outputColumn);
+  private int colNum;
+  private <HiveOperandType> value;
+  private int outputColumn;
+
+  public <ClassName>(int colNum, <HiveOperandType> value, int outputColumn) {
+    this.colNum = colNum;
+    this.value = value;
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #1 is type <OperandType>.
+    <InputColumnVectorType> inputColVector1 = (<InputColumnVectorType>) batch.cols[colNum];
+
+    LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] nullPos = inputColVector1.isNull;
+    boolean[] outNulls = outputColVector.isNull;
+    int n = batch.size;
+    long[] outputVector = outputColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    outputColVector.isRepeating = false;
+    outputColVector.noNulls = inputColVector1.noNulls;
+    if (inputColVector1.noNulls) {
+      if (inputColVector1.isRepeating) {
+        //All must be selected otherwise size would be zero
+        //Repeating property will not change.
+        outputVector[0] = inputColVector1.compareTo(0, value) <OperatorSymbol> 0 ? 1 : 0;
+        outputColVector.isRepeating = true;
+      } else if (batch.selectedInUse) {
+        for(int j=0; j != n; j++) {
+          int i = sel[j];
+          outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      }
+    } else {
+      if (inputColVector1.isRepeating) {
+        //All must be selected otherwise size would be zero
+        //Repeating property will not change.
+        if (!nullPos[0]) {
+          outputVector[0] = inputColVector1.compareTo(0, value) <OperatorSymbol> 0 ? 1 : 0;
+          outNulls[0] = false;
+        } else {
+          outNulls[0] = true;
+        }
+        outputColVector.isRepeating = true;
+      } else if (batch.selectedInUse) {
+        for(int j=0; j != n; j++) {
+          int i = sel[j];
+          if (!nullPos[i]) {
+            outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
+            outNulls[i] = false;
+          } else {
+            //comparison with null is null
+            outNulls[i] = true;
+          }
+        }
+      } else {
+        System.arraycopy(nullPos, 0, outNulls, 0, n);
+        for(int i = 0; i != n; i++) {
+          if (!nullPos[i]) {
+            outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
+          }
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "long";
   }
 
   @Override
@@ -49,8 +139,8 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"),
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"),
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.COLUMN,
             VectorExpressionDescriptor.InputExpressionType.SCALAR).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalarBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalarBase.txt
deleted file mode 100644
index ce940a4cd3..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampColumnCompareTimestampScalarBase.txt
+++ /dev/null
@@ -1,131 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template TimestampColumnCompareTimestampScalar.txt, which covers binary comparison
- * expressions between a column and a scalar. The boolean output is stored in a
- * separate boolean column.
- */
-public abstract class <ClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-
-  public <ClassName>(int colNum, PisaTimestamp value, int outputColumn) {
-    this.colNum = colNum;
-    this.value = value;
-    this.outputColumn = outputColumn;
-  }
-
-  public <ClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    TimestampColumnVector inputColVector1 = (TimestampColumnVector) batch.cols[colNum];
-    LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
-    int[] sel = batch.selected;
-    boolean[] nullPos = inputColVector1.isNull;
-    boolean[] outNulls = outputColVector.isNull;
-    int n = batch.size;
-    long[] outputVector = outputColVector.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    outputColVector.isRepeating = false;
-    outputColVector.noNulls = inputColVector1.noNulls;
-    if (inputColVector1.noNulls) {
-      if (inputColVector1.isRepeating) {
-        //All must be selected otherwise size would be zero
-        //Repeating property will not change.
-        outputVector[0] = inputColVector1.compareTo(0, value) <OperatorSymbol> 0 ? 1 : 0;
-        outputColVector.isRepeating = true;
-      } else if (batch.selectedInUse) {
-        for(int j=0; j != n; j++) {
-          int i = sel[j];
-          outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      }
-    } else {
-      if (inputColVector1.isRepeating) {
-        //All must be selected otherwise size would be zero
-        //Repeating property will not change.
-        if (!nullPos[0]) {
-          outputVector[0] = inputColVector1.compareTo(0, value) <OperatorSymbol> 0 ? 1 : 0;
-          outNulls[0] = false;
-        } else {
-          outNulls[0] = true;
-        }
-        outputColVector.isRepeating = true;
-      } else if (batch.selectedInUse) {
-        for(int j=0; j != n; j++) {
-          int i = sel[j];
-          if (!nullPos[i]) {
-            outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
-            outNulls[i] = false;
-          } else {
-            //comparison with null is null
-            outNulls[i] = true;
-          }
-        }
-      } else {
-        System.arraycopy(nullPos, 0, outNulls, 0, n);
-        for(int i = 0; i != n; i++) {
-          if (!nullPos[i]) {
-            outputVector[i] = inputColVector1.compareTo(i, value) <OperatorSymbol> 0 ? 1 : 0;
-          }
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "long";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumn.txt
index 8f89bd480c..f958be8542 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumn.txt
@@ -19,9 +19,8 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -31,28 +30,128 @@ import org.apache.hadoop.hive.ql.exec.vector.*;
  * of these ColumnVector imports may be needed. Listing both of them
  * rather than using ....vectorization.*;
  */
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
-import org.apache.hive.common.util.DateUtils;
+import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
- * Generated from template TimestampScalarArithmeticDateColumn.txt.
+ * Generated from template TimestampScalarArithmeticDateColumnBase.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  public <ClassName>(<ScalarHiveTimestampType1> value, int colNum, int outputColumn) {
-    super(<PisaTimestampConversion1>, colNum, outputColumn);
+  private int colNum;
+  private <HiveOperandType1> value;
+  private int outputColumn;
+  private Timestamp scratchTimestamp2;
+  private DateTimeMath dtm = new DateTimeMath();
+
+  public <ClassName>(<HiveOperandType1> value, int colNum, int outputColumn) {
+    this.colNum = colNum;
+    this.value = value;
+    this.outputColumn = outputColumn;
+    scratchTimestamp2 = new Timestamp(0);
   }
 
   public <ClassName>() {
   }
 
+  @Override
+  /**
+   * Method to evaluate scalar-column operation in vectorized fashion.
+   *
+   * @batch a package of rows with each column stored in a vector
+   */
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #2 is type date.
+    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum];
+
+     // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] inputIsNull = inputColVector2.isNull;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
+    int n = batch.size;
+
+    long[] vector2 = inputColVector2.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector2.isRepeating) {
+      scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      dtm.<OperatorMethod>(
+          value, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+      outputIsNull[0] = inputIsNull[0];
+    } else if (inputColVector2.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      }
+    } else {                         /* there are nulls */
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+          outputIsNull[i] = inputIsNull[i];
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.<OperatorMethod>(
+              value, scratchTimestamp2, outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
+      }
+    }
+
+    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "<ReturnType>";
+  }
+
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder())
@@ -61,7 +160,7 @@ public class <ClassName> extends <BaseClassName> {
         .setNumArguments(2)
         .setArgumentTypes(
             VectorExpressionDescriptor.ArgumentType.getType("<OperandType1>"),
-            VectorExpressionDescriptor.ArgumentType.getType("<OperandType2>"))
+            VectorExpressionDescriptor.ArgumentType.getType("date"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumnBase.txt
deleted file mode 100644
index 94be4f6ad2..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticDateColumnBase.txt
+++ /dev/null
@@ -1,151 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-/*
- * Because of the templatized nature of the code, either or both
- * of these ColumnVector imports may be needed. Listing both of them
- * rather than using ....vectorization.*;
- */
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
-
-/**
- * Generated from template TimestampScalarArithmeticDateColumnBase.txt.
- * Implements a vectorized arithmetic operator with a scalar on the left and a
- * column vector on the right. The result is output to an output column vector.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
-
-  public <BaseClassName>(PisaTimestamp value, int colNum, int outputColumn) {
-    this.colNum = colNum;
-    this.value = value;
-    this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  /**
-   * Method to evaluate scalar-column operation in vectorized fashion.
-   *
-   * @batch a package of rows with each column stored in a vector
-   */
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #2 is type date.
-    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum];
-
-        // Output is type Timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector2.isNull;
-    boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector2.noNulls;
-    outputColVector.isRepeating = inputColVector2.isRepeating;
-    int n = batch.size;
-
-    long[] vector2 = inputColVector2.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector2.isRepeating) {
-       outputColVector.<OperatorMethod>(
-         value,
-         scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0])),
-         0);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector2.noNulls) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value,
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value,
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-      }
-    } else {                         /* there are nulls */
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value,
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-          outputIsNull[i] = inputIsNull[i];
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value,
-            scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-            i);
-        }
-        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
-      }
-    }
-
-    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticIntervalYearMonthColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticIntervalYearMonthColumn.txt
index e9b9e67ab6..585027a7fa 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticIntervalYearMonthColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticIntervalYearMonthColumn.txt
@@ -18,11 +18,13 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 /*
  * Because of the templatized nature of the code, either or both
  * of these ColumnVector imports may be needed. Listing both of them
@@ -44,16 +46,16 @@ public class <ClassName> extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private PisaTimestamp value;
+  private Timestamp value;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
+  private HiveIntervalYearMonth scratchIntervalYearMonth2;
   private DateTimeMath dtm = new DateTimeMath();
 
-  public <ClassName>(PisaTimestamp value, int colNum, int outputColumn) {
+  public <ClassName>(Timestamp value, int colNum, int outputColumn) {
     this.colNum = colNum;
     this.value = value;
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchIntervalYearMonth2 = new HiveIntervalYearMonth();
   }
 
   public <ClassName>() {
@@ -72,61 +74,65 @@ public class <ClassName> extends VectorExpression {
     }
 
     // Input #2 is type Interval_Year_Month (months).
-    LongColumnVector inputColVector = (LongColumnVector) batch.cols[colNum];
+    LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum];
 
         // Output is type Timestamp.
     TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector.isNull;
+    boolean[] inputIsNull = inputColVector2.isNull;
     boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector.noNulls;
-    outputColVector.isRepeating = inputColVector.isRepeating;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
     int n = batch.size;
 
-    long[] vector = inputColVector.vector;
+    long[] vector2 = inputColVector2.vector;
 
     // return immediately if batch is empty
     if (n == 0) {
       return;
     }
 
-    if (inputColVector.isRepeating) {
-      outputColVector.set(0,
-         dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector[0],
-                 scratchPisaTimestamp));
-
+    if (inputColVector2.isRepeating) {
+      scratchIntervalYearMonth2.set((int) vector2[0]);
+      dtm.<OperatorMethod>(
+          value, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+      outputColVector.setFromScratchTimestamp(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector.noNulls) {
+    } else if (inputColVector2.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-             dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+             value, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+             value, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
       }
     } else {                         /* there are nulls */
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+             value, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i,
-              dtm.addMonthsToPisaTimestamp(value, <OperatorSymbol> (int) vector[i],
-                 scratchPisaTimestamp));
+          scratchIntervalYearMonth2.set((int) vector2[i]);
+          dtm.<OperatorMethod>(
+             value, scratchIntervalYearMonth2, outputColVector.getScratchTimestamp());
+          outputColVector.setFromScratchTimestamp(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumn.txt
index 6725908b9a..996c86a0a1 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumn.txt
@@ -19,10 +19,8 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hive.common.util.DateUtils;
 
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -38,21 +36,112 @@ import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.util.DateTimeMath;
 
 /**
- * Generated from template TimestampScalarArithmeticTimestampColumn.txt.
+ * Generated from template TimestampScalarArithmeticTimestampColumnBase.txt.
  * Implements a vectorized arithmetic operator with a scalar on the left and a
  * column vector on the right. The result is output to an output column vector.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  public <ClassName>(<ScalarHiveTimestampType1> value, int colNum, int outputColumn) {
-    super(<PisaTimestampConversion1>, colNum, outputColumn);
+  private int colNum;
+  private <HiveOperandType1> value;
+  private int outputColumn;
+  private DateTimeMath dtm = new DateTimeMath();
+
+  public <ClassName>(<HiveOperandType1> value, int colNum, int outputColumn) {
+    this.colNum = colNum;
+    this.value = value;
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
   }
 
+  @Override
+  /**
+   * Method to evaluate scalar-column operation in vectorized fashion.
+   *
+   * @batch a package of rows with each column stored in a vector
+   */
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    // Input #2 is type <OperandType2>.
+    <InputColumnVectorType2> inputColVector2 = (<InputColumnVectorType2>) batch.cols[colNum];
+
+    // Output is type <ReturnType>.
+    <OutputColumnVectorType> outputColVector = (<OutputColumnVectorType>) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] inputIsNull = inputColVector2.isNull;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    outputColVector.isRepeating = inputColVector2.isRepeating;
+    int n = batch.size;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (inputColVector2.isRepeating) {
+      dtm.<OperatorMethod>(
+          value, inputColVector2.asScratch<CamelOperandType2>(0), outputColVector.getScratch<CamelReturnType>());
+      outputColVector.setFromScratch<CamelReturnType>(0);
+      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
+      outputIsNull[0] = inputIsNull[0];
+    } else if (inputColVector2.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+
+        }
+      }
+    } else {                         /* there are nulls */
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+          outputIsNull[i] = inputIsNull[i];
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          dtm.<OperatorMethod>(
+              value, inputColVector2.asScratch<CamelOperandType2>(i), outputColVector.getScratch<CamelReturnType>());
+          outputColVector.setFromScratch<CamelReturnType>(i);
+        }
+        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
+      }
+    }
+
+    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "timestamp";
+  }
+
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder())
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumnBase.txt
deleted file mode 100644
index 0ff9226b3f..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarArithmeticTimestampColumnBase.txt
+++ /dev/null
@@ -1,139 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-import org.apache.hadoop.hive.ql.exec.vector.*;
-
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-/*
- * Because of the templatized nature of the code, either or both
- * of these ColumnVector imports may be needed. Listing both of them
- * rather than using ....vectorization.*;
- */
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
-import org.apache.hadoop.hive.ql.util.DateTimeMath;
-
-/**
- * Generated from template TimestampScalarArithmeticTimestampColumnBase.txt.
- * Implements a vectorized arithmetic operator with a scalar on the left and a
- * column vector on the right. The result is output to an output column vector.
- */
-public abstract class <BaseClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
-  private DateTimeMath dtm = new DateTimeMath();
-
-  public <BaseClassName>(PisaTimestamp value, int colNum, int outputColumn) {
-    this.colNum = colNum;
-    this.value = value;
-    this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
-  }
-
-  public <BaseClassName>() {
-  }
-
-  @Override
-  /**
-   * Method to evaluate scalar-column operation in vectorized fashion.
-   *
-   * @batch a package of rows with each column stored in a vector
-   */
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    // Input #2 is type timestamp/interval_day_time.
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum];
-
-    // Output is type timestamp/interval_day_time.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
-
-    int[] sel = batch.selected;
-    boolean[] inputIsNull = inputColVector2.isNull;
-    boolean[] outputIsNull = outputColVector.isNull;
-    outputColVector.noNulls = inputColVector2.noNulls;
-    outputColVector.isRepeating = inputColVector2.isRepeating;
-    int n = batch.size;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    if (inputColVector2.isRepeating) {
-       outputColVector.<OperatorMethod>(
-         value, inputColVector2.asScratchPisaTimestamp(0), 0);
-
-      // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0];
-    } else if (inputColVector2.noNulls) {
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value, inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value, inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-      }
-    } else {                         /* there are nulls */
-      if (batch.selectedInUse) {
-        for(int j = 0; j != n; j++) {
-          int i = sel[j];
-          outputColVector.<OperatorMethod>(
-            value, inputColVector2.asScratchPisaTimestamp(i), i);
-          outputIsNull[i] = inputIsNull[i];
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputColVector.<OperatorMethod>(
-            value, inputColVector2.asScratchPisaTimestamp(i), i);
-        }
-        System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
-      }
-    }
-
-    NullUtil.setNullOutputEntriesColScalar(outputColVector, batch.selectedInUse, sel, n);
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "timestamp";
-  }
-}
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareLongDoubleColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareLongDoubleColumn.txt
index 9e855e83b2..6815b5bc61 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareLongDoubleColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareLongDoubleColumn.txt
@@ -19,8 +19,8 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 
 import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 
+import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.*;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
@@ -35,7 +35,7 @@ public class <ClassName> extends <BaseClassName> {
   private static final long serialVersionUID = 1L;
 
   public <ClassName>(Timestamp value, int colNum, int outputColumn) {
-    super(new PisaTimestamp(value).<GetTimestampLongDoubleMethod>(), colNum, outputColumn);
+    super(TimestampColumnVector.<GetTimestampLongDoubleMethod>(value), colNum, outputColumn);
   }
 
   public <ClassName>() {
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumn.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumn.txt
index df9f3c90d2..6506c93fb2 100644
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumn.txt
+++ b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumn.txt
@@ -21,26 +21,117 @@ package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
 import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
-
 /**
- * Generated from template TimestampColumnCompareTimestampScalar.txt, which covers comparison
- * expressions between a datetime/interval column and a scalar of the same type. The boolean output
- * is stored in a separate boolean column.
+ * Generated from template ScalarCompareTimestamp.txt, which covers comparison
+ * expressions between a long/double scalar and a column. The boolean output is stored in a
+ * separate boolean column.
  */
-public class <ClassName> extends <BaseClassName> {
+public class <ClassName> extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
-  public <ClassName>(Timestamp value, int colNum, int outputColumn) {
-    super(new PisaTimestamp(value), colNum, outputColumn);
+  private int colNum;
+  private <HiveOperandType> value;
+  private int outputColumn;
+
+  public <ClassName>(<HiveOperandType> value, int colNum, int outputColumn) {
+    this.colNum = colNum;
+    this.value = value;
+    this.outputColumn = outputColumn;
   }
 
   public <ClassName>() {
-    super();
+  }
+
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+     // Input #2 is type <OperandType>.
+    <InputColumnVectorType> inputColVector2 = (<InputColumnVectorType>) batch.cols[colNum];
+
+    LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
+
+    int[] sel = batch.selected;
+    boolean[] nullPos = inputColVector2.isNull;
+    boolean[] outNulls = outputColVector.isNull;
+    int n = batch.size;
+    long[] outputVector = outputColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    outputColVector.isRepeating = false;
+    outputColVector.noNulls = inputColVector2.noNulls;
+    if (inputColVector2.noNulls) {
+      if (inputColVector2.isRepeating) {
+        //All must be selected otherwise size would be zero
+        //Repeating property will not change.
+        outputVector[0] = inputColVector2.compareTo(value, 0) <OperatorSymbol> 0 ? 1 : 0;
+        outputColVector.isRepeating = true;
+      } else if (batch.selectedInUse) {
+        for(int j=0; j != n; j++) {
+          int i = sel[j];
+          outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
+        }
+      }
+    } else {
+      if (inputColVector2.isRepeating) {
+        //All must be selected otherwise size would be zero
+        //Repeating property will not change.
+        if (!nullPos[0]) {
+          outputVector[0] = inputColVector2.compareTo(value, 0) <OperatorSymbol> 0 ? 1 : 0;
+          outNulls[0] = false;
+        } else {
+          outNulls[0] = true;
+        }
+        outputColVector.isRepeating = true;
+      } else if (batch.selectedInUse) {
+        for(int j=0; j != n; j++) {
+          int i = sel[j];
+          if (!nullPos[i]) {
+            outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
+            outNulls[i] = false;
+          } else {
+            //comparison with null is null
+            outNulls[i] = true;
+          }
+        }
+      } else {
+        System.arraycopy(nullPos, 0, outNulls, 0, n);
+        for(int i = 0; i != n; i++) {
+          if (!nullPos[i]) {
+            outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
+          }
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "long";
   }
 
   @Override
@@ -50,8 +141,8 @@ public class <ClassName> extends <BaseClassName> {
             VectorExpressionDescriptor.Mode.PROJECTION)
         .setNumArguments(2)
         .setArgumentTypes(
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"),
-            VectorExpressionDescriptor.ArgumentType.getType("timestamp"))
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"),
+            VectorExpressionDescriptor.ArgumentType.getType("<OperandType>"))
         .setInputExpressionTypes(
             VectorExpressionDescriptor.InputExpressionType.SCALAR,
             VectorExpressionDescriptor.InputExpressionType.COLUMN).build();
diff --git a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumnBase.txt b/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumnBase.txt
deleted file mode 100644
index bd345e722a..0000000000
--- a/ql/src/gen/vectorization/ExpressionTemplates/TimestampScalarCompareTimestampColumnBase.txt
+++ /dev/null
@@ -1,132 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.ql.exec.vector.expressions.gen;
-
-import java.sql.Timestamp;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
-
-/**
- * Generated from template ScalarCompareTimestamp.txt, which covers comparison
- * expressions between a long/double scalar and a column. The boolean output is stored in a
- * separate boolean column.
- */
-public abstract class <ClassName> extends VectorExpression {
-
-  private static final long serialVersionUID = 1L;
-
-  private int colNum;
-  private PisaTimestamp value;
-  private int outputColumn;
-
-  public <ClassName>(PisaTimestamp value, int colNum, int outputColumn) {
-    this.colNum = colNum;
-    this.value = value;
-    this.outputColumn = outputColumn;
-  }
-
-  public <ClassName>() {
-  }
-
-  @Override
-  public void evaluate(VectorizedRowBatch batch) {
-
-    if (childExpressions != null) {
-      super.evaluateChildren(batch);
-    }
-
-    TimestampColumnVector inputColVector2 = (TimestampColumnVector) batch.cols[colNum];
-    LongColumnVector outputColVector = (LongColumnVector) batch.cols[outputColumn];
-    int[] sel = batch.selected;
-    boolean[] nullPos = inputColVector2.isNull;
-    boolean[] outNulls = outputColVector.isNull;
-    int n = batch.size;
-    long[] outputVector = outputColVector.vector;
-
-    // return immediately if batch is empty
-    if (n == 0) {
-      return;
-    }
-
-    outputColVector.isRepeating = false;
-    outputColVector.noNulls = inputColVector2.noNulls;
-    if (inputColVector2.noNulls) {
-      if (inputColVector2.isRepeating) {
-        //All must be selected otherwise size would be zero
-        //Repeating property will not change.
-        outputVector[0] = inputColVector2.compareTo(value, 0) <OperatorSymbol> 0 ? 1 : 0;
-        outputColVector.isRepeating = true;
-      } else if (batch.selectedInUse) {
-        for(int j=0; j != n; j++) {
-          int i = sel[j];
-          outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      } else {
-        for(int i = 0; i != n; i++) {
-          outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
-        }
-      }
-    } else {
-      if (inputColVector2.isRepeating) {
-        //All must be selected otherwise size would be zero
-        //Repeating property will not change.
-        if (!nullPos[0]) {
-          outputVector[0] = inputColVector2.compareTo(value, 0) <OperatorSymbol> 0 ? 1 : 0;
-          outNulls[0] = false;
-        } else {
-          outNulls[0] = true;
-        }
-        outputColVector.isRepeating = true;
-      } else if (batch.selectedInUse) {
-        for(int j=0; j != n; j++) {
-          int i = sel[j];
-          if (!nullPos[i]) {
-            outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
-            outNulls[i] = false;
-          } else {
-            //comparison with null is null
-            outNulls[i] = true;
-          }
-        }
-      } else {
-        System.arraycopy(nullPos, 0, outNulls, 0, n);
-        for(int i = 0; i != n; i++) {
-          if (!nullPos[i]) {
-            outputVector[i] = inputColVector2.compareTo(value, i) <OperatorSymbol> 0 ? 1 : 0;
-          }
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getOutputColumn() {
-    return outputColumn;
-  }
-
-  @Override
-  public String getOutputType() {
-    return "long";
-  }
-}
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
new file mode 100644
index 0000000000..3cdf7e2358
--- /dev/null
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxIntervalDayTime.txt
@@ -0,0 +1,454 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
+
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory;
+import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.ql.plan.AggregationDesc;
+import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+
+/**
+* <ClassName>. Vectorized implementation for MIN/MAX aggregates.
+*/
+@Description(name = "<DescriptionName>",
+    value = "<DescriptionValue>")
+public class <ClassName> extends VectorAggregateExpression {
+
+    private static final long serialVersionUID = 1L;
+
+    /**
+     * class for storing the current aggregate value.
+     */
+    static private final class Aggregation implements AggregationBuffer {
+
+      private static final long serialVersionUID = 1L;
+
+      transient private final HiveIntervalDayTime value;
+
+      /**
+      * Value is explicitly (re)initialized in reset()
+      */
+      transient private boolean isNull = true;
+
+      public Aggregation() {
+        value = new HiveIntervalDayTime();
+      }
+
+      public void checkValue(IntervalDayTimeColumnVector colVector, int index) {
+        if (isNull) {
+          isNull = false;
+          colVector.intervalDayTimeUpdate(this.value, index);
+        } else if (colVector.compareTo(this.value, index) <OperatorSymbol> 0) {
+          colVector.intervalDayTimeUpdate(this.value, index);
+        }
+      }
+
+      @Override
+      public int getVariableSize() {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void reset () {
+        isNull = true;
+        this.value.set(0, 0);
+      }
+    }
+
+    private VectorExpression inputExpression;
+    private transient VectorExpressionWriter resultWriter;
+
+    public <ClassName>(VectorExpression inputExpression) {
+      this();
+      this.inputExpression = inputExpression;
+    }
+
+    public <ClassName>() {
+      super();
+    }
+
+    @Override
+    public void init(AggregationDesc desc) throws HiveException {
+      resultWriter = VectorExpressionWriterFactory.genVectorExpressionWritable(
+          desc.getParameters().get(0));
+    }
+
+    private Aggregation getCurrentAggregationBuffer(
+        VectorAggregationBufferRow[] aggregationBufferSets,
+        int aggregrateIndex,
+        int row) {
+      VectorAggregationBufferRow mySet = aggregationBufferSets[row];
+      Aggregation myagg = (Aggregation) mySet.getAggregationBuffer(aggregrateIndex);
+      return myagg;
+    }
+
+    @Override
+    public void aggregateInputSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      VectorizedRowBatch batch) throws HiveException {
+
+      int batchSize = batch.size;
+
+      if (batchSize == 0) {
+        return;
+      }
+
+      inputExpression.evaluate(batch);
+
+      IntervalDayTimeColumnVector inputColVector = (IntervalDayTimeColumnVector)batch.
+        cols[this.inputExpression.getOutputColumn()];
+
+      if (inputColVector.noNulls) {
+        if (inputColVector.isRepeating) {
+          iterateNoNullsRepeatingWithAggregationSelection(
+            aggregationBufferSets, aggregrateIndex,
+            inputColVector, batchSize);
+        } else {
+          if (batch.selectedInUse) {
+            iterateNoNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, aggregrateIndex,
+              inputColVector, batch.selected, batchSize);
+          } else {
+            iterateNoNullsWithAggregationSelection(
+              aggregationBufferSets, aggregrateIndex,
+              inputColVector, batchSize);
+          }
+        }
+      } else {
+        if (inputColVector.isRepeating) {
+          if (batch.selectedInUse) {
+            iterateHasNullsRepeatingSelectionWithAggregationSelection(
+              aggregationBufferSets, aggregrateIndex,
+              inputColVector, batchSize, batch.selected, inputColVector.isNull);
+          } else {
+            iterateHasNullsRepeatingWithAggregationSelection(
+              aggregationBufferSets, aggregrateIndex,
+              inputColVector, batchSize, inputColVector.isNull);
+          }
+        } else {
+          if (batch.selectedInUse) {
+            iterateHasNullsSelectionWithAggregationSelection(
+              aggregationBufferSets, aggregrateIndex,
+              inputColVector, batchSize, batch.selected, inputColVector.isNull);
+          } else {
+            iterateHasNullsWithAggregationSelection(
+              aggregationBufferSets, aggregrateIndex,
+              inputColVector, batchSize, inputColVector.isNull);
+          }
+        }
+      }
+    }
+
+    private void iterateNoNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregrateIndex,
+          i);
+        // Repeating use index 0.
+        myagg.checkValue(inputColVector, 0);
+      }
+    }
+
+    private void iterateNoNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int[] selection,
+      int batchSize) {
+
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregrateIndex,
+          i);
+        myagg.checkValue(inputColVector, selection[i]);
+      }
+    }
+
+    private void iterateNoNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int batchSize) {
+      for (int i=0; i < batchSize; ++i) {
+        Aggregation myagg = getCurrentAggregationBuffer(
+          aggregationBufferSets,
+          aggregrateIndex,
+          i);
+        myagg.checkValue(inputColVector, i);
+      }
+    }
+
+    private void iterateHasNullsRepeatingSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[selection[i]]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            aggregrateIndex,
+            i);
+          // Repeating use index 0.
+          myagg.checkValue(inputColVector, 0);
+        }
+      }
+
+    }
+
+    private void iterateHasNullsRepeatingWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int batchSize,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            aggregrateIndex,
+            i);
+          // Repeating use index 0.
+          myagg.checkValue(inputColVector, 0);
+        }
+      }
+    }
+
+    private void iterateHasNullsSelectionWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int batchSize,
+      int[] selection,
+      boolean[] isNull) {
+
+      for (int j=0; j < batchSize; ++j) {
+        int i = selection[j];
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            aggregrateIndex,
+            j);
+          myagg.checkValue(inputColVector, i);
+        }
+      }
+   }
+
+    private void iterateHasNullsWithAggregationSelection(
+      VectorAggregationBufferRow[] aggregationBufferSets,
+      int aggregrateIndex,
+      IntervalDayTimeColumnVector inputColVector,
+      int batchSize,
+      boolean[] isNull) {
+
+      for (int i=0; i < batchSize; ++i) {
+        if (!isNull[i]) {
+          Aggregation myagg = getCurrentAggregationBuffer(
+            aggregationBufferSets,
+            aggregrateIndex,
+            i);
+          myagg.checkValue(inputColVector, i);
+        }
+      }
+   }
+
+    @Override
+    public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
+      throws HiveException {
+
+        inputExpression.evaluate(batch);
+
+        IntervalDayTimeColumnVector inputColVector = (IntervalDayTimeColumnVector)batch.
+            cols[this.inputExpression.getOutputColumn()];
+
+        int batchSize = batch.size;
+
+        if (batchSize == 0) {
+          return;
+        }
+
+        Aggregation myagg = (Aggregation)agg;
+
+        if (inputColVector.isRepeating) {
+          if (inputColVector.noNulls &&
+            (myagg.isNull || (inputColVector.compareTo(myagg.value, 0) <OperatorSymbol> 0))) {
+            myagg.isNull = false;
+            inputColVector.intervalDayTimeUpdate(myagg.value, 0);
+          }
+          return;
+        }
+
+        if (!batch.selectedInUse && inputColVector.noNulls) {
+          iterateNoSelectionNoNulls(myagg, inputColVector, batchSize);
+        }
+        else if (!batch.selectedInUse) {
+          iterateNoSelectionHasNulls(myagg, inputColVector,
+            batchSize, inputColVector.isNull);
+        }
+        else if (inputColVector.noNulls){
+          iterateSelectionNoNulls(myagg, inputColVector, batchSize, batch.selected);
+        }
+        else {
+          iterateSelectionHasNulls(myagg, inputColVector,
+            batchSize, inputColVector.isNull, batch.selected);
+        }
+    }
+
+    private void iterateSelectionHasNulls(
+        Aggregation myagg,
+        IntervalDayTimeColumnVector inputColVector,
+        int batchSize,
+        boolean[] isNull,
+        int[] selected) {
+
+      for (int j=0; j< batchSize; ++j) {
+        int i = selected[j];
+        if (!isNull[i]) {
+          if (myagg.isNull) {
+            myagg.isNull = false;
+            inputColVector.intervalDayTimeUpdate(myagg.value, i);
+          }
+          else if (inputColVector.compareTo(myagg.value, i) <OperatorSymbol> 0) {
+            inputColVector.intervalDayTimeUpdate(myagg.value, i);
+          }
+        }
+      }
+    }
+
+    private void iterateSelectionNoNulls(
+        Aggregation myagg,
+        IntervalDayTimeColumnVector inputColVector,
+        int batchSize,
+        int[] selected) {
+
+      if (myagg.isNull) {
+        inputColVector.intervalDayTimeUpdate(myagg.value, selected[0]);
+        myagg.isNull = false;
+      }
+
+      for (int i=0; i< batchSize; ++i) {
+        int sel = selected[i];
+        if (inputColVector.compareTo(myagg.value, sel) <OperatorSymbol> 0) {
+          inputColVector.intervalDayTimeUpdate(myagg.value, sel);
+        }
+      }
+    }
+
+    private void iterateNoSelectionHasNulls(
+        Aggregation myagg,
+        IntervalDayTimeColumnVector inputColVector,
+        int batchSize,
+        boolean[] isNull) {
+
+      for(int i=0;i<batchSize;++i) {
+        if (!isNull[i]) {
+          if (myagg.isNull) {
+            inputColVector.intervalDayTimeUpdate(myagg.value, i);
+            myagg.isNull = false;
+          }
+          else if (inputColVector.compareTo(myagg.value, i) <OperatorSymbol> 0) {
+            inputColVector.intervalDayTimeUpdate(myagg.value, i);
+          }
+        }
+      }
+    }
+
+    private void iterateNoSelectionNoNulls(
+        Aggregation myagg,
+        IntervalDayTimeColumnVector inputColVector,
+        int batchSize) {
+      if (myagg.isNull) {
+        inputColVector.intervalDayTimeUpdate(myagg.value, 0);
+        myagg.isNull = false;
+      }
+
+      for (int i=0;i<batchSize;++i) {
+        if (inputColVector.compareTo(myagg.value, i) <OperatorSymbol> 0) {
+          inputColVector.intervalDayTimeUpdate(myagg.value, i);
+        }
+      }
+    }
+
+    @Override
+    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+      return new Aggregation();
+    }
+
+    @Override
+    public void reset(AggregationBuffer agg) throws HiveException {
+      Aggregation myAgg = (Aggregation) agg;
+      myAgg.reset();
+    }
+
+    @Override
+    public Object evaluateOutput(
+        AggregationBuffer agg) throws HiveException {
+    Aggregation myagg = (Aggregation) agg;
+      if (myagg.isNull) {
+        return null;
+      }
+      else {
+        return resultWriter.writeValue(myagg.value);
+      }
+    }
+
+    @Override
+    public ObjectInspector getOutputObjectInspector() {
+      return resultWriter.getObjectInspector();
+    }
+
+    @Override
+    public int getAggregationBufferFixedSize() {
+    JavaDataModel model = JavaDataModel.get();
+    return JavaDataModel.alignUp(
+      model.object() +
+      model.primitive2(),
+      model.memoryAlign());
+  }
+
+  public VectorExpression getInputExpression() {
+    return inputExpression;
+  }
+
+  public void setInputExpression(VectorExpression inputExpression) {
+    this.inputExpression = inputExpression;
+  }
+}
+
diff --git a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
index 3cdf405292..7e34965c63 100644
--- a/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
+++ b/ql/src/gen/vectorization/UDAFTemplates/VectorUDAFMinMaxTimestamp.txt
@@ -18,7 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
@@ -49,7 +50,7 @@ public class <ClassName> extends VectorAggregateExpression {
 
       private static final long serialVersionUID = 1L;
 
-      transient private final PisaTimestamp value;
+      transient private final Timestamp value;
 
       /**
       * Value is explicitly (re)initialized in reset()
@@ -57,15 +58,15 @@ public class <ClassName> extends VectorAggregateExpression {
       transient private boolean isNull = true;
 
       public Aggregation() {
-        value = new PisaTimestamp();
+        value = new Timestamp(0);
       }
 
       public void checkValue(TimestampColumnVector colVector, int index) {
         if (isNull) {
           isNull = false;
-          colVector.pisaTimestampUpdate(this.value, index);
+          colVector.timestampUpdate(this.value, index);
         } else if (colVector.compareTo(this.value, index) <OperatorSymbol> 0) {
-          colVector.pisaTimestampUpdate(this.value, index);
+          colVector.timestampUpdate(this.value, index);
         }
       }
 
@@ -77,7 +78,7 @@ public class <ClassName> extends VectorAggregateExpression {
       @Override
       public void reset () {
         isNull = true;
-        this.value.reset();
+        this.value.setTime(0);
       }
     }
 
@@ -311,7 +312,7 @@ public class <ClassName> extends VectorAggregateExpression {
           if (inputColVector.noNulls &&
             (myagg.isNull || (inputColVector.compareTo(myagg.value, 0) <OperatorSymbol> 0))) {
             myagg.isNull = false;
-            inputColVector.pisaTimestampUpdate(myagg.value, 0);
+            inputColVector.timestampUpdate(myagg.value, 0);
           }
           return;
         }
@@ -344,10 +345,10 @@ public class <ClassName> extends VectorAggregateExpression {
         if (!isNull[i]) {
           if (myagg.isNull) {
             myagg.isNull = false;
-            inputColVector.pisaTimestampUpdate(myagg.value, i);
+            inputColVector.timestampUpdate(myagg.value, i);
           }
           else if (inputColVector.compareTo(myagg.value, i) <OperatorSymbol> 0) {
-            inputColVector.pisaTimestampUpdate(myagg.value, i);
+            inputColVector.timestampUpdate(myagg.value, i);
           }
         }
       }
@@ -360,14 +361,14 @@ public class <ClassName> extends VectorAggregateExpression {
         int[] selected) {
 
       if (myagg.isNull) {
-        inputColVector.pisaTimestampUpdate(myagg.value, selected[0]);
+        inputColVector.timestampUpdate(myagg.value, selected[0]);
         myagg.isNull = false;
       }
 
       for (int i=0; i< batchSize; ++i) {
         int sel = selected[i];
         if (inputColVector.compareTo(myagg.value, sel) <OperatorSymbol> 0) {
-          inputColVector.pisaTimestampUpdate(myagg.value, sel);
+          inputColVector.timestampUpdate(myagg.value, sel);
         }
       }
     }
@@ -381,11 +382,11 @@ public class <ClassName> extends VectorAggregateExpression {
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
           if (myagg.isNull) {
-            inputColVector.pisaTimestampUpdate(myagg.value, i);
+            inputColVector.timestampUpdate(myagg.value, i);
             myagg.isNull = false;
           }
           else if (inputColVector.compareTo(myagg.value, i) <OperatorSymbol> 0) {
-            inputColVector.pisaTimestampUpdate(myagg.value, i);
+            inputColVector.timestampUpdate(myagg.value, i);
           }
         }
       }
@@ -396,13 +397,13 @@ public class <ClassName> extends VectorAggregateExpression {
         TimestampColumnVector inputColVector,
         int batchSize) {
       if (myagg.isNull) {
-        inputColVector.pisaTimestampUpdate(myagg.value, 0);
+        inputColVector.timestampUpdate(myagg.value, 0);
         myagg.isNull = false;
       }
 
       for (int i=0;i<batchSize;++i) {
         if (inputColVector.compareTo(myagg.value, i) <OperatorSymbol> 0) {
-          inputColVector.pisaTimestampUpdate(myagg.value, i);
+          inputColVector.timestampUpdate(myagg.value, i);
         }
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampUtils.java
index 5de055c4d0..bb795fa48f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampUtils.java
@@ -21,6 +21,7 @@
 import java.util.concurrent.TimeUnit;
 
 import org.apache.hadoop.hive.serde2.io.DateWritable;
+import org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 
 public final class TimestampUtils {
@@ -38,4 +39,11 @@ public static TimestampWritable timestampColumnVectorWritable(
     timestampWritable.set(timestampColVector.asScratchTimestamp(elementNum));
     return timestampWritable;
   }
+
+  public static HiveIntervalDayTimeWritable intervalDayTimeColumnVectorWritable(
+      IntervalDayTimeColumnVector intervalDayTimeColVector, int elementNum,
+      HiveIntervalDayTimeWritable intervalDayTimeWritable) {
+    intervalDayTimeWritable.set(intervalDayTimeColVector.asScratchIntervalDayTime(elementNum));
+    return intervalDayTimeWritable;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java
index 965c02714f..de0300ad65 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java
@@ -288,7 +288,26 @@ void assign(int batchIndex, Object object) {
     }
   }
 
-  private class IntervalDayTimeAssigner extends AbstractTimestampAssigner {
+  private abstract class AbstractIntervalDayTimeAssigner extends Assigner {
+
+    protected IntervalDayTimeColumnVector colVector;
+
+    AbstractIntervalDayTimeAssigner(int columnIndex) {
+      super(columnIndex);
+    }
+
+    @Override
+    void setColumnVector(VectorizedRowBatch batch) {
+      colVector = (IntervalDayTimeColumnVector) batch.cols[columnIndex];
+    }
+
+    @Override
+    void forgetColumnVector() {
+      colVector = null;
+    }
+  }
+
+  private class IntervalDayTimeAssigner extends AbstractIntervalDayTimeAssigner {
 
     IntervalDayTimeAssigner(int columnIndex) {
       super(columnIndex);
@@ -301,7 +320,7 @@ void assign(int batchIndex, Object object) {
       } else {
         HiveIntervalDayTimeWritable idtw = (HiveIntervalDayTimeWritable) object;
         HiveIntervalDayTime idt = idtw.getHiveIntervalDayTime();
-        colVector.set(batchIndex, idt.pisaTimestampUpdate(colVector.useScratchPisaTimestamp()));
+        colVector.set(batchIndex, idt);
         colVector.isNull[batchIndex] = false;
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java
index 463c8a6eeb..96b8f782d8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java
@@ -25,6 +25,7 @@
 
 import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.io.ByteWritable;
@@ -176,6 +177,16 @@ protected void assignTimestamp(TimestampWritable tw, int index) {
     }
   }
 
+  private static abstract class VectorIntervalDayTimeColumnAssign
+  extends VectorColumnAssignVectorBase<IntervalDayTimeColumnVector> {
+
+    protected void assignIntervalDayTime(HiveIntervalDayTime value, int index) {
+      outCol.set(index, value);
+    }
+    protected void assignIntervalDayTime(HiveIntervalDayTimeWritable tw, int index) {
+      outCol.set(index, tw.getHiveIntervalDayTime());
+    }
+  }
 
   public static VectorColumnAssign[] buildAssigners(VectorizedRowBatch outputBatch)
       throws HiveException {
@@ -364,7 +375,7 @@ public void assignObjectValue(Object val, int destIndex) throws HiveException {
           }
         }.init(outputBatch, (LongColumnVector) destCol);
         break;
-      case INTERVAL_DAY_TIME:outVCA = new VectorLongColumnAssign() {
+      case INTERVAL_DAY_TIME:outVCA = new VectorIntervalDayTimeColumnAssign() {
         @Override
         public void assignObjectValue(Object val, int destIndex) throws HiveException {
           if (val == null) {
@@ -372,12 +383,12 @@ public void assignObjectValue(Object val, int destIndex) throws HiveException {
           }
           else {
             HiveIntervalDayTimeWritable bw = (HiveIntervalDayTimeWritable) val;
-            assignLong(
-                DateUtils.getIntervalDayTimeTotalNanos(bw.getHiveIntervalDayTime()),
+            assignIntervalDayTime(
+                bw.getHiveIntervalDayTime(),
                 destIndex);
           }
         }
-      }.init(outputBatch, (LongColumnVector) destCol);
+      }.init(outputBatch, (IntervalDayTimeColumnVector) destCol);
       break;
       default:
         throw new HiveException("Incompatible Long vector column and primitive category " +
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
index 0949145973..935b47bd31 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java
@@ -59,6 +59,11 @@ public class VectorColumnSetInfo {
    */
   protected int[] timestampIndices;
 
+  /**
+   * indices of INTERVAL_DAY_TIME primitive keys.
+   */
+  protected int[] intervalDayTimeIndices;
+
   /**
    * Helper class for looking up a key value based on key index.
    */
@@ -68,12 +73,13 @@ public class KeyLookupHelper {
     public int stringIndex;
     public int decimalIndex;
     public int timestampIndex;
+    public int intervalDayTimeIndex;
 
     private static final int INDEX_UNUSED = -1;
 
     private void resetIndices() {
         this.longIndex = this.doubleIndex = this.stringIndex = this.decimalIndex =
-            timestampIndex = INDEX_UNUSED;
+            timestampIndex = intervalDayTimeIndex = INDEX_UNUSED;
     }
     public void setLong(int index) {
       resetIndices();
@@ -99,6 +105,11 @@ public void setTimestamp(int index) {
       resetIndices();
       this.timestampIndex= index;
     }
+
+    public void setIntervalDayTime(int index) {
+      resetIndices();
+      this.intervalDayTimeIndex= index;
+    }
   }
 
   /**
@@ -114,6 +125,7 @@ public void setTimestamp(int index) {
   protected int stringIndicesIndex;
   protected int decimalIndicesIndex;
   protected int timestampIndicesIndex;
+  protected int intervalDayTimeIndicesIndex;
 
   protected VectorColumnSetInfo(int keyCount) {
     this.keyCount = keyCount;
@@ -130,6 +142,8 @@ protected VectorColumnSetInfo(int keyCount) {
     decimalIndicesIndex = 0;
     timestampIndices = new int[this.keyCount];
     timestampIndicesIndex = 0;
+    intervalDayTimeIndices = new int[this.keyCount];
+    intervalDayTimeIndicesIndex = 0;
     indexLookup = new KeyLookupHelper[this.keyCount];
   }
 
@@ -172,6 +186,12 @@ protected void addKey(String outputType) throws HiveException {
       ++timestampIndicesIndex;
       break;
 
+    case INTERVAL_DAY_TIME:
+      intervalDayTimeIndices[intervalDayTimeIndicesIndex] = addIndex;
+      indexLookup[addIndex].setIntervalDayTime(intervalDayTimeIndicesIndex);
+      ++intervalDayTimeIndicesIndex;
+      break;
+
     default:
       throw new HiveException("Unexpected column vector type " + columnVectorType);
     }
@@ -185,5 +205,6 @@ protected void finishAdding() {
     stringIndices = Arrays.copyOf(stringIndices, stringIndicesIndex);
     decimalIndices = Arrays.copyOf(decimalIndices, decimalIndicesIndex);
     timestampIndices = Arrays.copyOf(timestampIndices, timestampIndicesIndex);
+    intervalDayTimeIndices = Arrays.copyOf(intervalDayTimeIndices, intervalDayTimeIndicesIndex);
   }
 }
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorCopyRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorCopyRow.java
index 73476a3960..c8e0284d27 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorCopyRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorCopyRow.java
@@ -223,6 +223,34 @@ void copy(VectorizedRowBatch inBatch, int inBatchIndex, VectorizedRowBatch outBa
       }
     }
   }
+
+  private class IntervalDayTimeCopyRow extends CopyRow {
+
+    IntervalDayTimeCopyRow(int inColumnIndex, int outColumnIndex) {
+      super(inColumnIndex, outColumnIndex);
+    }
+
+    @Override
+    void copy(VectorizedRowBatch inBatch, int inBatchIndex, VectorizedRowBatch outBatch, int outBatchIndex) {
+      IntervalDayTimeColumnVector inColVector = (IntervalDayTimeColumnVector) inBatch.cols[inColumnIndex];
+      IntervalDayTimeColumnVector outColVector = (IntervalDayTimeColumnVector) outBatch.cols[outColumnIndex];
+
+      if (inColVector.isRepeating) {
+        if (inColVector.noNulls || !inColVector.isNull[0]) {
+          outColVector.setElement(outBatchIndex, 0, inColVector);
+        } else {
+          VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex);
+        }
+      } else {
+        if (inColVector.noNulls || !inColVector.isNull[inBatchIndex]) {
+          outColVector.setElement(outBatchIndex, inBatchIndex, inColVector);
+        } else {
+          VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex);
+        }
+      }
+    }
+  }
+
   private CopyRow[] subRowToBatchCopiersByValue;
   private CopyRow[] subRowToBatchCopiersByReference;
 
@@ -250,6 +278,10 @@ public void init(VectorColumnMapping columnMapping) throws HiveException {
         copyRowByValue = new TimestampCopyRow(inputColumn, outputColumn);
         break;
 
+      case INTERVAL_DAY_TIME:
+        copyRowByValue = new IntervalDayTimeCopyRow(inputColumn, outputColumn);
+        break;
+
       case DOUBLE:
         copyRowByValue = new DoubleCopyRow(inputColumn, outputColumn);
         break;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
index 50881e75dd..3eadc12cc3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java
@@ -264,7 +264,14 @@ void apply(VectorizedRowBatch batch, int batchIndex) throws IOException {
     }
   }
 
-  private class IntervalDayTimeReader extends AbstractTimestampReader {
+  private abstract class AbstractIntervalDayTimeReader extends Reader<T> {
+
+    AbstractIntervalDayTimeReader(int columnIndex) {
+      super(columnIndex);
+    }
+  }
+
+  private class IntervalDayTimeReader extends AbstractIntervalDayTimeReader {
 
     DeserializeRead.ReadIntervalDayTimeResults readIntervalDayTimeResults;
 
@@ -275,14 +282,14 @@ private class IntervalDayTimeReader extends AbstractTimestampReader {
 
     @Override
     void apply(VectorizedRowBatch batch, int batchIndex) throws IOException {
-      TimestampColumnVector colVector = (TimestampColumnVector) batch.cols[columnIndex];
+      IntervalDayTimeColumnVector colVector = (IntervalDayTimeColumnVector) batch.cols[columnIndex];
 
       if (deserializeRead.readCheckNull()) {
         VectorizedBatchUtil.setNullColIsNullValue(colVector, batchIndex);
       } else {
         deserializeRead.readIntervalDayTime(readIntervalDayTimeResults);
         HiveIntervalDayTime idt = readIntervalDayTimeResults.getHiveIntervalDayTime();
-        colVector.set(batchIndex, idt.pisaTimestampUpdate(colVector.useScratchPisaTimestamp()));
+        colVector.set(batchIndex, idt);
         colVector.isNull[batchIndex] = false;
       }
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
index 0b9ad55c2e..7b3f781418 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java
@@ -59,6 +59,9 @@ public class VectorExpressionDescriptor {
   // TimestampColumnVector -->
   //    TIMESTAMP
   //
+  // IntervalDayTimeColumnVector -->
+  //    INTERVAL_DAY_TIME
+  //
   public enum ArgumentType {
     NONE                    (0x000),
     INT_FAMILY              (0x001),
@@ -76,7 +79,6 @@ public enum ArgumentType {
     INTERVAL_FAMILY         (INTERVAL_YEAR_MONTH.value | INTERVAL_DAY_TIME.value),
     INT_INTERVAL_YEAR_MONTH     (INT_FAMILY.value | INTERVAL_YEAR_MONTH.value),
     INT_DATE_INTERVAL_YEAR_MONTH  (INT_FAMILY.value | DATE.value | INTERVAL_YEAR_MONTH.value),
-    TIMESTAMP_INTERVAL_DAY_TIME (TIMESTAMP.value | INTERVAL_DAY_TIME.value),
     STRING_DATETIME_FAMILY  (STRING_FAMILY.value | DATETIME_FAMILY.value),
     ALL_FAMILY              (0xFFF);
 
@@ -346,7 +348,7 @@ public Class<?> getVectorExpressionClass(Class<?> udf, Descriptor descriptor) th
           return ve;
         }
       } catch (Exception ex) {
-        throw new HiveException(ex);
+        throw new HiveException("Could not instantiate VectorExpression class " + ve.getSimpleName(), ex);
       }
     }
     if (LOG.isDebugEnabled()) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java
index 622f4a33d0..e883f38d9b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -323,7 +322,26 @@ Object extract(int batchIndex) {
     }
   }
 
-  private class IntervalDayTimeExtractor extends AbstractTimestampExtractor {
+  private abstract class AbstractIntervalDayTimeExtractor extends Extractor {
+
+    protected IntervalDayTimeColumnVector colVector;
+
+    AbstractIntervalDayTimeExtractor(int columnIndex) {
+      super(columnIndex);
+    }
+
+    @Override
+    void setColumnVector(VectorizedRowBatch batch) {
+      colVector = (IntervalDayTimeColumnVector) batch.cols[columnIndex];
+    }
+
+    @Override
+    void forgetColumnVector() {
+      colVector = null;
+    }
+  }
+
+  private class IntervalDayTimeExtractor extends AbstractIntervalDayTimeExtractor {
 
     private HiveIntervalDayTime hiveIntervalDayTime;
 
@@ -337,7 +355,7 @@ private class IntervalDayTimeExtractor extends AbstractTimestampExtractor {
     Object extract(int batchIndex) {
       int adjustedIndex = (colVector.isRepeating ? 0 : batchIndex);
       if (colVector.noNulls || !colVector.isNull[adjustedIndex]) {
-        hiveIntervalDayTime.set(colVector.asScratchPisaTimestamp(adjustedIndex));
+        hiveIntervalDayTime.set(colVector.asScratchIntervalDayTime(adjustedIndex));
         PrimitiveObjectInspectorFactory.writableHiveIntervalDayTimeObjectInspector.set(object, hiveIntervalDayTime);
         return object;
       } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java
index 9f0ac11b99..50d04520fe 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupKeyHelper.java
@@ -131,5 +131,17 @@ public void copyGroupKey(VectorizedRowBatch inputBatch, VectorizedRowBatch outpu
         outputColumnVector.isNull[outputBatch.size] = true;
       }
     }
+    for(int i=0;i<intervalDayTimeIndices.length; ++i) {
+      int keyIndex = intervalDayTimeIndices[i];
+      IntervalDayTimeColumnVector inputColumnVector = (IntervalDayTimeColumnVector) inputBatch.cols[keyIndex];
+      IntervalDayTimeColumnVector outputColumnVector = (IntervalDayTimeColumnVector) outputBatch.cols[keyIndex];
+      if (inputColumnVector.noNulls || !inputColumnVector.isNull[0]) {
+
+        outputColumnVector.setElement(outputBatch.size, 0, inputColumnVector);
+      } else {
+        outputColumnVector.noNulls = false;
+        outputColumnVector.isNull[outputBatch.size] = true;
+      }
+    }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
index b5d81645e9..8a101a67da 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java
@@ -18,16 +18,16 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
+import java.sql.Timestamp;
 import java.util.Arrays;
 
-import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
-import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.KeyWrapper;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
 /**
@@ -44,7 +44,8 @@ public class VectorHashKeyWrapper extends KeyWrapper {
   private static final double[] EMPTY_DOUBLE_ARRAY = new double[0];
   private static final byte[][] EMPTY_BYTES_ARRAY = new byte[0][];
   private static final HiveDecimalWritable[] EMPTY_DECIMAL_ARRAY = new HiveDecimalWritable[0];
-  private static final PisaTimestamp[] EMPTY_TIMESTAMP_ARRAY = new PisaTimestamp[0];
+  private static final Timestamp[] EMPTY_TIMESTAMP_ARRAY = new Timestamp[0];
+  private static final HiveIntervalDayTime[] EMPTY_INTERVAL_DAY_TIME_ARRAY = new HiveIntervalDayTime[0];
 
   private long[] longValues;
   private double[] doubleValues;
@@ -55,17 +56,21 @@ public class VectorHashKeyWrapper extends KeyWrapper {
 
   private HiveDecimalWritable[] decimalValues;
 
-  private PisaTimestamp[] timestampValues;
+  private Timestamp[] timestampValues;
+
+  private HiveIntervalDayTime[] intervalDayTimeValues;
 
   private boolean[] isNull;
   private int hashcode;
 
   public VectorHashKeyWrapper(int longValuesCount, int doubleValuesCount,
-          int byteValuesCount, int decimalValuesCount, int timestampValuesCount) {
+          int byteValuesCount, int decimalValuesCount, int timestampValuesCount,
+          int intervalDayTimeValuesCount) {
     longValues = longValuesCount > 0 ? new long[longValuesCount] : EMPTY_LONG_ARRAY;
     doubleValues = doubleValuesCount > 0 ? new double[doubleValuesCount] : EMPTY_DOUBLE_ARRAY;
     decimalValues = decimalValuesCount > 0 ? new HiveDecimalWritable[decimalValuesCount] : EMPTY_DECIMAL_ARRAY;
-    timestampValues = timestampValuesCount > 0 ? new PisaTimestamp[timestampValuesCount] : EMPTY_TIMESTAMP_ARRAY;
+    timestampValues = timestampValuesCount > 0 ? new Timestamp[timestampValuesCount] : EMPTY_TIMESTAMP_ARRAY;
+    intervalDayTimeValues = intervalDayTimeValuesCount > 0 ? new HiveIntervalDayTime[intervalDayTimeValuesCount] : EMPTY_INTERVAL_DAY_TIME_ARRAY;
     for(int i = 0; i < decimalValuesCount; ++i) {
       decimalValues[i] = new HiveDecimalWritable(HiveDecimal.ZERO);
     }
@@ -79,10 +84,13 @@ public VectorHashKeyWrapper(int longValuesCount, int doubleValuesCount,
       byteLengths = EMPTY_INT_ARRAY;
     }
     for(int i = 0; i < timestampValuesCount; ++i) {
-      timestampValues[i] = new PisaTimestamp();
+      timestampValues[i] = new Timestamp(0);
+    }
+    for(int i = 0; i < intervalDayTimeValuesCount; ++i) {
+      intervalDayTimeValues[i] = new HiveIntervalDayTime();
     }
     isNull = new boolean[longValuesCount + doubleValuesCount + byteValuesCount +
-                         decimalValuesCount + timestampValuesCount];
+                         decimalValuesCount + timestampValuesCount + intervalDayTimeValuesCount];
     hashcode = 0;
   }
 
@@ -108,6 +116,10 @@ public void setHashKey() {
       hashcode ^= timestampValues[i].hashCode();
     }
 
+    for (int i = 0; i < intervalDayTimeValues.length; i++) {
+      hashcode ^= intervalDayTimeValues[i].hashCode();
+    }
+
     // This code, with branches and all, is not executed if there are no string keys
     for (int i = 0; i < byteValues.length; ++i) {
       /*
@@ -146,6 +158,7 @@ public boolean equals(Object that) {
           Arrays.equals(doubleValues, keyThat.doubleValues) &&
           Arrays.equals(decimalValues,  keyThat.decimalValues) &&
           Arrays.equals(timestampValues,  keyThat.timestampValues) &&
+          Arrays.equals(intervalDayTimeValues,  keyThat.intervalDayTimeValues) &&
           Arrays.equals(isNull, keyThat.isNull) &&
           byteValues.length == keyThat.byteValues.length &&
           (0 == byteValues.length || bytesEquals(keyThat));
@@ -212,14 +225,21 @@ public void duplicateTo(VectorHashKeyWrapper clone) {
       clone.byteLengths = EMPTY_INT_ARRAY;
     }
     if (timestampValues.length > 0) {
-      clone.timestampValues = new PisaTimestamp[timestampValues.length];
+      clone.timestampValues = new Timestamp[timestampValues.length];
       for(int i = 0; i < timestampValues.length; ++i) {
-        clone.timestampValues[i] = new PisaTimestamp();
-        clone.timestampValues[i].update(timestampValues[i]);
+        clone.timestampValues[i] = (Timestamp) timestampValues[i].clone();
       }
     } else {
       clone.timestampValues = EMPTY_TIMESTAMP_ARRAY;
     }
+    if (intervalDayTimeValues.length > 0) {
+      clone.intervalDayTimeValues = new HiveIntervalDayTime[intervalDayTimeValues.length];
+      for(int i = 0; i < intervalDayTimeValues.length; ++i) {
+        clone.intervalDayTimeValues[i] = (HiveIntervalDayTime) intervalDayTimeValues[i].clone();
+      }
+    } else {
+      clone.intervalDayTimeValues = EMPTY_INTERVAL_DAY_TIME_ARRAY;
+    }
 
     clone.hashcode = hashcode;
     assert clone.equals(this);
@@ -281,14 +301,14 @@ public void assignNullDecimal(int index) {
       isNull[longValues.length + doubleValues.length + byteValues.length + index] = true;
   }
 
-  public void assignTimestamp(int index, PisaTimestamp value) {
-    timestampValues[index].update(value);
+  public void assignTimestamp(int index, Timestamp value) {
+    timestampValues[index] = value;
     isNull[longValues.length + doubleValues.length + byteValues.length +
            decimalValues.length + index] = false;
   }
 
   public void assignTimestamp(int index, TimestampColumnVector colVector, int elementNum) {
-    colVector.pisaTimestampUpdate(timestampValues[index], elementNum);
+    colVector.timestampUpdate(timestampValues[index], elementNum);
     isNull[longValues.length + doubleValues.length + byteValues.length +
            decimalValues.length + index] = false;
   }
@@ -298,15 +318,33 @@ public void assignNullTimestamp(int index) {
              decimalValues.length + index] = true;
   }
 
+  public void assignIntervalDayTime(int index, HiveIntervalDayTime value) {
+    intervalDayTimeValues[index].set(value);
+    isNull[longValues.length + doubleValues.length + byteValues.length +
+           decimalValues.length + timestampValues.length + index] = false;
+  }
+
+  public void assignIntervalDayTime(int index, IntervalDayTimeColumnVector colVector, int elementNum) {
+    intervalDayTimeValues[index].set(colVector.asScratchIntervalDayTime(elementNum));
+    isNull[longValues.length + doubleValues.length + byteValues.length +
+           decimalValues.length + timestampValues.length + index] = false;
+  }
+
+  public void assignNullIntervalDayTime(int index) {
+      isNull[longValues.length + doubleValues.length + byteValues.length +
+             decimalValues.length + timestampValues.length + index] = true;
+  }
+
   @Override
   public String toString()
   {
-    return String.format("%d[%s] %d[%s] %d[%s] %d[%s] %d[%s]",
+    return String.format("%d[%s] %d[%s] %d[%s] %d[%s] %d[%s] %d[%s]",
         longValues.length, Arrays.toString(longValues),
         doubleValues.length, Arrays.toString(doubleValues),
         byteValues.length, Arrays.toString(byteValues),
         decimalValues.length, Arrays.toString(decimalValues),
-        timestampValues.length, Arrays.toString(timestampValues));
+        timestampValues.length, Arrays.toString(timestampValues),
+        intervalDayTimeValues.length, Arrays.toString(intervalDayTimeValues));
   }
 
   public boolean getIsLongNull(int i) {
@@ -364,9 +402,17 @@ public boolean getIsTimestampNull(int i) {
                   decimalValues.length + i];
   }
 
-  public PisaTimestamp getTimestamp(int i) {
+  public Timestamp getTimestamp(int i) {
     return timestampValues[i];
   }
 
+  public boolean getIsIntervalDayTimeNull(int i) {
+    return isNull[longValues.length + doubleValues.length + byteValues.length +
+                  decimalValues.length + timestampValues.length + i];
+  }
+
+  public HiveIntervalDayTime getIntervalDayTime(int i) {
+    return intervalDayTimeValues[i];
+  }
 }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
index 1c341244a4..bfd26aee80 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java
@@ -198,6 +198,28 @@ public void evaluateBatch(VectorizedRowBatch batch) throws HiveException {
             columnVector.noNulls, columnVector.isRepeating, batch.selectedInUse));
       }
     }
+    for(int i=0;i<intervalDayTimeIndices.length; ++i) {
+      int keyIndex = intervalDayTimeIndices[i];
+      int columnIndex = keyExpressions[keyIndex].getOutputColumn();
+      IntervalDayTimeColumnVector columnVector = (IntervalDayTimeColumnVector) batch.cols[columnIndex];
+      if (columnVector.noNulls && !columnVector.isRepeating && !batch.selectedInUse) {
+        assignIntervalDayTimeNoNullsNoRepeatingNoSelection(i, batch.size, columnVector);
+      } else if (columnVector.noNulls && !columnVector.isRepeating && batch.selectedInUse) {
+        assignIntervalDayTimeNoNullsNoRepeatingSelection(i, batch.size, columnVector, batch.selected);
+      } else if (columnVector.noNulls && columnVector.isRepeating) {
+        assignIntervalDayTimeNoNullsRepeating(i, batch.size, columnVector);
+      } else if (!columnVector.noNulls && !columnVector.isRepeating && !batch.selectedInUse) {
+        assignIntervalDayTimeNullsNoRepeatingNoSelection(i, batch.size, columnVector);
+      } else if (!columnVector.noNulls && columnVector.isRepeating) {
+        assignIntervalDayTimeNullsRepeating(i, batch.size, columnVector);
+      } else if (!columnVector.noNulls && !columnVector.isRepeating && batch.selectedInUse) {
+        assignIntervalDayTimeNullsNoRepeatingSelection (i, batch.size, columnVector, batch.selected);
+      } else {
+        throw new HiveException (String.format(
+            "Unimplemented intervalDayTime null/repeat/selected combination %b/%b/%b",
+            columnVector.noNulls, columnVector.isRepeating, batch.selectedInUse));
+      }
+    }
     for(int i=0;i<batch.size;++i) {
       vectorHashKeyWrappers[i].setHashKey();
     }
@@ -596,6 +618,81 @@ private void assignTimestampNoNullsNoRepeatingNoSelection(int index, int size,
       vectorHashKeyWrappers[r].assignTimestamp(index, columnVector, r);
     }
   }
+
+  /**
+   * Helper method to assign values from a vector column into the key wrapper.
+   * Optimized for IntervalDayTime type, possible nulls, no repeat values, batch selection vector.
+   */
+  private void assignIntervalDayTimeNullsNoRepeatingSelection(int index, int size,
+      IntervalDayTimeColumnVector columnVector, int[] selected) {
+    for(int i = 0; i < size; ++i) {
+      int row = selected[i];
+      if (!columnVector.isNull[row]) {
+        vectorHashKeyWrappers[i].assignIntervalDayTime(index, columnVector, row);
+      } else {
+        vectorHashKeyWrappers[i].assignNullIntervalDayTime(index);
+      }
+    }
+  }
+
+  /**
+   * Helper method to assign values from a vector column into the key wrapper.
+   * Optimized for IntervalDayTime type, repeat null values.
+   */
+  private void assignIntervalDayTimeNullsRepeating(int index, int size,
+      IntervalDayTimeColumnVector columnVector) {
+    for(int r = 0; r < size; ++r) {
+      vectorHashKeyWrappers[r].assignNullIntervalDayTime(index);
+    }
+  }
+
+  /**
+   * Helper method to assign values from a vector column into the key wrapper.
+   * Optimized for IntervalDayTime type, possible nulls, repeat values.
+   */
+  private void assignIntervalDayTimeNullsNoRepeatingNoSelection(int index, int size,
+      IntervalDayTimeColumnVector columnVector) {
+    for(int r = 0; r < size; ++r) {
+      if (!columnVector.isNull[r]) {
+        vectorHashKeyWrappers[r].assignIntervalDayTime(index, columnVector, r);
+      } else {
+        vectorHashKeyWrappers[r].assignNullIntervalDayTime(index);
+      }
+    }
+  }
+
+  /**
+   * Helper method to assign values from a vector column into the key wrapper.
+   * Optimized for IntervalDayTime type, no nulls, repeat values, no selection vector.
+   */
+  private void assignIntervalDayTimeNoNullsRepeating(int index, int size, IntervalDayTimeColumnVector columnVector) {
+    for(int r = 0; r < size; ++r) {
+      vectorHashKeyWrappers[r].assignIntervalDayTime(index, columnVector, 0);
+    }
+  }
+
+  /**
+   * Helper method to assign values from a vector column into the key wrapper.
+   * Optimized for IntervalDayTime type, no nulls, no repeat values, batch selection vector.
+   */
+  private void assignIntervalDayTimeNoNullsNoRepeatingSelection(int index, int size,
+      IntervalDayTimeColumnVector columnVector, int[] selected) {
+    for(int r = 0; r < size; ++r) {
+      vectorHashKeyWrappers[r].assignIntervalDayTime(index, columnVector, selected[r]);
+    }
+  }
+
+  /**
+   * Helper method to assign values from a vector column into the key wrapper.
+   * Optimized for IntervalDayTime type, no nulls, no repeat values, no selection vector.
+   */
+  private void assignIntervalDayTimeNoNullsNoRepeatingNoSelection(int index, int size,
+      IntervalDayTimeColumnVector columnVector) {
+    for(int r = 0; r < size; ++r) {
+      vectorHashKeyWrappers[r].assignIntervalDayTime(index, columnVector, r);
+    }
+  }
+
   /**
    * Prepares a VectorHashKeyWrapperBatch to work for a specific set of keys.
    * Computes the fast access lookup indices, preallocates all needed internal arrays.
@@ -638,6 +735,7 @@ public static VectorHashKeyWrapperBatch compileKeyWrapperBatch(VectorExpression[
     compiledKeyWrapperBatch.keysFixedSize += model.lengthForObjectArrayOfSize(compiledKeyWrapperBatch.stringIndices.length);
     compiledKeyWrapperBatch.keysFixedSize += model.lengthForObjectArrayOfSize(compiledKeyWrapperBatch.decimalIndices.length);
     compiledKeyWrapperBatch.keysFixedSize += model.lengthForObjectArrayOfSize(compiledKeyWrapperBatch.timestampIndices.length);
+    compiledKeyWrapperBatch.keysFixedSize += model.lengthForObjectArrayOfSize(compiledKeyWrapperBatch.intervalDayTimeIndices.length);
     compiledKeyWrapperBatch.keysFixedSize += model.lengthForIntArrayOfSize(compiledKeyWrapperBatch.longIndices.length) * 2;
     compiledKeyWrapperBatch.keysFixedSize +=
         model.lengthForBooleanArrayOfSize(keyExpressions.length);
@@ -647,7 +745,8 @@ public static VectorHashKeyWrapperBatch compileKeyWrapperBatch(VectorExpression[
 
   public VectorHashKeyWrapper allocateKeyWrapper() {
     return new VectorHashKeyWrapper(longIndices.length, doubleIndices.length,
-        stringIndices.length, decimalIndices.length, timestampIndices.length);
+        stringIndices.length, decimalIndices.length, timestampIndices.length,
+        intervalDayTimeIndices.length);
   }
 
   /**
@@ -679,12 +778,15 @@ public Object getWritableKeyValue(VectorHashKeyWrapper kw, int i,
       return kw.getIsTimestampNull(klh.timestampIndex)? null :
           keyOutputWriter.writeValue(
                 kw.getTimestamp(klh.timestampIndex));
-    }
-    else {
+    } else if (klh.intervalDayTimeIndex >= 0) {
+      return kw.getIsIntervalDayTimeNull(klh.intervalDayTimeIndex)? null :
+        keyOutputWriter.writeValue(
+              kw.getIntervalDayTime(klh.intervalDayTimeIndex));
+    } else {
       throw new HiveException(String.format(
-          "Internal inconsistent KeyLookupHelper at index [%d]:%d %d %d %d %d",
+          "Internal inconsistent KeyLookupHelper at index [%d]:%d %d %d %d %d %d",
           i, klh.longIndex, klh.doubleIndex, klh.stringIndex, klh.decimalIndex,
-          klh.timestampIndex));
+          klh.timestampIndex, klh.intervalDayTimeIndex));
     }
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java
index dea38e8f40..6af3d99914 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java
@@ -23,7 +23,6 @@
 import java.util.List;
 
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.ByteStream.Output;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
@@ -332,11 +331,11 @@ private class IntervalDayTimeWriter extends Writer {
 
     @Override
     boolean apply(VectorizedRowBatch batch, int batchIndex) throws IOException {
-      TimestampColumnVector colVector = (TimestampColumnVector) batch.cols[columnIndex];
+      IntervalDayTimeColumnVector colVector = (IntervalDayTimeColumnVector) batch.cols[columnIndex];
 
       if (colVector.isRepeating) {
         if (colVector.noNulls || !colVector.isNull[0]) {
-          hiveIntervalDayTime.set(colVector.asScratchPisaTimestamp(0));
+          hiveIntervalDayTime.set(colVector.asScratchIntervalDayTime(0));
           serializeWrite.writeHiveIntervalDayTime(hiveIntervalDayTime);
           return true;
         } else {
@@ -345,7 +344,7 @@ boolean apply(VectorizedRowBatch batch, int batchIndex) throws IOException {
         }
       } else {
         if (colVector.noNulls || !colVector.isNull[batchIndex]) {
-          hiveIntervalDayTime.set(colVector.asScratchPisaTimestamp(batchIndex));
+          hiveIntervalDayTime.set(colVector.asScratchIntervalDayTime(batchIndex));
           serializeWrite.writeHiveIntervalDayTime(hiveIntervalDayTime);
           return true;
         } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
index 3f95be2304..0552f9dec3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java
@@ -68,11 +68,13 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxString;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinLong;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinString;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMinIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFStdPopLong;
@@ -2333,10 +2335,12 @@ public static ColumnVector.Type getColumnVectorTypeFromTypeInfo(TypeInfo typeInf
           case INTERVAL_YEAR_MONTH:
             return ColumnVector.Type.LONG;
 
-          case INTERVAL_DAY_TIME:
           case TIMESTAMP:
             return ColumnVector.Type.TIMESTAMP;
 
+          case INTERVAL_DAY_TIME:
+            return ColumnVector.Type.INTERVAL_DAY_TIME;
+
           case FLOAT:
           case DOUBLE:
             return ColumnVector.Type.DOUBLE;
@@ -2369,19 +2373,20 @@ public static ColumnVector.Type getColumnVectorTypeFromTypeInfo(TypeInfo typeInf
     add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           null,                          VectorUDAFMinDouble.class));
     add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,          null,                          VectorUDAFMinString.class));
     add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                null,                          VectorUDAFMinDecimal.class));
-    add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP_INTERVAL_DAY_TIME,     null,                          VectorUDAFMinTimestamp.class));
+    add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              null,                          VectorUDAFMinTimestamp.class));
     add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.INT_DATE_INTERVAL_YEAR_MONTH,    null,                          VectorUDAFMaxLong.class));
     add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           null,                          VectorUDAFMaxDouble.class));
     add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,          null,                          VectorUDAFMaxString.class));
     add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                null,                          VectorUDAFMaxDecimal.class));
-    add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP_INTERVAL_DAY_TIME,     null,                          VectorUDAFMaxTimestamp.class));
+    add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              null,                          VectorUDAFMaxTimestamp.class));
     add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.NONE,                   GroupByDesc.Mode.HASH,         VectorUDAFCountStar.class));
     add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.INT_DATE_INTERVAL_YEAR_MONTH,    GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
     add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.MERGEPARTIAL, VectorUDAFCountMerge.class));
     add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
     add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,          GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
     add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
-    add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.TIMESTAMP_INTERVAL_DAY_TIME,     GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
+    add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
+    add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.INTERVAL_DAY_TIME,      GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
     add(new AggregateDefinition("sum",         VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             null,                          VectorUDAFSumLong.class));
     add(new AggregateDefinition("sum",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           null,                          VectorUDAFSumDouble.class));
     add(new AggregateDefinition("sum",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                null,                          VectorUDAFSumDecimal.class));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java
index a68d0cc762..be04da82ee 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java
@@ -144,9 +144,10 @@ public static ColumnVector createColumnVector(TypeInfo typeInfo) {
           case DATE:
           case INTERVAL_YEAR_MONTH:
             return new LongColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
-          case INTERVAL_DAY_TIME:
           case TIMESTAMP:
             return new TimestampColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
+          case INTERVAL_DAY_TIME:
+            return new IntervalDayTimeColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
           case FLOAT:
           case DOUBLE:
             return new DoubleColumnVector(VectorizedRowBatch.DEFAULT_SIZE);
@@ -417,14 +418,14 @@ private static void setVector(Object row,
     }
       break;
     case INTERVAL_DAY_TIME: {
-      LongColumnVector lcv = (LongColumnVector) batch.cols[offset + colIndex];
+      IntervalDayTimeColumnVector icv = (IntervalDayTimeColumnVector) batch.cols[offset + colIndex];
       if (writableCol != null) {
-        HiveIntervalDayTime i = ((HiveIntervalDayTimeWritable) writableCol).getHiveIntervalDayTime();
-        lcv.vector[rowIndex] = DateUtils.getIntervalDayTimeTotalNanos(i);
-        lcv.isNull[rowIndex] = false;
+        HiveIntervalDayTime idt = ((HiveIntervalDayTimeWritable) writableCol).getHiveIntervalDayTime();
+        icv.set(rowIndex, idt);
+        icv.isNull[rowIndex] = false;
       } else {
-        lcv.vector[rowIndex] = 1;
-        setNullColIsNullValue(lcv, rowIndex);
+        icv.setNullValue(rowIndex);
+        setNullColIsNullValue(icv, rowIndex);
       }
     }
       break;
@@ -585,6 +586,8 @@ static ColumnVector cloneColumnVector(ColumnVector source
           decColVector.scale);
     } else if (source instanceof TimestampColumnVector) {
       return new TimestampColumnVector(((TimestampColumnVector) source).getLength());
+    } else if (source instanceof IntervalDayTimeColumnVector) {
+      return new IntervalDayTimeColumnVector(((IntervalDayTimeColumnVector) source).getLength());
     } else if (source instanceof ListColumnVector) {
       ListColumnVector src = (ListColumnVector) source;
       ColumnVector child = cloneColumnVector(src.child);
@@ -688,6 +691,9 @@ public static void debugDisplayOneRow(VectorizedRowBatch batch, int index, Strin
             Timestamp timestamp = new Timestamp(0);
             ((TimestampColumnVector) colVector).timestampUpdate(timestamp, index);
             sb.append(timestamp.toString());
+          } else if (colVector instanceof IntervalDayTimeColumnVector) {
+            HiveIntervalDayTime intervalDayTime = ((IntervalDayTimeColumnVector) colVector).asScratchIntervalDayTime(index);
+            sb.append(intervalDayTime.toString());
           } else {
             sb.append("Unknown");
           }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
index 7e79e1e110..072419193d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java
@@ -400,14 +400,14 @@ public void addPartitionColsToBatch(VectorizedRowBatch batch, Object[] partition
         }
 
         case INTERVAL_DAY_TIME: {
-          TimestampColumnVector tcv = (TimestampColumnVector) batch.cols[colIndex];
+          IntervalDayTimeColumnVector icv = (IntervalDayTimeColumnVector) batch.cols[colIndex];
           if (value == null) {
-            tcv.noNulls = false;
-            tcv.isNull[0] = true;
-            tcv.isRepeating = true;
+            icv.noNulls = false;
+            icv.isNull[0] = true;
+            icv.isRepeating = true;
           } else {
-            tcv.fill(((HiveIntervalDayTime) value).pisaTimestampUpdate(tcv.useScratchPisaTimestamp()));
-            tcv.isNull[0] = false;
+            icv.fill(((HiveIntervalDayTime) value));
+            icv.isNull[0] = false;
           }
         }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java
index 2b0068da26..6225adeaef 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java
@@ -44,7 +44,6 @@ public CastDecimalToTimestamp() {
 
   @Override
   protected void func(TimestampColumnVector outV, DecimalColumnVector inV,  int i) {
-    Timestamp timestamp = TimestampWritable.decimalToTimestamp(inV.vector[i].getHiveDecimal());
-    outV.set(i, timestamp);
+    outV.set(i, TimestampWritable.decimalToTimestamp(inV.vector[i].getHiveDecimal()));
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToTimestamp.java
index 39823fe09e..31d2f783d8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToTimestamp.java
@@ -20,6 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 
 public class CastDoubleToTimestamp extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -37,9 +38,11 @@ public CastDoubleToTimestamp() {
     super();
   }
 
-  private void setSecondsWithFractionalNanoseconds(TimestampColumnVector timestampColVector,
+  private void setDouble(TimestampColumnVector timestampColVector,
       double[] vector, int elementNum) {
-    timestampColVector.setTimestampSecondsWithFractionalNanoseconds(elementNum, vector[elementNum]);
+    TimestampWritable.setTimestampFromDouble(
+        timestampColVector.getScratchTimestamp(), vector[elementNum]);
+    timestampColVector.setFromScratchTimestamp(elementNum);
   }
 
   @Override
@@ -66,7 +69,7 @@ public void evaluate(VectorizedRowBatch batch) {
     if (inputColVector.isRepeating) {
       //All must be selected otherwise size would be zero
       //Repeating property will not change.
-      setSecondsWithFractionalNanoseconds(outputColVector, vector, 0);
+      setDouble(outputColVector, vector, 0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
       outputColVector.isRepeating = true;
@@ -74,11 +77,11 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          setSecondsWithFractionalNanoseconds(outputColVector, vector, i);
+          setDouble(outputColVector, vector, i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          setSecondsWithFractionalNanoseconds(outputColVector, vector, i);
+          setDouble(outputColVector, vector, i);
         }
       }
       outputColVector.isRepeating = false;
@@ -86,12 +89,12 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          setSecondsWithFractionalNanoseconds(outputColVector, vector, i);
+          setDouble(outputColVector, vector, i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          setSecondsWithFractionalNanoseconds(outputColVector, vector, i);
+          setDouble(outputColVector, vector, i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastLongToTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastLongToTimestamp.java
index d344d4d6d6..a2ee52db11 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastLongToTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastLongToTimestamp.java
@@ -20,8 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 
 public class CastLongToTimestamp extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -40,7 +39,10 @@ public CastLongToTimestamp() {
   }
 
   private void setSeconds(TimestampColumnVector timestampColVector, long[] vector, int elementNum) {
-    timestampColVector.setTimestampSeconds(elementNum, vector[elementNum]);
+    TimestampWritable.setTimestampFromLong(
+        timestampColVector.getScratchTimestamp(), vector[elementNum],
+        /* intToTimestampInSeconds */ true);
+    timestampColVector.setFromScratchTimestamp(elementNum);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastMillisecondsLongToTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastMillisecondsLongToTimestamp.java
index a0c947f6ef..01c8810cfa 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastMillisecondsLongToTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastMillisecondsLongToTimestamp.java
@@ -20,8 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 
 public class CastMillisecondsLongToTimestamp extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -39,6 +38,13 @@ public CastMillisecondsLongToTimestamp() {
     super();
   }
 
+  private void setMilliseconds(TimestampColumnVector timestampColVector, long[] vector, int elementNum) {
+    TimestampWritable.setTimestampFromLong(
+        timestampColVector.getScratchTimestamp(), vector[elementNum],
+        /* intToTimestampInSeconds */ false);
+    timestampColVector.setFromScratchTimestamp(elementNum);
+  }
+
   @Override
   public void evaluate(VectorizedRowBatch batch) {
 
@@ -63,19 +69,19 @@ public void evaluate(VectorizedRowBatch batch) {
     if (inputColVector.isRepeating) {
       //All must be selected otherwise size would be zero
       //Repeating property will not change.
-      outputColVector.setTimestampMilliseconds(0, vector[0]);
+      setMilliseconds(outputColVector, vector, 0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0]; 
+      outputIsNull[0] = inputIsNull[0];
       outputColVector.isRepeating = true;
     } else if (inputColVector.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.setTimestampMilliseconds(i, vector[i]);
+          setMilliseconds(outputColVector, vector, i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.setTimestampMilliseconds(i, vector[i]);
+          setMilliseconds(outputColVector, vector, i);
         }
       }
       outputColVector.isRepeating = false;
@@ -83,12 +89,12 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.setTimestampMilliseconds(i, vector[i]);
+          setMilliseconds(outputColVector, vector, i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.setTimestampMilliseconds(i, vector[i]);
+          setMilliseconds(outputColVector, vector, i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToIntervalDayTime.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToIntervalDayTime.java
index a3ddf9f287..c8844c881c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToIntervalDayTime.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastStringToIntervalDayTime.java
@@ -21,7 +21,7 @@
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.serde.serdeConstants;
@@ -56,7 +56,7 @@ public void evaluate(VectorizedRowBatch batch) {
     BytesColumnVector inV = (BytesColumnVector) batch.cols[inputColumn];
     int[] sel = batch.selected;
     int n = batch.size;
-    TimestampColumnVector outV = (TimestampColumnVector) batch.cols[outputColumn];
+    IntervalDayTimeColumnVector outV = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
 
     if (n == 0) {
 
@@ -113,11 +113,11 @@ public void evaluate(VectorizedRowBatch batch) {
     }
   }
 
-  private void evaluate(TimestampColumnVector outV, BytesColumnVector inV, int i) {
+  private void evaluate(IntervalDayTimeColumnVector outV, BytesColumnVector inV, int i) {
     try {
       HiveIntervalDayTime interval = HiveIntervalDayTime.valueOf(
           new String(inV.vector[i], inV.start[i], inV.length[i], "UTF-8"));
-      outV.setEpochSecondsAndSignedNanos(i, interval.getTotalSeconds(), interval.getNanos());
+      outV.set(i, interval);
     } catch (Exception e) {
       outV.setNullValue(i);
       outV.isNull[i] = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToBoolean.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToBoolean.java
index 55b84b1fd8..b8a58cdc05 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToBoolean.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToBoolean.java
@@ -41,8 +41,8 @@ public CastTimestampToBoolean() {
   }
 
   private int toBool(TimestampColumnVector timestampColVector, int index) {
-    return (timestampColVector.getEpochDay(index) != 0 ||
-            timestampColVector.getNanoOfDay(index) != 0) ? 1 : 0;
+    return (timestampColVector.getTime(index) != 0 ||
+            timestampColVector.getNanos(index) != 0) ? 1 : 0;
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDate.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDate.java
index 00790b9bbb..4e3e62ced2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDate.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDate.java
@@ -44,6 +44,6 @@ public CastTimestampToDate(int inputColumn, int outputColumn) {
   @Override
   protected void func(LongColumnVector outV, TimestampColumnVector inV, int i) {
 
-    outV.vector[i] = DateWritable.millisToDays(inV.getTimestampMilliseconds(i));
+    outV.vector[i] = DateWritable.millisToDays(inV.getTime(i));
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDecimal.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDecimal.java
index aec104e8da..e5bfb15913 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDecimal.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDecimal.java
@@ -18,9 +18,9 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 
 /**
  * To be used to cast timestamp to decimal.
@@ -39,11 +39,6 @@ public CastTimestampToDecimal(int inputColumn, int outputColumn) {
 
   @Override
   protected void func(DecimalColumnVector outV, TimestampColumnVector inV, int i) {
-
-    // The BigDecimal class recommends not converting directly from double to BigDecimal,
-    // so we convert like the non-vectorized case and got through a string...
-    Double timestampDouble = inV.getTimestampSecondsWithFractionalNanos(i);
-    HiveDecimal result = HiveDecimal.create(timestampDouble.toString());
-    outV.set(i, result);
+    outV.set(i, TimestampWritable.getHiveDecimal(inV.asScratchTimestamp(i)));
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDouble.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDouble.java
index f8737f9a97..a955d79bb2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDouble.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToDouble.java
@@ -20,8 +20,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 
 public class CastTimestampToDouble extends VectorExpression {
   private static final long serialVersionUID = 1L;
@@ -63,7 +62,7 @@ public void evaluate(VectorizedRowBatch batch) {
     if (inputColVector.isRepeating) {
       //All must be selected otherwise size would be zero
       //Repeating property will not change.
-      outputVector[0] =  inputColVector.getTimestampSecondsWithFractionalNanos(0);
+      outputVector[0] = inputColVector.getDouble(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
       outputColVector.isRepeating = true;
@@ -71,11 +70,11 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] =  inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          outputVector[i] =  inputColVector.getDouble(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] =  inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          outputVector[i] =  inputColVector.getDouble(i);
         }
       }
       outputColVector.isRepeating = false;
@@ -83,12 +82,12 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] =  inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          outputVector[i] = inputColVector.getDouble(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] =  inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          outputVector[i] = inputColVector.getDouble(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToLong.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToLong.java
index 4f53f5c278..ba2e823727 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToLong.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastTimestampToLong.java
@@ -64,19 +64,19 @@ public void evaluate(VectorizedRowBatch batch) {
     if (inputColVector.isRepeating) {
       //All must be selected otherwise size would be zero
       //Repeating property will not change.
-      outputVector[0] =  inputColVector.getEpochSeconds(0);
+      outputVector[0] =  inputColVector.getTimestampAsLong(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
-      outputIsNull[0] = inputIsNull[0]; 
+      outputIsNull[0] = inputIsNull[0];
       outputColVector.isRepeating = true;
     } else if (inputColVector.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] =  inputColVector.getEpochSeconds(i);
+          outputVector[i] =  inputColVector.getTimestampAsLong(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] =  inputColVector.getEpochSeconds(i);
+          outputVector[i] =  inputColVector.getTimestampAsLong(i);
         }
       }
       outputColVector.isRepeating = false;
@@ -84,12 +84,12 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputVector[i] =  inputColVector.getEpochSeconds(i);
+          outputVector[i] =  inputColVector.getTimestampAsLong(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputVector[i] =  inputColVector.getEpochSeconds(i);
+          outputVector[i] =  inputColVector.getTimestampAsLong(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java
index 24ee9bccf7..8a743f6088 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java
@@ -24,13 +24,9 @@
 import org.apache.hadoop.hive.common.type.HiveChar;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.*;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector.Type;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
-import org.apache.hive.common.util.DateUtils;
 
 /**
  * Constant is represented as a vector with repeating values.
@@ -44,7 +40,8 @@ public class ConstantVectorExpression extends VectorExpression {
   private double doubleValue = 0;
   private byte[] bytesValue = null;
   private HiveDecimal decimalValue = null;
-  private PisaTimestamp timestampValue = null;
+  private Timestamp timestampValue = null;
+  private HiveIntervalDayTime intervalDayTimeValue = null;
   private boolean isNullValue = false;
 
   private ColumnVector.Type type;
@@ -97,7 +94,7 @@ public ConstantVectorExpression(int outputColumn, Timestamp value) {
   }
 
   public ConstantVectorExpression(int outputColumn, HiveIntervalDayTime value) {
-    this(outputColumn, "timestamp");
+    this(outputColumn, "interval_day_time");
     setIntervalDayTimeValue(value);
   }
 
@@ -165,6 +162,17 @@ private void evaluateTimestamp(VectorizedRowBatch vrg) {
     }
   }
 
+  private void evaluateIntervalDayTime(VectorizedRowBatch vrg) {
+    IntervalDayTimeColumnVector dcv = (IntervalDayTimeColumnVector) vrg.cols[outputColumn];
+    dcv.isRepeating = true;
+    dcv.noNulls = !isNullValue;
+    if (!isNullValue) {
+      dcv.set(0, intervalDayTimeValue);
+    } else {
+      dcv.isNull[0] = true;
+    }
+  }
+
   @Override
   public void evaluate(VectorizedRowBatch vrg) {
     switch (type) {
@@ -183,6 +191,9 @@ public void evaluate(VectorizedRowBatch vrg) {
     case TIMESTAMP:
       evaluateTimestamp(vrg);
       break;
+    case INTERVAL_DAY_TIME:
+      evaluateIntervalDayTime(vrg);
+      break;
     }
   }
 
@@ -225,16 +236,19 @@ public HiveDecimal getDecimalValue() {
   }
 
   public void setTimestampValue(Timestamp timestampValue) {
-    this.timestampValue = new PisaTimestamp(timestampValue);
+    this.timestampValue = timestampValue;
   }
 
-  public void setIntervalDayTimeValue(HiveIntervalDayTime intervalDayTimeValue) {
-    this.timestampValue = intervalDayTimeValue.pisaTimestampUpdate(new PisaTimestamp());
+  public Timestamp getTimestampValue() {
+    return timestampValue;
   }
 
+  public void setIntervalDayTimeValue(HiveIntervalDayTime intervalDayTimeValue) {
+    this.intervalDayTimeValue = intervalDayTimeValue;
+  }
 
-  public PisaTimestamp getTimestampValue() {
-    return timestampValue;
+  public HiveIntervalDayTime getIntervalDayTimeValue() {
+    return intervalDayTimeValue;
   }
 
   public String getTypeString() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateColumn.java
index 8d2a186ae2..fafacce029 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateColumn.java
@@ -18,7 +18,9 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
 import org.apache.hadoop.hive.ql.exec.vector.*;
@@ -26,7 +28,7 @@
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 // A type date (LongColumnVector storing epoch days) minus a type date produces a
-// type interval_day_time (TimestampColumnVector storing nanosecond interval in 2 longs).
+// type interval_day_time (IntervalDayTimeColumnVector storing nanosecond interval in 2 longs).
 public class DateColSubtractDateColumn extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
@@ -34,16 +36,16 @@ public class DateColSubtractDateColumn extends VectorExpression {
   private int colNum1;
   private int colNum2;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp1;
-  private PisaTimestamp scratchPisaTimestamp2;
+  private Timestamp scratchTimestamp1;
+  private Timestamp scratchTimestamp2;
   private DateTimeMath dtm = new DateTimeMath();
 
   public DateColSubtractDateColumn(int colNum1, int colNum2, int outputColumn) {
     this.colNum1 = colNum1;
     this.colNum2 = colNum2;
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp1 = new PisaTimestamp();
-    scratchPisaTimestamp2 = new PisaTimestamp();
+    scratchTimestamp1 = new Timestamp(0);
+    scratchTimestamp2 = new Timestamp(0);
   }
 
   public DateColSubtractDateColumn() {
@@ -63,7 +65,7 @@ public void evaluate(VectorizedRowBatch batch) {
     LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum2];
 
     // Output is type interval_day_time.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
     int n = batch.size;
@@ -80,73 +82,69 @@ public void evaluate(VectorizedRowBatch batch) {
       || inputColVector1.isRepeating && !inputColVector1.noNulls && inputColVector1.isNull[0]
       || inputColVector2.isRepeating && !inputColVector2.noNulls && inputColVector2.isNull[0];
 
-    // Handle nulls first  
+    // Handle nulls first
     NullUtil.propagateNullsColCol(
       inputColVector1, inputColVector2, outputColVector, sel, n, batch.selectedInUse);
 
+    HiveIntervalDayTime resultIntervalDayTime = outputColVector.getScratchIntervalDayTime();
+
     /* Disregard nulls for processing. In other words,
      * the arithmetic operation is performed even if one or
      * more inputs are null. This is to improve speed by avoiding
      * conditional checks in the inner loop.
      */
     if (inputColVector1.isRepeating && inputColVector2.isRepeating) {
-      outputColVector.subtract(
-          scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-          scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0])),
-          0);
+      scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+      outputColVector.setFromScratchIntervalDayTime(0);
     } else if (inputColVector1.isRepeating) {
+      scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[0]));
       if (batch.selectedInUse) {
-        scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0]));
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              scratchPisaTimestamp1,
-              scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       } else {
-        scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0]));
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              scratchPisaTimestamp1,
-              scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       }
     } else if (inputColVector2.isRepeating) {
+      scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[0]));
       if (batch.selectedInUse) {
-        scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0]));
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              scratchPisaTimestamp2,
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       } else {
-        scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0]));
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              scratchPisaTimestamp2,
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       }
     } else {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              scratchPisaTimestamp1.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[i])),
-              scratchPisaTimestamp2.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(scratchTimestamp1, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       }
     }
@@ -157,7 +155,7 @@ public void evaluate(VectorizedRowBatch batch) {
      * in complex arithmetic expressions like col2 / (col1 - 1)
      * in the case when some col1 entries are null.
      */
-    NullUtil.setNullDataEntriesTimestamp(outputColVector, batch.selectedInUse, sel, n);
+    NullUtil.setNullDataEntriesIntervalDayTime(outputColVector, batch.selectedInUse, sel, n);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateScalar.java
index 3ea9331fd1..a9ca93c768 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateColSubtractDateScalar.java
@@ -18,7 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
@@ -35,16 +36,17 @@ public class DateColSubtractDateScalar extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private PisaTimestamp value;
+  private Timestamp value;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
+  private Timestamp scratchTimestamp1;
   private DateTimeMath dtm = new DateTimeMath();
 
   public DateColSubtractDateScalar(int colNum, long value, int outputColumn) {
     this.colNum = colNum;
-    this.value = new PisaTimestamp().updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) value));
+    this.value = new Timestamp(0);
+    this.value.setTime(DateWritable.daysToMillis((int) value));
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchTimestamp1 = new Timestamp(0);
   }
 
   public DateColSubtractDateScalar() {
@@ -60,8 +62,8 @@ public void evaluate(VectorizedRowBatch batch) {
     // Input #1 is type date (epochDays).
     LongColumnVector inputColVector1 = (LongColumnVector) batch.cols[colNum];
 
-    // Output is type Timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
+    // Output is type HiveIntervalDayTime.
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
     boolean[] inputIsNull = inputColVector1.isNull;
@@ -77,45 +79,40 @@ public void evaluate(VectorizedRowBatch batch) {
     }
 
     if (inputColVector1.isRepeating) {
-      outputColVector.subtract(
-          scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-          value,
-          0);
+      scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[0]));
+      dtm.subtract(scratchTimestamp1, value, outputColVector.getScratchIntervalDayTime());
+      outputColVector.setFromScratchIntervalDayTime(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
     } else if (inputColVector1.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-              value,
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.subtract(scratchTimestamp1, value, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-              value,
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.subtract(scratchTimestamp1, value, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       }
     } else /* there are nulls */ {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-              value,
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.subtract(scratchTimestamp1, value, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector1[0])),
-              value,
-              i);
+          scratchTimestamp1.setTime(DateWritable.daysToMillis((int) vector1[i]));
+          dtm.subtract(scratchTimestamp1, value, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateScalarSubtractDateColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateScalarSubtractDateColumn.java
index a8cabb87ef..59cf9da3fe 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateScalarSubtractDateColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DateScalarSubtractDateColumn.java
@@ -18,7 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil;
@@ -32,16 +33,17 @@ public class DateScalarSubtractDateColumn extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int colNum;
-  private PisaTimestamp value;
+  private Timestamp value;
   private int outputColumn;
-  private PisaTimestamp scratchPisaTimestamp;
+  private Timestamp scratchTimestamp2;
   private DateTimeMath dtm = new DateTimeMath();
 
   public DateScalarSubtractDateColumn(long value, int colNum, int outputColumn) {
     this.colNum = colNum;
-    this.value = new PisaTimestamp().updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) value));
+    this.value = new Timestamp(0);
+    this.value.setTime(DateWritable.daysToMillis((int) value));
     this.outputColumn = outputColumn;
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchTimestamp2 = new Timestamp(0);
   }
 
   public DateScalarSubtractDateColumn() {
@@ -62,8 +64,8 @@ public void evaluate(VectorizedRowBatch batch) {
     // Input #2 is type date (epochDays).
     LongColumnVector inputColVector2 = (LongColumnVector) batch.cols[colNum];
 
-    // Output is type Timestamp.
-    TimestampColumnVector outputColVector = (TimestampColumnVector) batch.cols[outputColumn];
+    // Output is type HiveIntervalDayTime.
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
 
     int[] sel = batch.selected;
     boolean[] inputIsNull = inputColVector2.isNull;
@@ -80,46 +82,40 @@ public void evaluate(VectorizedRowBatch batch) {
     }
 
     if (inputColVector2.isRepeating) {
-      outputColVector.subtract(
-          value,
-          scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[0])),
-          0);
-
+      scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[0]));
+      dtm.subtract(value, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+      outputColVector.setFromScratchIntervalDayTime(0);
       // Even if there are no nulls, we always copy over entry 0. Simplifies code.
       outputIsNull[0] = inputIsNull[0];
     } else if (inputColVector2.noNulls) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              value,
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(value, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              value,
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(value, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
       }
     } else {                         /* there are nulls */
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.subtract(
-              value,
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(value, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
           outputIsNull[i] = inputIsNull[i];
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.subtract(
-              value,
-              scratchPisaTimestamp.updateFromTimestampMilliseconds(DateWritable.daysToMillis((int) vector2[i])),
-              i);
+          scratchTimestamp2.setTime(DateWritable.daysToMillis((int) vector2[i]));
+          dtm.subtract(value, scratchTimestamp2, outputColVector.getScratchIntervalDayTime());
+          outputColVector.setFromScratchIntervalDayTime(i);
         }
         System.arraycopy(inputIsNull, 0, outputIsNull, 0, n);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterTimestampColumnInList.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterTimestampColumnInList.java
index 42e49845c9..25a276af8f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterTimestampColumnInList.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterTimestampColumnInList.java
@@ -21,7 +21,6 @@
 import java.sql.Timestamp;
 import java.util.HashSet;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Descriptor;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
@@ -35,7 +34,7 @@ public class FilterTimestampColumnInList extends VectorExpression implements ITi
   private Timestamp[] inListValues;
 
   // The set object containing the IN list.
-  private transient HashSet<PisaTimestamp> inSet;
+  private transient HashSet<Timestamp> inSet;
 
   public FilterTimestampColumnInList() {
     super();
@@ -58,9 +57,9 @@ public void evaluate(VectorizedRowBatch batch) {
     }
 
     if (inSet == null) {
-      inSet = new HashSet<PisaTimestamp>(inListValues.length);
+      inSet = new HashSet<Timestamp>(inListValues.length);
       for (Timestamp val : inListValues) {
-        inSet.add(new PisaTimestamp(val));
+        inSet.add(val);
       }
     }
 
@@ -74,16 +73,13 @@ public void evaluate(VectorizedRowBatch batch) {
       return;
     }
 
-    PisaTimestamp scratchTimestamp = new PisaTimestamp();
-
     if (inputColVector.noNulls) {
       if (inputColVector.isRepeating) {
 
         // All must be selected otherwise size would be zero
         // Repeating property will not change.
 
-        inputColVector.pisaTimestampUpdate(scratchTimestamp, 0);
-        if (!(inSet.contains(scratchTimestamp))) {
+        if (!(inSet.contains(inputColVector.asScratchTimestamp(0)))) {
           //Entire batch is filtered out.
           batch.size = 0;
         }
@@ -91,8 +87,7 @@ public void evaluate(VectorizedRowBatch batch) {
         int newSize = 0;
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-          if (inSet.contains(scratchTimestamp)) {
+          if (inSet.contains(inputColVector.asScratchTimestamp(i))) {
             sel[newSize++] = i;
           }
         }
@@ -100,8 +95,7 @@ public void evaluate(VectorizedRowBatch batch) {
       } else {
         int newSize = 0;
         for(int i = 0; i != n; i++) {
-          inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-          if (inSet.contains(scratchTimestamp)) {
+          if (inSet.contains(inputColVector.asScratchTimestamp(i))) {
             sel[newSize++] = i;
           }
         }
@@ -116,8 +110,7 @@ public void evaluate(VectorizedRowBatch batch) {
         //All must be selected otherwise size would be zero
         //Repeating property will not change.
         if (!nullPos[0]) {
-          inputColVector.pisaTimestampUpdate(scratchTimestamp, 0);
-          if (!inSet.contains(scratchTimestamp)) {
+          if (!inSet.contains(inputColVector.asScratchTimestamp(0))) {
 
             //Entire batch is filtered out.
             batch.size = 0;
@@ -130,8 +123,7 @@ public void evaluate(VectorizedRowBatch batch) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
           if (!nullPos[i]) {
-            inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-           if (inSet.contains(scratchTimestamp)) {
+           if (inSet.contains(inputColVector.asScratchTimestamp(i))) {
              sel[newSize++] = i;
            }
           }
@@ -143,8 +135,7 @@ public void evaluate(VectorizedRowBatch batch) {
         int newSize = 0;
         for(int i = 0; i != n; i++) {
           if (!nullPos[i]) {
-            inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-            if (inSet.contains(scratchTimestamp)) {
+            if (inSet.contains(inputColVector.asScratchTimestamp(i))) {
               sel[newSize++] = i;
             }
           }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnColumn.java
index a6f80578d0..804923e7a7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnColumn.java
@@ -17,24 +17,123 @@
  */
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 /**
  * Compute IF(expr1, expr2, expr3) for 3 input column expressions.
  * The first is always a boolean (LongColumnVector).
  * The second and third are long columns or long expression results.
  */
-public class IfExprIntervalDayTimeColumnColumn extends IfExprTimestampColumnColumnBase {
+public class IfExprIntervalDayTimeColumnColumn extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int arg1Column, arg2Column, arg3Column;
+  private int outputColumn;
+
   public IfExprIntervalDayTimeColumnColumn(int arg1Column, int arg2Column, int arg3Column, int outputColumn) {
-    super(arg1Column, arg2Column, arg3Column, outputColumn);
+    this.arg1Column = arg1Column;
+    this.arg2Column = arg2Column;
+    this.arg3Column = arg3Column;
+    this.outputColumn = outputColumn;
   }
 
   public IfExprIntervalDayTimeColumnColumn() {
     super();
   }
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    LongColumnVector arg1ColVector = (LongColumnVector) batch.cols[arg1Column];
+    IntervalDayTimeColumnVector arg2ColVector = (IntervalDayTimeColumnVector) batch.cols[arg2Column];
+    IntervalDayTimeColumnVector arg3ColVector = (IntervalDayTimeColumnVector) batch.cols[arg3Column];
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
+    int[] sel = batch.selected;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = arg2ColVector.noNulls && arg3ColVector.noNulls;
+    outputColVector.isRepeating = false; // may override later
+    int n = batch.size;
+    long[] vector1 = arg1ColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    /* All the code paths below propagate nulls even if neither arg2 nor arg3
+     * have nulls. This is to reduce the number of code paths and shorten the
+     * code, at the expense of maybe doing unnecessary work if neither input
+     * has nulls. This could be improved in the future by expanding the number
+     * of code paths.
+     */
+    if (arg1ColVector.isRepeating) {
+      if (vector1[0] == 1) {
+        arg2ColVector.copySelected(batch.selectedInUse, sel, n, outputColVector);
+      } else {
+        arg3ColVector.copySelected(batch.selectedInUse, sel, n, outputColVector);
+      }
+      return;
+    }
+
+    // extend any repeating values and noNulls indicator in the inputs
+    arg2ColVector.flatten(batch.selectedInUse, sel, n);
+    arg3ColVector.flatten(batch.selectedInUse, sel, n);
+
+    if (arg1ColVector.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchIntervalDayTime(i) : arg3ColVector.asScratchIntervalDayTime(i));
+          outputIsNull[i] = (vector1[i] == 1 ?
+              arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchIntervalDayTime(i) : arg3ColVector.asScratchIntervalDayTime(i));
+          outputIsNull[i] = (vector1[i] == 1 ?
+              arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.asScratchIntervalDayTime(i) : arg3ColVector.asScratchIntervalDayTime(i));
+          outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.asScratchIntervalDayTime(i) : arg3ColVector.asScratchIntervalDayTime(i));
+          outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
+        }
+      }
+    }
+
+    // restore repeating and no nulls indicators
+    arg2ColVector.unFlatten();
+    arg3ColVector.unFlatten();
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "interval_day_time";
+  }
 
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnScalar.java
index 4beb50a4f4..8face7da3d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeColumnScalar.java
@@ -19,8 +19,10 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 /**
  * Compute IF(expr1, expr2, expr3) for 3 input column expressions.
@@ -28,19 +30,105 @@
  * The second is a column or non-constant expression result.
  * The third is a constant value.
  */
-public class IfExprIntervalDayTimeColumnScalar extends IfExprTimestampColumnScalarBase {
+public class IfExprIntervalDayTimeColumnScalar extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int arg1Column, arg2Column;
+  private HiveIntervalDayTime arg3Scalar;
+  private int outputColumn;
+
   public IfExprIntervalDayTimeColumnScalar(int arg1Column, int arg2Column, HiveIntervalDayTime arg3Scalar,
       int outputColumn) {
-    super(arg1Column, arg2Column, arg3Scalar.pisaTimestampUpdate(new PisaTimestamp()), outputColumn);
+    this.arg1Column = arg1Column;
+    this.arg2Column = arg2Column;
+    this.arg3Scalar = arg3Scalar;
+    this.outputColumn = outputColumn;
   }
 
   public IfExprIntervalDayTimeColumnScalar() {
     super();
   }
 
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    LongColumnVector arg1ColVector = (LongColumnVector) batch.cols[arg1Column];
+    IntervalDayTimeColumnVector arg2ColVector = (IntervalDayTimeColumnVector) batch.cols[arg2Column];
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
+    int[] sel = batch.selected;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = arg2ColVector.noNulls; // nulls can only come from arg2
+    outputColVector.isRepeating = false; // may override later
+    int n = batch.size;
+    long[] vector1 = arg1ColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (arg1ColVector.isRepeating) {
+      if (vector1[0] == 1) {
+        arg2ColVector.copySelected(batch.selectedInUse, sel, n, outputColVector);
+      } else {
+        outputColVector.fill(arg3Scalar);
+      }
+      return;
+    }
+
+    // Extend any repeating values and noNulls indicator in the inputs to
+    // reduce the number of code paths needed below.
+    arg2ColVector.flatten(batch.selectedInUse, sel, n);
+
+    if (arg1ColVector.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchIntervalDayTime(i) : arg3Scalar);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchIntervalDayTime(i) : arg3Scalar);
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.asScratchIntervalDayTime(i) : arg3Scalar);
+          outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.isNull[i] : false);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.asScratchIntervalDayTime(i) : arg3Scalar);
+          outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2ColVector.isNull[i] : false);
+        }
+      }
+    }
+
+    // restore repeating and no nulls indicators
+    arg2ColVector.unFlatten();
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "interval_day_time";
+  }
+
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder())
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarColumn.java
index 5463c7c388..40f2e08701 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarColumn.java
@@ -19,8 +19,10 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 /**
  * Compute IF(expr1, expr2, expr3) for 3 input column expressions.
@@ -28,19 +30,107 @@
  * The second is a column or non-constant expression result.
  * The third is a constant value.
  */
-public class IfExprIntervalDayTimeScalarColumn extends IfExprTimestampScalarColumnBase {
+public class IfExprIntervalDayTimeScalarColumn extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int arg1Column, arg3Column;
+  private HiveIntervalDayTime arg2Scalar;
+  private int outputColumn;
+
   public IfExprIntervalDayTimeScalarColumn(int arg1Column, HiveIntervalDayTime arg2Scalar, int arg3Column,
       int outputColumn) {
-    super(arg1Column, arg2Scalar.pisaTimestampUpdate(new PisaTimestamp()), arg3Column, outputColumn);
+    this.arg1Column = arg1Column;
+    this.arg2Scalar = arg2Scalar;
+    this.arg3Column = arg3Column;
+    this.outputColumn = outputColumn;
   }
 
   public IfExprIntervalDayTimeScalarColumn() {
     super();
   }
 
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    LongColumnVector arg1ColVector = (LongColumnVector) batch.cols[arg1Column];
+    IntervalDayTimeColumnVector arg3ColVector = (IntervalDayTimeColumnVector) batch.cols[arg3Column];
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
+    int[] sel = batch.selected;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = arg3ColVector.noNulls; // nulls can only come from arg3 column vector
+    outputColVector.isRepeating = false; // may override later
+    int n = batch.size;
+    long[] vector1 = arg1ColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (arg1ColVector.isRepeating) {
+      if (vector1[0] == 1) {
+        outputColVector.fill(arg2Scalar);
+      } else {
+        arg3ColVector.copySelected(batch.selectedInUse, sel, n, outputColVector);
+      }
+      return;
+    }
+
+    // Extend any repeating values and noNulls indicator in the inputs to
+    // reduce the number of code paths needed below.
+    // This could be optimized in the future by having separate paths
+    // for when arg3ColVector is repeating or has no nulls.
+    arg3ColVector.flatten(batch.selectedInUse, sel, n);
+
+    if (arg1ColVector.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3ColVector.asScratchIntervalDayTime(i));
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3ColVector.asScratchIntervalDayTime(i));
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2Scalar : arg3ColVector.asScratchIntervalDayTime(i));
+          outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              false : arg3ColVector.isNull[i]);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2Scalar : arg3ColVector.asScratchIntervalDayTime(i));
+          outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              false : arg3ColVector.isNull[i]);
+        }
+      }
+    }
+
+    // restore repeating and no nulls indicators
+    arg3ColVector.unFlatten();
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "interval_day_time";
+  }
+
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder())
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarScalar.java
index af2e0c0076..43676dd0c9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprIntervalDayTimeScalarScalar.java
@@ -18,9 +18,13 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
+import java.util.Arrays;
+
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 
 /**
  * Compute IF(expr1, expr2, expr3) for 3 input  expressions.
@@ -28,19 +32,93 @@
  * The second is a constant value.
  * The third is a constant value.
  */
-public class IfExprIntervalDayTimeScalarScalar extends IfExprTimestampScalarScalarBase {
+public class IfExprIntervalDayTimeScalarScalar extends VectorExpression {
 
   private static final long serialVersionUID = 1L;
 
+  private int arg1Column;
+  private HiveIntervalDayTime arg2Scalar;
+  private HiveIntervalDayTime arg3Scalar;
+  private int outputColumn;
+
   public IfExprIntervalDayTimeScalarScalar(int arg1Column, HiveIntervalDayTime arg2Scalar, HiveIntervalDayTime arg3Scalar,
       int outputColumn) {
-    super(arg1Column, arg2Scalar.pisaTimestampUpdate(new PisaTimestamp()), arg3Scalar.pisaTimestampUpdate(new PisaTimestamp()), outputColumn);
+    this.arg1Column = arg1Column;
+    this.arg2Scalar = arg2Scalar;
+    this.arg3Scalar = arg3Scalar;
+    this.outputColumn = outputColumn;
   }
 
   public IfExprIntervalDayTimeScalarScalar() {
     super();
   }
 
+  @Override
+  public void evaluate(VectorizedRowBatch batch) {
+
+    if (childExpressions != null) {
+      super.evaluateChildren(batch);
+    }
+
+    LongColumnVector arg1ColVector = (LongColumnVector) batch.cols[arg1Column];
+    IntervalDayTimeColumnVector outputColVector = (IntervalDayTimeColumnVector) batch.cols[outputColumn];
+    int[] sel = batch.selected;
+    boolean[] outputIsNull = outputColVector.isNull;
+    outputColVector.noNulls = false; // output is a scalar which we know is non null
+    outputColVector.isRepeating = false; // may override later
+    int n = batch.size;
+    long[] vector1 = arg1ColVector.vector;
+
+    // return immediately if batch is empty
+    if (n == 0) {
+      return;
+    }
+
+    if (arg1ColVector.isRepeating) {
+      if (vector1[0] == 1) {
+        outputColVector.fill(arg2Scalar);
+      } else {
+        outputColVector.fill(arg3Scalar);
+      }
+    } else if (arg1ColVector.noNulls) {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3Scalar);
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3Scalar);
+        }
+      }
+    } else /* there are nulls */ {
+      if (batch.selectedInUse) {
+        for(int j = 0; j != n; j++) {
+          int i = sel[j];
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2Scalar : arg3Scalar);
+          outputIsNull[i] = false;
+        }
+      } else {
+        for(int i = 0; i != n; i++) {
+          outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
+              arg2Scalar : arg3Scalar);
+        }
+        Arrays.fill(outputIsNull, 0, n, false);
+      }
+    }
+  }
+
+  @Override
+  public int getOutputColumn() {
+    return outputColumn;
+  }
+
+  @Override
+  public String getOutputType() {
+    return "timestamp";
+  }
+
   @Override
   public VectorExpressionDescriptor.Descriptor getDescriptor() {
     return (new VectorExpressionDescriptor.Builder())
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnColumnBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnColumnBase.java
index d3dd67d2bb..844186394a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnColumnBase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnColumnBase.java
@@ -89,13 +89,13 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchPisaTimestamp(i) : arg3ColVector.asScratchPisaTimestamp(i));
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchTimestamp(i) : arg3ColVector.asScratchTimestamp(i));
           outputIsNull[i] = (vector1[i] == 1 ?
               arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchPisaTimestamp(i) : arg3ColVector.asScratchPisaTimestamp(i));
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchTimestamp(i) : arg3ColVector.asScratchTimestamp(i));
           outputIsNull[i] = (vector1[i] == 1 ?
               arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
         }
@@ -105,14 +105,14 @@ public void evaluate(VectorizedRowBatch batch) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
           outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
-              arg2ColVector.asScratchPisaTimestamp(i) : arg3ColVector.asScratchPisaTimestamp(i));
+              arg2ColVector.asScratchTimestamp(i) : arg3ColVector.asScratchTimestamp(i));
           outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
               arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
         }
       } else {
         for(int i = 0; i != n; i++) {
           outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
-              arg2ColVector.asScratchPisaTimestamp(i) : arg3ColVector.asScratchPisaTimestamp(i));
+              arg2ColVector.asScratchTimestamp(i) : arg3ColVector.asScratchTimestamp(i));
           outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
               arg2ColVector.isNull[i] : arg3ColVector.isNull[i]);
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalar.java
index 0660038686..ae997e012e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalar.java
@@ -20,7 +20,6 @@
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -35,7 +34,7 @@ public class IfExprTimestampColumnScalar extends IfExprTimestampColumnScalarBase
 
   public IfExprTimestampColumnScalar(int arg1Column, int arg2Column, Timestamp arg3Scalar,
       int outputColumn) {
-    super(arg1Column, arg2Column, new PisaTimestamp(arg3Scalar), outputColumn);
+    super(arg1Column, arg2Column, arg3Scalar, outputColumn);
   }
 
   public IfExprTimestampColumnScalar() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalarBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalarBase.java
index 8aaad3f19a..6b87ff237c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalarBase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampColumnScalarBase.java
@@ -18,7 +18,7 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
@@ -38,10 +38,10 @@ public abstract class IfExprTimestampColumnScalarBase extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int arg1Column, arg2Column;
-  private PisaTimestamp arg3Scalar;
+  private Timestamp arg3Scalar;
   private int outputColumn;
 
-  public IfExprTimestampColumnScalarBase(int arg1Column, int arg2Column, PisaTimestamp arg3Scalar,
+  public IfExprTimestampColumnScalarBase(int arg1Column, int arg2Column, Timestamp arg3Scalar,
       int outputColumn) {
     this.arg1Column = arg1Column;
     this.arg2Column = arg2Column;
@@ -91,11 +91,11 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchPisaTimestamp(i) : arg3Scalar);
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchTimestamp(i) : arg3Scalar);
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchPisaTimestamp(i) : arg3Scalar);
+          outputColVector.set(i, vector1[i] == 1 ? arg2ColVector.asScratchTimestamp(i) : arg3Scalar);
         }
       }
     } else /* there are nulls */ {
@@ -103,14 +103,14 @@ public void evaluate(VectorizedRowBatch batch) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
           outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
-              arg2ColVector.asScratchPisaTimestamp(i) : arg3Scalar);
+              arg2ColVector.asScratchTimestamp(i) : arg3Scalar);
           outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
               arg2ColVector.isNull[i] : false);
         }
       } else {
         for(int i = 0; i != n; i++) {
           outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
-              arg2ColVector.asScratchPisaTimestamp(i) : arg3Scalar);
+              arg2ColVector.asScratchTimestamp(i) : arg3Scalar);
           outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
               arg2ColVector.isNull[i] : false);
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumn.java
index 7f618cb2c7..3d53df1dee 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumn.java
@@ -20,7 +20,6 @@
 
 import java.sql.Timestamp;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 /**
@@ -35,7 +34,7 @@ public class IfExprTimestampScalarColumn extends IfExprTimestampScalarColumnBase
 
   public IfExprTimestampScalarColumn(int arg1Column, Timestamp arg2Scalar, int arg3Column,
       int outputColumn) {
-    super(arg1Column, new PisaTimestamp(arg2Scalar), arg3Column, outputColumn);
+    super(arg1Column, arg2Scalar, arg3Column, outputColumn);
   }
 
   public IfExprTimestampScalarColumn() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumnBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumnBase.java
index 84d7655de9..2162f17434 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumnBase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarColumnBase.java
@@ -18,7 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import java.sql.Timestamp;
+
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
@@ -36,10 +37,10 @@ public abstract class IfExprTimestampScalarColumnBase extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int arg1Column, arg3Column;
-  private PisaTimestamp arg2Scalar;
+  private Timestamp arg2Scalar;
   private int outputColumn;
 
-  public IfExprTimestampScalarColumnBase(int arg1Column, PisaTimestamp arg2Scalar, int arg3Column,
+  public IfExprTimestampScalarColumnBase(int arg1Column, Timestamp arg2Scalar, int arg3Column,
       int outputColumn) {
     this.arg1Column = arg1Column;
     this.arg2Scalar = arg2Scalar;
@@ -91,11 +92,11 @@ public void evaluate(VectorizedRowBatch batch) {
       if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3ColVector.asScratchPisaTimestamp(i));
+          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3ColVector.asScratchTimestamp(i));
         }
       } else {
         for(int i = 0; i != n; i++) {
-          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3ColVector.asScratchPisaTimestamp(i));
+          outputColVector.set(i, vector1[i] == 1 ? arg2Scalar : arg3ColVector.asScratchTimestamp(i));
         }
       }
     } else /* there are nulls */ {
@@ -103,14 +104,14 @@ public void evaluate(VectorizedRowBatch batch) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
           outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
-              arg2Scalar : arg3ColVector.asScratchPisaTimestamp(i));
+              arg2Scalar : arg3ColVector.asScratchTimestamp(i));
           outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
               false : arg3ColVector.isNull[i]);
         }
       } else {
         for(int i = 0; i != n; i++) {
           outputColVector.set(i, !arg1ColVector.isNull[i] && vector1[i] == 1 ?
-              arg2Scalar : arg3ColVector.asScratchPisaTimestamp(i));
+              arg2Scalar : arg3ColVector.asScratchTimestamp(i));
           outputIsNull[i] = (!arg1ColVector.isNull[i] && vector1[i] == 1 ?
               false : arg3ColVector.isNull[i]);
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalar.java
index 5286ea34b9..cd00d3a9cb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalar.java
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor;
 
 import java.sql.Timestamp;
@@ -35,7 +34,7 @@ public class IfExprTimestampScalarScalar extends IfExprTimestampScalarScalarBase
 
   public IfExprTimestampScalarScalar(int arg1Column, Timestamp arg2Scalar, Timestamp arg3Scalar,
       int outputColumn) {
-    super(arg1Column, new PisaTimestamp(arg2Scalar), new PisaTimestamp(arg3Scalar), outputColumn);
+    super(arg1Column, arg2Scalar, arg3Scalar, outputColumn);
   }
 
   public IfExprTimestampScalarScalar() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalarBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalarBase.java
index 1aeabfcfd4..707f574d4d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalarBase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprTimestampScalarScalarBase.java
@@ -18,11 +18,12 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+
+import java.sql.Timestamp;
 import java.util.Arrays;
 
 /**
@@ -36,11 +37,11 @@ public abstract class IfExprTimestampScalarScalarBase extends VectorExpression {
   private static final long serialVersionUID = 1L;
 
   private int arg1Column;
-  private PisaTimestamp arg2Scalar;
-  private PisaTimestamp arg3Scalar;
+  private Timestamp arg2Scalar;
+  private Timestamp arg3Scalar;
   private int outputColumn;
 
-  public IfExprTimestampScalarScalarBase(int arg1Column, PisaTimestamp arg2Scalar, PisaTimestamp arg3Scalar,
+  public IfExprTimestampScalarScalarBase(int arg1Column, Timestamp arg2Scalar, Timestamp arg3Scalar,
       int outputColumn) {
     this.arg1Column = arg1Column;
     this.arg2Scalar = arg2Scalar;
@@ -116,8 +117,4 @@ public int getOutputColumn() {
   public String getOutputType() {
     return "timestamp";
   }
-
-  public int getArg1Column() {
-    return arg1Column;
-  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/NullUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/NullUtil.java
index 3c6824db26..eb493bf023 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/NullUtil.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/NullUtil.java
@@ -22,6 +22,7 @@
 
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
@@ -107,6 +108,31 @@ public static void setNullDataEntriesTimestamp(
     }
   }
 
+  /**
+   * Set the data value for all NULL entries to the designated NULL_VALUE.
+   */
+  public static void setNullDataEntriesIntervalDayTime(
+      IntervalDayTimeColumnVector v, boolean selectedInUse, int[] sel, int n) {
+    if (v.noNulls) {
+      return;
+    } else if (v.isRepeating && v.isNull[0]) {
+      v.setNullValue(0);
+    } else if (selectedInUse) {
+      for (int j = 0; j != n; j++) {
+        int i = sel[j];
+        if(v.isNull[i]) {
+          v.setNullValue(i);
+        }
+      }
+    } else {
+      for (int i = 0; i != n; i++) {
+        if(v.isNull[i]) {
+          v.setNullValue(i);
+        }
+      }
+    }
+  }
+
   // for use by Column-Scalar and Scalar-Column arithmetic for null propagation
   public static void setNullOutputEntriesColScalar(
       ColumnVector v, boolean selectedInUse, int[] sel, int n) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TimestampColumnInList.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TimestampColumnInList.java
index 2d7d0c2b4c..bc09a3a806 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TimestampColumnInList.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/TimestampColumnInList.java
@@ -21,7 +21,6 @@
 import java.sql.Timestamp;
 import java.util.HashSet;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Descriptor;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
@@ -36,11 +35,8 @@ public class TimestampColumnInList extends VectorExpression implements ITimestam
   private Timestamp[] inListValues;
   private int outputColumn;
 
-  private transient PisaTimestamp scratchTimestamp;
-
-
   // The set object containing the IN list.
-  private transient HashSet<PisaTimestamp> inSet;
+  private transient HashSet<Timestamp> inSet;
 
   public TimestampColumnInList() {
     super();
@@ -64,11 +60,10 @@ public void evaluate(VectorizedRowBatch batch) {
     }
 
     if (inSet == null) {
-      inSet = new HashSet<PisaTimestamp>(inListValues.length);
+      inSet = new HashSet<Timestamp>(inListValues.length);
       for (Timestamp val : inListValues) {
-        inSet.add(new PisaTimestamp(val));
+        inSet.add(val);
       }
-      scratchTimestamp = new PisaTimestamp();
     }
 
     TimestampColumnVector inputColVector = (TimestampColumnVector) batch.cols[inputCol];
@@ -91,19 +86,16 @@ public void evaluate(VectorizedRowBatch batch) {
 
         // All must be selected otherwise size would be zero
         // Repeating property will not change.
-        inputColVector.pisaTimestampUpdate(scratchTimestamp, 0);
-        outputVector[0] = inSet.contains(scratchTimestamp) ? 1 : 0;
+        outputVector[0] = inSet.contains(inputColVector.asScratchTimestamp(0)) ? 1 : 0;
         outputColVector.isRepeating = true;
       } else if (batch.selectedInUse) {
         for(int j = 0; j != n; j++) {
           int i = sel[j];
-          inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-          outputVector[i] = inSet.contains(scratchTimestamp) ? 1 : 0;
+          outputVector[i] = inSet.contains(inputColVector.asScratchTimestamp(i)) ? 1 : 0;
         }
       } else {
         for(int i = 0; i != n; i++) {
-          inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-          outputVector[i] = inSet.contains(scratchTimestamp) ? 1 : 0;
+          outputVector[i] = inSet.contains(inputColVector.asScratchTimestamp(i)) ? 1 : 0;
         }
       }
     } else {
@@ -112,8 +104,7 @@ public void evaluate(VectorizedRowBatch batch) {
         //All must be selected otherwise size would be zero
         //Repeating property will not change.
         if (!nullPos[0]) {
-          inputColVector.pisaTimestampUpdate(scratchTimestamp, 0);
-          outputVector[0] = inSet.contains(scratchTimestamp) ? 1 : 0;
+          outputVector[0] = inSet.contains(inputColVector.asScratchTimestamp(0)) ? 1 : 0;
           outNulls[0] = false;
         } else {
           outNulls[0] = true;
@@ -124,16 +115,14 @@ public void evaluate(VectorizedRowBatch batch) {
           int i = sel[j];
           outNulls[i] = nullPos[i];
           if (!nullPos[i]) {
-            inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-            outputVector[i] = inSet.contains(scratchTimestamp) ? 1 : 0;
+            outputVector[i] = inSet.contains(inputColVector.asScratchTimestamp(i)) ? 1 : 0;
           }
         }
       } else {
         System.arraycopy(nullPos, 0, outNulls, 0, n);
         for(int i = 0; i != n; i++) {
           if (!nullPos[i]) {
-            inputColVector.pisaTimestampUpdate(scratchTimestamp, i);
-            outputVector[i] = inSet.contains(scratchTimestamp) ? 1 : 0;
+            outputVector[i] = inSet.contains(inputColVector.asScratchTimestamp(i)) ? 1 : 0;
           }
         }
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriter.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriter.java
index 326bfb923e..85dacd76db 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriter.java
@@ -21,10 +21,11 @@
 import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
+import org.apache.hadoop.hive.serde2.io.HiveIntervalDayTimeWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 
@@ -42,7 +43,8 @@ public interface VectorExpressionWriter {
   Object writeValue(HiveDecimal value) throws HiveException;
   Object writeValue(TimestampWritable value) throws HiveException;
   Object writeValue(Timestamp value) throws HiveException;
-  Object writeValue(PisaTimestamp value) throws HiveException;
+  Object writeValue(HiveIntervalDayTimeWritable value) throws HiveException;
+  Object writeValue(HiveIntervalDayTime value) throws HiveException;
   Object setValue(Object row, ColumnVector column, int columnRow) throws HiveException;
   Object initValue(Object ost) throws HiveException;
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
index 9a1d7f3d92..c20bc685e4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java
@@ -31,7 +31,6 @@
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveVarchar;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.*;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
@@ -188,17 +187,39 @@ public Object setValue(Object field, TimestampWritable value) throws HiveExcepti
     }
 
     /**
-     * The base implementation must be overridden by the PisaTimestamp specialization
+     * The base implementation must be overridden by the Timestamp specialization
+     */
+    public Object setValue(Object field, Timestamp value) throws HiveException {
+      throw new HiveException("Internal error: should not reach here");
+    }
+
+    /**
+     * The base implementation must be overridden by the HiveIntervalDayTime specialization
      */
     @Override
-    public Object writeValue(PisaTimestamp value) throws HiveException {
+    public Object writeValue(HiveIntervalDayTimeWritable value) throws HiveException {
       throw new HiveException("Internal error: should not reach here");
     }
 
     /**
-     * The base implementation must be overridden by the Timestamp specialization
+     * The base implementation must be overridden by the HiveIntervalDayTime specialization
      */
-    public Object setValue(Object field, Timestamp value) throws HiveException {
+    @Override
+    public Object writeValue(HiveIntervalDayTime value) throws HiveException {
+      throw new HiveException("Internal error: should not reach here");
+    }
+
+    /**
+     * The base implementation must be overridden by the HiveIntervalDayTime specialization
+     */
+    public Object setValue(Object field, HiveIntervalDayTimeWritable value) throws HiveException {
+      throw new HiveException("Internal error: should not reach here");
+    }
+
+    /**
+     * The base implementation must be overridden by the HiveIntervalDayTime specialization
+     */
+    public Object setValue(Object field, HiveIntervalDayTime value) throws HiveException {
       throw new HiveException("Internal error: should not reach here");
     }
   }
@@ -465,6 +486,66 @@ public Object setValue(Object field, ColumnVector column, int row) throws HiveEx
     }
   }
 
+  /**
+   * Specialized writer for IntervalDayTimeColumnVector. Will throw cast exception
+   * if the wrong vector column is used.
+   */
+  private static abstract class VectorExpressionWriterIntervalDayTime extends VectorExpressionWriterBase {
+    @Override
+    public Object writeValue(ColumnVector column, int row) throws HiveException {
+      IntervalDayTimeColumnVector dcv = (IntervalDayTimeColumnVector) column;
+      HiveIntervalDayTimeWritable intervalDayTimeWritable = (HiveIntervalDayTimeWritable) dcv.getScratchWritable();
+      if (intervalDayTimeWritable == null) {
+        intervalDayTimeWritable = new HiveIntervalDayTimeWritable();
+        dcv.setScratchWritable(intervalDayTimeWritable);
+      }
+      if (dcv.noNulls && !dcv.isRepeating) {
+        return writeValue(TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, row, intervalDayTimeWritable));
+      } else if (dcv.noNulls && dcv.isRepeating) {
+        return writeValue(TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, 0, intervalDayTimeWritable));
+      } else if (!dcv.noNulls && !dcv.isRepeating && !dcv.isNull[row]) {
+        return writeValue(TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, row, intervalDayTimeWritable));
+      } else if (!dcv.noNulls && dcv.isRepeating && !dcv.isNull[0]) {
+        return writeValue(TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, 0, intervalDayTimeWritable));
+      } else if (!dcv.noNulls && dcv.isRepeating && dcv.isNull[0]) {
+        return null;
+      } else if (!dcv.noNulls && !dcv.isRepeating && dcv.isNull[row]) {
+        return null;
+      }
+      throw new HiveException(
+          String.format(
+              "Incorrect null/repeating: row:%d noNulls:%b isRepeating:%b isNull[row]:%b isNull[0]:%b",
+              row, dcv.noNulls, dcv.isRepeating, dcv.isNull[row], dcv.isNull[0]));
+    }
+
+    @Override
+    public Object setValue(Object field, ColumnVector column, int row) throws HiveException {
+      IntervalDayTimeColumnVector dcv = (IntervalDayTimeColumnVector) column;
+      HiveIntervalDayTimeWritable intervalDayTimeWritable = (HiveIntervalDayTimeWritable) dcv.getScratchWritable();
+      if (intervalDayTimeWritable == null) {
+        intervalDayTimeWritable = new HiveIntervalDayTimeWritable();
+        dcv.setScratchWritable(intervalDayTimeWritable);
+      }
+      if (dcv.noNulls && !dcv.isRepeating) {
+        return setValue(field, TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, row, intervalDayTimeWritable));
+      } else if (dcv.noNulls && dcv.isRepeating) {
+        return setValue(field, TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, 0, intervalDayTimeWritable));
+      } else if (!dcv.noNulls && !dcv.isRepeating && !dcv.isNull[row]) {
+        return setValue(field, TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, row, intervalDayTimeWritable));
+      } else if (!dcv.noNulls && !dcv.isRepeating && dcv.isNull[row]) {
+        return null;
+      } else if (!dcv.noNulls && dcv.isRepeating && !dcv.isNull[0]) {
+        return setValue(field, TimestampUtils.intervalDayTimeColumnVectorWritable(dcv, 0, intervalDayTimeWritable));
+      } else if (!dcv.noNulls && dcv.isRepeating && dcv.isNull[0]) {
+        return null;
+      }
+      throw new HiveException(
+          String.format(
+              "Incorrect null/repeating: row:%d noNulls:%b isRepeating:%b isNull[row]:%b isNull[0]:%b",
+              row, dcv.noNulls, dcv.isRepeating, dcv.isNull[row], dcv.isNull[0]));
+    }
+  }
+
     /**
      * Compiles the appropriate vector expression writer based on an expression info (ExprNodeDesc)
      */
@@ -697,8 +778,13 @@ public Object writeValue(Timestamp value) throws HiveException {
       }
 
       @Override
-      public Object writeValue(PisaTimestamp value) throws HiveException {
-        return ((SettableTimestampObjectInspector) this.objectInspector).set(obj, value.asScratchTimestamp());
+      public Object writeValue(HiveIntervalDayTimeWritable value) throws HiveException {
+        return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(obj, value);
+      }
+
+      @Override
+      public Object writeValue(HiveIntervalDayTime value) throws HiveException {
+        return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(obj, value);
       }
 
       @Override
@@ -766,53 +852,45 @@ public Object initValue(Object ignored) {
   private static VectorExpressionWriter genVectorExpressionWritableIntervalDayTime(
       SettableHiveIntervalDayTimeObjectInspector fieldObjInspector) throws HiveException {
 
-    return new VectorExpressionWriterTimestamp() {
+    return new VectorExpressionWriterIntervalDayTime() {
       private Object obj;
       private HiveIntervalDayTime interval;
-      private PisaTimestamp pisaTimestamp;
 
       public VectorExpressionWriter init(SettableHiveIntervalDayTimeObjectInspector objInspector)
           throws HiveException {
         super.init(objInspector);
         interval = new HiveIntervalDayTime();
         obj = initValue(null);
-        pisaTimestamp = new PisaTimestamp();
         return this;
       }
 
       @Override
-      public Object writeValue(TimestampWritable value) throws HiveException {
-        interval.set(pisaTimestamp.updateFromTimestamp(value.getTimestamp()));
+      public Object writeValue(HiveIntervalDayTimeWritable value) throws HiveException {
+        interval.set(value.getHiveIntervalDayTime());
         return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(obj, interval);
       }
 
       @Override
-      public Object writeValue(Timestamp value) throws HiveException {
-        interval.set(pisaTimestamp.updateFromTimestamp(value));
-        return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(obj, interval);
-      }
-
-      @Override
-      public Object writeValue(PisaTimestamp value) throws HiveException {
+      public Object writeValue(HiveIntervalDayTime value) throws HiveException {
         interval.set(value);
         return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(obj, interval);
       }
 
       @Override
-      public Object setValue(Object field, TimestampWritable value) {
+      public Object setValue(Object field, HiveIntervalDayTimeWritable value) {
         if (null == field) {
           field = initValue(null);
         }
-        interval.set(pisaTimestamp.updateFromTimestamp(value.getTimestamp()));
+        interval.set(value.getHiveIntervalDayTime());
         return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(field, interval);
       }
 
       @Override
-      public Object setValue(Object field, Timestamp value) {
+      public Object setValue(Object field, HiveIntervalDayTime value) {
         if (null == field) {
           field = initValue(null);
         }
-        interval.set(pisaTimestamp.updateFromTimestamp(value));
+        interval.set(value);
         return ((SettableHiveIntervalDayTimeObjectInspector) this.objectInspector).set(field, interval);
       }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColCol.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColCol.java
index 9f5c793166..05dd93e070 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColCol.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColCol.java
@@ -166,7 +166,7 @@ protected byte[] evaluateDate(ColumnVector columnVector, int index, long numDays
 
   protected byte[] evaluateTimestamp(ColumnVector columnVector, int index, long numDays) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    calendar.setTimeInMillis(tcv.getTimestampMilliseconds(index));
+    calendar.setTimeInMillis(tcv.getTime(index));
     if (isPositive) {
       calendar.add(Calendar.DATE, (int) numDays);
     } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColScalar.java
index 6390ecd03e..59ca61e1d4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateAddColScalar.java
@@ -210,7 +210,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   protected byte[] evaluateTimestamp(ColumnVector columnVector, int index) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    calendar.setTimeInMillis(tcv.getTimestampMilliseconds(index));
+    calendar.setTimeInMillis(tcv.getTime(index));
     if (isPositive) {
       calendar.add(Calendar.DATE, numDays);
     } else {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColCol.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColCol.java
index b22c31f637..4edf5588b7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColCol.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColCol.java
@@ -275,7 +275,7 @@ public void copySelected(
       output.isRepeating = true;
 
       if (!input.isNull[0]) {
-        date.setTime(input.getTimestampMilliseconds(0));
+        date.setTime(input.getTime(0));
         output.vector[0] = DateWritable.dateToDays(date);
       }
       return;
@@ -288,12 +288,12 @@ public void copySelected(
       if (selectedInUse) {
         for (int j = 0; j < size; j++) {
           int i = sel[j];
-          date.setTime(input.getTimestampMilliseconds(i));
+          date.setTime(input.getTime(i));
           output.vector[i] = DateWritable.dateToDays(date);
         }
       } else {
         for (int i = 0; i < size; i++) {
-          date.setTime(input.getTimestampMilliseconds(i));
+          date.setTime(input.getTime(i));
           output.vector[i] = DateWritable.dateToDays(date);
         }
       }
@@ -312,14 +312,14 @@ public void copySelected(
         for (int j = 0; j < size; j++) {
           int i = sel[j];
           if (!input.isNull[i]) {
-            date.setTime(input.getTimestampMilliseconds(i));
+            date.setTime(input.getTime(i));
             output.vector[i] = DateWritable.dateToDays(date);
           }
         }
       } else {
         for (int i = 0; i < size; i++) {
           if (!input.isNull[i]) {
-            date.setTime(input.getTimestampMilliseconds(i));
+            date.setTime(input.getTime(i));
             output.vector[i] = DateWritable.dateToDays(date);
           }
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColScalar.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColScalar.java
index ab71b47f77..71b3887fee 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColScalar.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffColScalar.java
@@ -238,7 +238,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   protected int evaluateTimestamp(ColumnVector columnVector, int index) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    date.setTime(tcv.getTimestampMilliseconds(index));
+    date.setTime(tcv.getTime(index));
     return DateWritable.dateToDays(date) - baseDate;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffScalarCol.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffScalarCol.java
index dea5444045..c733bc94ea 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffScalarCol.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateDiffScalarCol.java
@@ -237,7 +237,7 @@ public void evaluate(VectorizedRowBatch batch) {
 
   protected int evaluateTimestamp(ColumnVector columnVector, int index) {
     TimestampColumnVector tcv = (TimestampColumnVector) columnVector;
-    date.setTime(tcv.getTimestampMilliseconds(index));
+    date.setTime(tcv.getTime(index));
     return baseDate - DateWritable.dateToDays(date);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateTimestamp.java
index c29e22eb13..cde0be45da 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFDateTimestamp.java
@@ -45,7 +45,7 @@ public VectorUDFDateTimestamp(int inputColumn, int outputColumn) {
   protected void func(BytesColumnVector outV, TimestampColumnVector inV, int i) {
     switch (inputTypes[0]) {
       case TIMESTAMP:
-        date.setTime(inV.getTimestampMilliseconds(i));
+        date.setTime(inV.getTime(i));
         break;
 
       default:
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampDate.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampDate.java
index b7c4ff4484..3c693af516 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampDate.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampDate.java
@@ -28,15 +28,18 @@ public final class VectorUDFUnixTimeStampDate extends VectorUDFTimestampFieldDat
 
   private static final long serialVersionUID = 1L;
 
+  private DateWritable dateWritable;
+
   @Override
   protected long getDateField(long days) {
-    long ms = DateWritable.daysToMillis((int) days);
-    return ms / 1000;
+    dateWritable.set((int) days);
+    return dateWritable.getTimeInSeconds();
   }
 
   public VectorUDFUnixTimeStampDate(int colNum, int outputColumn) {
     /* not a real field */
     super(-1, colNum, outputColumn);
+    dateWritable = new DateWritable();
   }
 
   public VectorUDFUnixTimeStampDate() {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampTimestamp.java
index e4a31caad1..2bd7756f23 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorUDFUnixTimeStampTimestamp.java
@@ -18,10 +18,7 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
-import java.sql.Timestamp;
-
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
-import org.apache.hadoop.hive.serde2.io.DateWritable;
 
 /**
  * Return Unix Timestamp.
@@ -33,7 +30,7 @@ public final class VectorUDFUnixTimeStampTimestamp extends VectorUDFTimestampFie
 
   @Override
   protected long getTimestampField(TimestampColumnVector timestampColVector, int elementNum) {
-    return timestampColVector.getTimestampSeconds(elementNum);
+    return timestampColVector.asScratchTimestamp(elementNum).getTime() / 1000;
   }
 
   public VectorUDFUnixTimeStampTimestamp(int colNum, int outputColumn) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java
index 5c8db41874..d0a1d0d221 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgTimestamp.java
@@ -27,8 +27,6 @@
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
@@ -146,7 +144,8 @@ public void aggregateInputSelection(
         if (inputColVector.isRepeating) {
           iterateNoNullsRepeatingWithAggregationSelection(
             aggregationBufferSets, bufferIndex,
-            inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+            inputColVector.getDouble(0),
+            batchSize);
         } else {
           if (batch.selectedInUse) {
             iterateNoNullsSelectionWithAggregationSelection(
@@ -163,11 +162,11 @@ public void aggregateInputSelection(
           if (batch.selectedInUse) {
             iterateHasNullsRepeatingSelectionWithAggregationSelection(
               aggregationBufferSets, bufferIndex,
-              inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize, batch.selected, inputColVector.isNull);
+              inputColVector.getDouble(0), batchSize, batch.selected, inputColVector.isNull);
           } else {
             iterateHasNullsRepeatingWithAggregationSelection(
               aggregationBufferSets, bufferIndex,
-              inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize, inputColVector.isNull);
+              inputColVector.getDouble(0), batchSize, inputColVector.isNull);
           }
         } else {
           if (batch.selectedInUse) {
@@ -210,7 +209,8 @@ private void iterateNoNullsSelectionWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(inputColVector.getTimestampSecondsWithFractionalNanos(selection[i]));
+        myagg.sumValue(
+            inputColVector.getDouble(selection[i]));
       }
     }
 
@@ -224,7 +224,7 @@ private void iterateNoNullsWithAggregationSelection(
           aggregationBufferSets,
           bufferIndex,
           i);
-        myagg.sumValue(inputColVector.getTimestampSecondsWithFractionalNanos(i));
+        myagg.sumValue(inputColVector.getDouble(i));
       }
     }
 
@@ -281,7 +281,7 @@ private void iterateHasNullsSelectionWithAggregationSelection(
             aggregationBufferSets,
             bufferIndex,
             j);
-          myagg.sumValue(inputColVector.getTimestampSecondsWithFractionalNanos(i));
+          myagg.sumValue(inputColVector.getDouble(i));
         }
       }
    }
@@ -296,10 +296,10 @@ private void iterateHasNullsWithAggregationSelection(
       for (int i=0; i < batchSize; ++i) {
         if (!isNull[i]) {
           Aggregation myagg = getCurrentAggregationBuffer(
-            aggregationBufferSets, 
+            aggregationBufferSets,
             bufferIndex,
             i);
-          myagg.sumValue(inputColVector.getTimestampSecondsWithFractionalNanos(i));
+          myagg.sumValue(inputColVector.getDouble(i));
         }
       }
    }
@@ -328,7 +328,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
               myagg.sum = 0;
               myagg.count = 0;
             }
-            myagg.sum += inputColVector.getTimestampSecondsWithFractionalNanos(0)*batchSize;
+            myagg.sum += inputColVector.getDouble(0)*batchSize;
             myagg.count += batchSize;
           }
           return;
@@ -358,7 +358,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.isNull = false;
             myagg.sum = 0;
@@ -381,24 +381,24 @@ private void iterateSelectionNoNulls(
         myagg.sum = 0;
         myagg.count = 0;
       }
-      
+
       for (int i=0; i< batchSize; ++i) {
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        double value = inputColVector.getDouble(selected[i]);
         myagg.sum += value;
         myagg.count += 1;
       }
     }
 
     private void iterateNoSelectionHasNulls(
-        Aggregation myagg, 
-        TimestampColumnVector inputColVector, 
+        Aggregation myagg,
+        TimestampColumnVector inputColVector,
         int batchSize,
         boolean[] isNull) {
-      
+
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
-          if (myagg.isNull) { 
+          double value = inputColVector.getDouble(i);
+          if (myagg.isNull) {
             myagg.isNull = false;
             myagg.sum = 0;
             myagg.count = 0;
@@ -420,7 +420,7 @@ private void iterateNoSelectionNoNulls(
       }
 
       for (int i=0;i<batchSize;++i) {
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        double value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java
index 17906ec269..fa25e6aed2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdPopTimestamp.java
@@ -152,7 +152,7 @@ public void aggregateInputSelection(
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls || !inputColVector.isNull[0]) {
           iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -213,7 +213,7 @@ private void iterateSelectionHasNullsWithAggregationSelection(
           j);
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -239,7 +239,7 @@ private void iterateSelectionNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        double value = inputColVector.getDouble(selected[i]);
         if (myagg.isNull) {
           myagg.init ();
         }
@@ -265,7 +265,7 @@ private void iterateNoSelectionHasNullsWithAggregationSelection(
             aggregationBufferSets,
             aggregateIndex,
           i);
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -293,7 +293,7 @@ private void iterateNoSelectionNoNullsWithAggregationSelection(
         if (myagg.isNull) {
           myagg.init ();
         }
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        double value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         if(myagg.count > 1) {
@@ -322,7 +322,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
 
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -377,7 +377,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -401,7 +401,7 @@ private void iterateSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[0]);
+      double value = inputColVector.getDouble(selected[0]);
       myagg.sum += value;
       myagg.count += 1;
       if(myagg.count > 1) {
@@ -412,7 +412,7 @@ private void iterateSelectionNoNulls(
       // i=0 was pulled out to remove the count > 1 check in the loop
       //
       for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        value = inputColVector.getDouble(selected[i]);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
@@ -428,7 +428,7 @@ private void iterateNoSelectionHasNulls(
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -451,7 +451,7 @@ private void iterateNoSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(0);
+      double value = inputColVector.getDouble(0);
       myagg.sum += value;
       myagg.count += 1;
 
@@ -462,7 +462,7 @@ private void iterateNoSelectionNoNulls(
 
       // i=0 was pulled out to remove count > 1 check
       for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java
index 2e41e47b2f..b3e1faea52 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFStdSampTimestamp.java
@@ -26,7 +26,6 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression;
 import org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
-import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.AggregationDesc;
@@ -38,7 +37,7 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
 
 /**
-* VectorUDAFStdSampDouble. Vectorized implementation for VARIANCE aggregates.
+* VectorUDAFStdSampTimestamp. Vectorized implementation for VARIANCE aggregates.
 */
 @Description(name = "stddev_samp",
     value = "_FUNC_(x) - Returns the sample standard deviation of a set of numbers (vectorized, double)")
@@ -153,7 +152,7 @@ public void aggregateInputSelection(
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls || !inputColVector.isNull[0]) {
           iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -214,7 +213,7 @@ private void iterateSelectionHasNullsWithAggregationSelection(
           j);
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -240,7 +239,7 @@ private void iterateSelectionNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        double value = inputColVector.getDouble(selected[i]);
         if (myagg.isNull) {
           myagg.init ();
         }
@@ -266,7 +265,7 @@ private void iterateNoSelectionHasNullsWithAggregationSelection(
             aggregationBufferSets,
             aggregateIndex,
           i);
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -294,7 +293,7 @@ private void iterateNoSelectionNoNullsWithAggregationSelection(
         if (myagg.isNull) {
           myagg.init ();
         }
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        double value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         if(myagg.count > 1) {
@@ -323,7 +322,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
 
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -378,7 +377,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -402,7 +401,7 @@ private void iterateSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[0]);
+      double value = inputColVector.getDouble(selected[0]);
       myagg.sum += value;
       myagg.count += 1;
       if(myagg.count > 1) {
@@ -413,7 +412,7 @@ private void iterateSelectionNoNulls(
       // i=0 was pulled out to remove the count > 1 check in the loop
       //
       for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        value = inputColVector.getDouble(selected[i]);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
@@ -429,7 +428,7 @@ private void iterateNoSelectionHasNulls(
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -452,7 +451,7 @@ private void iterateNoSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(0);
+      double value = inputColVector.getDouble(0);
       myagg.sum += value;
       myagg.count += 1;
 
@@ -463,7 +462,7 @@ private void iterateNoSelectionNoNulls(
 
       // i=0 was pulled out to remove count > 1 check
       for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java
index d128b7c927..970ec223f4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarPopTimestamp.java
@@ -152,7 +152,7 @@ public void aggregateInputSelection(
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls || !inputColVector.isNull[0]) {
           iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -213,7 +213,7 @@ private void iterateSelectionHasNullsWithAggregationSelection(
           j);
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -239,7 +239,7 @@ private void iterateSelectionNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        double value = inputColVector.getDouble(selected[i]);
         if (myagg.isNull) {
           myagg.init ();
         }
@@ -265,7 +265,7 @@ private void iterateNoSelectionHasNullsWithAggregationSelection(
             aggregationBufferSets,
             aggregateIndex,
           i);
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -293,7 +293,7 @@ private void iterateNoSelectionNoNullsWithAggregationSelection(
         if (myagg.isNull) {
           myagg.init ();
         }
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        double value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         if(myagg.count > 1) {
@@ -322,7 +322,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
 
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -377,7 +377,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -401,7 +401,7 @@ private void iterateSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[0]);
+      double value = inputColVector.getDouble(selected[0]);
       myagg.sum += value;
       myagg.count += 1;
       if(myagg.count > 1) {
@@ -412,7 +412,7 @@ private void iterateSelectionNoNulls(
       // i=0 was pulled out to remove the count > 1 check in the loop
       //
       for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        value = inputColVector.getDouble(selected[i]);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
@@ -428,7 +428,7 @@ private void iterateNoSelectionHasNulls(
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -451,7 +451,7 @@ private void iterateNoSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(0);
+      double value = inputColVector.getDouble(0);
       myagg.sum += value;
       myagg.count += 1;
 
@@ -462,7 +462,7 @@ private void iterateNoSelectionNoNulls(
 
       // i=0 was pulled out to remove count > 1 check
       for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java
index cf76f208da..9af1a2840b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFVarSampTimestamp.java
@@ -152,7 +152,7 @@ public void aggregateInputSelection(
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls || !inputColVector.isNull[0]) {
           iterateRepeatingNoNullsWithAggregationSelection(
-            aggregationBufferSets, aggregateIndex, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+            aggregationBufferSets, aggregateIndex, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -213,7 +213,7 @@ private void iterateSelectionHasNullsWithAggregationSelection(
           j);
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -239,7 +239,7 @@ private void iterateSelectionNoNullsWithAggregationSelection(
           aggregationBufferSets,
           aggregateIndex,
           i);
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        double value = inputColVector.getDouble(selected[i]);
         if (myagg.isNull) {
           myagg.init ();
         }
@@ -265,7 +265,7 @@ private void iterateNoSelectionHasNullsWithAggregationSelection(
             aggregationBufferSets,
             aggregateIndex,
           i);
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -293,7 +293,7 @@ private void iterateNoSelectionNoNullsWithAggregationSelection(
         if (myagg.isNull) {
           myagg.init ();
         }
-        double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        double value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         if(myagg.count > 1) {
@@ -322,7 +322,7 @@ public void aggregateInput(AggregationBuffer agg, VectorizedRowBatch batch)
 
       if (inputColVector.isRepeating) {
         if (inputColVector.noNulls) {
-          iterateRepeatingNoNulls(myagg, inputColVector.getTimestampSecondsWithFractionalNanos(0), batchSize);
+          iterateRepeatingNoNulls(myagg, inputColVector.getDouble(0), batchSize);
         }
       }
       else if (!batch.selectedInUse && inputColVector.noNulls) {
@@ -377,7 +377,7 @@ private void iterateSelectionHasNulls(
       for (int j=0; j< batchSize; ++j) {
         int i = selected[j];
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -401,7 +401,7 @@ private void iterateSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[0]);
+      double value = inputColVector.getDouble(selected[0]);
       myagg.sum += value;
       myagg.count += 1;
       if(myagg.count > 1) {
@@ -412,7 +412,7 @@ private void iterateSelectionNoNulls(
       // i=0 was pulled out to remove the count > 1 check in the loop
       //
       for (int i=1; i< batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(selected[i]);
+        value = inputColVector.getDouble(selected[i]);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
@@ -428,7 +428,7 @@ private void iterateNoSelectionHasNulls(
 
       for(int i=0;i<batchSize;++i) {
         if (!isNull[i]) {
-          double value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+          double value = inputColVector.getDouble(i);
           if (myagg.isNull) {
             myagg.init ();
           }
@@ -451,7 +451,7 @@ private void iterateNoSelectionNoNulls(
         myagg.init ();
       }
 
-      double value = inputColVector.getTimestampSecondsWithFractionalNanos(0);
+      double value = inputColVector.getDouble(0);
       myagg.sum += value;
       myagg.count += 1;
 
@@ -462,7 +462,7 @@ private void iterateNoSelectionNoNulls(
 
       // i=0 was pulled out to remove count > 1 check
       for (int i=1; i<batchSize; ++i) {
-        value = inputColVector.getTimestampSecondsWithFractionalNanos(i);
+        value = inputColVector.getDouble(i);
         myagg.sum += value;
         myagg.count += 1;
         double t = myagg.count*value - myagg.sum;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java
index d3a0f9f627..20cfb89164 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java
@@ -297,23 +297,14 @@ private void setOutputCol(ColumnVector colVec, int i, Object value) {
         lv.vector[i] = ((WritableByteObjectInspector) outputOI).get(value);
       }
     } else if (outputOI instanceof WritableTimestampObjectInspector) {
-      LongColumnVector lv = (LongColumnVector) colVec;
+      TimestampColumnVector tv = (TimestampColumnVector) colVec;
       Timestamp ts;
       if (value instanceof Timestamp) {
         ts = (Timestamp) value;
       } else {
         ts = ((WritableTimestampObjectInspector) outputOI).getPrimitiveJavaObject(value);
       }
-      /* Calculate the number of nanoseconds since the epoch as a long integer. By convention
-       * that is how Timestamp values are operated on in a vector.
-       */
-      long l = ts.getTime() * 1000000  // Shift the milliseconds value over by 6 digits
-                                       // to scale for nanosecond precision.
-                                       // The milliseconds digits will by convention be all 0s.
-            + ts.getNanos() % 1000000; // Add on the remaining nanos.
-                                       // The % 1000000 operation removes the ms values
-                                       // so that the milliseconds are not counted twice.
-      lv.vector[i] = l;
+      tv.set(i, ts);
     } else if (outputOI instanceof WritableDateObjectInspector) {
       LongColumnVector lv = (LongColumnVector) colVec;
       Date ts;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/util/DateTimeMath.java b/ql/src/java/org/apache/hadoop/hive/ql/util/DateTimeMath.java
index e092ac21f7..98b1ded522 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/util/DateTimeMath.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/util/DateTimeMath.java
@@ -25,7 +25,6 @@
 
 import org.apache.hadoop.hive.common.type.HiveIntervalYearMonth;
 import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hive.common.util.DateUtils;
 
@@ -91,26 +90,6 @@ public long addMonthsToNanosLocal(long nanos, int months) {
     return result;
   }
 
-  /**
-   * Perform month arithmetic to millis value using local time zone.
-   * @param pisaTimestamp
-   * @param months
-   * @return
-   */
-  public PisaTimestamp addMonthsToPisaTimestamp(PisaTimestamp pisaTimestamp, int months,
-      PisaTimestamp scratchPisaTimestamp) {
-    calLocal.setTimeInMillis(pisaTimestamp.getTimestampMilliseconds());
-    calLocal.add(Calendar.MONTH, months);
-    scratchPisaTimestamp.updateFromTimestampMilliseconds(calLocal.getTimeInMillis());
-
-    // Add in portion of nanos below a millisecond...
-    PisaTimestamp.add(
-        scratchPisaTimestamp.getEpochDay(), scratchPisaTimestamp.getNanoOfDay(),
-        0, pisaTimestamp.getNanoOfDay() % 1000000,
-        scratchPisaTimestamp);
-    return scratchPisaTimestamp;
-  }
-
   public long addMonthsToDays(long days, int months) {
     long millis = DateWritable.daysToMillis((int) days);
     millis = addMonthsToMillisLocal(millis, months);
@@ -123,24 +102,95 @@ public Timestamp add(Timestamp ts, HiveIntervalYearMonth interval) {
       return null;
     }
 
+    Timestamp tsResult = new Timestamp(0);
+    add(ts, interval, tsResult);
+
+    return tsResult;
+  }
+
+  public boolean add(Timestamp ts, HiveIntervalYearMonth interval, Timestamp result) {
+    if (ts == null || interval == null) {
+      return false;
+    }
+
     // Attempt to match Oracle semantics for timestamp arithmetic,
     // where timestamp arithmetic is done in UTC, then converted back to local timezone
     long resultMillis = addMonthsToMillisUtc(ts.getTime(), interval.getTotalMonths());
-    Timestamp tsResult = new Timestamp(resultMillis);
-    tsResult.setNanos(ts.getNanos());
+    result.setTime(resultMillis);
+    result.setNanos(ts.getNanos());
+
+    return true;
+  }
+
+  public Timestamp add(HiveIntervalYearMonth interval, Timestamp ts) {
+    if (ts == null || interval == null) {
+      return null;
+    }
+
+    Timestamp tsResult = new Timestamp(0);
+    add(interval, ts, tsResult);
 
     return tsResult;
   }
 
+  public boolean add(HiveIntervalYearMonth interval, Timestamp ts, Timestamp result) {
+    if (ts == null || interval == null) {
+      return false;
+    }
+
+    // Attempt to match Oracle semantics for timestamp arithmetic,
+    // where timestamp arithmetic is done in UTC, then converted back to local timezone
+    long resultMillis = addMonthsToMillisUtc(ts.getTime(), interval.getTotalMonths());
+    result.setTime(resultMillis);
+    result.setNanos(ts.getNanos());
+
+    return true;
+  }
+
   public Date add(Date dt, HiveIntervalYearMonth interval) {
     if (dt == null || interval == null) {
       return null;
     }
 
+    Date dtResult = new Date(0);
+    add(dt, interval, dtResult);
+
+    return dtResult;
+  }
+
+  public boolean add(Date dt, HiveIntervalYearMonth interval, Date result) {
+    if (dt == null || interval == null) {
+      return false;
+    }
+
+    // Since Date millis value is in local timezone representation, do date arithmetic
+    // using local timezone so the time remains at the start of the day.
+    long resultMillis = addMonthsToMillisLocal(dt.getTime(), interval.getTotalMonths());
+    result.setTime(resultMillis);
+    return true;
+  }
+
+  public Date add(HiveIntervalYearMonth interval, Date dt) {
+    if (dt == null || interval == null) {
+      return null;
+    }
+
+    Date dtResult = new Date(0);
+    add(interval, dt, dtResult);
+
+    return dtResult;
+  }
+
+  public boolean add(HiveIntervalYearMonth interval, Date dt, Date result) {
+    if (dt == null || interval == null) {
+      return false;
+    }
+
     // Since Date millis value is in local timezone representation, do date arithmetic
     // using local timezone so the time remains at the start of the day.
     long resultMillis = addMonthsToMillisLocal(dt.getTime(), interval.getTotalMonths());
-    return new Date(resultMillis);
+    result.setTime(resultMillis);
+    return true;
   }
 
   public HiveIntervalYearMonth add(HiveIntervalYearMonth left, HiveIntervalYearMonth right) {
@@ -157,14 +207,36 @@ public Timestamp subtract(Timestamp left, HiveIntervalYearMonth right) {
     if (left == null || right == null) {
       return null;
     }
-    return add(left, right.negate());
+
+    Timestamp tsResult = new Timestamp(0);
+    subtract(left, right, tsResult);
+
+    return tsResult;
+  }
+
+  public boolean subtract(Timestamp left, HiveIntervalYearMonth right, Timestamp result) {
+    if (left == null || right == null) {
+      return false;
+    }
+    return add(left, right.negate(), result);
   }
 
   public Date subtract(Date left, HiveIntervalYearMonth right) {
     if (left == null || right == null) {
       return null;
     }
-    return add(left, right.negate());
+
+    Date dtResult = new Date(0);
+    subtract(left, right, dtResult);
+
+    return dtResult;
+  }
+
+  public boolean subtract(Date left, HiveIntervalYearMonth right, Date result) {
+    if (left == null || right == null) {
+      return false;
+    }
+    return add(left, right.negate(), result);
   }
 
   public HiveIntervalYearMonth subtract(HiveIntervalYearMonth left, HiveIntervalYearMonth right) {
@@ -183,26 +255,74 @@ public Timestamp add(Timestamp ts, HiveIntervalDayTime interval) {
       return null;
     }
 
+    Timestamp tsResult = new Timestamp(0);
+    add(ts, interval, tsResult);
+
+    return tsResult;
+  }
+
+  public boolean add(Timestamp ts, HiveIntervalDayTime interval,
+      Timestamp result) {
+    if (ts == null || interval == null) {
+      return false;
+    }
+
     nanosResult.addNanos(ts.getNanos(), interval.getNanos());
 
     long newMillis = ts.getTime()
         + TimeUnit.SECONDS.toMillis(interval.getTotalSeconds() + nanosResult.seconds);
-    Timestamp tsResult = new Timestamp(newMillis);
-    tsResult.setNanos(nanosResult.nanos);
+    result.setTime(newMillis);
+    result.setNanos(nanosResult.nanos);
+    return true;
+  }
+
+  public Timestamp add(HiveIntervalDayTime interval, Timestamp ts) {
+    if (ts == null || interval == null) {
+      return null;
+    }
+
+    Timestamp tsResult = new Timestamp(0);
+    add(interval, ts, tsResult);
     return tsResult;
   }
 
+  public boolean add(HiveIntervalDayTime interval, Timestamp ts,
+      Timestamp result) {
+    if (ts == null || interval == null) {
+      return false;
+    }
+
+    nanosResult.addNanos(ts.getNanos(), interval.getNanos());
+
+    long newMillis = ts.getTime()
+        + TimeUnit.SECONDS.toMillis(interval.getTotalSeconds() + nanosResult.seconds);
+    result.setTime(newMillis);
+    result.setNanos(nanosResult.nanos);
+    return true;
+  }
+
   public HiveIntervalDayTime add(HiveIntervalDayTime left, HiveIntervalDayTime right) {
-    HiveIntervalDayTime result = null;
     if (left == null || right == null) {
       return null;
     }
 
+    HiveIntervalDayTime result = new HiveIntervalDayTime();
+    add(left, right, result);
+ 
+    return result;
+  }
+
+  public boolean add(HiveIntervalDayTime left, HiveIntervalDayTime right,
+      HiveIntervalDayTime result) {
+    if (left == null || right == null) {
+      return false;
+    }
+
     nanosResult.addNanos(left.getNanos(), right.getNanos());
 
     long totalSeconds = left.getTotalSeconds() + right.getTotalSeconds() + nanosResult.seconds;
-    result = new HiveIntervalDayTime(totalSeconds, nanosResult.nanos);
-    return result;
+    result.set(totalSeconds, nanosResult.nanos);
+    return true;
   }
 
   public Timestamp subtract(Timestamp left, HiveIntervalDayTime right) {
@@ -212,6 +332,13 @@ public Timestamp subtract(Timestamp left, HiveIntervalDayTime right) {
     return add(left, right.negate());
   }
 
+  public boolean subtract(Timestamp left, HiveIntervalDayTime right, Timestamp result) {
+    if (left == null || right == null) {
+      return false;
+    }
+    return add(left, right.negate(), result);
+  }
+
   public HiveIntervalDayTime subtract(HiveIntervalDayTime left, HiveIntervalDayTime right) {
     if (left == null || right == null) {
       return null;
@@ -219,17 +346,36 @@ public HiveIntervalDayTime subtract(HiveIntervalDayTime left, HiveIntervalDayTim
     return add(left, right.negate());
   }
 
+  public boolean subtract(HiveIntervalDayTime left, HiveIntervalDayTime right,
+      HiveIntervalDayTime result) {
+    if (left == null || right == null) {
+      return false;
+    }
+    return add(left, right.negate(), result);
+  }
+
   public HiveIntervalDayTime subtract(Timestamp left, Timestamp right) {
-    HiveIntervalDayTime result = null;
     if (left == null || right == null) {
       return null;
     }
 
+    HiveIntervalDayTime result = new HiveIntervalDayTime();
+    subtract(left, right, result);
+
+    return result;
+  }
+
+  public boolean subtract(Timestamp left, Timestamp right,
+      HiveIntervalDayTime result) {
+    if (left == null || right == null) {
+      return false;
+    }
+
     nanosResult.addNanos(left.getNanos(), -(right.getNanos()));
 
     long totalSeconds = TimeUnit.MILLISECONDS.toSeconds(left.getTime())
         - TimeUnit.MILLISECONDS.toSeconds(right.getTime()) + nanosResult.seconds;
-    result = new HiveIntervalDayTime(totalSeconds, nanosResult.nanos);
-    return result;
+    result.set(totalSeconds, nanosResult.nanos);
+    return true;
   }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestTimestampWritableAndColumnVector.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestTimestampWritableAndColumnVector.java
new file mode 100644
index 0000000000..6c462574fd
--- /dev/null
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestTimestampWritableAndColumnVector.java
@@ -0,0 +1,68 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector;
+
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.sql.Timestamp;
+import java.util.Date;
+import java.util.Random;
+
+import org.apache.hadoop.hive.common.type.RandomTypeUtil;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
+
+import static org.junit.Assert.*;
+
+/**
+ * Test for ListColumnVector
+ */
+public class TestTimestampWritableAndColumnVector {
+
+  private static int TEST_COUNT = 5000;
+
+  private static int fake = 0;
+
+  @Test
+  public void testDouble() throws Exception {
+
+    Random r = new Random(1234);
+    TimestampColumnVector timestampColVector = new TimestampColumnVector();
+    Timestamp[] randTimestamps = new Timestamp[VectorizedRowBatch.DEFAULT_SIZE];
+
+    for (int i = 0; i < VectorizedRowBatch.DEFAULT_SIZE; i++) {
+      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r);
+      randTimestamps[i] = randTimestamp;
+      timestampColVector.set(i, randTimestamp);
+    }
+    for (int i = 0; i < VectorizedRowBatch.DEFAULT_SIZE; i++) {
+      Timestamp retrievedTimestamp = timestampColVector.asScratchTimestamp(i);
+      Timestamp randTimestamp = randTimestamps[i];
+      if (!retrievedTimestamp.equals(randTimestamp)) {
+        assertTrue(false);
+      }
+      double randDouble = TimestampWritable.getDouble(randTimestamp);
+      double retrievedDouble = timestampColVector.getDouble(i);
+      if (randDouble != retrievedDouble) {
+        assertTrue(false);
+      }
+    }
+  }
+}
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java
index fc38dd3b17..02602f48bb 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java
@@ -229,18 +229,19 @@ private void testSetterLong(TypeInfo type) throws HiveException {
   }
 
   private void testWriterTimestamp(TypeInfo type) throws HiveException {
-    TimestampColumnVector tcv = VectorizedRowGroupGenUtil.generateTimestampColumnVector(true, false,
-        vectorSize, new Random(10));
+    Timestamp[] timestampValues = new Timestamp[vectorSize];
+    TimestampColumnVector tcv =
+        VectorizedRowGroupGenUtil.generateTimestampColumnVector(true, false,
+        vectorSize, new Random(10), timestampValues);
     tcv.isNull[3] = true;
     VectorExpressionWriter vew = getWriter(type);
     for (int i = 0; i < vectorSize; i++) {
       Writable w = (Writable) vew.writeValue(tcv, i);
       if (w != null) {
-        Writable expected = getWritableValue(type, tcv.asScratchTimestamp(i));
+        Writable expected = getWritableValue(type, timestampValues[i]);
         TimestampWritable t1 = (TimestampWritable) expected;
         TimestampWritable t2 = (TimestampWritable) w;
-        Assert.assertTrue(t1.getNanos() == t2.getNanos());
-        Assert.assertTrue(t1.getSeconds() == t2.getSeconds());
+        Assert.assertTrue(t1.equals(t2));
        } else {
         Assert.assertTrue(tcv.isNull[i]);
       }
@@ -248,8 +249,10 @@ private void testWriterTimestamp(TypeInfo type) throws HiveException {
   }
 
   private void testSetterTimestamp(TypeInfo type) throws HiveException {
-    TimestampColumnVector tcv = VectorizedRowGroupGenUtil.generateTimestampColumnVector(true, false,
-        vectorSize, new Random(10));
+    Timestamp[] timestampValues = new Timestamp[vectorSize];
+    TimestampColumnVector tcv =
+        VectorizedRowGroupGenUtil.generateTimestampColumnVector(true, false,
+        vectorSize, new Random(10), timestampValues);
     tcv.isNull[3] = true;
 
     Object[] values = new Object[this.vectorSize];
@@ -259,12 +262,10 @@ private void testSetterTimestamp(TypeInfo type) throws HiveException {
       values[i] = null;  // setValue() should be able to handle null input
       values[i] = vew.setValue(values[i], tcv, i);
       if (values[i] != null) {
-        Timestamp scratchTimestamp = tcv.asScratchTimestamp(i);
-        Writable expected = getWritableValue(type, scratchTimestamp);
+        Writable expected = getWritableValue(type, timestampValues[i]);
         TimestampWritable t1 = (TimestampWritable) expected;
         TimestampWritable t2 = (TimestampWritable) values[i];
-        Assert.assertTrue(t1.getNanos() == t2.getNanos());
-        Assert.assertTrue(t1.getSeconds() == t2.getSeconds());
+        Assert.assertTrue(t1.equals(t2));
       } else {
         Assert.assertTrue(tcv.isNull[i]);
       }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterExpressions.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterExpressions.java
index 819cc27a02..80f55dc7db 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterExpressions.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorFilterExpressions.java
@@ -25,7 +25,6 @@
 import java.sql.Timestamp;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorMathFunctions.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorMathFunctions.java
index c14eb4af54..31add6e2e5 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorMathFunctions.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorMathFunctions.java
@@ -19,11 +19,13 @@
 package org.apache.hadoop.hive.ql.exec.vector.expressions;
 
 import java.io.UnsupportedEncodingException;
+import java.sql.Timestamp;
 import java.util.Arrays;
+import java.util.Random;
 
 import junit.framework.Assert;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.common.type.RandomTypeUtil;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;
@@ -52,6 +54,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncSinDoubleToDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncSqrtDoubleToDouble;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FuncTanDoubleToDouble;
+import org.apache.hadoop.hive.serde2.io.TimestampWritable;
 import org.junit.Test;
 
 
@@ -194,22 +197,22 @@ public static VectorizedRowBatch getVectorizedRowBatchLongInDoubleOut() {
     return batch;
   }
 
-  public static VectorizedRowBatch getVectorizedRowBatchTimestampInDoubleOut() {
+  public static VectorizedRowBatch getVectorizedRowBatchTimestampInDoubleOut(double[] doubleValues) {
+    Random r = new Random(45993);
     VectorizedRowBatch batch = new VectorizedRowBatch(2);
     TimestampColumnVector tcv;
     DoubleColumnVector dcv;
-    tcv = new TimestampColumnVector();
-    dcv = new DoubleColumnVector();
-    tcv.set(0, new PisaTimestamp(0, -2));
-    tcv.set(1, new PisaTimestamp(0, -1));
-    tcv.set(2, new PisaTimestamp(0, 0));
-    tcv.set(3, new PisaTimestamp(0, 1));
-    tcv.set(4, new PisaTimestamp(0, 2));
+    tcv = new TimestampColumnVector(doubleValues.length);
+    dcv = new DoubleColumnVector(doubleValues.length);
+    for (int i = 0; i < doubleValues.length; i++) {
+      doubleValues[i] = r.nextDouble() % (double) SECONDS_LIMIT;
+      dcv.vector[i] = doubleValues[i];
+    }
 
     batch.cols[0] = tcv;
     batch.cols[1] = dcv;
 
-    batch.size = 5;
+    batch.size = doubleValues.length;
     return batch;
   }
 
@@ -228,35 +231,45 @@ public static VectorizedRowBatch getVectorizedRowBatchLongInLongOut() {
     return batch;
   }
 
-  public static VectorizedRowBatch getVectorizedRowBatchTimestampInLongOut() {
+  public static VectorizedRowBatch getVectorizedRowBatchTimestampInLongOut(long[] longValues) {
+    Random r = new Random(345);
     VectorizedRowBatch batch = new VectorizedRowBatch(2);
     TimestampColumnVector inV;
     LongColumnVector outV;
-    inV = new TimestampColumnVector();
-    outV = new LongColumnVector();
-    inV.setTimestampSeconds(0, 2);
-    inV.setTimestampSeconds(1, 2);
+    inV = new TimestampColumnVector(longValues.length);
+    outV = new LongColumnVector(longValues.length);
+    for (int i = 0; i < longValues.length; i++) {
+      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r);
+      longValues[i] = TimestampWritable.getLong(randTimestamp);
+      inV.set(0, randTimestamp);
+    }
 
     batch.cols[0] = inV;
     batch.cols[1] = outV;
 
-    batch.size = 2;
+    batch.size = longValues.length;
     return batch;
   }
 
-  public static VectorizedRowBatch getVectorizedRowBatchLongInTimestampOut() {
+  static long SECONDS_LIMIT = 60L * 24L * 365L * 9999L;
+
+  public static VectorizedRowBatch getVectorizedRowBatchLongInTimestampOut(long[] longValues) {
+    Random r = new Random(12099);
     VectorizedRowBatch batch = new VectorizedRowBatch(2);
     LongColumnVector inV;
     TimestampColumnVector outV;
     inV = new LongColumnVector();
     outV = new TimestampColumnVector();
-    inV.vector[0] = -2;
-    inV.vector[1] = 2;
+
+    for (int i = 0; i < longValues.length; i++) {
+      longValues[i] = r.nextLong() % SECONDS_LIMIT;
+      inV.vector[i] = longValues[i];
+    }
 
     batch.cols[0] = inV;
     batch.cols[1] = outV;
 
-    batch.size = 2;
+    batch.size = longValues.length;
     return batch;
   }
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExpressions.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExpressions.java
index 375f369893..d4f1f6f160 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExpressions.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTimestampExpressions.java
@@ -32,7 +32,6 @@
 import junit.framework.Assert;
 
 import org.apache.commons.lang.ArrayUtils;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.common.type.RandomTypeUtil;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
@@ -84,7 +83,7 @@ private Timestamp[] getAllBoundaries(int minYear, int maxYear) {
   }
 
   private Timestamp[] getAllBoundaries() {
-    return getAllBoundaries(0000, 9999);
+    return getAllBoundaries(RandomTypeUtil.MIN_YEAR, RandomTypeUtil.MAX_YEAR);
   }
 
   private VectorizedRowBatch getVectorizedRandomRowBatchTimestampLong(int seed, int size) {
@@ -742,27 +741,14 @@ public void testVectorUDFSecondString() {
     testVectorUDFSecond(TestType.STRING_LONG);
   }
 
-  private LongWritable getLongWritable(TimestampWritable i) {
-    LongWritable result = new LongWritable();
-    if (i == null) {
-      return null;
-    } else {
-      result.set(i.getSeconds());
-      return result;
+  private void compareToUDFUnixTimeStampLong(Timestamp ts, long y) {
+    long seconds = ts.getTime() / 1000;
+    if(seconds != y) {
+      System.out.printf("%d vs %d for %s\n", seconds, y, ts.toString());
+      Assert.assertTrue(false);
     }
   }
 
-  private void compareToUDFUnixTimeStampLong(Timestamp t, long y) {
-    TimestampWritable tsw = new TimestampWritable(t);
-    LongWritable res = getLongWritable(tsw);
-    if(res.get() != y) {
-      System.out.printf("%d vs %d for %s, %d\n", res.get(), y, t.toString(),
-          tsw.getTimestamp().getTime()/1000);
-    }
-
-    Assert.assertEquals(res.get(), y);
-  }
-
   private void verifyUDFUnixTimeStamp(VectorizedRowBatch batch, TestType testType) {
     VectorExpression udf;
     if (testType == TestType.TIMESTAMP_LONG) {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java
index 038e382e48..1e41fce9ac 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorTypeCasts.java
@@ -23,13 +23,19 @@
 import static org.junit.Assert.assertTrue;
 
 import java.io.UnsupportedEncodingException;
+import java.math.BigDecimal;
+import java.math.MathContext;
+import java.math.RoundingMode;
+import java.sql.Timestamp;
 import java.util.Arrays;
+import java.util.Random;
+import java.util.concurrent.TimeUnit;
 
 import junit.framework.Assert;
 
 import org.apache.hadoop.hive.common.type.Decimal128;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
+import org.apache.hadoop.hive.common.type.RandomTypeUtil;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
@@ -39,6 +45,7 @@
 import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.*;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.*;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;
 import org.junit.Test;
 
 /**
@@ -84,8 +91,8 @@ public void testCastDoubleToTimestamp() {
     b.cols[0].noNulls = true;
     VectorExpression expr = new CastDoubleToTimestamp(0, 1);
     expr.evaluate(b);
-    Assert.assertEquals(0.0, resultV.getTimestampSecondsWithFractionalNanos(3));
-    Assert.assertEquals(0.5d, resultV.getTimestampSecondsWithFractionalNanos(4));
+    Assert.assertEquals(0.0, TimestampWritable.getDouble(resultV.asScratchTimestamp(3)));
+    Assert.assertEquals(0.5d, TimestampWritable.getDouble(resultV.asScratchTimestamp(4)));
   }
 
   @Test
@@ -103,39 +110,51 @@ public void testCastLongToBoolean() {
 
   @Test
   public void testCastLongToTimestamp() {
-    VectorizedRowBatch b = TestVectorMathFunctions.getVectorizedRowBatchLongInTimestampOut();
+    long[] longValues = new long[500];
+    VectorizedRowBatch b = TestVectorMathFunctions.getVectorizedRowBatchLongInTimestampOut(longValues);
     TimestampColumnVector resultV = (TimestampColumnVector) b.cols[1];
     b.cols[0].noNulls = true;
     VectorExpression expr = new CastLongToTimestamp(0, 1);
     expr.evaluate(b);
-    Assert.assertEquals(-2, resultV.getTimestampSeconds(0));
-    Assert.assertEquals(2, resultV.getTimestampSeconds(1));
+    for (int i = 0; i < longValues.length; i++) {
+      Timestamp timestamp = resultV.asScratchTimestamp(i);
+      long actual = TimestampWritable.getLong(timestamp);
+      assertEquals(actual, longValues[i]);
+    }
   }
 
   @Test
   public void testCastTimestampToLong() {
-    VectorizedRowBatch b = TestVectorMathFunctions.getVectorizedRowBatchTimestampInLongOut();
+    long[] longValues = new long[500];
+    VectorizedRowBatch b = TestVectorMathFunctions.getVectorizedRowBatchTimestampInLongOut(longValues);
     TimestampColumnVector inV = (TimestampColumnVector) b.cols[0];
-    inV.set(0, new PisaTimestamp(0, PisaTimestamp.NANOSECONDS_PER_SECOND));  // Make one entry produce interesting result
-      // (1 sec after epoch).
-
     LongColumnVector resultV = (LongColumnVector) b.cols[1];
     b.cols[0].noNulls = true;
     VectorExpression expr = new CastTimestampToLong(0, 1);
     expr.evaluate(b);
-    Assert.assertEquals(1, resultV.vector[0]);
+    for (int i = 0; i < longValues.length; i++) {
+      long actual = resultV.vector[i];
+      long timestampLong = inV.getTimestampAsLong(i);
+      if (actual != timestampLong) {
+        assertTrue(false);
+      }
+    }
   }
 
   @Test
   public void testCastTimestampToDouble() {
-    VectorizedRowBatch b = TestVectorMathFunctions.getVectorizedRowBatchTimestampInDoubleOut();
+    double[] doubleValues = new double[500];
+    VectorizedRowBatch b = TestVectorMathFunctions.getVectorizedRowBatchTimestampInDoubleOut(doubleValues);
     TimestampColumnVector inV = (TimestampColumnVector) b.cols[0];
     DoubleColumnVector resultV = (DoubleColumnVector) b.cols[1];
     b.cols[0].noNulls = true;
     VectorExpression expr = new CastTimestampToDouble(0, 1);
     expr.evaluate(b);
-    Assert.assertEquals(-1E-9D , resultV.vector[1]);
-    Assert.assertEquals(1E-9D, resultV.vector[3]);
+    for (int i = 0; i < doubleValues.length; i++) {
+      double actual = resultV.vector[i];
+      double doubleValue = TimestampWritable.getDouble(inV.asScratchTimestamp(i));
+      assertEquals(actual, doubleValue, 0.000000001F);
+    }
   }
 
   public byte[] toBytes(String s) {
@@ -356,16 +375,19 @@ private VectorizedRowBatch getBatchDecimalString() {
 
   @Test
   public void testCastDecimalToTimestamp() {
-    VectorizedRowBatch b = getBatchDecimalTimestamp();
+    double[] doubleValues = new double[500];
+    VectorizedRowBatch b = getBatchDecimalTimestamp(doubleValues);
     VectorExpression expr = new CastDecimalToTimestamp(0, 1);
     expr.evaluate(b);
     TimestampColumnVector r = (TimestampColumnVector) b.cols[1];
-    assertEquals(1111111111L, r.getNanoOfDay(0));
-    assertEquals(0L, r.getEpochDay(0));
-    assertEquals(-2222222222L, r.getNanoOfDay(1));
-    assertEquals(0L, r.getEpochDay(1));
-    assertEquals(999999999L, r.getNanoOfDay(2));
-    assertEquals(365L, r.getEpochDay(2));
+    for (int i = 0; i < doubleValues.length; i++) {
+      Timestamp timestamp = r.asScratchTimestamp(i);
+      double asDouble = TimestampWritable.getDouble(timestamp);
+      double expectedDouble = doubleValues[i];
+      if (expectedDouble != asDouble) {
+        assertTrue(false);
+      }
+    }
   }
 
   private VectorizedRowBatch getBatchDecimalLong2() {
@@ -384,19 +406,25 @@ private VectorizedRowBatch getBatchDecimalLong2() {
     return b;
   }
 
-  private VectorizedRowBatch getBatchDecimalTimestamp() {
+  private VectorizedRowBatch getBatchDecimalTimestamp(double[] doubleValues) {
     VectorizedRowBatch b = new VectorizedRowBatch(2);
     DecimalColumnVector dv;
-    short scale = 9;
-    b.cols[0] = dv = new DecimalColumnVector(18, scale);
-    b.cols[1] = new TimestampColumnVector();
-
-    b.size = 3;
-
-    dv.vector[0].set(HiveDecimal.create("1.111111111").setScale(scale));
-    dv.vector[1].set(HiveDecimal.create("-2.222222222").setScale(scale));
-    dv.vector[2].set(HiveDecimal.create("31536000.999999999").setScale(scale));
-
+    b.cols[0] = dv = new DecimalColumnVector(doubleValues.length, HiveDecimal.SYSTEM_DEFAULT_PRECISION, HiveDecimal.SYSTEM_DEFAULT_SCALE);
+    b.cols[1] = new TimestampColumnVector(doubleValues.length);
+    dv.noNulls = true;
+    Random r = new Random(94830);
+    for (int i = 0; i < doubleValues.length; i++) {
+      long millis = RandomTypeUtil.randomMillis(r);
+      Timestamp ts = new Timestamp(millis);
+      int nanos = RandomTypeUtil.randomNanos(r);
+      ts.setNanos(nanos);
+      TimestampWritable tsw = new TimestampWritable(ts);
+      double asDouble = tsw.getDouble();
+      doubleValues[i] = asDouble;
+      HiveDecimal hiveDecimal = HiveDecimal.create(new BigDecimal(asDouble));
+      dv.set(i, hiveDecimal);
+    }
+    b.size = doubleValues.length;
     return b;
   }
 
@@ -422,14 +450,44 @@ private VectorizedRowBatch getBatchLongDecimal() {
     return b;
   }
 
-  private VectorizedRowBatch getBatchTimestampDecimal() {
+
+  public static final long NANOSECONDS_PER_SECOND = TimeUnit.SECONDS.toNanos(1);
+  public static final long MILLISECONDS_PER_SECOND = TimeUnit.SECONDS.toMillis(1);
+  public static final long NANOSECONDS_PER_MILLISSECOND = TimeUnit.MILLISECONDS.toNanos(1);
+
+  private VectorizedRowBatch getBatchTimestampDecimal(HiveDecimal[] hiveDecimalValues) {
+    Random r = new Random(994);
     VectorizedRowBatch b = new VectorizedRowBatch(2);
     TimestampColumnVector tcv;
-    b.cols[0] = tcv = new TimestampColumnVector();
-    b.cols[1] = new DecimalColumnVector(18, 2);
-    tcv.set(0, new PisaTimestamp( 0, 0));
-    tcv.set(1, new PisaTimestamp( 0, -1));
-    tcv.set(2, new PisaTimestamp( 99999999999999L / PisaTimestamp.NANOSECONDS_PER_DAY, 99999999999999L % PisaTimestamp.NANOSECONDS_PER_DAY));
+    b.cols[0] = tcv = new TimestampColumnVector(hiveDecimalValues.length);
+    b.cols[1] = new DecimalColumnVector(hiveDecimalValues.length, HiveDecimal.SYSTEM_DEFAULT_PRECISION, HiveDecimal.SYSTEM_DEFAULT_SCALE);
+    for (int i = 0; i < hiveDecimalValues.length; i++) {
+      int optionalNanos = 0;
+      switch (r.nextInt(4)) {
+      case 0:
+        // No nanos.
+        break;
+      case 1:
+        optionalNanos = r.nextInt((int) NANOSECONDS_PER_SECOND);
+        break;
+      case 2:
+        // Limit to milliseconds only...
+        optionalNanos = r.nextInt((int) MILLISECONDS_PER_SECOND) * (int) NANOSECONDS_PER_MILLISSECOND;
+        break;
+      case 3:
+        // Limit to below milliseconds only...
+        optionalNanos = r.nextInt((int) NANOSECONDS_PER_MILLISSECOND);
+        break;
+      }
+      long millis = RandomTypeUtil.randomMillis(r);
+      Timestamp ts = new Timestamp(millis);
+      ts.setNanos(optionalNanos);
+      TimestampWritable tsw = new TimestampWritable(ts);
+      hiveDecimalValues[i] = tsw.getHiveDecimal();
+
+      tcv.set(i, ts);
+    }
+    b.size = hiveDecimalValues.length;
     return b;
   }
 
@@ -440,9 +498,18 @@ public void testCastDoubleToDecimal() {
     expr.evaluate(b);
     DecimalColumnVector r = (DecimalColumnVector) b.cols[1];
 
-    assertTrue(r.vector[0].getHiveDecimal().equals(HiveDecimal.create("0.0")));
-    assertTrue(r.vector[1].getHiveDecimal().equals(HiveDecimal.create("-1.0")));
-    assertTrue(r.vector[2].getHiveDecimal().equals(HiveDecimal.create("99999999999999")));
+    HiveDecimal hd0 = HiveDecimal.create("0.0");
+    if (!hd0.equals(r.vector[0].getHiveDecimal())) {
+      assertTrue(false);
+    }
+    HiveDecimal hd1 = HiveDecimal.create("-1.0");
+    if (!hd1.equals(r.vector[1].getHiveDecimal())) {
+      assertTrue(false);
+    }
+    HiveDecimal hd2 = HiveDecimal.create("99999999999999");
+    if (!hd2.equals(r.vector[2].getHiveDecimal())) {
+      assertTrue(false);
+    }
   }
 
   private VectorizedRowBatch getBatchDoubleDecimal() {
@@ -496,25 +563,37 @@ public void testCastTimestampToDecimal() {
 
     // The input timestamps are stored as long values
     // measured in nanoseconds from the epoch.
-    VectorizedRowBatch b = getBatchTimestampDecimal();
+    HiveDecimal[] hiveDecimalValues = new HiveDecimal[500];
+    VectorizedRowBatch b = getBatchTimestampDecimal(hiveDecimalValues);
     VectorExpression expr = new CastTimestampToDecimal(0, 1);
     TimestampColumnVector inT = (TimestampColumnVector) b.cols[0];
-    inT.set(1, new PisaTimestamp(0, -1990000000L));
     expr.evaluate(b);
     DecimalColumnVector r = (DecimalColumnVector) b.cols[1];
-    assertTrue(r.vector[0].getHiveDecimal().equals(HiveDecimal.create("0.00")));
-    assertTrue(r.vector[1].getHiveDecimal().equals(HiveDecimal.create("-1.99")));
-    assertTrue(r.vector[2].getHiveDecimal().equals(HiveDecimal.create("100000.00")));
+    for (int i = 0; i < hiveDecimalValues.length; i++) {
+      HiveDecimal hiveDecimal = r.vector[i].getHiveDecimal();
+      HiveDecimal expectedHiveDecimal = hiveDecimalValues[i];
+      if (!hiveDecimal.equals(expectedHiveDecimal)) {
+        assertTrue(false);
+      }
+    }
 
     // Try again with a value that won't fit in 5 digits, to make
     // sure that NULL is produced.
-    b = getBatchTimestampDecimalPrec5Scale2();
+    b.cols[1] = r = new DecimalColumnVector(hiveDecimalValues.length, 5, 2);
     expr.evaluate(b);
     r = (DecimalColumnVector) b.cols[1];
-    assertFalse(r.noNulls);
-    assertFalse(r.isNull[0]);
-    assertFalse(r.isNull[1]);
-    assertTrue(r.isNull[2]);
+    for (int i = 0; i < hiveDecimalValues.length; i++) {
+      HiveDecimal hiveDecimal = r.vector[i].getHiveDecimal();
+      HiveDecimal expectedHiveDecimal = hiveDecimalValues[i];
+      if (HiveDecimal.enforcePrecisionScale(expectedHiveDecimal, 5, 2) == null) {
+        assertTrue(r.isNull[i]);
+      } else {
+        assertTrue(!r.isNull[i]);
+        if (!hiveDecimal.equals(expectedHiveDecimal)) {
+          assertTrue(false);
+        }
+      }
+    }
   }
 
   /* This batch has output decimal column precision 5 and scale 2.
@@ -533,41 +612,6 @@ private VectorizedRowBatch getBatchLongDecimalPrec5Scale2() {
     return b;
   }
 
-  private VectorizedRowBatch getBatchTimestampDecimalPrec5Scale2() {
-    VectorizedRowBatch b = new VectorizedRowBatch(2);
-    TimestampColumnVector tcv;
-    b.cols[0] = tcv = new TimestampColumnVector();
-    b.cols[1] = new DecimalColumnVector(5, 2);
-    tcv.set(0, new PisaTimestamp(0, 0));
-    tcv.set(1, new PisaTimestamp(0, -1));
-    tcv.set(2, new PisaTimestamp(99999999999999L / PisaTimestamp.NANOSECONDS_PER_DAY, 99999999999999L % PisaTimestamp.NANOSECONDS_PER_DAY));
-    return b;
-  }
-
-  /*
-  @Test
-  public void testCastDecimalToDecimal() {
-
-    // test casting from one precision and scale to another.
-    VectorizedRowBatch b = getBatchDecimalDecimal();
-    VectorExpression expr = new CastDecimalToDecimal(0, 1);
-    expr.evaluate(b);
-    DecimalColumnVector r = (DecimalColumnVector) b.cols[1];
-    assertTrue(r.vector[0].getHiveDecimal().equals(HiveDecimal.create("10.00", (short) 2)));
-    assertFalse(r.noNulls);
-    assertTrue(r.isNull[1]);
-
-    // test an increase in precision/scale
-    b = getBatchDecimalDecimal();
-    expr = new CastDecimalToDecimal(1, 0);
-    expr.evaluate(b);
-    r = (DecimalColumnVector) b.cols[0];
-    assertTrue(r.vector[0].getHiveDecimal().equals(HiveDecimal.create("100.01", (short) 4)));
-    assertTrue(r.vector[1].getHiveDecimal().equals(HiveDecimal.create("-200.02", (short) 4)));
-    assertTrue(r.noNulls);
-  }
-  */
-
   private VectorizedRowBatch getBatchDecimalDecimal() {
     VectorizedRowBatch b = new VectorizedRowBatch(2);
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/FakeVectorRowBatchFromObjectIterables.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/FakeVectorRowBatchFromObjectIterables.java
index ab860821db..98849c309b 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/FakeVectorRowBatchFromObjectIterables.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/FakeVectorRowBatchFromObjectIterables.java
@@ -27,7 +27,6 @@
 import java.util.regex.Pattern;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
@@ -111,7 +110,7 @@ public void assign(
               Object value) {
             TimestampColumnVector lcv = (TimestampColumnVector) columnVector;
             Timestamp t = (Timestamp) value;
-            lcv.set(row, new PisaTimestamp(t));
+            lcv.set(row, t);
           }
         };
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/VectorizedRowGroupGenUtil.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/VectorizedRowGroupGenUtil.java
index 649e52b996..84717b19d6 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/VectorizedRowGroupGenUtil.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/VectorizedRowGroupGenUtil.java
@@ -22,7 +22,6 @@
 import java.util.Random;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.common.type.RandomTypeUtil;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
@@ -81,7 +80,7 @@ public static LongColumnVector generateLongColumnVector(
   }
 
   public static TimestampColumnVector generateTimestampColumnVector(
-      boolean nulls, boolean repeating, int size, Random rand) {
+      boolean nulls, boolean repeating, int size, Random rand, Timestamp[] timestampValues) {
     TimestampColumnVector tcv = new TimestampColumnVector(size);
 
     tcv.noNulls = !nulls;
@@ -95,10 +94,17 @@ public static TimestampColumnVector generateTimestampColumnVector(
       if(nulls && (repeating || i % nullFrequency == 0)) {
         tcv.isNull[i] = true;
         tcv.setNullValue(i);
-
+        timestampValues[i] = null;
       }else {
         tcv.isNull[i] = false;
-        tcv.set(i, repeating ? repeatingTimestamp : RandomTypeUtil.getRandTimestamp(rand));
+        if (!repeating) {
+          Timestamp randomTimestamp = RandomTypeUtil.getRandTimestamp(rand);
+          tcv.set(i,  randomTimestamp);
+          timestampValues[i] = randomTimestamp;
+        } else {
+          tcv.set(i, repeatingTimestamp);
+          timestampValues[i] = repeatingTimestamp;
+        }
       }
     }
     return tcv;
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
index c88f6d841f..85923a81db 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java
@@ -1946,7 +1946,7 @@ public void testVectorizationWithAcid() throws Exception {
       long millis = (long) i * MILLIS_IN_DAY;
       millis -= LOCAL_TIMEZONE.getOffset(millis);
       assertEquals("checking timestamp " + i, millis,
-          timestampColumn.getTimestampMilliseconds(i));
+          timestampColumn.getTime(i));
     }
     assertEquals(false, reader.next(key, value));
   }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
index 3843c6d4e8..1a97a6dfff 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestOrcFile.java
@@ -541,7 +541,10 @@ public void testTimestamp() throws Exception {
     int idx = 0;
     while (rows.hasNext()) {
       Object row = rows.next(null);
-      assertEquals(tslist.get(idx++).getNanos(), ((TimestampWritable) row).getNanos());
+      Timestamp tlistTimestamp = tslist.get(idx++);
+      if (tlistTimestamp.getNanos() != ((TimestampWritable) row).getNanos()) {
+        assertTrue(false);
+      }
     }
     assertEquals(0, writer.getSchema().getMaximumId());
     boolean[] expected = new boolean[] {false};
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorOrcFile.java b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorOrcFile.java
index 4ca20c53ea..a82d67294a 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorOrcFile.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestVectorOrcFile.java
@@ -24,7 +24,6 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;
 import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;
@@ -527,7 +526,7 @@ public void testTimestamp() throws Exception {
     batch.size = tslist.size();
     for (int i=0; i < tslist.size(); ++i) {
       Timestamp ts = tslist.get(i);
-      vec.set(i, new PisaTimestamp(ts));
+      vec.set(i, ts);
     }
     writer.addRowBatch(batch);
     writer.close();
@@ -1345,8 +1344,8 @@ public void createOrcDateFile(Path file, int minYear, int maxYear
       for (int ms = 1000; ms < 2000; ++ms) {
         TimestampColumnVector timestampColVector = (TimestampColumnVector) batch.cols[0];
         timestampColVector.set(ms - 1000,
-            new PisaTimestamp(Timestamp.valueOf(year +
-                "-05-05 12:34:56." + ms)));
+            Timestamp.valueOf(year +
+                "-05-05 12:34:56." + ms));
         ((LongColumnVector) batch.cols[1]).vector[ms - 1000] =
             new DateWritable(new Date(year - 1900, 11, 25)).getDays();
       }
@@ -1385,7 +1384,7 @@ private static void setUnion(VectorizedRowBatch batch, int rowId,
     UnionColumnVector union = (UnionColumnVector) batch.cols[1];
     if (ts != null) {
       TimestampColumnVector timestampColVector = (TimestampColumnVector) batch.cols[0];
-      timestampColVector.set(rowId, new PisaTimestamp(ts));
+      timestampColVector.set(rowId, ts);
     } else {
       batch.cols[0].isNull[rowId] = true;
       batch.cols[0].noNulls = false;
@@ -2178,8 +2177,8 @@ public void testRepeating() throws Exception {
     ((LongColumnVector) batch.cols[6]).vector[0] =
         new DateWritable(new Date(111, 6, 1)).getDays();
     ((TimestampColumnVector) batch.cols[7]).set(0,
-        new PisaTimestamp(new Timestamp(115, 9, 23, 10, 11, 59,
-            999999999)));
+        new Timestamp(115, 9, 23, 10, 11, 59,
+            999999999));
     ((DecimalColumnVector) batch.cols[8]).vector[0] =
         new HiveDecimalWritable("1.234567");
     ((BytesColumnVector) batch.cols[9]).setVal(0, "Echelon".getBytes());
@@ -2234,9 +2233,8 @@ public void testRepeating() throws Exception {
           new DateWritable(new Date(111, 6, 1)).getDays() + r;
 
       Timestamp ts = new Timestamp(115, 9, 23, 10, 11, 59, 999999999);
-      PisaTimestamp pisaTimestamp = new PisaTimestamp(ts);
-      pisaTimestamp.addSeconds(pisaTimestamp, r, pisaTimestamp);
-      ((TimestampColumnVector) batch.cols[7]).set(r, pisaTimestamp);
+      ts.setTime(ts.getTime() + r * 1000);
+      ((TimestampColumnVector) batch.cols[7]).set(r, ts);
       ((DecimalColumnVector) batch.cols[8]).vector[r] =
           new HiveDecimalWritable("1.234567");
       ((BytesColumnVector) batch.cols[9]).setVal(r,
@@ -2378,8 +2376,10 @@ public void testRepeating() throws Exception {
           row.getFieldValue(5).toString());
       assertEquals("row " + r, new Date(111, 6, 1 + r).toString(),
           row.getFieldValue(6).toString());
+      Timestamp ts = new Timestamp(115, 9, 23, 10, 11, 59, 999999999);
+      ts.setTime(ts.getTime() + r * 1000);
       assertEquals("row " + r,
-          new Timestamp(115, 9, 23, 10, 11, 59 + r, 999999999).toString(),
+          ts.toString(),
           row.getFieldValue(7).toString());
       assertEquals("row " + r, "1.234567", row.getFieldValue(8).toString());
       assertEquals("row " + r, Integer.toString(r),
diff --git a/ql/src/test/queries/clientpositive/vector_interval_arithmetic.q b/ql/src/test/queries/clientpositive/vector_interval_arithmetic.q
new file mode 100644
index 0000000000..40c4c03e72
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/vector_interval_arithmetic.q
@@ -0,0 +1,174 @@
+set hive.cli.print.header=true;
+set hive.explain.user=false;
+set hive.fetch.task.conversion=none;
+
+create table unique_timestamps (tsval timestamp) STORED AS TEXTFILE;
+
+LOAD DATA LOCAL INPATH '../../data/files/timestamps.txt' OVERWRITE INTO TABLE unique_timestamps;
+
+create table interval_arithmetic_1 (dateval date, tsval timestamp) stored as orc;
+insert overwrite table interval_arithmetic_1
+  select cast(tsval as date), tsval from unique_timestamps;
+
+SET hive.vectorized.execution.enabled=true;
+
+-- interval year-month arithmetic
+explain
+select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval;
+
+select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval;
+
+explain
+select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval;
+
+select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval;
+
+explain
+select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval;
+
+select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval;
+
+explain
+select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2;
+
+select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2;
+
+
+-- interval day-time arithmetic
+explain
+select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval;
+
+select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval;
+
+explain
+select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval;
+
+select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval;
+
+explain
+select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval;
+
+select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval;
+
+explain
+select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2;
+
+select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2;
+
+drop table interval_arithmetic_1;
diff --git a/ql/src/test/results/clientpositive/tez/vector_interval_arithmetic.q.out b/ql/src/test/results/clientpositive/tez/vector_interval_arithmetic.q.out
new file mode 100644
index 0000000000..8409a015f2
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/vector_interval_arithmetic.q.out
@@ -0,0 +1,1086 @@
+PREHOOK: query: create table unique_timestamps (tsval timestamp) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@unique_timestamps
+POSTHOOK: query: create table unique_timestamps (tsval timestamp) STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@unique_timestamps
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/timestamps.txt' OVERWRITE INTO TABLE unique_timestamps
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@unique_timestamps
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/timestamps.txt' OVERWRITE INTO TABLE unique_timestamps
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@unique_timestamps
+PREHOOK: query: create table interval_arithmetic_1 (dateval date, tsval timestamp) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: query: create table interval_arithmetic_1 (dateval date, tsval timestamp) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@interval_arithmetic_1
+PREHOOK: query: insert overwrite table interval_arithmetic_1
+  select cast(tsval as date), tsval from unique_timestamps
+PREHOOK: type: QUERY
+PREHOOK: Input: default@unique_timestamps
+PREHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: query: insert overwrite table interval_arithmetic_1
+  select cast(tsval as date), tsval from unique_timestamps
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@unique_timestamps
+POSTHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: Lineage: interval_arithmetic_1.dateval EXPRESSION [(unique_timestamps)unique_timestamps.FieldSchema(name:tsval, type:timestamp, comment:null), ]
+POSTHOOK: Lineage: interval_arithmetic_1.tsval SIMPLE [(unique_timestamps)unique_timestamps.FieldSchema(name:tsval, type:timestamp, comment:null), ]
+_c0	tsval
+PREHOOK: query: -- interval year-month arithmetic
+explain
+select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: -- interval year-month arithmetic
+explain
+select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: dateval (type: date), (dateval - 2-2) (type: date), (dateval - -2-2) (type: date), (dateval + 2-2) (type: date), (dateval + -2-2) (type: date), (-2-2 + dateval) (type: date), (2-2 + dateval) (type: date)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                    Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: date)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: date), _col2 (type: date), _col3 (type: date), _col4 (type: date), _col5 (type: date), _col6 (type: date)
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: date), VALUE._col1 (type: date), VALUE._col2 (type: date), VALUE._col3 (type: date), VALUE._col4 (type: date), VALUE._col5 (type: date)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	c1	c2	c3	c4	c5	c6
+0004-09-22	0002-07-22	0006-11-22	0006-11-22	0002-07-22	0002-07-22	0006-11-22
+0528-10-27	0526-08-27	0530-12-27	0530-12-27	0526-08-27	0526-08-27	0530-12-27
+1319-02-02	1316-12-02	1321-04-02	1321-04-02	1316-12-02	1316-12-02	1321-04-02
+1404-07-23	1402-05-23	1406-09-23	1406-09-23	1402-05-23	1402-05-23	1406-09-23
+1815-05-06	1813-03-06	1817-07-06	1817-07-06	1813-03-06	1813-03-06	1817-07-06
+1883-04-17	1881-02-17	1885-06-17	1885-06-17	1881-02-17	1881-02-17	1885-06-17
+1966-08-16	1964-06-16	1968-10-16	1968-10-16	1964-06-16	1964-06-16	1968-10-16
+1973-04-17	1971-02-17	1975-06-17	1975-06-17	1971-02-17	1971-02-17	1975-06-17
+1974-10-04	1972-08-04	1976-12-04	1976-12-04	1972-08-04	1972-08-04	1976-12-04
+1976-03-03	1974-01-03	1978-05-03	1978-05-03	1974-01-03	1974-01-03	1978-05-03
+1976-05-06	1974-03-06	1978-07-06	1978-07-06	1974-03-06	1974-03-06	1978-07-06
+1978-08-05	1976-06-05	1980-10-05	1980-10-05	1976-06-05	1976-06-05	1980-10-05
+1981-04-25	1979-02-25	1983-06-25	1983-06-25	1979-02-25	1979-02-25	1983-06-25
+1981-11-15	1979-09-15	1984-01-15	1984-01-15	1979-09-15	1979-09-15	1984-01-15
+1985-07-20	1983-05-20	1987-09-20	1987-09-20	1983-05-20	1983-05-20	1987-09-20
+1985-11-18	1983-09-18	1988-01-18	1988-01-18	1983-09-18	1983-09-18	1988-01-18
+1987-02-21	1984-12-21	1989-04-21	1989-04-21	1984-12-21	1984-12-21	1989-04-21
+1987-05-28	1985-03-28	1989-07-28	1989-07-28	1985-03-28	1985-03-28	1989-07-28
+1998-10-16	1996-08-16	2000-12-16	2000-12-16	1996-08-16	1996-08-16	2000-12-16
+1999-10-03	1997-08-03	2001-12-03	2001-12-03	1997-08-03	1997-08-03	2001-12-03
+2000-12-18	1998-10-18	2003-02-18	2003-02-18	1998-10-18	1998-10-18	2003-02-18
+2002-05-10	2000-03-10	2004-07-10	2004-07-10	2000-03-10	2000-03-10	2004-07-10
+2003-09-23	2001-07-23	2005-11-23	2005-11-23	2001-07-23	2001-07-23	2005-11-23
+2004-03-07	2002-01-07	2006-05-07	2006-05-07	2002-01-07	2002-01-07	2006-05-07
+2007-02-09	2004-12-09	2009-04-09	2009-04-09	2004-12-09	2004-12-09	2009-04-09
+2009-01-21	2006-11-21	2011-03-21	2011-03-21	2006-11-21	2006-11-21	2011-03-21
+2010-04-08	2008-02-08	2012-06-08	2012-06-08	2008-02-08	2008-02-08	2012-06-08
+2013-04-07	2011-02-07	2015-06-07	2015-06-07	2011-02-07	2011-02-07	2015-06-07
+2013-04-10	2011-02-10	2015-06-10	2015-06-10	2011-02-10	2011-02-10	2015-06-10
+2021-09-24	2019-07-24	2023-11-24	2023-11-24	2019-07-24	2019-07-24	2023-11-24
+2024-11-11	2022-09-11	2027-01-11	2027-01-11	2022-09-11	2022-09-11	2027-01-11
+4143-07-08	4141-05-08	4145-09-08	4145-09-08	4141-05-08	4141-05-08	4145-09-08
+4966-12-04	4964-10-04	4969-02-04	4969-02-04	4964-10-04	4964-10-04	4969-02-04
+5339-02-01	5336-12-01	5341-04-01	5341-04-01	5336-12-01	5336-12-01	5341-04-01
+5344-10-04	5342-08-04	5346-12-04	5346-12-04	5342-08-04	5342-08-04	5346-12-04
+5397-07-13	5395-05-13	5399-09-13	5399-09-13	5395-05-13	5395-05-13	5399-09-13
+5966-07-09	5964-05-09	5968-09-09	5968-09-09	5964-05-09	5964-05-09	5968-09-09
+6229-06-28	6227-04-28	6231-08-28	6231-08-28	6227-04-28	6227-04-28	6231-08-28
+6482-04-27	6480-02-27	6484-06-27	6484-06-27	6480-02-27	6480-02-27	6484-06-27
+6631-11-13	6629-09-13	6634-01-13	6634-01-13	6629-09-13	6629-09-13	6634-01-13
+6705-09-28	6703-07-28	6707-11-28	6707-11-28	6703-07-28	6703-07-28	6707-11-28
+6731-02-12	6728-12-12	6733-04-12	6733-04-12	6728-12-12	6728-12-12	6733-04-12
+7160-12-02	7158-10-02	7163-02-02	7163-02-02	7158-10-02	7158-10-02	7163-02-02
+7409-09-07	7407-07-07	7411-11-07	7411-11-07	7407-07-07	7407-07-07	7411-11-07
+7503-06-23	7501-04-23	7505-08-23	7505-08-23	7501-04-23	7501-04-23	7505-08-23
+8422-07-22	8420-05-22	8424-09-22	8424-09-22	8420-05-22	8420-05-22	8424-09-22
+8521-01-16	8518-11-16	8523-03-16	8523-03-16	8518-11-16	8518-11-16	8523-03-16
+9075-06-13	9073-04-13	9077-08-13	9077-08-13	9073-04-13	9073-04-13	9077-08-13
+9209-11-11	9207-09-11	9212-01-11	9212-01-11	9207-09-11	9207-09-11	9212-01-11
+9403-01-09	9400-11-09	9405-03-09	9405-03-09	9400-11-09	9400-11-09	9405-03-09
+PREHOOK: query: explain
+select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: dateval (type: date), (dateval - 1999-06-07) (type: interval_day_time), (1999-06-07 - dateval) (type: interval_day_time), (dateval - dateval) (type: interval_day_time)
+                    outputColumnNames: _col0, _col1, _col2, _col3
+                    Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: date)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: interval_day_time), _col2 (type: interval_day_time), _col3 (type: interval_day_time)
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: interval_day_time), VALUE._col1 (type: interval_day_time), VALUE._col2 (type: interval_day_time)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	c1	c2	c3
+0004-09-22	-728552 23:00:00.000000000	728552 23:00:00.000000000	0 00:00:00.000000000
+0528-10-27	-537126 23:00:00.000000000	537126 23:00:00.000000000	0 00:00:00.000000000
+1319-02-02	-248481 23:00:00.000000000	248481 23:00:00.000000000	0 00:00:00.000000000
+1404-07-23	-217263 23:00:00.000000000	217263 23:00:00.000000000	0 00:00:00.000000000
+1815-05-06	-67236 23:00:00.000000000	67236 23:00:00.000000000	0 00:00:00.000000000
+1883-04-17	-42418 23:00:00.000000000	42418 23:00:00.000000000	0 00:00:00.000000000
+1966-08-16	-11983 00:00:00.000000000	11983 00:00:00.000000000	0 00:00:00.000000000
+1973-04-17	-9546 23:00:00.000000000	9546 23:00:00.000000000	0 00:00:00.000000000
+1974-10-04	-9012 00:00:00.000000000	9012 00:00:00.000000000	0 00:00:00.000000000
+1976-03-03	-8495 23:00:00.000000000	8495 23:00:00.000000000	0 00:00:00.000000000
+1976-05-06	-8432 00:00:00.000000000	8432 00:00:00.000000000	0 00:00:00.000000000
+1978-08-05	-7611 00:00:00.000000000	7611 00:00:00.000000000	0 00:00:00.000000000
+1981-04-25	-6616 23:00:00.000000000	6616 23:00:00.000000000	0 00:00:00.000000000
+1981-11-15	-6412 23:00:00.000000000	6412 23:00:00.000000000	0 00:00:00.000000000
+1985-07-20	-5070 00:00:00.000000000	5070 00:00:00.000000000	0 00:00:00.000000000
+1985-11-18	-4948 23:00:00.000000000	4948 23:00:00.000000000	0 00:00:00.000000000
+1987-02-21	-4488 23:00:00.000000000	4488 23:00:00.000000000	0 00:00:00.000000000
+1987-05-28	-4393 00:00:00.000000000	4393 00:00:00.000000000	0 00:00:00.000000000
+1998-10-16	-234 00:00:00.000000000	234 00:00:00.000000000	0 00:00:00.000000000
+1999-10-03	118 00:00:00.000000000	-118 00:00:00.000000000	0 00:00:00.000000000
+2000-12-18	560 01:00:00.000000000	-560 01:00:00.000000000	0 00:00:00.000000000
+2002-05-10	1068 00:00:00.000000000	-1068 00:00:00.000000000	0 00:00:00.000000000
+2003-09-23	1569 00:00:00.000000000	-1569 00:00:00.000000000	0 00:00:00.000000000
+2004-03-07	1735 01:00:00.000000000	-1735 01:00:00.000000000	0 00:00:00.000000000
+2007-02-09	2804 01:00:00.000000000	-2804 01:00:00.000000000	0 00:00:00.000000000
+2009-01-21	3516 01:00:00.000000000	-3516 01:00:00.000000000	0 00:00:00.000000000
+2010-04-08	3958 00:00:00.000000000	-3958 00:00:00.000000000	0 00:00:00.000000000
+2013-04-07	5053 00:00:00.000000000	-5053 00:00:00.000000000	0 00:00:00.000000000
+2013-04-10	5056 00:00:00.000000000	-5056 00:00:00.000000000	0 00:00:00.000000000
+2021-09-24	8145 00:00:00.000000000	-8145 00:00:00.000000000	0 00:00:00.000000000
+2024-11-11	9289 01:00:00.000000000	-9289 01:00:00.000000000	0 00:00:00.000000000
+4143-07-08	783111 00:00:00.000000000	-783111 00:00:00.000000000	0 00:00:00.000000000
+4966-12-04	1083855 01:00:00.000000000	-1083855 01:00:00.000000000	0 00:00:00.000000000
+5339-02-01	1219784 01:00:00.000000000	-1219784 01:00:00.000000000	0 00:00:00.000000000
+5344-10-04	1221856 00:00:00.000000000	-1221856 00:00:00.000000000	0 00:00:00.000000000
+5397-07-13	1241131 00:00:00.000000000	-1241131 00:00:00.000000000	0 00:00:00.000000000
+5966-07-09	1448949 00:00:00.000000000	-1448949 00:00:00.000000000	0 00:00:00.000000000
+6229-06-28	1544997 00:00:00.000000000	-1544997 00:00:00.000000000	0 00:00:00.000000000
+6482-04-27	1637342 00:00:00.000000000	-1637342 00:00:00.000000000	0 00:00:00.000000000
+6631-11-13	1691962 01:00:00.000000000	-1691962 01:00:00.000000000	0 00:00:00.000000000
+6705-09-28	1718944 00:00:00.000000000	-1718944 00:00:00.000000000	0 00:00:00.000000000
+6731-02-12	1728212 01:00:00.000000000	-1728212 01:00:00.000000000	0 00:00:00.000000000
+7160-12-02	1885195 01:00:00.000000000	-1885195 01:00:00.000000000	0 00:00:00.000000000
+7409-09-07	1976054 00:00:00.000000000	-1976054 00:00:00.000000000	0 00:00:00.000000000
+7503-06-23	2010310 00:00:00.000000000	-2010310 00:00:00.000000000	0 00:00:00.000000000
+8422-07-22	2345998 00:00:00.000000000	-2345998 00:00:00.000000000	0 00:00:00.000000000
+8521-01-16	2381970 01:00:00.000000000	-2381970 01:00:00.000000000	0 00:00:00.000000000
+9075-06-13	2584462 00:00:00.000000000	-2584462 00:00:00.000000000	0 00:00:00.000000000
+9209-11-11	2633556 01:00:00.000000000	-2633556 01:00:00.000000000	0 00:00:00.000000000
+9403-01-09	2704106 01:00:00.000000000	-2704106 01:00:00.000000000	0 00:00:00.000000000
+PREHOOK: query: explain
+select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: tsval (type: timestamp), (tsval - 2-2) (type: timestamp), (tsval - -2-2) (type: timestamp), (tsval + 2-2) (type: timestamp), (tsval + -2-2) (type: timestamp), (-2-2 + tsval) (type: timestamp), (2-2 + tsval) (type: timestamp)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                    Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: timestamp)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: timestamp), _col2 (type: timestamp), _col3 (type: timestamp), _col4 (type: timestamp), _col5 (type: timestamp), _col6 (type: timestamp)
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: timestamp), VALUE._col0 (type: timestamp), VALUE._col1 (type: timestamp), VALUE._col2 (type: timestamp), VALUE._col3 (type: timestamp), VALUE._col4 (type: timestamp), VALUE._col5 (type: timestamp)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+tsval	c1	c2	c3	c4	c5	c6
+0004-09-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222
+0528-10-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273
+1319-02-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778	1321-04-02 16:31:57.778	1316-12-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778
+1404-07-23 15:32:16.059185026	1402-05-23 15:32:16.059185026	1406-09-23 15:32:16.059185026	1406-09-23 15:32:16.059185026	1402-05-23 15:32:16.059185026	1402-05-23 15:32:16.059185026	1406-09-23 15:32:16.059185026
+1815-05-06 00:12:37.543584705	1813-03-06 00:12:37.543584705	1817-07-06 00:12:37.543584705	1817-07-06 00:12:37.543584705	1813-03-06 00:12:37.543584705	1813-03-06 00:12:37.543584705	1817-07-06 00:12:37.543584705
+1883-04-17 04:14:34.647766229	1881-02-17 04:14:34.647766229	1885-06-17 04:14:34.647766229	1885-06-17 04:14:34.647766229	1881-02-17 04:14:34.647766229	1881-02-17 04:14:34.647766229	1885-06-17 04:14:34.647766229
+1966-08-16 13:36:50.183618031	1964-06-16 13:36:50.183618031	1968-10-16 13:36:50.183618031	1968-10-16 13:36:50.183618031	1964-06-16 13:36:50.183618031	1964-06-16 13:36:50.183618031	1968-10-16 13:36:50.183618031
+1973-04-17 06:30:38.596784156	1971-02-17 06:30:38.596784156	1975-06-17 07:30:38.596784156	1975-06-17 07:30:38.596784156	1971-02-17 06:30:38.596784156	1971-02-17 06:30:38.596784156	1975-06-17 07:30:38.596784156
+1974-10-04 17:21:03.989	1972-08-04 17:21:03.989	1976-12-04 16:21:03.989	1976-12-04 16:21:03.989	1972-08-04 17:21:03.989	1972-08-04 17:21:03.989	1976-12-04 16:21:03.989
+1976-03-03 04:54:33.000895162	1974-01-03 04:54:33.000895162	1978-05-03 05:54:33.000895162	1978-05-03 05:54:33.000895162	1974-01-03 04:54:33.000895162	1974-01-03 04:54:33.000895162	1978-05-03 05:54:33.000895162
+1976-05-06 00:42:30.910786948	1974-03-06 00:42:30.910786948	1978-07-06 00:42:30.910786948	1978-07-06 00:42:30.910786948	1974-03-06 00:42:30.910786948	1974-03-06 00:42:30.910786948	1978-07-06 00:42:30.910786948
+1978-08-05 14:41:05.501	1976-06-05 14:41:05.501	1980-10-05 14:41:05.501	1980-10-05 14:41:05.501	1976-06-05 14:41:05.501	1976-06-05 14:41:05.501	1980-10-05 14:41:05.501
+1981-04-25 09:01:12.077192689	1979-02-25 09:01:12.077192689	1983-06-25 10:01:12.077192689	1983-06-25 10:01:12.077192689	1979-02-25 09:01:12.077192689	1979-02-25 09:01:12.077192689	1983-06-25 10:01:12.077192689
+1981-11-15 23:03:10.999338387	1979-09-16 00:03:10.999338387	1984-01-15 23:03:10.999338387	1984-01-15 23:03:10.999338387	1979-09-16 00:03:10.999338387	1979-09-16 00:03:10.999338387	1984-01-15 23:03:10.999338387
+1985-07-20 09:30:11	1983-05-20 09:30:11	1987-09-20 09:30:11	1987-09-20 09:30:11	1983-05-20 09:30:11	1983-05-20 09:30:11	1987-09-20 09:30:11
+1985-11-18 16:37:54	1983-09-18 17:37:54	1988-01-18 16:37:54	1988-01-18 16:37:54	1983-09-18 17:37:54	1983-09-18 17:37:54	1988-01-18 16:37:54
+1987-02-21 19:48:29	1984-12-21 19:48:29	1989-04-21 20:48:29	1989-04-21 20:48:29	1984-12-21 19:48:29	1984-12-21 19:48:29	1989-04-21 20:48:29
+1987-05-28 13:52:07.900916635	1985-03-28 12:52:07.900916635	1989-07-28 13:52:07.900916635	1989-07-28 13:52:07.900916635	1985-03-28 12:52:07.900916635	1985-03-28 12:52:07.900916635	1989-07-28 13:52:07.900916635
+1998-10-16 20:05:29.397591987	1996-08-16 20:05:29.397591987	2000-12-16 19:05:29.397591987	2000-12-16 19:05:29.397591987	1996-08-16 20:05:29.397591987	1996-08-16 20:05:29.397591987	2000-12-16 19:05:29.397591987
+1999-10-03 16:59:10.396903939	1997-08-03 16:59:10.396903939	2001-12-03 15:59:10.396903939	2001-12-03 15:59:10.396903939	1997-08-03 16:59:10.396903939	1997-08-03 16:59:10.396903939	2001-12-03 15:59:10.396903939
+2000-12-18 08:42:30.000595596	1998-10-18 09:42:30.000595596	2003-02-18 08:42:30.000595596	2003-02-18 08:42:30.000595596	1998-10-18 09:42:30.000595596	1998-10-18 09:42:30.000595596	2003-02-18 08:42:30.000595596
+2002-05-10 05:29:48.990818073	2000-03-10 04:29:48.990818073	2004-07-10 05:29:48.990818073	2004-07-10 05:29:48.990818073	2000-03-10 04:29:48.990818073	2000-03-10 04:29:48.990818073	2004-07-10 05:29:48.990818073
+2003-09-23 22:33:17.00003252	2001-07-23 22:33:17.00003252	2005-11-23 21:33:17.00003252	2005-11-23 21:33:17.00003252	2001-07-23 22:33:17.00003252	2001-07-23 22:33:17.00003252	2005-11-23 21:33:17.00003252
+2004-03-07 20:14:13	2002-01-07 20:14:13	2006-05-07 21:14:13	2006-05-07 21:14:13	2002-01-07 20:14:13	2002-01-07 20:14:13	2006-05-07 21:14:13
+2007-02-09 05:17:29.368756876	2004-12-09 05:17:29.368756876	2009-04-09 06:17:29.368756876	2009-04-09 06:17:29.368756876	2004-12-09 05:17:29.368756876	2004-12-09 05:17:29.368756876	2009-04-09 06:17:29.368756876
+2009-01-21 10:49:07.108	2006-11-21 10:49:07.108	2011-03-21 11:49:07.108	2011-03-21 11:49:07.108	2006-11-21 10:49:07.108	2006-11-21 10:49:07.108	2011-03-21 11:49:07.108
+2010-04-08 02:43:35.861742727	2008-02-08 01:43:35.861742727	2012-06-08 02:43:35.861742727	2012-06-08 02:43:35.861742727	2008-02-08 01:43:35.861742727	2008-02-08 01:43:35.861742727	2012-06-08 02:43:35.861742727
+2013-04-07 02:44:43.00086821	2011-02-07 01:44:43.00086821	2015-06-07 02:44:43.00086821	2015-06-07 02:44:43.00086821	2011-02-07 01:44:43.00086821	2011-02-07 01:44:43.00086821	2015-06-07 02:44:43.00086821
+2013-04-10 00:43:46.854731546	2011-02-09 23:43:46.854731546	2015-06-10 00:43:46.854731546	2015-06-10 00:43:46.854731546	2011-02-09 23:43:46.854731546	2011-02-09 23:43:46.854731546	2015-06-10 00:43:46.854731546
+2021-09-24 03:18:32.413655165	2019-07-24 03:18:32.413655165	2023-11-24 02:18:32.413655165	2023-11-24 02:18:32.413655165	2019-07-24 03:18:32.413655165	2019-07-24 03:18:32.413655165	2023-11-24 02:18:32.413655165
+2024-11-11 16:42:41.101	2022-09-11 17:42:41.101	2027-01-11 16:42:41.101	2027-01-11 16:42:41.101	2022-09-11 17:42:41.101	2022-09-11 17:42:41.101	2027-01-11 16:42:41.101
+4143-07-08 10:53:27.252802259	4141-05-08 10:53:27.252802259	4145-09-08 10:53:27.252802259	4145-09-08 10:53:27.252802259	4141-05-08 10:53:27.252802259	4141-05-08 10:53:27.252802259	4145-09-08 10:53:27.252802259
+4966-12-04 09:30:55.202	4964-10-04 10:30:55.202	4969-02-04 09:30:55.202	4969-02-04 09:30:55.202	4964-10-04 10:30:55.202	4964-10-04 10:30:55.202	4969-02-04 09:30:55.202
+5339-02-01 14:10:01.085678691	5336-12-01 14:10:01.085678691	5341-04-01 15:10:01.085678691	5341-04-01 15:10:01.085678691	5336-12-01 14:10:01.085678691	5336-12-01 14:10:01.085678691	5341-04-01 15:10:01.085678691
+5344-10-04 18:40:08.165	5342-08-04 18:40:08.165	5346-12-04 17:40:08.165	5346-12-04 17:40:08.165	5342-08-04 18:40:08.165	5342-08-04 18:40:08.165	5346-12-04 17:40:08.165
+5397-07-13 07:12:32.000896438	5395-05-13 07:12:32.000896438	5399-09-13 07:12:32.000896438	5399-09-13 07:12:32.000896438	5395-05-13 07:12:32.000896438	5395-05-13 07:12:32.000896438	5399-09-13 07:12:32.000896438
+5966-07-09 03:30:50.597	5964-05-09 03:30:50.597	5968-09-09 03:30:50.597	5968-09-09 03:30:50.597	5964-05-09 03:30:50.597	5964-05-09 03:30:50.597	5968-09-09 03:30:50.597
+6229-06-28 02:54:28.970117179	6227-04-28 02:54:28.970117179	6231-08-28 02:54:28.970117179	6231-08-28 02:54:28.970117179	6227-04-28 02:54:28.970117179	6227-04-28 02:54:28.970117179	6231-08-28 02:54:28.970117179
+6482-04-27 12:07:38.073915413	6480-02-27 11:07:38.073915413	6484-06-27 12:07:38.073915413	6484-06-27 12:07:38.073915413	6480-02-27 11:07:38.073915413	6480-02-27 11:07:38.073915413	6484-06-27 12:07:38.073915413
+6631-11-13 16:31:29.702202248	6629-09-13 17:31:29.702202248	6634-01-13 16:31:29.702202248	6634-01-13 16:31:29.702202248	6629-09-13 17:31:29.702202248	6629-09-13 17:31:29.702202248	6634-01-13 16:31:29.702202248
+6705-09-28 18:27:28.000845672	6703-07-28 18:27:28.000845672	6707-11-28 17:27:28.000845672	6707-11-28 17:27:28.000845672	6703-07-28 18:27:28.000845672	6703-07-28 18:27:28.000845672	6707-11-28 17:27:28.000845672
+6731-02-12 08:12:48.287783702	6728-12-12 08:12:48.287783702	6733-04-12 09:12:48.287783702	6733-04-12 09:12:48.287783702	6728-12-12 08:12:48.287783702	6728-12-12 08:12:48.287783702	6733-04-12 09:12:48.287783702
+7160-12-02 06:00:24.81200852	7158-10-02 07:00:24.81200852	7163-02-02 06:00:24.81200852	7163-02-02 06:00:24.81200852	7158-10-02 07:00:24.81200852	7158-10-02 07:00:24.81200852	7163-02-02 06:00:24.81200852
+7409-09-07 23:33:32.459349602	7407-07-07 23:33:32.459349602	7411-11-07 22:33:32.459349602	7411-11-07 22:33:32.459349602	7407-07-07 23:33:32.459349602	7407-07-07 23:33:32.459349602	7411-11-07 22:33:32.459349602
+7503-06-23 23:14:17.486	7501-04-23 23:14:17.486	7505-08-23 23:14:17.486	7505-08-23 23:14:17.486	7501-04-23 23:14:17.486	7501-04-23 23:14:17.486	7505-08-23 23:14:17.486
+8422-07-22 03:21:45.745036084	8420-05-22 03:21:45.745036084	8424-09-22 03:21:45.745036084	8424-09-22 03:21:45.745036084	8420-05-22 03:21:45.745036084	8420-05-22 03:21:45.745036084	8424-09-22 03:21:45.745036084
+8521-01-16 20:42:05.668832388	8518-11-16 20:42:05.668832388	8523-03-16 21:42:05.668832388	8523-03-16 21:42:05.668832388	8518-11-16 20:42:05.668832388	8518-11-16 20:42:05.668832388	8523-03-16 21:42:05.668832388
+9075-06-13 16:20:09.218517797	9073-04-13 16:20:09.218517797	9077-08-13 16:20:09.218517797	9077-08-13 16:20:09.218517797	9073-04-13 16:20:09.218517797	9073-04-13 16:20:09.218517797	9077-08-13 16:20:09.218517797
+9209-11-11 04:08:58.223768453	9207-09-11 05:08:58.223768453	9212-01-11 04:08:58.223768453	9212-01-11 04:08:58.223768453	9207-09-11 05:08:58.223768453	9207-09-11 05:08:58.223768453	9212-01-11 04:08:58.223768453
+9403-01-09 18:12:33.547	9400-11-09 18:12:33.547	9405-03-09 18:12:33.547	9405-03-09 18:12:33.547	9400-11-09 18:12:33.547	9400-11-09 18:12:33.547	9405-03-09 18:12:33.547
+PREHOOK: query: explain
+select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                    Reduce Output Operator
+                      key expressions: 5-5 (type: interval_year_month)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                      TopN Hash Memory Usage: 0.1
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: 5-5 (type: interval_year_month), -1-1 (type: interval_year_month)
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                Limit
+                  Number of rows: 2
+                  Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                    table:
+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 2
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+c0	c1
+5-5	-1-1
+5-5	-1-1
+PREHOOK: query: -- interval day-time arithmetic
+explain
+select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: -- interval day-time arithmetic
+explain
+select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: dateval (type: date), (dateval - 99 11:22:33.123456789) (type: timestamp), (dateval - -99 11:22:33.123456789) (type: timestamp), (dateval + 99 11:22:33.123456789) (type: timestamp), (dateval + -99 11:22:33.123456789) (type: timestamp), (-99 11:22:33.123456789 + dateval) (type: timestamp), (99 11:22:33.123456789 + dateval) (type: timestamp)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                    Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: date)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: timestamp), _col2 (type: timestamp), _col3 (type: timestamp), _col4 (type: timestamp), _col5 (type: timestamp), _col6 (type: timestamp)
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: timestamp), VALUE._col1 (type: timestamp), VALUE._col2 (type: timestamp), VALUE._col3 (type: timestamp), VALUE._col4 (type: timestamp), VALUE._col5 (type: timestamp)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	_c1	_c2	_c3	_c4	_c5	_c6
+0004-09-22	0004-06-14 12:37:26.876543211	0004-12-30 11:22:33.123456789	0004-12-30 11:22:33.123456789	0004-06-14 12:37:26.876543211	0004-06-14 12:37:26.876543211	0004-12-30 11:22:33.123456789
+0528-10-27	0528-07-19 12:37:26.876543211	0529-02-03 11:22:33.123456789	0529-02-03 11:22:33.123456789	0528-07-19 12:37:26.876543211	0528-07-19 12:37:26.876543211	0529-02-03 11:22:33.123456789
+1319-02-02	1318-10-25 12:37:26.876543211	1319-05-12 11:22:33.123456789	1319-05-12 11:22:33.123456789	1318-10-25 12:37:26.876543211	1318-10-25 12:37:26.876543211	1319-05-12 11:22:33.123456789
+1404-07-23	1404-04-14 12:37:26.876543211	1404-10-30 11:22:33.123456789	1404-10-30 11:22:33.123456789	1404-04-14 12:37:26.876543211	1404-04-14 12:37:26.876543211	1404-10-30 11:22:33.123456789
+1815-05-06	1815-01-26 12:37:26.876543211	1815-08-13 11:22:33.123456789	1815-08-13 11:22:33.123456789	1815-01-26 12:37:26.876543211	1815-01-26 12:37:26.876543211	1815-08-13 11:22:33.123456789
+1883-04-17	1883-01-07 12:37:26.876543211	1883-07-25 11:22:33.123456789	1883-07-25 11:22:33.123456789	1883-01-07 12:37:26.876543211	1883-01-07 12:37:26.876543211	1883-07-25 11:22:33.123456789
+1966-08-16	1966-05-08 12:37:26.876543211	1966-11-23 10:22:33.123456789	1966-11-23 10:22:33.123456789	1966-05-08 12:37:26.876543211	1966-05-08 12:37:26.876543211	1966-11-23 10:22:33.123456789
+1973-04-17	1973-01-07 12:37:26.876543211	1973-07-25 12:22:33.123456789	1973-07-25 12:22:33.123456789	1973-01-07 12:37:26.876543211	1973-01-07 12:37:26.876543211	1973-07-25 12:22:33.123456789
+1974-10-04	1974-06-26 12:37:26.876543211	1975-01-11 10:22:33.123456789	1975-01-11 10:22:33.123456789	1974-06-26 12:37:26.876543211	1974-06-26 12:37:26.876543211	1975-01-11 10:22:33.123456789
+1976-03-03	1975-11-24 12:37:26.876543211	1976-06-10 12:22:33.123456789	1976-06-10 12:22:33.123456789	1975-11-24 12:37:26.876543211	1975-11-24 12:37:26.876543211	1976-06-10 12:22:33.123456789
+1976-05-06	1976-01-27 11:37:26.876543211	1976-08-13 11:22:33.123456789	1976-08-13 11:22:33.123456789	1976-01-27 11:37:26.876543211	1976-01-27 11:37:26.876543211	1976-08-13 11:22:33.123456789
+1978-08-05	1978-04-27 11:37:26.876543211	1978-11-12 10:22:33.123456789	1978-11-12 10:22:33.123456789	1978-04-27 11:37:26.876543211	1978-04-27 11:37:26.876543211	1978-11-12 10:22:33.123456789
+1981-04-25	1981-01-15 12:37:26.876543211	1981-08-02 12:22:33.123456789	1981-08-02 12:22:33.123456789	1981-01-15 12:37:26.876543211	1981-01-15 12:37:26.876543211	1981-08-02 12:22:33.123456789
+1981-11-15	1981-08-07 13:37:26.876543211	1982-02-22 11:22:33.123456789	1982-02-22 11:22:33.123456789	1981-08-07 13:37:26.876543211	1981-08-07 13:37:26.876543211	1982-02-22 11:22:33.123456789
+1985-07-20	1985-04-11 11:37:26.876543211	1985-10-27 10:22:33.123456789	1985-10-27 10:22:33.123456789	1985-04-11 11:37:26.876543211	1985-04-11 11:37:26.876543211	1985-10-27 10:22:33.123456789
+1985-11-18	1985-08-10 13:37:26.876543211	1986-02-25 11:22:33.123456789	1986-02-25 11:22:33.123456789	1985-08-10 13:37:26.876543211	1985-08-10 13:37:26.876543211	1986-02-25 11:22:33.123456789
+1987-02-21	1986-11-13 12:37:26.876543211	1987-05-31 12:22:33.123456789	1987-05-31 12:22:33.123456789	1986-11-13 12:37:26.876543211	1986-11-13 12:37:26.876543211	1987-05-31 12:22:33.123456789
+1987-05-28	1987-02-17 11:37:26.876543211	1987-09-04 11:22:33.123456789	1987-09-04 11:22:33.123456789	1987-02-17 11:37:26.876543211	1987-02-17 11:37:26.876543211	1987-09-04 11:22:33.123456789
+1998-10-16	1998-07-08 12:37:26.876543211	1999-01-23 10:22:33.123456789	1999-01-23 10:22:33.123456789	1998-07-08 12:37:26.876543211	1998-07-08 12:37:26.876543211	1999-01-23 10:22:33.123456789
+1999-10-03	1999-06-25 12:37:26.876543211	2000-01-10 10:22:33.123456789	2000-01-10 10:22:33.123456789	1999-06-25 12:37:26.876543211	1999-06-25 12:37:26.876543211	2000-01-10 10:22:33.123456789
+2000-12-18	2000-09-09 13:37:26.876543211	2001-03-27 11:22:33.123456789	2001-03-27 11:22:33.123456789	2000-09-09 13:37:26.876543211	2000-09-09 13:37:26.876543211	2001-03-27 11:22:33.123456789
+2002-05-10	2002-01-30 11:37:26.876543211	2002-08-17 11:22:33.123456789	2002-08-17 11:22:33.123456789	2002-01-30 11:37:26.876543211	2002-01-30 11:37:26.876543211	2002-08-17 11:22:33.123456789
+2003-09-23	2003-06-15 12:37:26.876543211	2003-12-31 10:22:33.123456789	2003-12-31 10:22:33.123456789	2003-06-15 12:37:26.876543211	2003-06-15 12:37:26.876543211	2003-12-31 10:22:33.123456789
+2004-03-07	2003-11-28 12:37:26.876543211	2004-06-14 12:22:33.123456789	2004-06-14 12:22:33.123456789	2003-11-28 12:37:26.876543211	2003-11-28 12:37:26.876543211	2004-06-14 12:22:33.123456789
+2007-02-09	2006-11-01 12:37:26.876543211	2007-05-19 12:22:33.123456789	2007-05-19 12:22:33.123456789	2006-11-01 12:37:26.876543211	2006-11-01 12:37:26.876543211	2007-05-19 12:22:33.123456789
+2009-01-21	2008-10-13 13:37:26.876543211	2009-04-30 12:22:33.123456789	2009-04-30 12:22:33.123456789	2008-10-13 13:37:26.876543211	2008-10-13 13:37:26.876543211	2009-04-30 12:22:33.123456789
+2010-04-08	2009-12-29 11:37:26.876543211	2010-07-16 11:22:33.123456789	2010-07-16 11:22:33.123456789	2009-12-29 11:37:26.876543211	2009-12-29 11:37:26.876543211	2010-07-16 11:22:33.123456789
+2013-04-07	2012-12-28 11:37:26.876543211	2013-07-15 11:22:33.123456789	2013-07-15 11:22:33.123456789	2012-12-28 11:37:26.876543211	2012-12-28 11:37:26.876543211	2013-07-15 11:22:33.123456789
+2013-04-10	2012-12-31 11:37:26.876543211	2013-07-18 11:22:33.123456789	2013-07-18 11:22:33.123456789	2012-12-31 11:37:26.876543211	2012-12-31 11:37:26.876543211	2013-07-18 11:22:33.123456789
+2021-09-24	2021-06-16 12:37:26.876543211	2022-01-01 10:22:33.123456789	2022-01-01 10:22:33.123456789	2021-06-16 12:37:26.876543211	2021-06-16 12:37:26.876543211	2022-01-01 10:22:33.123456789
+2024-11-11	2024-08-03 13:37:26.876543211	2025-02-18 11:22:33.123456789	2025-02-18 11:22:33.123456789	2024-08-03 13:37:26.876543211	2024-08-03 13:37:26.876543211	2025-02-18 11:22:33.123456789
+4143-07-08	4143-03-30 12:37:26.876543211	4143-10-15 11:22:33.123456789	4143-10-15 11:22:33.123456789	4143-03-30 12:37:26.876543211	4143-03-30 12:37:26.876543211	4143-10-15 11:22:33.123456789
+4966-12-04	4966-08-26 13:37:26.876543211	4967-03-13 12:22:33.123456789	4967-03-13 12:22:33.123456789	4966-08-26 13:37:26.876543211	4966-08-26 13:37:26.876543211	4967-03-13 12:22:33.123456789
+5339-02-01	5338-10-24 13:37:26.876543211	5339-05-11 12:22:33.123456789	5339-05-11 12:22:33.123456789	5338-10-24 13:37:26.876543211	5338-10-24 13:37:26.876543211	5339-05-11 12:22:33.123456789
+5344-10-04	5344-06-26 12:37:26.876543211	5345-01-11 10:22:33.123456789	5345-01-11 10:22:33.123456789	5344-06-26 12:37:26.876543211	5344-06-26 12:37:26.876543211	5345-01-11 10:22:33.123456789
+5397-07-13	5397-04-04 12:37:26.876543211	5397-10-20 11:22:33.123456789	5397-10-20 11:22:33.123456789	5397-04-04 12:37:26.876543211	5397-04-04 12:37:26.876543211	5397-10-20 11:22:33.123456789
+5966-07-09	5966-03-31 12:37:26.876543211	5966-10-16 11:22:33.123456789	5966-10-16 11:22:33.123456789	5966-03-31 12:37:26.876543211	5966-03-31 12:37:26.876543211	5966-10-16 11:22:33.123456789
+6229-06-28	6229-03-20 12:37:26.876543211	6229-10-05 11:22:33.123456789	6229-10-05 11:22:33.123456789	6229-03-20 12:37:26.876543211	6229-03-20 12:37:26.876543211	6229-10-05 11:22:33.123456789
+6482-04-27	6482-01-17 11:37:26.876543211	6482-08-04 11:22:33.123456789	6482-08-04 11:22:33.123456789	6482-01-17 11:37:26.876543211	6482-01-17 11:37:26.876543211	6482-08-04 11:22:33.123456789
+6631-11-13	6631-08-05 13:37:26.876543211	6632-02-20 11:22:33.123456789	6632-02-20 11:22:33.123456789	6631-08-05 13:37:26.876543211	6631-08-05 13:37:26.876543211	6632-02-20 11:22:33.123456789
+6705-09-28	6705-06-20 12:37:26.876543211	6706-01-05 10:22:33.123456789	6706-01-05 10:22:33.123456789	6705-06-20 12:37:26.876543211	6705-06-20 12:37:26.876543211	6706-01-05 10:22:33.123456789
+6731-02-12	6730-11-04 12:37:26.876543211	6731-05-22 12:22:33.123456789	6731-05-22 12:22:33.123456789	6730-11-04 12:37:26.876543211	6730-11-04 12:37:26.876543211	6731-05-22 12:22:33.123456789
+7160-12-02	7160-08-24 13:37:26.876543211	7161-03-11 11:22:33.123456789	7161-03-11 11:22:33.123456789	7160-08-24 13:37:26.876543211	7160-08-24 13:37:26.876543211	7161-03-11 11:22:33.123456789
+7409-09-07	7409-05-30 12:37:26.876543211	7409-12-15 10:22:33.123456789	7409-12-15 10:22:33.123456789	7409-05-30 12:37:26.876543211	7409-05-30 12:37:26.876543211	7409-12-15 10:22:33.123456789
+7503-06-23	7503-03-15 12:37:26.876543211	7503-09-30 11:22:33.123456789	7503-09-30 11:22:33.123456789	7503-03-15 12:37:26.876543211	7503-03-15 12:37:26.876543211	7503-09-30 11:22:33.123456789
+8422-07-22	8422-04-13 12:37:26.876543211	8422-10-29 11:22:33.123456789	8422-10-29 11:22:33.123456789	8422-04-13 12:37:26.876543211	8422-04-13 12:37:26.876543211	8422-10-29 11:22:33.123456789
+8521-01-16	8520-10-08 13:37:26.876543211	8521-04-25 12:22:33.123456789	8521-04-25 12:22:33.123456789	8520-10-08 13:37:26.876543211	8520-10-08 13:37:26.876543211	8521-04-25 12:22:33.123456789
+9075-06-13	9075-03-05 11:37:26.876543211	9075-09-20 11:22:33.123456789	9075-09-20 11:22:33.123456789	9075-03-05 11:37:26.876543211	9075-03-05 11:37:26.876543211	9075-09-20 11:22:33.123456789
+9209-11-11	9209-08-03 13:37:26.876543211	9210-02-18 11:22:33.123456789	9210-02-18 11:22:33.123456789	9209-08-03 13:37:26.876543211	9209-08-03 13:37:26.876543211	9210-02-18 11:22:33.123456789
+9403-01-09	9402-10-01 13:37:26.876543211	9403-04-18 12:22:33.123456789	9403-04-18 12:22:33.123456789	9402-10-01 13:37:26.876543211	9402-10-01 13:37:26.876543211	9403-04-18 12:22:33.123456789
+PREHOOK: query: explain
+select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: dateval (type: date), tsval (type: timestamp), (dateval - tsval) (type: interval_day_time), (tsval - dateval) (type: interval_day_time), (tsval - tsval) (type: interval_day_time)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                    Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: date)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: timestamp), _col2 (type: interval_day_time), _col3 (type: interval_day_time), _col4 (type: interval_day_time)
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: timestamp), VALUE._col1 (type: interval_day_time), VALUE._col2 (type: interval_day_time), VALUE._col3 (type: interval_day_time)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	tsval	c2	c3	c4
+0004-09-22	0004-09-22 18:26:29.519542222	-0 18:26:30.519542222	0 18:26:30.519542222	0 00:00:00.000000000
+0528-10-27	0528-10-27 08:15:18.941718273	-0 08:15:19.941718273	0 08:15:19.941718273	0 00:00:00.000000000
+1319-02-02	1319-02-02 16:31:57.778	-0 16:31:58.778000000	0 16:31:58.778000000	0 00:00:00.000000000
+1404-07-23	1404-07-23 15:32:16.059185026	-0 15:32:17.059185026	0 15:32:17.059185026	0 00:00:00.000000000
+1815-05-06	1815-05-06 00:12:37.543584705	-0 00:12:38.543584705	0 00:12:38.543584705	0 00:00:00.000000000
+1883-04-17	1883-04-17 04:14:34.647766229	-0 04:14:35.647766229	0 04:14:35.647766229	0 00:00:00.000000000
+1966-08-16	1966-08-16 13:36:50.183618031	-0 13:36:51.183618031	0 13:36:51.183618031	0 00:00:00.000000000
+1973-04-17	1973-04-17 06:30:38.596784156	-0 06:30:38.596784156	0 06:30:38.596784156	0 00:00:00.000000000
+1974-10-04	1974-10-04 17:21:03.989	-0 17:21:03.989000000	0 17:21:03.989000000	0 00:00:00.000000000
+1976-03-03	1976-03-03 04:54:33.000895162	-0 04:54:33.000895162	0 04:54:33.000895162	0 00:00:00.000000000
+1976-05-06	1976-05-06 00:42:30.910786948	-0 00:42:30.910786948	0 00:42:30.910786948	0 00:00:00.000000000
+1978-08-05	1978-08-05 14:41:05.501	-0 14:41:05.501000000	0 14:41:05.501000000	0 00:00:00.000000000
+1981-04-25	1981-04-25 09:01:12.077192689	-0 09:01:12.077192689	0 09:01:12.077192689	0 00:00:00.000000000
+1981-11-15	1981-11-15 23:03:10.999338387	-0 23:03:10.999338387	0 23:03:10.999338387	0 00:00:00.000000000
+1985-07-20	1985-07-20 09:30:11	-0 09:30:11.000000000	0 09:30:11.000000000	0 00:00:00.000000000
+1985-11-18	1985-11-18 16:37:54	-0 16:37:54.000000000	0 16:37:54.000000000	0 00:00:00.000000000
+1987-02-21	1987-02-21 19:48:29	-0 19:48:29.000000000	0 19:48:29.000000000	0 00:00:00.000000000
+1987-05-28	1987-05-28 13:52:07.900916635	-0 13:52:07.900916635	0 13:52:07.900916635	0 00:00:00.000000000
+1998-10-16	1998-10-16 20:05:29.397591987	-0 20:05:29.397591987	0 20:05:29.397591987	0 00:00:00.000000000
+1999-10-03	1999-10-03 16:59:10.396903939	-0 16:59:10.396903939	0 16:59:10.396903939	0 00:00:00.000000000
+2000-12-18	2000-12-18 08:42:30.000595596	-0 08:42:30.000595596	0 08:42:30.000595596	0 00:00:00.000000000
+2002-05-10	2002-05-10 05:29:48.990818073	-0 05:29:48.990818073	0 05:29:48.990818073	0 00:00:00.000000000
+2003-09-23	2003-09-23 22:33:17.00003252	-0 22:33:17.000032520	0 22:33:17.000032520	0 00:00:00.000000000
+2004-03-07	2004-03-07 20:14:13	-0 20:14:13.000000000	0 20:14:13.000000000	0 00:00:00.000000000
+2007-02-09	2007-02-09 05:17:29.368756876	-0 05:17:29.368756876	0 05:17:29.368756876	0 00:00:00.000000000
+2009-01-21	2009-01-21 10:49:07.108	-0 10:49:07.108000000	0 10:49:07.108000000	0 00:00:00.000000000
+2010-04-08	2010-04-08 02:43:35.861742727	-0 02:43:35.861742727	0 02:43:35.861742727	0 00:00:00.000000000
+2013-04-07	2013-04-07 02:44:43.00086821	-0 02:44:43.000868210	0 02:44:43.000868210	0 00:00:00.000000000
+2013-04-10	2013-04-10 00:43:46.854731546	-0 00:43:46.854731546	0 00:43:46.854731546	0 00:00:00.000000000
+2021-09-24	2021-09-24 03:18:32.413655165	-0 03:18:32.413655165	0 03:18:32.413655165	0 00:00:00.000000000
+2024-11-11	2024-11-11 16:42:41.101	-0 16:42:41.101000000	0 16:42:41.101000000	0 00:00:00.000000000
+4143-07-08	4143-07-08 10:53:27.252802259	-0 10:53:27.252802259	0 10:53:27.252802259	0 00:00:00.000000000
+4966-12-04	4966-12-04 09:30:55.202	-0 09:30:55.202000000	0 09:30:55.202000000	0 00:00:00.000000000
+5339-02-01	5339-02-01 14:10:01.085678691	-0 14:10:01.085678691	0 14:10:01.085678691	0 00:00:00.000000000
+5344-10-04	5344-10-04 18:40:08.165	-0 18:40:08.165000000	0 18:40:08.165000000	0 00:00:00.000000000
+5397-07-13	5397-07-13 07:12:32.000896438	-0 07:12:32.000896438	0 07:12:32.000896438	0 00:00:00.000000000
+5966-07-09	5966-07-09 03:30:50.597	-0 03:30:50.597000000	0 03:30:50.597000000	0 00:00:00.000000000
+6229-06-28	6229-06-28 02:54:28.970117179	-0 02:54:28.970117179	0 02:54:28.970117179	0 00:00:00.000000000
+6482-04-27	6482-04-27 12:07:38.073915413	-0 12:07:38.073915413	0 12:07:38.073915413	0 00:00:00.000000000
+6631-11-13	6631-11-13 16:31:29.702202248	-0 16:31:29.702202248	0 16:31:29.702202248	0 00:00:00.000000000
+6705-09-28	6705-09-28 18:27:28.000845672	-0 18:27:28.000845672	0 18:27:28.000845672	0 00:00:00.000000000
+6731-02-12	6731-02-12 08:12:48.287783702	-0 08:12:48.287783702	0 08:12:48.287783702	0 00:00:00.000000000
+7160-12-02	7160-12-02 06:00:24.81200852	-0 06:00:24.812008520	0 06:00:24.812008520	0 00:00:00.000000000
+7409-09-07	7409-09-07 23:33:32.459349602	-0 23:33:32.459349602	0 23:33:32.459349602	0 00:00:00.000000000
+7503-06-23	7503-06-23 23:14:17.486	-0 23:14:17.486000000	0 23:14:17.486000000	0 00:00:00.000000000
+8422-07-22	8422-07-22 03:21:45.745036084	-0 03:21:45.745036084	0 03:21:45.745036084	0 00:00:00.000000000
+8521-01-16	8521-01-16 20:42:05.668832388	-0 20:42:05.668832388	0 20:42:05.668832388	0 00:00:00.000000000
+9075-06-13	9075-06-13 16:20:09.218517797	-0 16:20:09.218517797	0 16:20:09.218517797	0 00:00:00.000000000
+9209-11-11	9209-11-11 04:08:58.223768453	-0 04:08:58.223768453	0 04:08:58.223768453	0 00:00:00.000000000
+9403-01-09	9403-01-09 18:12:33.547	-0 18:12:33.547000000	0 18:12:33.547000000	0 00:00:00.000000000
+PREHOOK: query: explain
+select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: tsval (type: timestamp), (tsval - 99 11:22:33.123456789) (type: timestamp), (tsval - -99 11:22:33.123456789) (type: timestamp), (tsval + 99 11:22:33.123456789) (type: timestamp), (tsval + -99 11:22:33.123456789) (type: timestamp), (-99 11:22:33.123456789 + tsval) (type: timestamp), (99 11:22:33.123456789 + tsval) (type: timestamp)
+                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                    Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                    Reduce Output Operator
+                      key expressions: _col0 (type: timestamp)
+                      sort order: +
+                      Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                      value expressions: _col1 (type: timestamp), _col2 (type: timestamp), _col3 (type: timestamp), _col4 (type: timestamp), _col5 (type: timestamp), _col6 (type: timestamp)
+            Execution mode: vectorized
+        Reducer 2 
+            Execution mode: vectorized
+            Reduce Operator Tree:
+              Select Operator
+                expressions: KEY.reducesinkkey0 (type: timestamp), VALUE._col0 (type: timestamp), VALUE._col1 (type: timestamp), VALUE._col2 (type: timestamp), VALUE._col3 (type: timestamp), VALUE._col4 (type: timestamp), VALUE._col5 (type: timestamp)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+tsval	_c1	_c2	_c3	_c4	_c5	_c6
+0004-09-22 18:26:29.519542222	0004-06-15 07:03:56.396085433	0004-12-31 05:49:02.642999011	0004-12-31 05:49:02.642999011	0004-06-15 07:03:56.396085433	0004-06-15 07:03:56.396085433	0004-12-31 05:49:02.642999011
+0528-10-27 08:15:18.941718273	0528-07-19 20:52:45.818261484	0529-02-03 19:37:52.065175062	0529-02-03 19:37:52.065175062	0528-07-19 20:52:45.818261484	0528-07-19 20:52:45.818261484	0529-02-03 19:37:52.065175062
+1319-02-02 16:31:57.778	1318-10-26 05:09:24.654543211	1319-05-13 03:54:30.901456789	1319-05-13 03:54:30.901456789	1318-10-26 05:09:24.654543211	1318-10-26 05:09:24.654543211	1319-05-13 03:54:30.901456789
+1404-07-23 15:32:16.059185026	1404-04-15 04:09:42.935728237	1404-10-31 02:54:49.182641815	1404-10-31 02:54:49.182641815	1404-04-15 04:09:42.935728237	1404-04-15 04:09:42.935728237	1404-10-31 02:54:49.182641815
+1815-05-06 00:12:37.543584705	1815-01-26 12:50:04.420127916	1815-08-13 11:35:10.667041494	1815-08-13 11:35:10.667041494	1815-01-26 12:50:04.420127916	1815-01-26 12:50:04.420127916	1815-08-13 11:35:10.667041494
+1883-04-17 04:14:34.647766229	1883-01-07 16:52:01.52430944	1883-07-25 15:37:07.771223018	1883-07-25 15:37:07.771223018	1883-01-07 16:52:01.52430944	1883-01-07 16:52:01.52430944	1883-07-25 15:37:07.771223018
+1966-08-16 13:36:50.183618031	1966-05-09 02:14:17.060161242	1966-11-23 23:59:23.30707482	1966-11-23 23:59:23.30707482	1966-05-09 02:14:17.060161242	1966-05-09 02:14:17.060161242	1966-11-23 23:59:23.30707482
+1973-04-17 06:30:38.596784156	1973-01-07 19:08:05.473327367	1973-07-25 18:53:11.720240945	1973-07-25 18:53:11.720240945	1973-01-07 19:08:05.473327367	1973-01-07 19:08:05.473327367	1973-07-25 18:53:11.720240945
+1974-10-04 17:21:03.989	1974-06-27 05:58:30.865543211	1975-01-12 03:43:37.112456789	1975-01-12 03:43:37.112456789	1974-06-27 05:58:30.865543211	1974-06-27 05:58:30.865543211	1975-01-12 03:43:37.112456789
+1976-03-03 04:54:33.000895162	1975-11-24 17:31:59.877438373	1976-06-10 17:17:06.124351951	1976-06-10 17:17:06.124351951	1975-11-24 17:31:59.877438373	1975-11-24 17:31:59.877438373	1976-06-10 17:17:06.124351951
+1976-05-06 00:42:30.910786948	1976-01-27 12:19:57.787330159	1976-08-13 12:05:04.034243737	1976-08-13 12:05:04.034243737	1976-01-27 12:19:57.787330159	1976-01-27 12:19:57.787330159	1976-08-13 12:05:04.034243737
+1978-08-05 14:41:05.501	1978-04-28 02:18:32.377543211	1978-11-13 01:03:38.624456789	1978-11-13 01:03:38.624456789	1978-04-28 02:18:32.377543211	1978-04-28 02:18:32.377543211	1978-11-13 01:03:38.624456789
+1981-04-25 09:01:12.077192689	1981-01-15 21:38:38.9537359	1981-08-02 21:23:45.200649478	1981-08-02 21:23:45.200649478	1981-01-15 21:38:38.9537359	1981-01-15 21:38:38.9537359	1981-08-02 21:23:45.200649478
+1981-11-15 23:03:10.999338387	1981-08-08 12:40:37.875881598	1982-02-23 10:25:44.122795176	1982-02-23 10:25:44.122795176	1981-08-08 12:40:37.875881598	1981-08-08 12:40:37.875881598	1982-02-23 10:25:44.122795176
+1985-07-20 09:30:11	1985-04-11 21:07:37.876543211	1985-10-27 19:52:44.123456789	1985-10-27 19:52:44.123456789	1985-04-11 21:07:37.876543211	1985-04-11 21:07:37.876543211	1985-10-27 19:52:44.123456789
+1985-11-18 16:37:54	1985-08-11 06:15:20.876543211	1986-02-26 04:00:27.123456789	1986-02-26 04:00:27.123456789	1985-08-11 06:15:20.876543211	1985-08-11 06:15:20.876543211	1986-02-26 04:00:27.123456789
+1987-02-21 19:48:29	1986-11-14 08:25:55.876543211	1987-06-01 08:11:02.123456789	1987-06-01 08:11:02.123456789	1986-11-14 08:25:55.876543211	1986-11-14 08:25:55.876543211	1987-06-01 08:11:02.123456789
+1987-05-28 13:52:07.900916635	1987-02-18 01:29:34.777459846	1987-09-05 01:14:41.024373424	1987-09-05 01:14:41.024373424	1987-02-18 01:29:34.777459846	1987-02-18 01:29:34.777459846	1987-09-05 01:14:41.024373424
+1998-10-16 20:05:29.397591987	1998-07-09 08:42:56.274135198	1999-01-24 06:28:02.521048776	1999-01-24 06:28:02.521048776	1998-07-09 08:42:56.274135198	1998-07-09 08:42:56.274135198	1999-01-24 06:28:02.521048776
+1999-10-03 16:59:10.396903939	1999-06-26 05:36:37.27344715	2000-01-11 03:21:43.520360728	2000-01-11 03:21:43.520360728	1999-06-26 05:36:37.27344715	1999-06-26 05:36:37.27344715	2000-01-11 03:21:43.520360728
+2000-12-18 08:42:30.000595596	2000-09-09 22:19:56.877138807	2001-03-27 20:05:03.124052385	2001-03-27 20:05:03.124052385	2000-09-09 22:19:56.877138807	2000-09-09 22:19:56.877138807	2001-03-27 20:05:03.124052385
+2002-05-10 05:29:48.990818073	2002-01-30 17:07:15.867361284	2002-08-17 16:52:22.114274862	2002-08-17 16:52:22.114274862	2002-01-30 17:07:15.867361284	2002-01-30 17:07:15.867361284	2002-08-17 16:52:22.114274862
+2003-09-23 22:33:17.00003252	2003-06-16 11:10:43.876575731	2004-01-01 08:55:50.123489309	2004-01-01 08:55:50.123489309	2003-06-16 11:10:43.876575731	2003-06-16 11:10:43.876575731	2004-01-01 08:55:50.123489309
+2004-03-07 20:14:13	2003-11-29 08:51:39.876543211	2004-06-15 08:36:46.123456789	2004-06-15 08:36:46.123456789	2003-11-29 08:51:39.876543211	2003-11-29 08:51:39.876543211	2004-06-15 08:36:46.123456789
+2007-02-09 05:17:29.368756876	2006-11-01 17:54:56.245300087	2007-05-19 17:40:02.492213665	2007-05-19 17:40:02.492213665	2006-11-01 17:54:56.245300087	2006-11-01 17:54:56.245300087	2007-05-19 17:40:02.492213665
+2009-01-21 10:49:07.108	2008-10-14 00:26:33.984543211	2009-04-30 23:11:40.231456789	2009-04-30 23:11:40.231456789	2008-10-14 00:26:33.984543211	2008-10-14 00:26:33.984543211	2009-04-30 23:11:40.231456789
+2010-04-08 02:43:35.861742727	2009-12-29 14:21:02.738285938	2010-07-16 14:06:08.985199516	2010-07-16 14:06:08.985199516	2009-12-29 14:21:02.738285938	2009-12-29 14:21:02.738285938	2010-07-16 14:06:08.985199516
+2013-04-07 02:44:43.00086821	2012-12-28 14:22:09.877411421	2013-07-15 14:07:16.124324999	2013-07-15 14:07:16.124324999	2012-12-28 14:22:09.877411421	2012-12-28 14:22:09.877411421	2013-07-15 14:07:16.124324999
+2013-04-10 00:43:46.854731546	2012-12-31 12:21:13.731274757	2013-07-18 12:06:19.978188335	2013-07-18 12:06:19.978188335	2012-12-31 12:21:13.731274757	2012-12-31 12:21:13.731274757	2013-07-18 12:06:19.978188335
+2021-09-24 03:18:32.413655165	2021-06-16 15:55:59.290198376	2022-01-01 13:41:05.537111954	2022-01-01 13:41:05.537111954	2021-06-16 15:55:59.290198376	2021-06-16 15:55:59.290198376	2022-01-01 13:41:05.537111954
+2024-11-11 16:42:41.101	2024-08-04 06:20:07.977543211	2025-02-19 04:05:14.224456789	2025-02-19 04:05:14.224456789	2024-08-04 06:20:07.977543211	2024-08-04 06:20:07.977543211	2025-02-19 04:05:14.224456789
+4143-07-08 10:53:27.252802259	4143-03-30 23:30:54.12934547	4143-10-15 22:16:00.376259048	4143-10-15 22:16:00.376259048	4143-03-30 23:30:54.12934547	4143-03-30 23:30:54.12934547	4143-10-15 22:16:00.376259048
+4966-12-04 09:30:55.202	4966-08-26 23:08:22.078543211	4967-03-13 21:53:28.325456789	4967-03-13 21:53:28.325456789	4966-08-26 23:08:22.078543211	4966-08-26 23:08:22.078543211	4967-03-13 21:53:28.325456789
+5339-02-01 14:10:01.085678691	5338-10-25 03:47:27.962221902	5339-05-12 02:32:34.20913548	5339-05-12 02:32:34.20913548	5338-10-25 03:47:27.962221902	5338-10-25 03:47:27.962221902	5339-05-12 02:32:34.20913548
+5344-10-04 18:40:08.165	5344-06-27 07:17:35.041543211	5345-01-12 05:02:41.288456789	5345-01-12 05:02:41.288456789	5344-06-27 07:17:35.041543211	5344-06-27 07:17:35.041543211	5345-01-12 05:02:41.288456789
+5397-07-13 07:12:32.000896438	5397-04-04 19:49:58.877439649	5397-10-20 18:35:05.124353227	5397-10-20 18:35:05.124353227	5397-04-04 19:49:58.877439649	5397-04-04 19:49:58.877439649	5397-10-20 18:35:05.124353227
+5966-07-09 03:30:50.597	5966-03-31 16:08:17.473543211	5966-10-16 14:53:23.720456789	5966-10-16 14:53:23.720456789	5966-03-31 16:08:17.473543211	5966-03-31 16:08:17.473543211	5966-10-16 14:53:23.720456789
+6229-06-28 02:54:28.970117179	6229-03-20 15:31:55.84666039	6229-10-05 14:17:02.093573968	6229-10-05 14:17:02.093573968	6229-03-20 15:31:55.84666039	6229-03-20 15:31:55.84666039	6229-10-05 14:17:02.093573968
+6482-04-27 12:07:38.073915413	6482-01-17 23:45:04.950458624	6482-08-04 23:30:11.197372202	6482-08-04 23:30:11.197372202	6482-01-17 23:45:04.950458624	6482-01-17 23:45:04.950458624	6482-08-04 23:30:11.197372202
+6631-11-13 16:31:29.702202248	6631-08-06 06:08:56.578745459	6632-02-21 03:54:02.825659037	6632-02-21 03:54:02.825659037	6631-08-06 06:08:56.578745459	6631-08-06 06:08:56.578745459	6632-02-21 03:54:02.825659037
+6705-09-28 18:27:28.000845672	6705-06-21 07:04:54.877388883	6706-01-06 04:50:01.124302461	6706-01-06 04:50:01.124302461	6705-06-21 07:04:54.877388883	6705-06-21 07:04:54.877388883	6706-01-06 04:50:01.124302461
+6731-02-12 08:12:48.287783702	6730-11-04 20:50:15.164326913	6731-05-22 20:35:21.411240491	6731-05-22 20:35:21.411240491	6730-11-04 20:50:15.164326913	6730-11-04 20:50:15.164326913	6731-05-22 20:35:21.411240491
+7160-12-02 06:00:24.81200852	7160-08-24 19:37:51.688551731	7161-03-11 17:22:57.935465309	7161-03-11 17:22:57.935465309	7160-08-24 19:37:51.688551731	7160-08-24 19:37:51.688551731	7161-03-11 17:22:57.935465309
+7409-09-07 23:33:32.459349602	7409-05-31 12:10:59.335892813	7409-12-16 09:56:05.582806391	7409-12-16 09:56:05.582806391	7409-05-31 12:10:59.335892813	7409-05-31 12:10:59.335892813	7409-12-16 09:56:05.582806391
+7503-06-23 23:14:17.486	7503-03-16 11:51:44.362543211	7503-10-01 10:36:50.609456789	7503-10-01 10:36:50.609456789	7503-03-16 11:51:44.362543211	7503-03-16 11:51:44.362543211	7503-10-01 10:36:50.609456789
+8422-07-22 03:21:45.745036084	8422-04-13 15:59:12.621579295	8422-10-29 14:44:18.868492873	8422-10-29 14:44:18.868492873	8422-04-13 15:59:12.621579295	8422-04-13 15:59:12.621579295	8422-10-29 14:44:18.868492873
+8521-01-16 20:42:05.668832388	8520-10-09 10:19:32.545375599	8521-04-26 09:04:38.792289177	8521-04-26 09:04:38.792289177	8520-10-09 10:19:32.545375599	8520-10-09 10:19:32.545375599	8521-04-26 09:04:38.792289177
+9075-06-13 16:20:09.218517797	9075-03-06 03:57:36.095061008	9075-09-21 03:42:42.341974586	9075-09-21 03:42:42.341974586	9075-03-06 03:57:36.095061008	9075-03-06 03:57:36.095061008	9075-09-21 03:42:42.341974586
+9209-11-11 04:08:58.223768453	9209-08-03 17:46:25.100311664	9210-02-18 15:31:31.347225242	9210-02-18 15:31:31.347225242	9209-08-03 17:46:25.100311664	9209-08-03 17:46:25.100311664	9210-02-18 15:31:31.347225242
+9403-01-09 18:12:33.547	9402-10-02 07:50:00.423543211	9403-04-19 06:35:06.670456789	9403-04-19 06:35:06.670456789	9402-10-02 07:50:00.423543211	9402-10-02 07:50:00.423543211	9403-04-19 06:35:06.670456789
+PREHOOK: query: explain
+select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: interval_arithmetic_1
+                  Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: COMPLETE
+                  Select Operator
+                    expressions: 109 20:30:40.246913578 (type: interval_day_time), 89 02:14:26.000000000 (type: interval_day_time)
+                    outputColumnNames: _col0, _col1
+                    Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                    Limit
+                      Number of rows: 2
+                      Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                      File Output Operator
+                        compressed: false
+                        Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                        table:
+                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+            Execution mode: vectorized
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 2
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+_c0	_c1
+109 20:30:40.246913578	89 02:14:26.000000000
+109 20:30:40.246913578	89 02:14:26.000000000
+PREHOOK: query: drop table interval_arithmetic_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@interval_arithmetic_1
+PREHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: query: drop table interval_arithmetic_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@interval_arithmetic_1
+POSTHOOK: Output: default@interval_arithmetic_1
diff --git a/ql/src/test/results/clientpositive/tez/vectorized_casts.q.out b/ql/src/test/results/clientpositive/tez/vectorized_casts.q.out
index b7b17de781..cf6f4c7b0c 100644
--- a/ql/src/test/results/clientpositive/tez/vectorized_casts.q.out
+++ b/ql/src/test/results/clientpositive/tez/vectorized_casts.q.out
@@ -353,18 +353,18 @@ true	NULL	true	true	true	NULL	true	false	true	true	11	NULL	-64615982	1803053750
 true	NULL	true	true	true	NULL	true	false	true	true	8	NULL	890988972	-1862301000	8	NULL	1	15	NULL	NULL	8	8	8	8.0	NULL	8.90988972E8	-1.862301E9	8.0	NULL	1.0	15.892	NULL	NULL	8.9098899E8	NULL	1969-12-31 16:00:00.008	NULL	1970-01-10 23:29:48.972	1969-12-10 02:41:39	1969-12-31 16:00:08	NULL	1969-12-31 16:00:00.001	1969-12-31 16:00:00	1969-12-31 16:00:15.892	NULL	NULL	8	NULL	890988972	-1862301000	8.0	NULL	TRUE	0	1969-12-31 16:00:15.892	XylAH4	XylAH4	XylAH4	8.0	1.781977944E9	0.9893582466233818	8.90988973E8
 true	NULL	true	true	true	NULL	true	false	true	true	8	NULL	930867246	1205399250	8	NULL	1	15	NULL	NULL	8	8	8	8.0	NULL	9.30867246E8	1.20539925E9	8.0	NULL	1.0	15.892	NULL	NULL	9.3086726E8	NULL	1969-12-31 16:00:00.008	NULL	1970-01-11 10:34:27.246	1970-01-14 14:49:59.25	1969-12-31 16:00:08	NULL	1969-12-31 16:00:00.001	1969-12-31 16:00:00	1969-12-31 16:00:15.892	NULL	NULL	8	NULL	930867246	1205399250	8.0	NULL	TRUE	0	1969-12-31 16:00:15.892	c1V8o1A	c1V8o1A	c1V8o1A	8.0	1.861734492E9	0.9893582466233818	9.30867247E8
 true	true	NULL	true	true	true	NULL	false	true	NULL	-14	-7196	NULL	-1552199500	-14	-7196	NULL	11	NULL	NULL	-14	-14	-14	-14.0	-7196.0	NULL	-1.5521995E9	-14.0	-7196.0	NULL	11.065	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.986	1969-12-31 15:59:52.804	NULL	1969-12-13 16:50:00.5	1969-12-31 15:59:46	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:11.065	NULL	NULL	-14	-7196	NULL	-1552199500	-14.0	-7196.0	NULL	0	1969-12-31 16:00:11.065	NULL	NULL	NULL	-14.0	NULL	-0.9906073556948704	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-21	-7196	NULL	1542429000	-21	-7196	NULL	-4	NULL	NULL	-21	-21	-21	-21.0	-7196.0	NULL	1.542429E9	-21.0	-7196.0	NULL	-4.1	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.979	1969-12-31 15:59:52.804	NULL	1970-01-18 12:27:09	1969-12-31 15:59:39	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:55.9	NULL	NULL	-21	-7196	NULL	1542429000	-21.0	-7196.0	NULL	0	1969-12-31 15:59:55.9	NULL	NULL	NULL	-21.0	NULL	-0.8366556385360561	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-24	-7196	NULL	829111000	-24	-7196	NULL	-6	NULL	NULL	-24	-24	-24	-24.0	-7196.0	NULL	8.29111E8	-24.0	-7196.0	NULL	-6.855	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.976	1969-12-31 15:59:52.804	NULL	1970-01-10 06:18:31	1969-12-31 15:59:36	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.145	NULL	NULL	-24	-7196	NULL	829111000	-24.0	-7196.0	NULL	0	1969-12-31 15:59:53.145	NULL	NULL	NULL	-24.0	NULL	0.9055783620066238	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-21	-7196	NULL	1542429000	-21	-7196	NULL	-5	NULL	NULL	-21	-21	-21	-21.0	-7196.0	NULL	1.542429E9	-21.0	-7196.0	NULL	-4.1	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.979	1969-12-31 15:59:52.804	NULL	1970-01-18 12:27:09	1969-12-31 15:59:39	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:55.9	NULL	NULL	-21	-7196	NULL	1542429000	-21.0	-7196.0	NULL	0	1969-12-31 15:59:55.9	NULL	NULL	NULL	-21.0	NULL	-0.8366556385360561	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-24	-7196	NULL	829111000	-24	-7196	NULL	-7	NULL	NULL	-24	-24	-24	-24.0	-7196.0	NULL	8.29111E8	-24.0	-7196.0	NULL	-6.855	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.976	1969-12-31 15:59:52.804	NULL	1970-01-10 06:18:31	1969-12-31 15:59:36	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.145	NULL	NULL	-24	-7196	NULL	829111000	-24.0	-7196.0	NULL	0	1969-12-31 15:59:53.145	NULL	NULL	NULL	-24.0	NULL	0.9055783620066238	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-30	-200	NULL	1429852250	-30	-200	NULL	12	NULL	NULL	-30	-30	-30	-30.0	-200.0	NULL	1.42985225E9	-30.0	-200.0	NULL	12.935	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.97	1969-12-31 15:59:59.8	NULL	1970-01-17 05:10:52.25	1969-12-31 15:59:30	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 16:00:12.935	NULL	NULL	-30	-200	NULL	1429852250	-30.0	-200.0	NULL	0	1969-12-31 16:00:12.935	NULL	NULL	NULL	-30.0	NULL	0.9880316240928618	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	-2006216750	-36	-200	NULL	-14	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	-2.00621675E9	-36.0	-200.0	NULL	-14.252	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1969-12-08 10:43:03.25	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.748	NULL	NULL	-36	-200	NULL	-2006216750	-36.0	-200.0	NULL	0	1969-12-31 15:59:45.748	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	1599879000	-36	-200	NULL	-6	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	1.599879E9	-36.0	-200.0	NULL	-6.183	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1970-01-19 04:24:39	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.817	NULL	NULL	-36	-200	NULL	1599879000	-36.0	-200.0	NULL	0	1969-12-31 15:59:53.817	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-38	15601	NULL	-1858689000	-38	15601	NULL	-1	NULL	NULL	-38	-38	-38	-38.0	15601.0	NULL	-1.858689E9	-38.0	15601.0	NULL	-1.386	NULL	NULL	NULL	15601.0	1969-12-31 15:59:59.962	1969-12-31 16:00:15.601	NULL	1969-12-10 03:41:51	1969-12-31 15:59:22	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:58.614	NULL	NULL	-38	15601	NULL	-1858689000	-38.0	15601.0	NULL	0	1969-12-31 15:59:58.614	NULL	NULL	NULL	-38.0	NULL	-0.2963685787093853	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	-2006216750	-36	-200	NULL	-15	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	-2.00621675E9	-36.0	-200.0	NULL	-14.252	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1969-12-08 10:43:03.25	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.748	NULL	NULL	-36	-200	NULL	-2006216750	-36.0	-200.0	NULL	0	1969-12-31 15:59:45.748	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	1599879000	-36	-200	NULL	-7	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	1.599879E9	-36.0	-200.0	NULL	-6.183	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1970-01-19 04:24:39	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.817	NULL	NULL	-36	-200	NULL	1599879000	-36.0	-200.0	NULL	0	1969-12-31 15:59:53.817	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-38	15601	NULL	-1858689000	-38	15601	NULL	-2	NULL	NULL	-38	-38	-38	-38.0	15601.0	NULL	-1.858689E9	-38.0	15601.0	NULL	-1.3860000000000001	NULL	NULL	NULL	15601.0	1969-12-31 15:59:59.962	1969-12-31 16:00:15.601	NULL	1969-12-10 03:41:51	1969-12-31 15:59:22	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:58.614	NULL	NULL	-38	15601	NULL	-1858689000	-38.0	15601.0	NULL	0	1969-12-31 15:59:58.614	NULL	NULL	NULL	-38.0	NULL	-0.2963685787093853	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-5	15601	NULL	612416000	-5	15601	NULL	4	NULL	NULL	-5	-5	-5	-5.0	15601.0	NULL	6.12416E8	-5.0	15601.0	NULL	4.679	NULL	NULL	NULL	15601.0	1969-12-31 15:59:59.995	1969-12-31 16:00:15.601	NULL	1970-01-07 18:06:56	1969-12-31 15:59:55	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 16:00:04.679	NULL	NULL	-5	15601	NULL	612416000	-5.0	15601.0	NULL	0	1969-12-31 16:00:04.679	NULL	NULL	NULL	-5.0	NULL	0.9589242746631385	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-50	-7196	NULL	-1031187250	-50	-7196	NULL	-5	NULL	NULL	-50	-50	-50	-50.0	-7196.0	NULL	-1.03118725E9	-50.0	-7196.0	NULL	-5.267	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.95	1969-12-31 15:59:52.804	NULL	1969-12-19 17:33:32.75	1969-12-31 15:59:10	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:54.733	NULL	NULL	-50	-7196	NULL	-1031187250	-50.0	-7196.0	NULL	0	1969-12-31 15:59:54.733	NULL	NULL	NULL	-50.0	NULL	0.26237485370392877	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-50	-7196	NULL	-1031187250	-50	-7196	NULL	-6	NULL	NULL	-50	-50	-50	-50.0	-7196.0	NULL	-1.03118725E9	-50.0	-7196.0	NULL	-5.267	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.95	1969-12-31 15:59:52.804	NULL	1969-12-19 17:33:32.75	1969-12-31 15:59:10	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:54.733	NULL	NULL	-50	-7196	NULL	-1031187250	-50.0	-7196.0	NULL	0	1969-12-31 15:59:54.733	NULL	NULL	NULL	-50.0	NULL	0.26237485370392877	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-59	-7196	NULL	-1604890000	-59	-7196	NULL	13	NULL	NULL	-59	-59	-59	-59.0	-7196.0	NULL	-1.60489E9	-59.0	-7196.0	NULL	13.15	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.941	1969-12-31 15:59:52.804	NULL	1969-12-13 02:11:50	1969-12-31 15:59:01	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:13.15	NULL	NULL	-59	-7196	NULL	-1604890000	-59.0	-7196.0	NULL	0	1969-12-31 16:00:13.15	NULL	NULL	NULL	-59.0	NULL	-0.6367380071391379	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-60	-7196	NULL	1516314750	-60	-7196	NULL	-7	NULL	NULL	-60	-60	-60	-60.0	-7196.0	NULL	1.51631475E9	-60.0	-7196.0	NULL	-7.592	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.94	1969-12-31 15:59:52.804	NULL	1970-01-18 05:11:54.75	1969-12-31 15:59:00	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:52.408	NULL	NULL	-60	-7196	NULL	1516314750	-60.0	-7196.0	NULL	0	1969-12-31 15:59:52.408	NULL	NULL	NULL	-60.0	NULL	0.3048106211022167	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-60	-7196	NULL	1516314750	-60	-7196	NULL	-8	NULL	NULL	-60	-60	-60	-60.0	-7196.0	NULL	1.51631475E9	-60.0	-7196.0	NULL	-7.592	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.94	1969-12-31 15:59:52.804	NULL	1970-01-18 05:11:54.75	1969-12-31 15:59:00	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:52.408	NULL	NULL	-60	-7196	NULL	1516314750	-60.0	-7196.0	NULL	0	1969-12-31 15:59:52.408	NULL	NULL	NULL	-60.0	NULL	0.3048106211022167	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-8	-7196	NULL	-1849991500	-8	-7196	NULL	3	NULL	NULL	-8	-8	-8	-8.0	-7196.0	NULL	-1.8499915E9	-8.0	-7196.0	NULL	3.136	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.992	1969-12-31 15:59:52.804	NULL	1969-12-10 06:06:48.5	1969-12-31 15:59:52	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:03.136	NULL	NULL	-8	-7196	NULL	-1849991500	-8.0	-7196.0	NULL	0	1969-12-31 16:00:03.136	NULL	NULL	NULL	-8.0	NULL	-0.9893582466233818	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	20	15601	NULL	-362433250	20	15601	NULL	-14	NULL	NULL	20	20	20	20.0	15601.0	NULL	-3.6243325E8	20.0	15601.0	NULL	-14.871	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.02	1969-12-31 16:00:15.601	NULL	1969-12-27 11:19:26.75	1969-12-31 16:00:20	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.129	NULL	NULL	20	15601	NULL	-362433250	20.0	15601.0	NULL	0	1969-12-31 15:59:45.129	NULL	NULL	NULL	20.0	NULL	0.9129452507276277	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	48	15601	NULL	-795361000	48	15601	NULL	-9	NULL	NULL	48	48	48	48.0	15601.0	NULL	-7.95361E8	48.0	15601.0	NULL	-9.765	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.048	1969-12-31 16:00:15.601	NULL	1969-12-22 11:03:59	1969-12-31 16:00:48	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:50.235	NULL	NULL	48	15601	NULL	-795361000	48.0	15601.0	NULL	0	1969-12-31 15:59:50.235	NULL	NULL	NULL	48.0	NULL	-0.7682546613236668	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	20	15601	NULL	-362433250	20	15601	NULL	-15	NULL	NULL	20	20	20	20.0	15601.0	NULL	-3.6243325E8	20.0	15601.0	NULL	-14.871	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.02	1969-12-31 16:00:15.601	NULL	1969-12-27 11:19:26.75	1969-12-31 16:00:20	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.129	NULL	NULL	20	15601	NULL	-362433250	20.0	15601.0	NULL	0	1969-12-31 15:59:45.129	NULL	NULL	NULL	20.0	NULL	0.9129452507276277	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	48	15601	NULL	-795361000	48	15601	NULL	-10	NULL	NULL	48	48	48	48.0	15601.0	NULL	-7.95361E8	48.0	15601.0	NULL	-9.765	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.048	1969-12-31 16:00:15.601	NULL	1969-12-22 11:03:59	1969-12-31 16:00:48	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:50.235	NULL	NULL	48	15601	NULL	-795361000	48.0	15601.0	NULL	0	1969-12-31 15:59:50.235	NULL	NULL	NULL	48.0	NULL	-0.7682546613236668	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	5	-7196	NULL	-1015607500	5	-7196	NULL	10	NULL	NULL	5	5	5	5.0	-7196.0	NULL	-1.0156075E9	5.0	-7196.0	NULL	10.973	NULL	NULL	NULL	-7196.0	1969-12-31 16:00:00.005	1969-12-31 15:59:52.804	NULL	1969-12-19 21:53:12.5	1969-12-31 16:00:05	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:10.973	NULL	NULL	5	-7196	NULL	-1015607500	5.0	-7196.0	NULL	0	1969-12-31 16:00:10.973	NULL	NULL	NULL	5.0	NULL	-0.9589242746631385	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	59	-7196	NULL	-1137754500	59	-7196	NULL	10	NULL	NULL	59	59	59	59.0	-7196.0	NULL	-1.1377545E9	59.0	-7196.0	NULL	10.956	NULL	NULL	NULL	-7196.0	1969-12-31 16:00:00.059	1969-12-31 15:59:52.804	NULL	1969-12-18 11:57:25.5	1969-12-31 16:00:59	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:10.956	NULL	NULL	59	-7196	NULL	-1137754500	59.0	-7196.0	NULL	0	1969-12-31 16:00:10.956	NULL	NULL	NULL	59.0	NULL	0.6367380071391379	NULL
diff --git a/ql/src/test/results/clientpositive/tez/vectorized_timestamp.q.out b/ql/src/test/results/clientpositive/tez/vectorized_timestamp.q.out
new file mode 100644
index 0000000000..5382865e2e
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/vectorized_timestamp.q.out
@@ -0,0 +1,157 @@
+PREHOOK: query: DROP TABLE IF EXISTS test
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE IF EXISTS test
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE test(ts TIMESTAMP) STORED AS ORC
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test
+POSTHOOK: query: CREATE TABLE test(ts TIMESTAMP) STORED AS ORC
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test
+PREHOOK: query: INSERT INTO TABLE test VALUES ('0001-01-01 00:00:00.000000000'), ('9999-12-31 23:59:59.999999999')
+PREHOOK: type: QUERY
+PREHOOK: Input: default@values__tmp__table__1
+PREHOOK: Output: default@test
+POSTHOOK: query: INSERT INTO TABLE test VALUES ('0001-01-01 00:00:00.000000000'), ('9999-12-31 23:59:59.999999999')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@values__tmp__table__1
+POSTHOOK: Output: default@test
+POSTHOOK: Lineage: test.ts EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+PREHOOK: query: EXPLAIN
+SELECT ts FROM test
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT ts FROM test
+POSTHOOK: type: QUERY
+Plan optimized by CBO.
+
+Stage-0
+  Fetch Operator
+    limit:-1
+    Stage-1
+      Map 1
+      File Output Operator [FS_2]
+        Select Operator [SEL_1] (rows=2 width=40)
+          Output:["_col0"]
+          TableScan [TS_0] (rows=2 width=40)
+            default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+
+PREHOOK: query: SELECT ts FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT ts FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00
+9999-12-31 23:59:59.999999999
+PREHOOK: query: EXPLAIN
+SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: type: QUERY
+Plan optimized by CBO.
+
+Vertex dependency in root stage
+Reducer 2 <- Map 1 (SIMPLE_EDGE)
+
+Stage-0
+  Fetch Operator
+    limit:-1
+    Stage-1
+      Reducer 2
+      File Output Operator [FS_6]
+        Select Operator [SEL_5] (rows=1 width=80)
+          Output:["_col0","_col1","_col2"]
+          Group By Operator [GBY_4] (rows=1 width=80)
+            Output:["_col0","_col1"],aggregations:["min(VALUE._col0)","max(VALUE._col1)"]
+          <-Map 1 [SIMPLE_EDGE]
+            SHUFFLE [RS_3]
+              Group By Operator [GBY_2] (rows=1 width=80)
+                Output:["_col0","_col1"],aggregations:["min(ts)","max(ts)"]
+                Select Operator [SEL_1] (rows=2 width=40)
+                  Output:["ts"]
+                  TableScan [TS_0] (rows=2 width=40)
+                    default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+
+PREHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
+PREHOOK: query: EXPLAIN
+SELECT ts FROM test
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT ts FROM test
+POSTHOOK: type: QUERY
+Plan optimized by CBO.
+
+Stage-0
+  Fetch Operator
+    limit:-1
+    Stage-1
+      Map 1 vectorized
+      File Output Operator [FS_4]
+        Select Operator [OP_3] (rows=2 width=40)
+          Output:["_col0"]
+          TableScan [TS_0] (rows=2 width=40)
+            default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+
+PREHOOK: query: SELECT ts FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT ts FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00
+9999-12-31 23:59:59.999999999
+PREHOOK: query: EXPLAIN
+SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: type: QUERY
+Plan optimized by CBO.
+
+Vertex dependency in root stage
+Reducer 2 <- Map 1 (SIMPLE_EDGE)
+
+Stage-0
+  Fetch Operator
+    limit:-1
+    Stage-1
+      Reducer 2 vectorized
+      File Output Operator [FS_6]
+        Select Operator [SEL_5] (rows=1 width=80)
+          Output:["_col0","_col1","_col2"]
+          Group By Operator [OP_9] (rows=1 width=80)
+            Output:["_col0","_col1"],aggregations:["min(VALUE._col0)","max(VALUE._col1)"]
+          <-Map 1 [SIMPLE_EDGE] vectorized
+            SHUFFLE [RS_3]
+              Group By Operator [OP_8] (rows=1 width=80)
+                Output:["_col0","_col1"],aggregations:["min(ts)","max(ts)"]
+                Select Operator [OP_7] (rows=2 width=40)
+                  Output:["ts"]
+                  TableScan [TS_0] (rows=2 width=40)
+                    default@test,test,Tbl:COMPLETE,Col:NONE,Output:["ts"]
+
+PREHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+0001-01-01 00:00:00	9999-12-31 23:59:59.999999999	3652060 23:59:59.999999999
diff --git a/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out b/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out
new file mode 100644
index 0000000000..cd8111d4a6
--- /dev/null
+++ b/ql/src/test/results/clientpositive/vector_interval_arithmetic.q.out
@@ -0,0 +1,1027 @@
+PREHOOK: query: create table unique_timestamps (tsval timestamp) STORED AS TEXTFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@unique_timestamps
+POSTHOOK: query: create table unique_timestamps (tsval timestamp) STORED AS TEXTFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@unique_timestamps
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/timestamps.txt' OVERWRITE INTO TABLE unique_timestamps
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@unique_timestamps
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/timestamps.txt' OVERWRITE INTO TABLE unique_timestamps
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@unique_timestamps
+PREHOOK: query: create table interval_arithmetic_1 (dateval date, tsval timestamp) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: query: create table interval_arithmetic_1 (dateval date, tsval timestamp) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@interval_arithmetic_1
+PREHOOK: query: insert overwrite table interval_arithmetic_1
+  select cast(tsval as date), tsval from unique_timestamps
+PREHOOK: type: QUERY
+PREHOOK: Input: default@unique_timestamps
+PREHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: query: insert overwrite table interval_arithmetic_1
+  select cast(tsval as date), tsval from unique_timestamps
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@unique_timestamps
+POSTHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: Lineage: interval_arithmetic_1.dateval EXPRESSION [(unique_timestamps)unique_timestamps.FieldSchema(name:tsval, type:timestamp, comment:null), ]
+POSTHOOK: Lineage: interval_arithmetic_1.tsval SIMPLE [(unique_timestamps)unique_timestamps.FieldSchema(name:tsval, type:timestamp, comment:null), ]
+_c0	tsval
+PREHOOK: query: -- interval year-month arithmetic
+explain
+select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: -- interval year-month arithmetic
+explain
+select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: dateval (type: date), (dateval - 2-2) (type: date), (dateval - -2-2) (type: date), (dateval + 2-2) (type: date), (dateval + -2-2) (type: date), (-2-2 + dateval) (type: date), (2-2 + dateval) (type: date)
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+              Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col0 (type: date)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col1 (type: date), _col2 (type: date), _col3 (type: date), _col4 (type: date), _col5 (type: date), _col6 (type: date)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: date), VALUE._col1 (type: date), VALUE._col2 (type: date), VALUE._col3 (type: date), VALUE._col4 (type: date), VALUE._col5 (type: date)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+          Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  dateval - interval '2-2' year to month,
+  dateval - interval '-2-2' year to month,
+  dateval + interval '2-2' year to month,
+  dateval + interval '-2-2' year to month,
+  - interval '2-2' year to month + dateval,
+  interval '2-2' year to month + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	c1	c2	c3	c4	c5	c6
+0004-09-22	0002-07-22	0006-11-22	0006-11-22	0002-07-22	0002-07-22	0006-11-22
+0528-10-27	0526-08-27	0530-12-27	0530-12-27	0526-08-27	0526-08-27	0530-12-27
+1319-02-02	1316-12-02	1321-04-02	1321-04-02	1316-12-02	1316-12-02	1321-04-02
+1404-07-23	1402-05-23	1406-09-23	1406-09-23	1402-05-23	1402-05-23	1406-09-23
+1815-05-06	1813-03-06	1817-07-06	1817-07-06	1813-03-06	1813-03-06	1817-07-06
+1883-04-17	1881-02-17	1885-06-17	1885-06-17	1881-02-17	1881-02-17	1885-06-17
+1966-08-16	1964-06-16	1968-10-16	1968-10-16	1964-06-16	1964-06-16	1968-10-16
+1973-04-17	1971-02-17	1975-06-17	1975-06-17	1971-02-17	1971-02-17	1975-06-17
+1974-10-04	1972-08-04	1976-12-04	1976-12-04	1972-08-04	1972-08-04	1976-12-04
+1976-03-03	1974-01-03	1978-05-03	1978-05-03	1974-01-03	1974-01-03	1978-05-03
+1976-05-06	1974-03-06	1978-07-06	1978-07-06	1974-03-06	1974-03-06	1978-07-06
+1978-08-05	1976-06-05	1980-10-05	1980-10-05	1976-06-05	1976-06-05	1980-10-05
+1981-04-25	1979-02-25	1983-06-25	1983-06-25	1979-02-25	1979-02-25	1983-06-25
+1981-11-15	1979-09-15	1984-01-15	1984-01-15	1979-09-15	1979-09-15	1984-01-15
+1985-07-20	1983-05-20	1987-09-20	1987-09-20	1983-05-20	1983-05-20	1987-09-20
+1985-11-18	1983-09-18	1988-01-18	1988-01-18	1983-09-18	1983-09-18	1988-01-18
+1987-02-21	1984-12-21	1989-04-21	1989-04-21	1984-12-21	1984-12-21	1989-04-21
+1987-05-28	1985-03-28	1989-07-28	1989-07-28	1985-03-28	1985-03-28	1989-07-28
+1998-10-16	1996-08-16	2000-12-16	2000-12-16	1996-08-16	1996-08-16	2000-12-16
+1999-10-03	1997-08-03	2001-12-03	2001-12-03	1997-08-03	1997-08-03	2001-12-03
+2000-12-18	1998-10-18	2003-02-18	2003-02-18	1998-10-18	1998-10-18	2003-02-18
+2002-05-10	2000-03-10	2004-07-10	2004-07-10	2000-03-10	2000-03-10	2004-07-10
+2003-09-23	2001-07-23	2005-11-23	2005-11-23	2001-07-23	2001-07-23	2005-11-23
+2004-03-07	2002-01-07	2006-05-07	2006-05-07	2002-01-07	2002-01-07	2006-05-07
+2007-02-09	2004-12-09	2009-04-09	2009-04-09	2004-12-09	2004-12-09	2009-04-09
+2009-01-21	2006-11-21	2011-03-21	2011-03-21	2006-11-21	2006-11-21	2011-03-21
+2010-04-08	2008-02-08	2012-06-08	2012-06-08	2008-02-08	2008-02-08	2012-06-08
+2013-04-07	2011-02-07	2015-06-07	2015-06-07	2011-02-07	2011-02-07	2015-06-07
+2013-04-10	2011-02-10	2015-06-10	2015-06-10	2011-02-10	2011-02-10	2015-06-10
+2021-09-24	2019-07-24	2023-11-24	2023-11-24	2019-07-24	2019-07-24	2023-11-24
+2024-11-11	2022-09-11	2027-01-11	2027-01-11	2022-09-11	2022-09-11	2027-01-11
+4143-07-08	4141-05-08	4145-09-08	4145-09-08	4141-05-08	4141-05-08	4145-09-08
+4966-12-04	4964-10-04	4969-02-04	4969-02-04	4964-10-04	4964-10-04	4969-02-04
+5339-02-01	5336-12-01	5341-04-01	5341-04-01	5336-12-01	5336-12-01	5341-04-01
+5344-10-04	5342-08-04	5346-12-04	5346-12-04	5342-08-04	5342-08-04	5346-12-04
+5397-07-13	5395-05-13	5399-09-13	5399-09-13	5395-05-13	5395-05-13	5399-09-13
+5966-07-09	5964-05-09	5968-09-09	5968-09-09	5964-05-09	5964-05-09	5968-09-09
+6229-06-28	6227-04-28	6231-08-28	6231-08-28	6227-04-28	6227-04-28	6231-08-28
+6482-04-27	6480-02-27	6484-06-27	6484-06-27	6480-02-27	6480-02-27	6484-06-27
+6631-11-13	6629-09-13	6634-01-13	6634-01-13	6629-09-13	6629-09-13	6634-01-13
+6705-09-28	6703-07-28	6707-11-28	6707-11-28	6703-07-28	6703-07-28	6707-11-28
+6731-02-12	6728-12-12	6733-04-12	6733-04-12	6728-12-12	6728-12-12	6733-04-12
+7160-12-02	7158-10-02	7163-02-02	7163-02-02	7158-10-02	7158-10-02	7163-02-02
+7409-09-07	7407-07-07	7411-11-07	7411-11-07	7407-07-07	7407-07-07	7411-11-07
+7503-06-23	7501-04-23	7505-08-23	7505-08-23	7501-04-23	7501-04-23	7505-08-23
+8422-07-22	8420-05-22	8424-09-22	8424-09-22	8420-05-22	8420-05-22	8424-09-22
+8521-01-16	8518-11-16	8523-03-16	8523-03-16	8518-11-16	8518-11-16	8523-03-16
+9075-06-13	9073-04-13	9077-08-13	9077-08-13	9073-04-13	9073-04-13	9077-08-13
+9209-11-11	9207-09-11	9212-01-11	9212-01-11	9207-09-11	9207-09-11	9212-01-11
+9403-01-09	9400-11-09	9405-03-09	9405-03-09	9400-11-09	9400-11-09	9405-03-09
+PREHOOK: query: explain
+select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: dateval (type: date), (dateval - 1999-06-07) (type: interval_day_time), (1999-06-07 - dateval) (type: interval_day_time), (dateval - dateval) (type: interval_day_time)
+              outputColumnNames: _col0, _col1, _col2, _col3
+              Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col0 (type: date)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col1 (type: interval_day_time), _col2 (type: interval_day_time), _col3 (type: interval_day_time)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: interval_day_time), VALUE._col1 (type: interval_day_time), VALUE._col2 (type: interval_day_time)
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  dateval - date '1999-06-07',
+  date '1999-06-07' - dateval,
+  dateval - dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	c1	c2	c3
+0004-09-22	-728552 23:00:00.000000000	728552 23:00:00.000000000	0 00:00:00.000000000
+0528-10-27	-537126 23:00:00.000000000	537126 23:00:00.000000000	0 00:00:00.000000000
+1319-02-02	-248481 23:00:00.000000000	248481 23:00:00.000000000	0 00:00:00.000000000
+1404-07-23	-217263 23:00:00.000000000	217263 23:00:00.000000000	0 00:00:00.000000000
+1815-05-06	-67236 23:00:00.000000000	67236 23:00:00.000000000	0 00:00:00.000000000
+1883-04-17	-42418 23:00:00.000000000	42418 23:00:00.000000000	0 00:00:00.000000000
+1966-08-16	-11983 00:00:00.000000000	11983 00:00:00.000000000	0 00:00:00.000000000
+1973-04-17	-9546 23:00:00.000000000	9546 23:00:00.000000000	0 00:00:00.000000000
+1974-10-04	-9012 00:00:00.000000000	9012 00:00:00.000000000	0 00:00:00.000000000
+1976-03-03	-8495 23:00:00.000000000	8495 23:00:00.000000000	0 00:00:00.000000000
+1976-05-06	-8432 00:00:00.000000000	8432 00:00:00.000000000	0 00:00:00.000000000
+1978-08-05	-7611 00:00:00.000000000	7611 00:00:00.000000000	0 00:00:00.000000000
+1981-04-25	-6616 23:00:00.000000000	6616 23:00:00.000000000	0 00:00:00.000000000
+1981-11-15	-6412 23:00:00.000000000	6412 23:00:00.000000000	0 00:00:00.000000000
+1985-07-20	-5070 00:00:00.000000000	5070 00:00:00.000000000	0 00:00:00.000000000
+1985-11-18	-4948 23:00:00.000000000	4948 23:00:00.000000000	0 00:00:00.000000000
+1987-02-21	-4488 23:00:00.000000000	4488 23:00:00.000000000	0 00:00:00.000000000
+1987-05-28	-4393 00:00:00.000000000	4393 00:00:00.000000000	0 00:00:00.000000000
+1998-10-16	-234 00:00:00.000000000	234 00:00:00.000000000	0 00:00:00.000000000
+1999-10-03	118 00:00:00.000000000	-118 00:00:00.000000000	0 00:00:00.000000000
+2000-12-18	560 01:00:00.000000000	-560 01:00:00.000000000	0 00:00:00.000000000
+2002-05-10	1068 00:00:00.000000000	-1068 00:00:00.000000000	0 00:00:00.000000000
+2003-09-23	1569 00:00:00.000000000	-1569 00:00:00.000000000	0 00:00:00.000000000
+2004-03-07	1735 01:00:00.000000000	-1735 01:00:00.000000000	0 00:00:00.000000000
+2007-02-09	2804 01:00:00.000000000	-2804 01:00:00.000000000	0 00:00:00.000000000
+2009-01-21	3516 01:00:00.000000000	-3516 01:00:00.000000000	0 00:00:00.000000000
+2010-04-08	3958 00:00:00.000000000	-3958 00:00:00.000000000	0 00:00:00.000000000
+2013-04-07	5053 00:00:00.000000000	-5053 00:00:00.000000000	0 00:00:00.000000000
+2013-04-10	5056 00:00:00.000000000	-5056 00:00:00.000000000	0 00:00:00.000000000
+2021-09-24	8145 00:00:00.000000000	-8145 00:00:00.000000000	0 00:00:00.000000000
+2024-11-11	9289 01:00:00.000000000	-9289 01:00:00.000000000	0 00:00:00.000000000
+4143-07-08	783111 00:00:00.000000000	-783111 00:00:00.000000000	0 00:00:00.000000000
+4966-12-04	1083855 01:00:00.000000000	-1083855 01:00:00.000000000	0 00:00:00.000000000
+5339-02-01	1219784 01:00:00.000000000	-1219784 01:00:00.000000000	0 00:00:00.000000000
+5344-10-04	1221856 00:00:00.000000000	-1221856 00:00:00.000000000	0 00:00:00.000000000
+5397-07-13	1241131 00:00:00.000000000	-1241131 00:00:00.000000000	0 00:00:00.000000000
+5966-07-09	1448949 00:00:00.000000000	-1448949 00:00:00.000000000	0 00:00:00.000000000
+6229-06-28	1544997 00:00:00.000000000	-1544997 00:00:00.000000000	0 00:00:00.000000000
+6482-04-27	1637342 00:00:00.000000000	-1637342 00:00:00.000000000	0 00:00:00.000000000
+6631-11-13	1691962 01:00:00.000000000	-1691962 01:00:00.000000000	0 00:00:00.000000000
+6705-09-28	1718944 00:00:00.000000000	-1718944 00:00:00.000000000	0 00:00:00.000000000
+6731-02-12	1728212 01:00:00.000000000	-1728212 01:00:00.000000000	0 00:00:00.000000000
+7160-12-02	1885195 01:00:00.000000000	-1885195 01:00:00.000000000	0 00:00:00.000000000
+7409-09-07	1976054 00:00:00.000000000	-1976054 00:00:00.000000000	0 00:00:00.000000000
+7503-06-23	2010310 00:00:00.000000000	-2010310 00:00:00.000000000	0 00:00:00.000000000
+8422-07-22	2345998 00:00:00.000000000	-2345998 00:00:00.000000000	0 00:00:00.000000000
+8521-01-16	2381970 01:00:00.000000000	-2381970 01:00:00.000000000	0 00:00:00.000000000
+9075-06-13	2584462 00:00:00.000000000	-2584462 00:00:00.000000000	0 00:00:00.000000000
+9209-11-11	2633556 01:00:00.000000000	-2633556 01:00:00.000000000	0 00:00:00.000000000
+9403-01-09	2704106 01:00:00.000000000	-2704106 01:00:00.000000000	0 00:00:00.000000000
+PREHOOK: query: explain
+select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: tsval (type: timestamp), (tsval - 2-2) (type: timestamp), (tsval - -2-2) (type: timestamp), (tsval + 2-2) (type: timestamp), (tsval + -2-2) (type: timestamp), (-2-2 + tsval) (type: timestamp), (2-2 + tsval) (type: timestamp)
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+              Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col0 (type: timestamp)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col1 (type: timestamp), _col2 (type: timestamp), _col3 (type: timestamp), _col4 (type: timestamp), _col5 (type: timestamp), _col6 (type: timestamp)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: timestamp), VALUE._col0 (type: timestamp), VALUE._col1 (type: timestamp), VALUE._col2 (type: timestamp), VALUE._col3 (type: timestamp), VALUE._col4 (type: timestamp), VALUE._col5 (type: timestamp)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+          Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  tsval,
+  tsval - interval '2-2' year to month,
+  tsval - interval '-2-2' year to month,
+  tsval + interval '2-2' year to month,
+  tsval + interval '-2-2' year to month,
+  - interval '2-2' year to month + tsval,
+  interval '2-2' year to month + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+tsval	c1	c2	c3	c4	c5	c6
+0004-09-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0006-11-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0002-07-22 18:26:29.519542222	0006-11-22 18:26:29.519542222
+0528-10-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0530-12-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0526-08-27 08:15:18.941718273	0530-12-27 08:15:18.941718273
+1319-02-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778	1321-04-02 16:31:57.778	1316-12-02 16:31:57.778	1316-12-02 16:31:57.778	1321-04-02 16:31:57.778
+1404-07-23 15:32:16.059185026	1402-05-23 15:32:16.059185026	1406-09-23 15:32:16.059185026	1406-09-23 15:32:16.059185026	1402-05-23 15:32:16.059185026	1402-05-23 15:32:16.059185026	1406-09-23 15:32:16.059185026
+1815-05-06 00:12:37.543584705	1813-03-06 00:12:37.543584705	1817-07-06 00:12:37.543584705	1817-07-06 00:12:37.543584705	1813-03-06 00:12:37.543584705	1813-03-06 00:12:37.543584705	1817-07-06 00:12:37.543584705
+1883-04-17 04:14:34.647766229	1881-02-17 04:14:34.647766229	1885-06-17 04:14:34.647766229	1885-06-17 04:14:34.647766229	1881-02-17 04:14:34.647766229	1881-02-17 04:14:34.647766229	1885-06-17 04:14:34.647766229
+1966-08-16 13:36:50.183618031	1964-06-16 13:36:50.183618031	1968-10-16 13:36:50.183618031	1968-10-16 13:36:50.183618031	1964-06-16 13:36:50.183618031	1964-06-16 13:36:50.183618031	1968-10-16 13:36:50.183618031
+1973-04-17 06:30:38.596784156	1971-02-17 06:30:38.596784156	1975-06-17 07:30:38.596784156	1975-06-17 07:30:38.596784156	1971-02-17 06:30:38.596784156	1971-02-17 06:30:38.596784156	1975-06-17 07:30:38.596784156
+1974-10-04 17:21:03.989	1972-08-04 17:21:03.989	1976-12-04 16:21:03.989	1976-12-04 16:21:03.989	1972-08-04 17:21:03.989	1972-08-04 17:21:03.989	1976-12-04 16:21:03.989
+1976-03-03 04:54:33.000895162	1974-01-03 04:54:33.000895162	1978-05-03 05:54:33.000895162	1978-05-03 05:54:33.000895162	1974-01-03 04:54:33.000895162	1974-01-03 04:54:33.000895162	1978-05-03 05:54:33.000895162
+1976-05-06 00:42:30.910786948	1974-03-06 00:42:30.910786948	1978-07-06 00:42:30.910786948	1978-07-06 00:42:30.910786948	1974-03-06 00:42:30.910786948	1974-03-06 00:42:30.910786948	1978-07-06 00:42:30.910786948
+1978-08-05 14:41:05.501	1976-06-05 14:41:05.501	1980-10-05 14:41:05.501	1980-10-05 14:41:05.501	1976-06-05 14:41:05.501	1976-06-05 14:41:05.501	1980-10-05 14:41:05.501
+1981-04-25 09:01:12.077192689	1979-02-25 09:01:12.077192689	1983-06-25 10:01:12.077192689	1983-06-25 10:01:12.077192689	1979-02-25 09:01:12.077192689	1979-02-25 09:01:12.077192689	1983-06-25 10:01:12.077192689
+1981-11-15 23:03:10.999338387	1979-09-16 00:03:10.999338387	1984-01-15 23:03:10.999338387	1984-01-15 23:03:10.999338387	1979-09-16 00:03:10.999338387	1979-09-16 00:03:10.999338387	1984-01-15 23:03:10.999338387
+1985-07-20 09:30:11	1983-05-20 09:30:11	1987-09-20 09:30:11	1987-09-20 09:30:11	1983-05-20 09:30:11	1983-05-20 09:30:11	1987-09-20 09:30:11
+1985-11-18 16:37:54	1983-09-18 17:37:54	1988-01-18 16:37:54	1988-01-18 16:37:54	1983-09-18 17:37:54	1983-09-18 17:37:54	1988-01-18 16:37:54
+1987-02-21 19:48:29	1984-12-21 19:48:29	1989-04-21 20:48:29	1989-04-21 20:48:29	1984-12-21 19:48:29	1984-12-21 19:48:29	1989-04-21 20:48:29
+1987-05-28 13:52:07.900916635	1985-03-28 12:52:07.900916635	1989-07-28 13:52:07.900916635	1989-07-28 13:52:07.900916635	1985-03-28 12:52:07.900916635	1985-03-28 12:52:07.900916635	1989-07-28 13:52:07.900916635
+1998-10-16 20:05:29.397591987	1996-08-16 20:05:29.397591987	2000-12-16 19:05:29.397591987	2000-12-16 19:05:29.397591987	1996-08-16 20:05:29.397591987	1996-08-16 20:05:29.397591987	2000-12-16 19:05:29.397591987
+1999-10-03 16:59:10.396903939	1997-08-03 16:59:10.396903939	2001-12-03 15:59:10.396903939	2001-12-03 15:59:10.396903939	1997-08-03 16:59:10.396903939	1997-08-03 16:59:10.396903939	2001-12-03 15:59:10.396903939
+2000-12-18 08:42:30.000595596	1998-10-18 09:42:30.000595596	2003-02-18 08:42:30.000595596	2003-02-18 08:42:30.000595596	1998-10-18 09:42:30.000595596	1998-10-18 09:42:30.000595596	2003-02-18 08:42:30.000595596
+2002-05-10 05:29:48.990818073	2000-03-10 04:29:48.990818073	2004-07-10 05:29:48.990818073	2004-07-10 05:29:48.990818073	2000-03-10 04:29:48.990818073	2000-03-10 04:29:48.990818073	2004-07-10 05:29:48.990818073
+2003-09-23 22:33:17.00003252	2001-07-23 22:33:17.00003252	2005-11-23 21:33:17.00003252	2005-11-23 21:33:17.00003252	2001-07-23 22:33:17.00003252	2001-07-23 22:33:17.00003252	2005-11-23 21:33:17.00003252
+2004-03-07 20:14:13	2002-01-07 20:14:13	2006-05-07 21:14:13	2006-05-07 21:14:13	2002-01-07 20:14:13	2002-01-07 20:14:13	2006-05-07 21:14:13
+2007-02-09 05:17:29.368756876	2004-12-09 05:17:29.368756876	2009-04-09 06:17:29.368756876	2009-04-09 06:17:29.368756876	2004-12-09 05:17:29.368756876	2004-12-09 05:17:29.368756876	2009-04-09 06:17:29.368756876
+2009-01-21 10:49:07.108	2006-11-21 10:49:07.108	2011-03-21 11:49:07.108	2011-03-21 11:49:07.108	2006-11-21 10:49:07.108	2006-11-21 10:49:07.108	2011-03-21 11:49:07.108
+2010-04-08 02:43:35.861742727	2008-02-08 01:43:35.861742727	2012-06-08 02:43:35.861742727	2012-06-08 02:43:35.861742727	2008-02-08 01:43:35.861742727	2008-02-08 01:43:35.861742727	2012-06-08 02:43:35.861742727
+2013-04-07 02:44:43.00086821	2011-02-07 01:44:43.00086821	2015-06-07 02:44:43.00086821	2015-06-07 02:44:43.00086821	2011-02-07 01:44:43.00086821	2011-02-07 01:44:43.00086821	2015-06-07 02:44:43.00086821
+2013-04-10 00:43:46.854731546	2011-02-09 23:43:46.854731546	2015-06-10 00:43:46.854731546	2015-06-10 00:43:46.854731546	2011-02-09 23:43:46.854731546	2011-02-09 23:43:46.854731546	2015-06-10 00:43:46.854731546
+2021-09-24 03:18:32.413655165	2019-07-24 03:18:32.413655165	2023-11-24 02:18:32.413655165	2023-11-24 02:18:32.413655165	2019-07-24 03:18:32.413655165	2019-07-24 03:18:32.413655165	2023-11-24 02:18:32.413655165
+2024-11-11 16:42:41.101	2022-09-11 17:42:41.101	2027-01-11 16:42:41.101	2027-01-11 16:42:41.101	2022-09-11 17:42:41.101	2022-09-11 17:42:41.101	2027-01-11 16:42:41.101
+4143-07-08 10:53:27.252802259	4141-05-08 10:53:27.252802259	4145-09-08 10:53:27.252802259	4145-09-08 10:53:27.252802259	4141-05-08 10:53:27.252802259	4141-05-08 10:53:27.252802259	4145-09-08 10:53:27.252802259
+4966-12-04 09:30:55.202	4964-10-04 10:30:55.202	4969-02-04 09:30:55.202	4969-02-04 09:30:55.202	4964-10-04 10:30:55.202	4964-10-04 10:30:55.202	4969-02-04 09:30:55.202
+5339-02-01 14:10:01.085678691	5336-12-01 14:10:01.085678691	5341-04-01 15:10:01.085678691	5341-04-01 15:10:01.085678691	5336-12-01 14:10:01.085678691	5336-12-01 14:10:01.085678691	5341-04-01 15:10:01.085678691
+5344-10-04 18:40:08.165	5342-08-04 18:40:08.165	5346-12-04 17:40:08.165	5346-12-04 17:40:08.165	5342-08-04 18:40:08.165	5342-08-04 18:40:08.165	5346-12-04 17:40:08.165
+5397-07-13 07:12:32.000896438	5395-05-13 07:12:32.000896438	5399-09-13 07:12:32.000896438	5399-09-13 07:12:32.000896438	5395-05-13 07:12:32.000896438	5395-05-13 07:12:32.000896438	5399-09-13 07:12:32.000896438
+5966-07-09 03:30:50.597	5964-05-09 03:30:50.597	5968-09-09 03:30:50.597	5968-09-09 03:30:50.597	5964-05-09 03:30:50.597	5964-05-09 03:30:50.597	5968-09-09 03:30:50.597
+6229-06-28 02:54:28.970117179	6227-04-28 02:54:28.970117179	6231-08-28 02:54:28.970117179	6231-08-28 02:54:28.970117179	6227-04-28 02:54:28.970117179	6227-04-28 02:54:28.970117179	6231-08-28 02:54:28.970117179
+6482-04-27 12:07:38.073915413	6480-02-27 11:07:38.073915413	6484-06-27 12:07:38.073915413	6484-06-27 12:07:38.073915413	6480-02-27 11:07:38.073915413	6480-02-27 11:07:38.073915413	6484-06-27 12:07:38.073915413
+6631-11-13 16:31:29.702202248	6629-09-13 17:31:29.702202248	6634-01-13 16:31:29.702202248	6634-01-13 16:31:29.702202248	6629-09-13 17:31:29.702202248	6629-09-13 17:31:29.702202248	6634-01-13 16:31:29.702202248
+6705-09-28 18:27:28.000845672	6703-07-28 18:27:28.000845672	6707-11-28 17:27:28.000845672	6707-11-28 17:27:28.000845672	6703-07-28 18:27:28.000845672	6703-07-28 18:27:28.000845672	6707-11-28 17:27:28.000845672
+6731-02-12 08:12:48.287783702	6728-12-12 08:12:48.287783702	6733-04-12 09:12:48.287783702	6733-04-12 09:12:48.287783702	6728-12-12 08:12:48.287783702	6728-12-12 08:12:48.287783702	6733-04-12 09:12:48.287783702
+7160-12-02 06:00:24.81200852	7158-10-02 07:00:24.81200852	7163-02-02 06:00:24.81200852	7163-02-02 06:00:24.81200852	7158-10-02 07:00:24.81200852	7158-10-02 07:00:24.81200852	7163-02-02 06:00:24.81200852
+7409-09-07 23:33:32.459349602	7407-07-07 23:33:32.459349602	7411-11-07 22:33:32.459349602	7411-11-07 22:33:32.459349602	7407-07-07 23:33:32.459349602	7407-07-07 23:33:32.459349602	7411-11-07 22:33:32.459349602
+7503-06-23 23:14:17.486	7501-04-23 23:14:17.486	7505-08-23 23:14:17.486	7505-08-23 23:14:17.486	7501-04-23 23:14:17.486	7501-04-23 23:14:17.486	7505-08-23 23:14:17.486
+8422-07-22 03:21:45.745036084	8420-05-22 03:21:45.745036084	8424-09-22 03:21:45.745036084	8424-09-22 03:21:45.745036084	8420-05-22 03:21:45.745036084	8420-05-22 03:21:45.745036084	8424-09-22 03:21:45.745036084
+8521-01-16 20:42:05.668832388	8518-11-16 20:42:05.668832388	8523-03-16 21:42:05.668832388	8523-03-16 21:42:05.668832388	8518-11-16 20:42:05.668832388	8518-11-16 20:42:05.668832388	8523-03-16 21:42:05.668832388
+9075-06-13 16:20:09.218517797	9073-04-13 16:20:09.218517797	9077-08-13 16:20:09.218517797	9077-08-13 16:20:09.218517797	9073-04-13 16:20:09.218517797	9073-04-13 16:20:09.218517797	9077-08-13 16:20:09.218517797
+9209-11-11 04:08:58.223768453	9207-09-11 05:08:58.223768453	9212-01-11 04:08:58.223768453	9212-01-11 04:08:58.223768453	9207-09-11 05:08:58.223768453	9207-09-11 05:08:58.223768453	9212-01-11 04:08:58.223768453
+9403-01-09 18:12:33.547	9400-11-09 18:12:33.547	9405-03-09 18:12:33.547	9405-03-09 18:12:33.547	9400-11-09 18:12:33.547	9400-11-09 18:12:33.547	9405-03-09 18:12:33.547
+PREHOOK: query: explain
+select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: COMPLETE
+            Select Operator
+              Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+              Reduce Output Operator
+                key expressions: 5-5 (type: interval_year_month)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                TopN Hash Memory Usage: 0.1
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: 5-5 (type: interval_year_month), -1-1 (type: interval_year_month)
+          outputColumnNames: _col0, _col1
+          Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+          Limit
+            Number of rows: 2
+            Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 2
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  interval '2-2' year to month + interval '3-3' year to month,
+  interval '2-2' year to month - interval '3-3' year to month
+from interval_arithmetic_1
+order by interval '2-2' year to month + interval '3-3' year to month
+limit 2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+c0	c1
+5-5	-1-1
+5-5	-1-1
+PREHOOK: query: -- interval day-time arithmetic
+explain
+select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: -- interval day-time arithmetic
+explain
+select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: dateval (type: date), (dateval - 99 11:22:33.123456789) (type: timestamp), (dateval - -99 11:22:33.123456789) (type: timestamp), (dateval + 99 11:22:33.123456789) (type: timestamp), (dateval + -99 11:22:33.123456789) (type: timestamp), (-99 11:22:33.123456789 + dateval) (type: timestamp), (99 11:22:33.123456789 + dateval) (type: timestamp)
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+              Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col0 (type: date)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col1 (type: timestamp), _col2 (type: timestamp), _col3 (type: timestamp), _col4 (type: timestamp), _col5 (type: timestamp), _col6 (type: timestamp)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: timestamp), VALUE._col1 (type: timestamp), VALUE._col2 (type: timestamp), VALUE._col3 (type: timestamp), VALUE._col4 (type: timestamp), VALUE._col5 (type: timestamp)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+          Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  dateval - interval '99 11:22:33.123456789' day to second,
+  dateval - interval '-99 11:22:33.123456789' day to second,
+  dateval + interval '99 11:22:33.123456789' day to second,
+  dateval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + dateval,
+  interval '99 11:22:33.123456789' day to second + dateval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	_c1	_c2	_c3	_c4	_c5	_c6
+0004-09-22	0004-06-14 12:37:26.876543211	0004-12-30 11:22:33.123456789	0004-12-30 11:22:33.123456789	0004-06-14 12:37:26.876543211	0004-06-14 12:37:26.876543211	0004-12-30 11:22:33.123456789
+0528-10-27	0528-07-19 12:37:26.876543211	0529-02-03 11:22:33.123456789	0529-02-03 11:22:33.123456789	0528-07-19 12:37:26.876543211	0528-07-19 12:37:26.876543211	0529-02-03 11:22:33.123456789
+1319-02-02	1318-10-25 12:37:26.876543211	1319-05-12 11:22:33.123456789	1319-05-12 11:22:33.123456789	1318-10-25 12:37:26.876543211	1318-10-25 12:37:26.876543211	1319-05-12 11:22:33.123456789
+1404-07-23	1404-04-14 12:37:26.876543211	1404-10-30 11:22:33.123456789	1404-10-30 11:22:33.123456789	1404-04-14 12:37:26.876543211	1404-04-14 12:37:26.876543211	1404-10-30 11:22:33.123456789
+1815-05-06	1815-01-26 12:37:26.876543211	1815-08-13 11:22:33.123456789	1815-08-13 11:22:33.123456789	1815-01-26 12:37:26.876543211	1815-01-26 12:37:26.876543211	1815-08-13 11:22:33.123456789
+1883-04-17	1883-01-07 12:37:26.876543211	1883-07-25 11:22:33.123456789	1883-07-25 11:22:33.123456789	1883-01-07 12:37:26.876543211	1883-01-07 12:37:26.876543211	1883-07-25 11:22:33.123456789
+1966-08-16	1966-05-08 12:37:26.876543211	1966-11-23 10:22:33.123456789	1966-11-23 10:22:33.123456789	1966-05-08 12:37:26.876543211	1966-05-08 12:37:26.876543211	1966-11-23 10:22:33.123456789
+1973-04-17	1973-01-07 12:37:26.876543211	1973-07-25 12:22:33.123456789	1973-07-25 12:22:33.123456789	1973-01-07 12:37:26.876543211	1973-01-07 12:37:26.876543211	1973-07-25 12:22:33.123456789
+1974-10-04	1974-06-26 12:37:26.876543211	1975-01-11 10:22:33.123456789	1975-01-11 10:22:33.123456789	1974-06-26 12:37:26.876543211	1974-06-26 12:37:26.876543211	1975-01-11 10:22:33.123456789
+1976-03-03	1975-11-24 12:37:26.876543211	1976-06-10 12:22:33.123456789	1976-06-10 12:22:33.123456789	1975-11-24 12:37:26.876543211	1975-11-24 12:37:26.876543211	1976-06-10 12:22:33.123456789
+1976-05-06	1976-01-27 11:37:26.876543211	1976-08-13 11:22:33.123456789	1976-08-13 11:22:33.123456789	1976-01-27 11:37:26.876543211	1976-01-27 11:37:26.876543211	1976-08-13 11:22:33.123456789
+1978-08-05	1978-04-27 11:37:26.876543211	1978-11-12 10:22:33.123456789	1978-11-12 10:22:33.123456789	1978-04-27 11:37:26.876543211	1978-04-27 11:37:26.876543211	1978-11-12 10:22:33.123456789
+1981-04-25	1981-01-15 12:37:26.876543211	1981-08-02 12:22:33.123456789	1981-08-02 12:22:33.123456789	1981-01-15 12:37:26.876543211	1981-01-15 12:37:26.876543211	1981-08-02 12:22:33.123456789
+1981-11-15	1981-08-07 13:37:26.876543211	1982-02-22 11:22:33.123456789	1982-02-22 11:22:33.123456789	1981-08-07 13:37:26.876543211	1981-08-07 13:37:26.876543211	1982-02-22 11:22:33.123456789
+1985-07-20	1985-04-11 11:37:26.876543211	1985-10-27 10:22:33.123456789	1985-10-27 10:22:33.123456789	1985-04-11 11:37:26.876543211	1985-04-11 11:37:26.876543211	1985-10-27 10:22:33.123456789
+1985-11-18	1985-08-10 13:37:26.876543211	1986-02-25 11:22:33.123456789	1986-02-25 11:22:33.123456789	1985-08-10 13:37:26.876543211	1985-08-10 13:37:26.876543211	1986-02-25 11:22:33.123456789
+1987-02-21	1986-11-13 12:37:26.876543211	1987-05-31 12:22:33.123456789	1987-05-31 12:22:33.123456789	1986-11-13 12:37:26.876543211	1986-11-13 12:37:26.876543211	1987-05-31 12:22:33.123456789
+1987-05-28	1987-02-17 11:37:26.876543211	1987-09-04 11:22:33.123456789	1987-09-04 11:22:33.123456789	1987-02-17 11:37:26.876543211	1987-02-17 11:37:26.876543211	1987-09-04 11:22:33.123456789
+1998-10-16	1998-07-08 12:37:26.876543211	1999-01-23 10:22:33.123456789	1999-01-23 10:22:33.123456789	1998-07-08 12:37:26.876543211	1998-07-08 12:37:26.876543211	1999-01-23 10:22:33.123456789
+1999-10-03	1999-06-25 12:37:26.876543211	2000-01-10 10:22:33.123456789	2000-01-10 10:22:33.123456789	1999-06-25 12:37:26.876543211	1999-06-25 12:37:26.876543211	2000-01-10 10:22:33.123456789
+2000-12-18	2000-09-09 13:37:26.876543211	2001-03-27 11:22:33.123456789	2001-03-27 11:22:33.123456789	2000-09-09 13:37:26.876543211	2000-09-09 13:37:26.876543211	2001-03-27 11:22:33.123456789
+2002-05-10	2002-01-30 11:37:26.876543211	2002-08-17 11:22:33.123456789	2002-08-17 11:22:33.123456789	2002-01-30 11:37:26.876543211	2002-01-30 11:37:26.876543211	2002-08-17 11:22:33.123456789
+2003-09-23	2003-06-15 12:37:26.876543211	2003-12-31 10:22:33.123456789	2003-12-31 10:22:33.123456789	2003-06-15 12:37:26.876543211	2003-06-15 12:37:26.876543211	2003-12-31 10:22:33.123456789
+2004-03-07	2003-11-28 12:37:26.876543211	2004-06-14 12:22:33.123456789	2004-06-14 12:22:33.123456789	2003-11-28 12:37:26.876543211	2003-11-28 12:37:26.876543211	2004-06-14 12:22:33.123456789
+2007-02-09	2006-11-01 12:37:26.876543211	2007-05-19 12:22:33.123456789	2007-05-19 12:22:33.123456789	2006-11-01 12:37:26.876543211	2006-11-01 12:37:26.876543211	2007-05-19 12:22:33.123456789
+2009-01-21	2008-10-13 13:37:26.876543211	2009-04-30 12:22:33.123456789	2009-04-30 12:22:33.123456789	2008-10-13 13:37:26.876543211	2008-10-13 13:37:26.876543211	2009-04-30 12:22:33.123456789
+2010-04-08	2009-12-29 11:37:26.876543211	2010-07-16 11:22:33.123456789	2010-07-16 11:22:33.123456789	2009-12-29 11:37:26.876543211	2009-12-29 11:37:26.876543211	2010-07-16 11:22:33.123456789
+2013-04-07	2012-12-28 11:37:26.876543211	2013-07-15 11:22:33.123456789	2013-07-15 11:22:33.123456789	2012-12-28 11:37:26.876543211	2012-12-28 11:37:26.876543211	2013-07-15 11:22:33.123456789
+2013-04-10	2012-12-31 11:37:26.876543211	2013-07-18 11:22:33.123456789	2013-07-18 11:22:33.123456789	2012-12-31 11:37:26.876543211	2012-12-31 11:37:26.876543211	2013-07-18 11:22:33.123456789
+2021-09-24	2021-06-16 12:37:26.876543211	2022-01-01 10:22:33.123456789	2022-01-01 10:22:33.123456789	2021-06-16 12:37:26.876543211	2021-06-16 12:37:26.876543211	2022-01-01 10:22:33.123456789
+2024-11-11	2024-08-03 13:37:26.876543211	2025-02-18 11:22:33.123456789	2025-02-18 11:22:33.123456789	2024-08-03 13:37:26.876543211	2024-08-03 13:37:26.876543211	2025-02-18 11:22:33.123456789
+4143-07-08	4143-03-30 12:37:26.876543211	4143-10-15 11:22:33.123456789	4143-10-15 11:22:33.123456789	4143-03-30 12:37:26.876543211	4143-03-30 12:37:26.876543211	4143-10-15 11:22:33.123456789
+4966-12-04	4966-08-26 13:37:26.876543211	4967-03-13 12:22:33.123456789	4967-03-13 12:22:33.123456789	4966-08-26 13:37:26.876543211	4966-08-26 13:37:26.876543211	4967-03-13 12:22:33.123456789
+5339-02-01	5338-10-24 13:37:26.876543211	5339-05-11 12:22:33.123456789	5339-05-11 12:22:33.123456789	5338-10-24 13:37:26.876543211	5338-10-24 13:37:26.876543211	5339-05-11 12:22:33.123456789
+5344-10-04	5344-06-26 12:37:26.876543211	5345-01-11 10:22:33.123456789	5345-01-11 10:22:33.123456789	5344-06-26 12:37:26.876543211	5344-06-26 12:37:26.876543211	5345-01-11 10:22:33.123456789
+5397-07-13	5397-04-04 12:37:26.876543211	5397-10-20 11:22:33.123456789	5397-10-20 11:22:33.123456789	5397-04-04 12:37:26.876543211	5397-04-04 12:37:26.876543211	5397-10-20 11:22:33.123456789
+5966-07-09	5966-03-31 12:37:26.876543211	5966-10-16 11:22:33.123456789	5966-10-16 11:22:33.123456789	5966-03-31 12:37:26.876543211	5966-03-31 12:37:26.876543211	5966-10-16 11:22:33.123456789
+6229-06-28	6229-03-20 12:37:26.876543211	6229-10-05 11:22:33.123456789	6229-10-05 11:22:33.123456789	6229-03-20 12:37:26.876543211	6229-03-20 12:37:26.876543211	6229-10-05 11:22:33.123456789
+6482-04-27	6482-01-17 11:37:26.876543211	6482-08-04 11:22:33.123456789	6482-08-04 11:22:33.123456789	6482-01-17 11:37:26.876543211	6482-01-17 11:37:26.876543211	6482-08-04 11:22:33.123456789
+6631-11-13	6631-08-05 13:37:26.876543211	6632-02-20 11:22:33.123456789	6632-02-20 11:22:33.123456789	6631-08-05 13:37:26.876543211	6631-08-05 13:37:26.876543211	6632-02-20 11:22:33.123456789
+6705-09-28	6705-06-20 12:37:26.876543211	6706-01-05 10:22:33.123456789	6706-01-05 10:22:33.123456789	6705-06-20 12:37:26.876543211	6705-06-20 12:37:26.876543211	6706-01-05 10:22:33.123456789
+6731-02-12	6730-11-04 12:37:26.876543211	6731-05-22 12:22:33.123456789	6731-05-22 12:22:33.123456789	6730-11-04 12:37:26.876543211	6730-11-04 12:37:26.876543211	6731-05-22 12:22:33.123456789
+7160-12-02	7160-08-24 13:37:26.876543211	7161-03-11 11:22:33.123456789	7161-03-11 11:22:33.123456789	7160-08-24 13:37:26.876543211	7160-08-24 13:37:26.876543211	7161-03-11 11:22:33.123456789
+7409-09-07	7409-05-30 12:37:26.876543211	7409-12-15 10:22:33.123456789	7409-12-15 10:22:33.123456789	7409-05-30 12:37:26.876543211	7409-05-30 12:37:26.876543211	7409-12-15 10:22:33.123456789
+7503-06-23	7503-03-15 12:37:26.876543211	7503-09-30 11:22:33.123456789	7503-09-30 11:22:33.123456789	7503-03-15 12:37:26.876543211	7503-03-15 12:37:26.876543211	7503-09-30 11:22:33.123456789
+8422-07-22	8422-04-13 12:37:26.876543211	8422-10-29 11:22:33.123456789	8422-10-29 11:22:33.123456789	8422-04-13 12:37:26.876543211	8422-04-13 12:37:26.876543211	8422-10-29 11:22:33.123456789
+8521-01-16	8520-10-08 13:37:26.876543211	8521-04-25 12:22:33.123456789	8521-04-25 12:22:33.123456789	8520-10-08 13:37:26.876543211	8520-10-08 13:37:26.876543211	8521-04-25 12:22:33.123456789
+9075-06-13	9075-03-05 11:37:26.876543211	9075-09-20 11:22:33.123456789	9075-09-20 11:22:33.123456789	9075-03-05 11:37:26.876543211	9075-03-05 11:37:26.876543211	9075-09-20 11:22:33.123456789
+9209-11-11	9209-08-03 13:37:26.876543211	9210-02-18 11:22:33.123456789	9210-02-18 11:22:33.123456789	9209-08-03 13:37:26.876543211	9209-08-03 13:37:26.876543211	9210-02-18 11:22:33.123456789
+9403-01-09	9402-10-01 13:37:26.876543211	9403-04-18 12:22:33.123456789	9403-04-18 12:22:33.123456789	9402-10-01 13:37:26.876543211	9402-10-01 13:37:26.876543211	9403-04-18 12:22:33.123456789
+PREHOOK: query: explain
+select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: dateval (type: date), tsval (type: timestamp), (dateval - tsval) (type: interval_day_time), (tsval - dateval) (type: interval_day_time), (tsval - tsval) (type: interval_day_time)
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4
+              Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col0 (type: date)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col1 (type: timestamp), _col2 (type: interval_day_time), _col3 (type: interval_day_time), _col4 (type: interval_day_time)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: date), VALUE._col0 (type: timestamp), VALUE._col1 (type: interval_day_time), VALUE._col2 (type: interval_day_time), VALUE._col3 (type: interval_day_time)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4
+          Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  dateval,
+  tsval,
+  dateval - tsval,
+  tsval - dateval,
+  tsval - tsval
+from interval_arithmetic_1
+order by dateval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+dateval	tsval	c2	c3	c4
+0004-09-22	0004-09-22 18:26:29.519542222	-0 18:26:30.519542222	0 18:26:30.519542222	0 00:00:00.000000000
+0528-10-27	0528-10-27 08:15:18.941718273	-0 08:15:19.941718273	0 08:15:19.941718273	0 00:00:00.000000000
+1319-02-02	1319-02-02 16:31:57.778	-0 16:31:58.778000000	0 16:31:58.778000000	0 00:00:00.000000000
+1404-07-23	1404-07-23 15:32:16.059185026	-0 15:32:17.059185026	0 15:32:17.059185026	0 00:00:00.000000000
+1815-05-06	1815-05-06 00:12:37.543584705	-0 00:12:38.543584705	0 00:12:38.543584705	0 00:00:00.000000000
+1883-04-17	1883-04-17 04:14:34.647766229	-0 04:14:35.647766229	0 04:14:35.647766229	0 00:00:00.000000000
+1966-08-16	1966-08-16 13:36:50.183618031	-0 13:36:51.183618031	0 13:36:51.183618031	0 00:00:00.000000000
+1973-04-17	1973-04-17 06:30:38.596784156	-0 06:30:38.596784156	0 06:30:38.596784156	0 00:00:00.000000000
+1974-10-04	1974-10-04 17:21:03.989	-0 17:21:03.989000000	0 17:21:03.989000000	0 00:00:00.000000000
+1976-03-03	1976-03-03 04:54:33.000895162	-0 04:54:33.000895162	0 04:54:33.000895162	0 00:00:00.000000000
+1976-05-06	1976-05-06 00:42:30.910786948	-0 00:42:30.910786948	0 00:42:30.910786948	0 00:00:00.000000000
+1978-08-05	1978-08-05 14:41:05.501	-0 14:41:05.501000000	0 14:41:05.501000000	0 00:00:00.000000000
+1981-04-25	1981-04-25 09:01:12.077192689	-0 09:01:12.077192689	0 09:01:12.077192689	0 00:00:00.000000000
+1981-11-15	1981-11-15 23:03:10.999338387	-0 23:03:10.999338387	0 23:03:10.999338387	0 00:00:00.000000000
+1985-07-20	1985-07-20 09:30:11	-0 09:30:11.000000000	0 09:30:11.000000000	0 00:00:00.000000000
+1985-11-18	1985-11-18 16:37:54	-0 16:37:54.000000000	0 16:37:54.000000000	0 00:00:00.000000000
+1987-02-21	1987-02-21 19:48:29	-0 19:48:29.000000000	0 19:48:29.000000000	0 00:00:00.000000000
+1987-05-28	1987-05-28 13:52:07.900916635	-0 13:52:07.900916635	0 13:52:07.900916635	0 00:00:00.000000000
+1998-10-16	1998-10-16 20:05:29.397591987	-0 20:05:29.397591987	0 20:05:29.397591987	0 00:00:00.000000000
+1999-10-03	1999-10-03 16:59:10.396903939	-0 16:59:10.396903939	0 16:59:10.396903939	0 00:00:00.000000000
+2000-12-18	2000-12-18 08:42:30.000595596	-0 08:42:30.000595596	0 08:42:30.000595596	0 00:00:00.000000000
+2002-05-10	2002-05-10 05:29:48.990818073	-0 05:29:48.990818073	0 05:29:48.990818073	0 00:00:00.000000000
+2003-09-23	2003-09-23 22:33:17.00003252	-0 22:33:17.000032520	0 22:33:17.000032520	0 00:00:00.000000000
+2004-03-07	2004-03-07 20:14:13	-0 20:14:13.000000000	0 20:14:13.000000000	0 00:00:00.000000000
+2007-02-09	2007-02-09 05:17:29.368756876	-0 05:17:29.368756876	0 05:17:29.368756876	0 00:00:00.000000000
+2009-01-21	2009-01-21 10:49:07.108	-0 10:49:07.108000000	0 10:49:07.108000000	0 00:00:00.000000000
+2010-04-08	2010-04-08 02:43:35.861742727	-0 02:43:35.861742727	0 02:43:35.861742727	0 00:00:00.000000000
+2013-04-07	2013-04-07 02:44:43.00086821	-0 02:44:43.000868210	0 02:44:43.000868210	0 00:00:00.000000000
+2013-04-10	2013-04-10 00:43:46.854731546	-0 00:43:46.854731546	0 00:43:46.854731546	0 00:00:00.000000000
+2021-09-24	2021-09-24 03:18:32.413655165	-0 03:18:32.413655165	0 03:18:32.413655165	0 00:00:00.000000000
+2024-11-11	2024-11-11 16:42:41.101	-0 16:42:41.101000000	0 16:42:41.101000000	0 00:00:00.000000000
+4143-07-08	4143-07-08 10:53:27.252802259	-0 10:53:27.252802259	0 10:53:27.252802259	0 00:00:00.000000000
+4966-12-04	4966-12-04 09:30:55.202	-0 09:30:55.202000000	0 09:30:55.202000000	0 00:00:00.000000000
+5339-02-01	5339-02-01 14:10:01.085678691	-0 14:10:01.085678691	0 14:10:01.085678691	0 00:00:00.000000000
+5344-10-04	5344-10-04 18:40:08.165	-0 18:40:08.165000000	0 18:40:08.165000000	0 00:00:00.000000000
+5397-07-13	5397-07-13 07:12:32.000896438	-0 07:12:32.000896438	0 07:12:32.000896438	0 00:00:00.000000000
+5966-07-09	5966-07-09 03:30:50.597	-0 03:30:50.597000000	0 03:30:50.597000000	0 00:00:00.000000000
+6229-06-28	6229-06-28 02:54:28.970117179	-0 02:54:28.970117179	0 02:54:28.970117179	0 00:00:00.000000000
+6482-04-27	6482-04-27 12:07:38.073915413	-0 12:07:38.073915413	0 12:07:38.073915413	0 00:00:00.000000000
+6631-11-13	6631-11-13 16:31:29.702202248	-0 16:31:29.702202248	0 16:31:29.702202248	0 00:00:00.000000000
+6705-09-28	6705-09-28 18:27:28.000845672	-0 18:27:28.000845672	0 18:27:28.000845672	0 00:00:00.000000000
+6731-02-12	6731-02-12 08:12:48.287783702	-0 08:12:48.287783702	0 08:12:48.287783702	0 00:00:00.000000000
+7160-12-02	7160-12-02 06:00:24.81200852	-0 06:00:24.812008520	0 06:00:24.812008520	0 00:00:00.000000000
+7409-09-07	7409-09-07 23:33:32.459349602	-0 23:33:32.459349602	0 23:33:32.459349602	0 00:00:00.000000000
+7503-06-23	7503-06-23 23:14:17.486	-0 23:14:17.486000000	0 23:14:17.486000000	0 00:00:00.000000000
+8422-07-22	8422-07-22 03:21:45.745036084	-0 03:21:45.745036084	0 03:21:45.745036084	0 00:00:00.000000000
+8521-01-16	8521-01-16 20:42:05.668832388	-0 20:42:05.668832388	0 20:42:05.668832388	0 00:00:00.000000000
+9075-06-13	9075-06-13 16:20:09.218517797	-0 16:20:09.218517797	0 16:20:09.218517797	0 00:00:00.000000000
+9209-11-11	9209-11-11 04:08:58.223768453	-0 04:08:58.223768453	0 04:08:58.223768453	0 00:00:00.000000000
+9403-01-09	9403-01-09 18:12:33.547	-0 18:12:33.547000000	0 18:12:33.547000000	0 00:00:00.000000000
+PREHOOK: query: explain
+select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: tsval (type: timestamp), (tsval - 99 11:22:33.123456789) (type: timestamp), (tsval - -99 11:22:33.123456789) (type: timestamp), (tsval + 99 11:22:33.123456789) (type: timestamp), (tsval + -99 11:22:33.123456789) (type: timestamp), (-99 11:22:33.123456789 + tsval) (type: timestamp), (99 11:22:33.123456789 + tsval) (type: timestamp)
+              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+              Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col0 (type: timestamp)
+                sort order: +
+                Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col1 (type: timestamp), _col2 (type: timestamp), _col3 (type: timestamp), _col4 (type: timestamp), _col5 (type: timestamp), _col6 (type: timestamp)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: timestamp), VALUE._col0 (type: timestamp), VALUE._col1 (type: timestamp), VALUE._col2 (type: timestamp), VALUE._col3 (type: timestamp), VALUE._col4 (type: timestamp), VALUE._col5 (type: timestamp)
+          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
+          Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  tsval,
+  tsval - interval '99 11:22:33.123456789' day to second,
+  tsval - interval '-99 11:22:33.123456789' day to second,
+  tsval + interval '99 11:22:33.123456789' day to second,
+  tsval + interval '-99 11:22:33.123456789' day to second,
+  -interval '99 11:22:33.123456789' day to second + tsval,
+  interval '99 11:22:33.123456789' day to second + tsval
+from interval_arithmetic_1
+order by tsval
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+tsval	_c1	_c2	_c3	_c4	_c5	_c6
+0004-09-22 18:26:29.519542222	0004-06-15 07:03:56.396085433	0004-12-31 05:49:02.642999011	0004-12-31 05:49:02.642999011	0004-06-15 07:03:56.396085433	0004-06-15 07:03:56.396085433	0004-12-31 05:49:02.642999011
+0528-10-27 08:15:18.941718273	0528-07-19 20:52:45.818261484	0529-02-03 19:37:52.065175062	0529-02-03 19:37:52.065175062	0528-07-19 20:52:45.818261484	0528-07-19 20:52:45.818261484	0529-02-03 19:37:52.065175062
+1319-02-02 16:31:57.778	1318-10-26 05:09:24.654543211	1319-05-13 03:54:30.901456789	1319-05-13 03:54:30.901456789	1318-10-26 05:09:24.654543211	1318-10-26 05:09:24.654543211	1319-05-13 03:54:30.901456789
+1404-07-23 15:32:16.059185026	1404-04-15 04:09:42.935728237	1404-10-31 02:54:49.182641815	1404-10-31 02:54:49.182641815	1404-04-15 04:09:42.935728237	1404-04-15 04:09:42.935728237	1404-10-31 02:54:49.182641815
+1815-05-06 00:12:37.543584705	1815-01-26 12:50:04.420127916	1815-08-13 11:35:10.667041494	1815-08-13 11:35:10.667041494	1815-01-26 12:50:04.420127916	1815-01-26 12:50:04.420127916	1815-08-13 11:35:10.667041494
+1883-04-17 04:14:34.647766229	1883-01-07 16:52:01.52430944	1883-07-25 15:37:07.771223018	1883-07-25 15:37:07.771223018	1883-01-07 16:52:01.52430944	1883-01-07 16:52:01.52430944	1883-07-25 15:37:07.771223018
+1966-08-16 13:36:50.183618031	1966-05-09 02:14:17.060161242	1966-11-23 23:59:23.30707482	1966-11-23 23:59:23.30707482	1966-05-09 02:14:17.060161242	1966-05-09 02:14:17.060161242	1966-11-23 23:59:23.30707482
+1973-04-17 06:30:38.596784156	1973-01-07 19:08:05.473327367	1973-07-25 18:53:11.720240945	1973-07-25 18:53:11.720240945	1973-01-07 19:08:05.473327367	1973-01-07 19:08:05.473327367	1973-07-25 18:53:11.720240945
+1974-10-04 17:21:03.989	1974-06-27 05:58:30.865543211	1975-01-12 03:43:37.112456789	1975-01-12 03:43:37.112456789	1974-06-27 05:58:30.865543211	1974-06-27 05:58:30.865543211	1975-01-12 03:43:37.112456789
+1976-03-03 04:54:33.000895162	1975-11-24 17:31:59.877438373	1976-06-10 17:17:06.124351951	1976-06-10 17:17:06.124351951	1975-11-24 17:31:59.877438373	1975-11-24 17:31:59.877438373	1976-06-10 17:17:06.124351951
+1976-05-06 00:42:30.910786948	1976-01-27 12:19:57.787330159	1976-08-13 12:05:04.034243737	1976-08-13 12:05:04.034243737	1976-01-27 12:19:57.787330159	1976-01-27 12:19:57.787330159	1976-08-13 12:05:04.034243737
+1978-08-05 14:41:05.501	1978-04-28 02:18:32.377543211	1978-11-13 01:03:38.624456789	1978-11-13 01:03:38.624456789	1978-04-28 02:18:32.377543211	1978-04-28 02:18:32.377543211	1978-11-13 01:03:38.624456789
+1981-04-25 09:01:12.077192689	1981-01-15 21:38:38.9537359	1981-08-02 21:23:45.200649478	1981-08-02 21:23:45.200649478	1981-01-15 21:38:38.9537359	1981-01-15 21:38:38.9537359	1981-08-02 21:23:45.200649478
+1981-11-15 23:03:10.999338387	1981-08-08 12:40:37.875881598	1982-02-23 10:25:44.122795176	1982-02-23 10:25:44.122795176	1981-08-08 12:40:37.875881598	1981-08-08 12:40:37.875881598	1982-02-23 10:25:44.122795176
+1985-07-20 09:30:11	1985-04-11 21:07:37.876543211	1985-10-27 19:52:44.123456789	1985-10-27 19:52:44.123456789	1985-04-11 21:07:37.876543211	1985-04-11 21:07:37.876543211	1985-10-27 19:52:44.123456789
+1985-11-18 16:37:54	1985-08-11 06:15:20.876543211	1986-02-26 04:00:27.123456789	1986-02-26 04:00:27.123456789	1985-08-11 06:15:20.876543211	1985-08-11 06:15:20.876543211	1986-02-26 04:00:27.123456789
+1987-02-21 19:48:29	1986-11-14 08:25:55.876543211	1987-06-01 08:11:02.123456789	1987-06-01 08:11:02.123456789	1986-11-14 08:25:55.876543211	1986-11-14 08:25:55.876543211	1987-06-01 08:11:02.123456789
+1987-05-28 13:52:07.900916635	1987-02-18 01:29:34.777459846	1987-09-05 01:14:41.024373424	1987-09-05 01:14:41.024373424	1987-02-18 01:29:34.777459846	1987-02-18 01:29:34.777459846	1987-09-05 01:14:41.024373424
+1998-10-16 20:05:29.397591987	1998-07-09 08:42:56.274135198	1999-01-24 06:28:02.521048776	1999-01-24 06:28:02.521048776	1998-07-09 08:42:56.274135198	1998-07-09 08:42:56.274135198	1999-01-24 06:28:02.521048776
+1999-10-03 16:59:10.396903939	1999-06-26 05:36:37.27344715	2000-01-11 03:21:43.520360728	2000-01-11 03:21:43.520360728	1999-06-26 05:36:37.27344715	1999-06-26 05:36:37.27344715	2000-01-11 03:21:43.520360728
+2000-12-18 08:42:30.000595596	2000-09-09 22:19:56.877138807	2001-03-27 20:05:03.124052385	2001-03-27 20:05:03.124052385	2000-09-09 22:19:56.877138807	2000-09-09 22:19:56.877138807	2001-03-27 20:05:03.124052385
+2002-05-10 05:29:48.990818073	2002-01-30 17:07:15.867361284	2002-08-17 16:52:22.114274862	2002-08-17 16:52:22.114274862	2002-01-30 17:07:15.867361284	2002-01-30 17:07:15.867361284	2002-08-17 16:52:22.114274862
+2003-09-23 22:33:17.00003252	2003-06-16 11:10:43.876575731	2004-01-01 08:55:50.123489309	2004-01-01 08:55:50.123489309	2003-06-16 11:10:43.876575731	2003-06-16 11:10:43.876575731	2004-01-01 08:55:50.123489309
+2004-03-07 20:14:13	2003-11-29 08:51:39.876543211	2004-06-15 08:36:46.123456789	2004-06-15 08:36:46.123456789	2003-11-29 08:51:39.876543211	2003-11-29 08:51:39.876543211	2004-06-15 08:36:46.123456789
+2007-02-09 05:17:29.368756876	2006-11-01 17:54:56.245300087	2007-05-19 17:40:02.492213665	2007-05-19 17:40:02.492213665	2006-11-01 17:54:56.245300087	2006-11-01 17:54:56.245300087	2007-05-19 17:40:02.492213665
+2009-01-21 10:49:07.108	2008-10-14 00:26:33.984543211	2009-04-30 23:11:40.231456789	2009-04-30 23:11:40.231456789	2008-10-14 00:26:33.984543211	2008-10-14 00:26:33.984543211	2009-04-30 23:11:40.231456789
+2010-04-08 02:43:35.861742727	2009-12-29 14:21:02.738285938	2010-07-16 14:06:08.985199516	2010-07-16 14:06:08.985199516	2009-12-29 14:21:02.738285938	2009-12-29 14:21:02.738285938	2010-07-16 14:06:08.985199516
+2013-04-07 02:44:43.00086821	2012-12-28 14:22:09.877411421	2013-07-15 14:07:16.124324999	2013-07-15 14:07:16.124324999	2012-12-28 14:22:09.877411421	2012-12-28 14:22:09.877411421	2013-07-15 14:07:16.124324999
+2013-04-10 00:43:46.854731546	2012-12-31 12:21:13.731274757	2013-07-18 12:06:19.978188335	2013-07-18 12:06:19.978188335	2012-12-31 12:21:13.731274757	2012-12-31 12:21:13.731274757	2013-07-18 12:06:19.978188335
+2021-09-24 03:18:32.413655165	2021-06-16 15:55:59.290198376	2022-01-01 13:41:05.537111954	2022-01-01 13:41:05.537111954	2021-06-16 15:55:59.290198376	2021-06-16 15:55:59.290198376	2022-01-01 13:41:05.537111954
+2024-11-11 16:42:41.101	2024-08-04 06:20:07.977543211	2025-02-19 04:05:14.224456789	2025-02-19 04:05:14.224456789	2024-08-04 06:20:07.977543211	2024-08-04 06:20:07.977543211	2025-02-19 04:05:14.224456789
+4143-07-08 10:53:27.252802259	4143-03-30 23:30:54.12934547	4143-10-15 22:16:00.376259048	4143-10-15 22:16:00.376259048	4143-03-30 23:30:54.12934547	4143-03-30 23:30:54.12934547	4143-10-15 22:16:00.376259048
+4966-12-04 09:30:55.202	4966-08-26 23:08:22.078543211	4967-03-13 21:53:28.325456789	4967-03-13 21:53:28.325456789	4966-08-26 23:08:22.078543211	4966-08-26 23:08:22.078543211	4967-03-13 21:53:28.325456789
+5339-02-01 14:10:01.085678691	5338-10-25 03:47:27.962221902	5339-05-12 02:32:34.20913548	5339-05-12 02:32:34.20913548	5338-10-25 03:47:27.962221902	5338-10-25 03:47:27.962221902	5339-05-12 02:32:34.20913548
+5344-10-04 18:40:08.165	5344-06-27 07:17:35.041543211	5345-01-12 05:02:41.288456789	5345-01-12 05:02:41.288456789	5344-06-27 07:17:35.041543211	5344-06-27 07:17:35.041543211	5345-01-12 05:02:41.288456789
+5397-07-13 07:12:32.000896438	5397-04-04 19:49:58.877439649	5397-10-20 18:35:05.124353227	5397-10-20 18:35:05.124353227	5397-04-04 19:49:58.877439649	5397-04-04 19:49:58.877439649	5397-10-20 18:35:05.124353227
+5966-07-09 03:30:50.597	5966-03-31 16:08:17.473543211	5966-10-16 14:53:23.720456789	5966-10-16 14:53:23.720456789	5966-03-31 16:08:17.473543211	5966-03-31 16:08:17.473543211	5966-10-16 14:53:23.720456789
+6229-06-28 02:54:28.970117179	6229-03-20 15:31:55.84666039	6229-10-05 14:17:02.093573968	6229-10-05 14:17:02.093573968	6229-03-20 15:31:55.84666039	6229-03-20 15:31:55.84666039	6229-10-05 14:17:02.093573968
+6482-04-27 12:07:38.073915413	6482-01-17 23:45:04.950458624	6482-08-04 23:30:11.197372202	6482-08-04 23:30:11.197372202	6482-01-17 23:45:04.950458624	6482-01-17 23:45:04.950458624	6482-08-04 23:30:11.197372202
+6631-11-13 16:31:29.702202248	6631-08-06 06:08:56.578745459	6632-02-21 03:54:02.825659037	6632-02-21 03:54:02.825659037	6631-08-06 06:08:56.578745459	6631-08-06 06:08:56.578745459	6632-02-21 03:54:02.825659037
+6705-09-28 18:27:28.000845672	6705-06-21 07:04:54.877388883	6706-01-06 04:50:01.124302461	6706-01-06 04:50:01.124302461	6705-06-21 07:04:54.877388883	6705-06-21 07:04:54.877388883	6706-01-06 04:50:01.124302461
+6731-02-12 08:12:48.287783702	6730-11-04 20:50:15.164326913	6731-05-22 20:35:21.411240491	6731-05-22 20:35:21.411240491	6730-11-04 20:50:15.164326913	6730-11-04 20:50:15.164326913	6731-05-22 20:35:21.411240491
+7160-12-02 06:00:24.81200852	7160-08-24 19:37:51.688551731	7161-03-11 17:22:57.935465309	7161-03-11 17:22:57.935465309	7160-08-24 19:37:51.688551731	7160-08-24 19:37:51.688551731	7161-03-11 17:22:57.935465309
+7409-09-07 23:33:32.459349602	7409-05-31 12:10:59.335892813	7409-12-16 09:56:05.582806391	7409-12-16 09:56:05.582806391	7409-05-31 12:10:59.335892813	7409-05-31 12:10:59.335892813	7409-12-16 09:56:05.582806391
+7503-06-23 23:14:17.486	7503-03-16 11:51:44.362543211	7503-10-01 10:36:50.609456789	7503-10-01 10:36:50.609456789	7503-03-16 11:51:44.362543211	7503-03-16 11:51:44.362543211	7503-10-01 10:36:50.609456789
+8422-07-22 03:21:45.745036084	8422-04-13 15:59:12.621579295	8422-10-29 14:44:18.868492873	8422-10-29 14:44:18.868492873	8422-04-13 15:59:12.621579295	8422-04-13 15:59:12.621579295	8422-10-29 14:44:18.868492873
+8521-01-16 20:42:05.668832388	8520-10-09 10:19:32.545375599	8521-04-26 09:04:38.792289177	8521-04-26 09:04:38.792289177	8520-10-09 10:19:32.545375599	8520-10-09 10:19:32.545375599	8521-04-26 09:04:38.792289177
+9075-06-13 16:20:09.218517797	9075-03-06 03:57:36.095061008	9075-09-21 03:42:42.341974586	9075-09-21 03:42:42.341974586	9075-03-06 03:57:36.095061008	9075-03-06 03:57:36.095061008	9075-09-21 03:42:42.341974586
+9209-11-11 04:08:58.223768453	9209-08-03 17:46:25.100311664	9210-02-18 15:31:31.347225242	9210-02-18 15:31:31.347225242	9209-08-03 17:46:25.100311664	9209-08-03 17:46:25.100311664	9210-02-18 15:31:31.347225242
+9403-01-09 18:12:33.547	9402-10-02 07:50:00.423543211	9403-04-19 06:35:06.670456789	9403-04-19 06:35:06.670456789	9402-10-02 07:50:00.423543211	9402-10-02 07:50:00.423543211	9403-04-19 06:35:06.670456789
+PREHOOK: query: explain
+select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+POSTHOOK: type: QUERY
+Explain
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: interval_arithmetic_1
+            Statistics: Num rows: 50 Data size: 4800 Basic stats: COMPLETE Column stats: COMPLETE
+            Select Operator
+              expressions: 109 20:30:40.246913578 (type: interval_day_time), 89 02:14:26.000000000 (type: interval_day_time)
+              outputColumnNames: _col0, _col1
+              Statistics: Num rows: 50 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+              Limit
+                Number of rows: 2
+                Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 2 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+      Execution mode: vectorized
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 2
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+POSTHOOK: query: select
+  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
+  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
+from interval_arithmetic_1
+limit 2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@interval_arithmetic_1
+#### A masked pattern was here ####
+_c0	_c1
+109 20:30:40.246913578	89 02:14:26.000000000
+109 20:30:40.246913578	89 02:14:26.000000000
+PREHOOK: query: drop table interval_arithmetic_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@interval_arithmetic_1
+PREHOOK: Output: default@interval_arithmetic_1
+POSTHOOK: query: drop table interval_arithmetic_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@interval_arithmetic_1
+POSTHOOK: Output: default@interval_arithmetic_1
diff --git a/ql/src/test/results/clientpositive/vectorized_casts.q.out b/ql/src/test/results/clientpositive/vectorized_casts.q.out
index 6256400d8b..804653c758 100644
--- a/ql/src/test/results/clientpositive/vectorized_casts.q.out
+++ b/ql/src/test/results/clientpositive/vectorized_casts.q.out
@@ -350,18 +350,18 @@ true	NULL	true	true	true	NULL	true	false	true	true	11	NULL	-64615982	1803053750
 true	NULL	true	true	true	NULL	true	false	true	true	8	NULL	890988972	-1862301000	8	NULL	1	15	NULL	NULL	8	8	8	8.0	NULL	8.90988972E8	-1.862301E9	8.0	NULL	1.0	15.892	NULL	NULL	8.9098899E8	NULL	1969-12-31 16:00:00.008	NULL	1970-01-10 23:29:48.972	1969-12-10 02:41:39	1969-12-31 16:00:08	NULL	1969-12-31 16:00:00.001	1969-12-31 16:00:00	1969-12-31 16:00:15.892	NULL	NULL	8	NULL	890988972	-1862301000	8.0	NULL	TRUE	0	1969-12-31 16:00:15.892	XylAH4	XylAH4	XylAH4	8.0	1.781977944E9	0.9893582466233818	8.90988973E8
 true	NULL	true	true	true	NULL	true	false	true	true	8	NULL	930867246	1205399250	8	NULL	1	15	NULL	NULL	8	8	8	8.0	NULL	9.30867246E8	1.20539925E9	8.0	NULL	1.0	15.892	NULL	NULL	9.3086726E8	NULL	1969-12-31 16:00:00.008	NULL	1970-01-11 10:34:27.246	1970-01-14 14:49:59.25	1969-12-31 16:00:08	NULL	1969-12-31 16:00:00.001	1969-12-31 16:00:00	1969-12-31 16:00:15.892	NULL	NULL	8	NULL	930867246	1205399250	8.0	NULL	TRUE	0	1969-12-31 16:00:15.892	c1V8o1A	c1V8o1A	c1V8o1A	8.0	1.861734492E9	0.9893582466233818	9.30867247E8
 true	true	NULL	true	true	true	NULL	false	true	NULL	-14	-7196	NULL	-1552199500	-14	-7196	NULL	11	NULL	NULL	-14	-14	-14	-14.0	-7196.0	NULL	-1.5521995E9	-14.0	-7196.0	NULL	11.065	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.986	1969-12-31 15:59:52.804	NULL	1969-12-13 16:50:00.5	1969-12-31 15:59:46	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:11.065	NULL	NULL	-14	-7196	NULL	-1552199500	-14.0	-7196.0	NULL	0	1969-12-31 16:00:11.065	NULL	NULL	NULL	-14.0	NULL	-0.9906073556948704	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-21	-7196	NULL	1542429000	-21	-7196	NULL	-4	NULL	NULL	-21	-21	-21	-21.0	-7196.0	NULL	1.542429E9	-21.0	-7196.0	NULL	-4.1	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.979	1969-12-31 15:59:52.804	NULL	1970-01-18 12:27:09	1969-12-31 15:59:39	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:55.9	NULL	NULL	-21	-7196	NULL	1542429000	-21.0	-7196.0	NULL	0	1969-12-31 15:59:55.9	NULL	NULL	NULL	-21.0	NULL	-0.8366556385360561	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-24	-7196	NULL	829111000	-24	-7196	NULL	-6	NULL	NULL	-24	-24	-24	-24.0	-7196.0	NULL	8.29111E8	-24.0	-7196.0	NULL	-6.855	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.976	1969-12-31 15:59:52.804	NULL	1970-01-10 06:18:31	1969-12-31 15:59:36	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.145	NULL	NULL	-24	-7196	NULL	829111000	-24.0	-7196.0	NULL	0	1969-12-31 15:59:53.145	NULL	NULL	NULL	-24.0	NULL	0.9055783620066238	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-21	-7196	NULL	1542429000	-21	-7196	NULL	-5	NULL	NULL	-21	-21	-21	-21.0	-7196.0	NULL	1.542429E9	-21.0	-7196.0	NULL	-4.1	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.979	1969-12-31 15:59:52.804	NULL	1970-01-18 12:27:09	1969-12-31 15:59:39	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:55.9	NULL	NULL	-21	-7196	NULL	1542429000	-21.0	-7196.0	NULL	0	1969-12-31 15:59:55.9	NULL	NULL	NULL	-21.0	NULL	-0.8366556385360561	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-24	-7196	NULL	829111000	-24	-7196	NULL	-7	NULL	NULL	-24	-24	-24	-24.0	-7196.0	NULL	8.29111E8	-24.0	-7196.0	NULL	-6.855	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.976	1969-12-31 15:59:52.804	NULL	1970-01-10 06:18:31	1969-12-31 15:59:36	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.145	NULL	NULL	-24	-7196	NULL	829111000	-24.0	-7196.0	NULL	0	1969-12-31 15:59:53.145	NULL	NULL	NULL	-24.0	NULL	0.9055783620066238	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-30	-200	NULL	1429852250	-30	-200	NULL	12	NULL	NULL	-30	-30	-30	-30.0	-200.0	NULL	1.42985225E9	-30.0	-200.0	NULL	12.935	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.97	1969-12-31 15:59:59.8	NULL	1970-01-17 05:10:52.25	1969-12-31 15:59:30	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 16:00:12.935	NULL	NULL	-30	-200	NULL	1429852250	-30.0	-200.0	NULL	0	1969-12-31 16:00:12.935	NULL	NULL	NULL	-30.0	NULL	0.9880316240928618	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	-2006216750	-36	-200	NULL	-14	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	-2.00621675E9	-36.0	-200.0	NULL	-14.252	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1969-12-08 10:43:03.25	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.748	NULL	NULL	-36	-200	NULL	-2006216750	-36.0	-200.0	NULL	0	1969-12-31 15:59:45.748	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	1599879000	-36	-200	NULL	-6	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	1.599879E9	-36.0	-200.0	NULL	-6.183	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1970-01-19 04:24:39	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.817	NULL	NULL	-36	-200	NULL	1599879000	-36.0	-200.0	NULL	0	1969-12-31 15:59:53.817	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-38	15601	NULL	-1858689000	-38	15601	NULL	-1	NULL	NULL	-38	-38	-38	-38.0	15601.0	NULL	-1.858689E9	-38.0	15601.0	NULL	-1.386	NULL	NULL	NULL	15601.0	1969-12-31 15:59:59.962	1969-12-31 16:00:15.601	NULL	1969-12-10 03:41:51	1969-12-31 15:59:22	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:58.614	NULL	NULL	-38	15601	NULL	-1858689000	-38.0	15601.0	NULL	0	1969-12-31 15:59:58.614	NULL	NULL	NULL	-38.0	NULL	-0.2963685787093853	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	-2006216750	-36	-200	NULL	-15	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	-2.00621675E9	-36.0	-200.0	NULL	-14.252	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1969-12-08 10:43:03.25	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.748	NULL	NULL	-36	-200	NULL	-2006216750	-36.0	-200.0	NULL	0	1969-12-31 15:59:45.748	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-36	-200	NULL	1599879000	-36	-200	NULL	-7	NULL	NULL	-36	-36	-36	-36.0	-200.0	NULL	1.599879E9	-36.0	-200.0	NULL	-6.183	NULL	NULL	NULL	-200.0	1969-12-31 15:59:59.964	1969-12-31 15:59:59.8	NULL	1970-01-19 04:24:39	1969-12-31 15:59:24	1969-12-31 15:56:40	NULL	1969-12-31 16:00:00	1969-12-31 15:59:53.817	NULL	NULL	-36	-200	NULL	1599879000	-36.0	-200.0	NULL	0	1969-12-31 15:59:53.817	NULL	NULL	NULL	-36.0	NULL	0.9917788534431158	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-38	15601	NULL	-1858689000	-38	15601	NULL	-2	NULL	NULL	-38	-38	-38	-38.0	15601.0	NULL	-1.858689E9	-38.0	15601.0	NULL	-1.3860000000000001	NULL	NULL	NULL	15601.0	1969-12-31 15:59:59.962	1969-12-31 16:00:15.601	NULL	1969-12-10 03:41:51	1969-12-31 15:59:22	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:58.614	NULL	NULL	-38	15601	NULL	-1858689000	-38.0	15601.0	NULL	0	1969-12-31 15:59:58.614	NULL	NULL	NULL	-38.0	NULL	-0.2963685787093853	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-5	15601	NULL	612416000	-5	15601	NULL	4	NULL	NULL	-5	-5	-5	-5.0	15601.0	NULL	6.12416E8	-5.0	15601.0	NULL	4.679	NULL	NULL	NULL	15601.0	1969-12-31 15:59:59.995	1969-12-31 16:00:15.601	NULL	1970-01-07 18:06:56	1969-12-31 15:59:55	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 16:00:04.679	NULL	NULL	-5	15601	NULL	612416000	-5.0	15601.0	NULL	0	1969-12-31 16:00:04.679	NULL	NULL	NULL	-5.0	NULL	0.9589242746631385	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-50	-7196	NULL	-1031187250	-50	-7196	NULL	-5	NULL	NULL	-50	-50	-50	-50.0	-7196.0	NULL	-1.03118725E9	-50.0	-7196.0	NULL	-5.267	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.95	1969-12-31 15:59:52.804	NULL	1969-12-19 17:33:32.75	1969-12-31 15:59:10	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:54.733	NULL	NULL	-50	-7196	NULL	-1031187250	-50.0	-7196.0	NULL	0	1969-12-31 15:59:54.733	NULL	NULL	NULL	-50.0	NULL	0.26237485370392877	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-50	-7196	NULL	-1031187250	-50	-7196	NULL	-6	NULL	NULL	-50	-50	-50	-50.0	-7196.0	NULL	-1.03118725E9	-50.0	-7196.0	NULL	-5.267	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.95	1969-12-31 15:59:52.804	NULL	1969-12-19 17:33:32.75	1969-12-31 15:59:10	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:54.733	NULL	NULL	-50	-7196	NULL	-1031187250	-50.0	-7196.0	NULL	0	1969-12-31 15:59:54.733	NULL	NULL	NULL	-50.0	NULL	0.26237485370392877	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-59	-7196	NULL	-1604890000	-59	-7196	NULL	13	NULL	NULL	-59	-59	-59	-59.0	-7196.0	NULL	-1.60489E9	-59.0	-7196.0	NULL	13.15	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.941	1969-12-31 15:59:52.804	NULL	1969-12-13 02:11:50	1969-12-31 15:59:01	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:13.15	NULL	NULL	-59	-7196	NULL	-1604890000	-59.0	-7196.0	NULL	0	1969-12-31 16:00:13.15	NULL	NULL	NULL	-59.0	NULL	-0.6367380071391379	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	-60	-7196	NULL	1516314750	-60	-7196	NULL	-7	NULL	NULL	-60	-60	-60	-60.0	-7196.0	NULL	1.51631475E9	-60.0	-7196.0	NULL	-7.592	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.94	1969-12-31 15:59:52.804	NULL	1970-01-18 05:11:54.75	1969-12-31 15:59:00	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:52.408	NULL	NULL	-60	-7196	NULL	1516314750	-60.0	-7196.0	NULL	0	1969-12-31 15:59:52.408	NULL	NULL	NULL	-60.0	NULL	0.3048106211022167	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	-60	-7196	NULL	1516314750	-60	-7196	NULL	-8	NULL	NULL	-60	-60	-60	-60.0	-7196.0	NULL	1.51631475E9	-60.0	-7196.0	NULL	-7.592	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.94	1969-12-31 15:59:52.804	NULL	1970-01-18 05:11:54.75	1969-12-31 15:59:00	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 15:59:52.408	NULL	NULL	-60	-7196	NULL	1516314750	-60.0	-7196.0	NULL	0	1969-12-31 15:59:52.408	NULL	NULL	NULL	-60.0	NULL	0.3048106211022167	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	-8	-7196	NULL	-1849991500	-8	-7196	NULL	3	NULL	NULL	-8	-8	-8	-8.0	-7196.0	NULL	-1.8499915E9	-8.0	-7196.0	NULL	3.136	NULL	NULL	NULL	-7196.0	1969-12-31 15:59:59.992	1969-12-31 15:59:52.804	NULL	1969-12-10 06:06:48.5	1969-12-31 15:59:52	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:03.136	NULL	NULL	-8	-7196	NULL	-1849991500	-8.0	-7196.0	NULL	0	1969-12-31 16:00:03.136	NULL	NULL	NULL	-8.0	NULL	-0.9893582466233818	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	20	15601	NULL	-362433250	20	15601	NULL	-14	NULL	NULL	20	20	20	20.0	15601.0	NULL	-3.6243325E8	20.0	15601.0	NULL	-14.871	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.02	1969-12-31 16:00:15.601	NULL	1969-12-27 11:19:26.75	1969-12-31 16:00:20	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.129	NULL	NULL	20	15601	NULL	-362433250	20.0	15601.0	NULL	0	1969-12-31 15:59:45.129	NULL	NULL	NULL	20.0	NULL	0.9129452507276277	NULL
-true	true	NULL	true	true	true	NULL	false	true	NULL	48	15601	NULL	-795361000	48	15601	NULL	-9	NULL	NULL	48	48	48	48.0	15601.0	NULL	-7.95361E8	48.0	15601.0	NULL	-9.765	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.048	1969-12-31 16:00:15.601	NULL	1969-12-22 11:03:59	1969-12-31 16:00:48	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:50.235	NULL	NULL	48	15601	NULL	-795361000	48.0	15601.0	NULL	0	1969-12-31 15:59:50.235	NULL	NULL	NULL	48.0	NULL	-0.7682546613236668	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	20	15601	NULL	-362433250	20	15601	NULL	-15	NULL	NULL	20	20	20	20.0	15601.0	NULL	-3.6243325E8	20.0	15601.0	NULL	-14.871	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.02	1969-12-31 16:00:15.601	NULL	1969-12-27 11:19:26.75	1969-12-31 16:00:20	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:45.129	NULL	NULL	20	15601	NULL	-362433250	20.0	15601.0	NULL	0	1969-12-31 15:59:45.129	NULL	NULL	NULL	20.0	NULL	0.9129452507276277	NULL
+true	true	NULL	true	true	true	NULL	false	true	NULL	48	15601	NULL	-795361000	48	15601	NULL	-10	NULL	NULL	48	48	48	48.0	15601.0	NULL	-7.95361E8	48.0	15601.0	NULL	-9.765	NULL	NULL	NULL	15601.0	1969-12-31 16:00:00.048	1969-12-31 16:00:15.601	NULL	1969-12-22 11:03:59	1969-12-31 16:00:48	1969-12-31 20:20:01	NULL	1969-12-31 16:00:00	1969-12-31 15:59:50.235	NULL	NULL	48	15601	NULL	-795361000	48.0	15601.0	NULL	0	1969-12-31 15:59:50.235	NULL	NULL	NULL	48.0	NULL	-0.7682546613236668	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	5	-7196	NULL	-1015607500	5	-7196	NULL	10	NULL	NULL	5	5	5	5.0	-7196.0	NULL	-1.0156075E9	5.0	-7196.0	NULL	10.973	NULL	NULL	NULL	-7196.0	1969-12-31 16:00:00.005	1969-12-31 15:59:52.804	NULL	1969-12-19 21:53:12.5	1969-12-31 16:00:05	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:10.973	NULL	NULL	5	-7196	NULL	-1015607500	5.0	-7196.0	NULL	0	1969-12-31 16:00:10.973	NULL	NULL	NULL	5.0	NULL	-0.9589242746631385	NULL
 true	true	NULL	true	true	true	NULL	false	true	NULL	59	-7196	NULL	-1137754500	59	-7196	NULL	10	NULL	NULL	59	59	59	59.0	-7196.0	NULL	-1.1377545E9	59.0	-7196.0	NULL	10.956	NULL	NULL	NULL	-7196.0	1969-12-31 16:00:00.059	1969-12-31 15:59:52.804	NULL	1969-12-18 11:57:25.5	1969-12-31 16:00:59	1969-12-31 14:00:04	NULL	1969-12-31 16:00:00	1969-12-31 16:00:10.956	NULL	NULL	59	-7196	NULL	-1137754500	59.0	-7196.0	NULL	0	1969-12-31 16:00:10.956	NULL	NULL	NULL	59.0	NULL	0.6367380071391379	NULL
diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java b/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
index fdc64e7d4b..305fdbefb0 100644
--- a/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
+++ b/serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java
@@ -28,7 +28,6 @@
 import java.util.Date;
 
 import org.apache.hadoop.hive.common.type.HiveDecimal;
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
 import org.apache.hadoop.hive.serde2.ByteStream.RandomAccessOutput;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils;
 import org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.VInt;
@@ -323,7 +322,20 @@ public double getDouble() {
     return seconds + nanos / 1000000000;
   }
 
+  public static long getLong(Timestamp timestamp) {
+    return timestamp.getTime() / 1000;
+  }
 
+  /**
+  *
+  * @return double representation of the timestamp, accurate to nanoseconds
+  */
+ public static double getDouble(Timestamp timestamp) {
+   double seconds, nanos;
+   seconds = millisToSeconds(timestamp.getTime());
+   nanos = timestamp.getNanos();
+   return seconds + nanos / 1000000000;
+ }
 
   public void readFields(DataInput in) throws IOException {
     in.readFully(internalBytes, 0, 4);
@@ -543,6 +555,21 @@ public static Timestamp decimalToTimestamp(HiveDecimal d) {
     return t;
   }
 
+  public HiveDecimal getHiveDecimal() {
+    if (timestampEmpty) {
+      populateTimestamp();
+    }
+    return getHiveDecimal(timestamp);
+  }
+
+  public static HiveDecimal getHiveDecimal(Timestamp timestamp) {
+    // The BigDecimal class recommends not converting directly from double to BigDecimal,
+    // so we convert through a string...
+    Double timestampDouble = TimestampWritable.getDouble(timestamp);
+    HiveDecimal result = HiveDecimal.create(timestampDouble.toString());
+    return result;
+  }
+
   /**
    * Converts the time in seconds or milliseconds to a timestamp.
    * @param time time in seconds or in milliseconds
@@ -553,6 +580,17 @@ public static Timestamp longToTimestamp(long time, boolean intToTimestampInSecon
       return new Timestamp(intToTimestampInSeconds ?  time * 1000 : time);
   }
 
+  /**
+   * Converts the time in seconds or milliseconds to a timestamp.
+   * @param time time in seconds or in milliseconds
+   * @return the timestamp
+   */
+  public static void setTimestampFromLong(Timestamp timestamp, long time,
+      boolean intToTimestampInSeconds) {
+      // If the time is in seconds, converts it to milliseconds first.
+    timestamp.setTime(intToTimestampInSeconds ?  time * 1000 : time);
+  }
+
   public static Timestamp doubleToTimestamp(double f) {
     long seconds = (long) f;
 
@@ -576,6 +614,37 @@ public static Timestamp doubleToTimestamp(double f) {
     return t;
   }
 
+  public static void setTimestampFromDouble(Timestamp timestamp, double f) {
+    // Otherwise, BigDecimal throws an exception.  (Support vector operations that sometimes
+    // do work on double Not-a-Number NaN values).
+    if (Double.isNaN(f)) {
+      timestamp.setTime(0);
+      return;
+    }
+    // Algorithm used by TimestampWritable.doubleToTimestamp method.
+    // Allocates a BigDecimal object!
+
+    long seconds = (long) f;
+
+    // We must ensure the exactness of the double's fractional portion.
+    // 0.6 as the fraction part will be converted to 0.59999... and
+    // significantly reduce the savings from binary serialization
+    BigDecimal bd = new BigDecimal(String.valueOf(f));
+    bd = bd.subtract(new BigDecimal(seconds)).multiply(new BigDecimal(1000000000));
+    int nanos = bd.intValue();
+
+    // Convert to millis
+    long millis = seconds * 1000;
+    if (nanos < 0) {
+      millis -= 1000;
+      nanos += 1000000000;
+    }
+    timestamp.setTime(millis);
+
+    // Set remaining fractional portion to nanos
+    timestamp.setNanos(nanos);
+  }
+
   public static void setTimestamp(Timestamp t, byte[] bytes, int offset) {
     boolean hasDecimalOrSecondVInt = hasDecimalOrSecondVInt(bytes[offset]);
     long seconds = (long) TimestampWritable.getSeconds(bytes, offset);
diff --git a/common/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java b/storage-api/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java
similarity index 85%
rename from common/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java
rename to storage-api/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java
index e262f010eb..b891e27547 100644
--- a/common/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/common/type/HiveIntervalDayTime.java
@@ -18,12 +18,16 @@
 package org.apache.hadoop.hive.common.type;
 
 import java.math.BigDecimal;
+import java.sql.Timestamp;
+import java.util.Date;
 import java.util.concurrent.TimeUnit;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
 import org.apache.commons.lang.builder.HashCodeBuilder;
-import org.apache.hive.common.util.DateUtils;
+import org.apache.hive.common.util.IntervalDayTimeUtils;
+
+import sun.util.calendar.BaseCalendar;
 
 /**
  * Day-time interval type representing an offset in days/hours/minutes/seconds,
@@ -84,16 +88,24 @@ public long getTotalSeconds() {
     return totalSeconds;
   }
 
+  /**
+   *
+   * @return double representation of the interval day time, accurate to nanoseconds
+   */
+  public double getDouble() {
+    return totalSeconds + nanos / 1000000000;
+  }
+
   /**
    * Ensures that the seconds and nanoseconds fields have consistent sign
    */
   protected void normalizeSecondsAndNanos() {
     if (totalSeconds > 0 && nanos < 0) {
       --totalSeconds;
-      nanos += DateUtils.NANOS_PER_SEC;
+      nanos += IntervalDayTimeUtils.NANOS_PER_SEC;
     } else if (totalSeconds < 0 && nanos > 0) {
       ++totalSeconds;
-      nanos -= DateUtils.NANOS_PER_SEC;
+      nanos -= IntervalDayTimeUtils.NANOS_PER_SEC;
     }
   }
 
@@ -103,7 +115,7 @@ public void set(int days, int hours, int minutes, int seconds, int nanos) {
     totalSeconds += TimeUnit.HOURS.toSeconds(hours);
     totalSeconds += TimeUnit.MINUTES.toSeconds(minutes);
     totalSeconds += TimeUnit.NANOSECONDS.toSeconds(nanos);
-    nanos = nanos % DateUtils.NANOS_PER_SEC;
+    nanos = nanos % IntervalDayTimeUtils.NANOS_PER_SEC;
 
     this.totalSeconds = totalSeconds;
     this.nanos = nanos;
@@ -117,16 +129,10 @@ public void set(long seconds, int nanos) {
     normalizeSecondsAndNanos();
   }
 
-  public void set(PisaTimestamp pisaTimestamp) {
-    this.totalSeconds = pisaTimestamp.getEpochSeconds();
-    this.nanos = pisaTimestamp.getSignedNanos();
-    normalizeSecondsAndNanos();
-  }
-
   public void set(BigDecimal totalSecondsBd) {
     long totalSeconds = totalSecondsBd.longValue();
     BigDecimal fractionalSecs = totalSecondsBd.remainder(BigDecimal.ONE);
-    int nanos = fractionalSecs.multiply(DateUtils.NANOS_PER_SEC_BD).intValue();
+    int nanos = fractionalSecs.multiply(IntervalDayTimeUtils.NANOS_PER_SEC_BD).intValue();
     set(totalSeconds, nanos);
   }
 
@@ -138,11 +144,6 @@ public HiveIntervalDayTime negate() {
     return new HiveIntervalDayTime(-getTotalSeconds(), -getNanos());
   }
 
-  public PisaTimestamp pisaTimestampUpdate(PisaTimestamp pisaTimestamp) {
-    // NOTE: Our nanos here are *SIGNED*.
-    return pisaTimestamp.updateFromEpochSecondsAndSignedNanos(totalSeconds, nanos);
-  }
-
   @Override
   public int compareTo(HiveIntervalDayTime other) {
     long cmp = this.totalSeconds - other.totalSeconds;
@@ -166,6 +167,13 @@ public boolean equals(Object obj) {
     return 0 == compareTo((HiveIntervalDayTime) obj);
   }
 
+  /**
+   * Return a copy of this object.
+   */
+  public Object clone() {
+      return new HiveIntervalDayTime(totalSeconds, nanos);
+  }
+
   @Override
   public int hashCode() {
     return new HashCodeBuilder().append(totalSeconds).append(nanos).toHashCode();
@@ -201,23 +209,23 @@ public static HiveIntervalDayTime valueOf(String strVal) {
           sign = -1;
         }
         int days = sign *
-            DateUtils.parseNumericValueWithRange("day", patternMatcher.group(2),
+            IntervalDayTimeUtils.parseNumericValueWithRange("day", patternMatcher.group(2),
                 0, Integer.MAX_VALUE);
         byte hours = (byte) (sign *
-            DateUtils.parseNumericValueWithRange("hour", patternMatcher.group(3), 0, 23));
+            IntervalDayTimeUtils.parseNumericValueWithRange("hour", patternMatcher.group(3), 0, 23));
         byte minutes = (byte) (sign *
-            DateUtils.parseNumericValueWithRange("minute", patternMatcher.group(4), 0, 59));
+            IntervalDayTimeUtils.parseNumericValueWithRange("minute", patternMatcher.group(4), 0, 59));
         int seconds = 0;
         int nanos = 0;
         field = patternMatcher.group(5);
         if (field != null) {
           BigDecimal bdSeconds = new BigDecimal(field);
-          if (bdSeconds.compareTo(DateUtils.MAX_INT_BD) > 0) {
+          if (bdSeconds.compareTo(IntervalDayTimeUtils.MAX_INT_BD) > 0) {
             throw new IllegalArgumentException("seconds value of " + bdSeconds + " too large");
           }
           seconds = sign * bdSeconds.intValue();
           nanos = sign * bdSeconds.subtract(new BigDecimal(bdSeconds.toBigInteger()))
-              .multiply(DateUtils.NANOS_PER_SEC_BD).intValue();
+              .multiply(IntervalDayTimeUtils.NANOS_PER_SEC_BD).intValue();
         }
 
         result = new HiveIntervalDayTime(days, hours, minutes, seconds, nanos);
diff --git a/storage-api/src/java/org/apache/hadoop/hive/common/type/PisaTimestamp.java b/storage-api/src/java/org/apache/hadoop/hive/common/type/PisaTimestamp.java
deleted file mode 100644
index ac1e38a51b..0000000000
--- a/storage-api/src/java/org/apache/hadoop/hive/common/type/PisaTimestamp.java
+++ /dev/null
@@ -1,609 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.hive.common.type;
-
-import java.math.BigDecimal;
-import java.sql.Timestamp;
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import com.google.common.base.Preconditions;
-
-/**
- * Pisa project is named after the famous Leonardo of Pisa, or better known as Fibanacci.
- *
- * A Pisa timestamp is a timestamp without a time-zone (i.e. local) in the ISO-8601 calendar system,
- * such as 2007-12-03 10:15:30.0123456789, with accuracy to the nanosecond (1 billionth of a
- * second).
- *
- * Pisa timestamps use the same starting point as a java.sql.Timestamp -- the number of nanoseconds
- * since the epoch (1970-01-01, or the day Unix roared awake) where negative numbers represent
- * earlier days.
- *
- * However, we use the PisaTimestamp class which has different design requirements than
- * java.sql.Timestamp.  It is designed to be mutable and NOT thread-safe to avoid high memory
- * allocation / garbage collection costs.  And, provides for ease of use by our vectorization
- * code to avoid the high CPU data cache miss cost for small objects, too.  We do this by allowing
- * the epoch day and nano of day to be stored externally (i.e. vector arrays).
- *
- * And, importantly, PisaTimestamp is a light-weight class similar to the epochDay/NanoOfDay of
- * the newer Java 8 LocalDateTime class, except the timestamp is *indifferent* to timezone.
- *
- * A common usage would be to treat it as UTC.
- *
- * You can work with days, seconds, milliseconds, nanoseconds, etc.  But to work with months you
- * will need to convert to an external timestamp object and use calendars, etc.
- * *
- * The storage for a PisaTimestamp is:
- *
- *        long epochDay
- *            // The number of days since 1970-01-01 (==> similar to Java 8 LocalDate).
- *        long nanoOfDay
- *            // The number of nanoseconds within the day, with the range of
- *            //  0 to 24 * 60 * 60 * 1,000,000,000 - 1 (==> similar to Java 8 LocalTime).
- *
- * Both epochDay and nanoOfDay are signed.
- *
- * We when both epochDay and nanoOfDay are non-zero, we will maintain them so they have the
- * same sign.
- *
- */
-
-public class PisaTimestamp {
-
-  private static final long serialVersionUID = 1L;
-
-  private long epochDay;
-  private long nanoOfDay;
-
-  private Timestamp scratchTimestamp;
-
-  public static final long NANOSECONDS_PER_SECOND = TimeUnit.SECONDS.toNanos(1);
-  public static final long NANOSECONDS_PER_MILLISECOND = TimeUnit.MILLISECONDS.toNanos(1);
-  public static final long NANOSECONDS_PER_DAY = TimeUnit.DAYS.toNanos(1);
-
-  public static final long MILLISECONDS_PER_SECOND = TimeUnit.SECONDS.toMillis(1);
-  public static final long MILLISECONDS_PER_DAY = TimeUnit.DAYS.toMillis(1);
-
-  public static final long SECONDS_PER_DAY = TimeUnit.DAYS.toSeconds(1);
-
-  public static final long MIN_NANO_OF_DAY = -NANOSECONDS_PER_DAY;
-  public static final long MAX_NANO_OF_DAY = NANOSECONDS_PER_DAY;
-
-  public static final BigDecimal BIG_NANOSECONDS_PER_SECOND = new BigDecimal(NANOSECONDS_PER_SECOND);
-
-  public long getEpochDay() {
-    return epochDay;
-  }
-
-  public long getNanoOfDay() {
-    return nanoOfDay;
-  }
-
-  public PisaTimestamp() {
-    epochDay = 0;
-    nanoOfDay = 0;
-    scratchTimestamp = new Timestamp(0);
-  }
-
-  public PisaTimestamp(long epochDay, long nanoOfDay) {
-
-    Preconditions.checkState(validateIntegrity(epochDay, nanoOfDay),
-        "epochDay " + epochDay + ", nanoOfDay " + nanoOfDay + " not valid");
-
-    this.epochDay = epochDay;
-    this.nanoOfDay = nanoOfDay;
-    scratchTimestamp = new Timestamp(0);
-  }
-
-  public PisaTimestamp(Timestamp timestamp) {
-    super();
-    updateFromTimestamp(timestamp);
-  }
-
-  public void reset() {
-    epochDay = 0;
-    nanoOfDay = 0;
-  }
-
-  /**
-   * NOTE: This method validates the integrity rules between epoch day and nano of day,
-   * but not overflow/underflow of epoch day.  Since epoch day overflow/underflow can result
-   * from to client data input, that must be checked manually with <undone> as this
-   * class do not throw data range exceptions as a rule.  It leaves that choice to the caller.
-   * @param epochDay
-   * @param nanoOfDay
-   * @return true if epoch day and nano of day have integrity.
-   */
-  public static boolean validateIntegrity(long epochDay, long nanoOfDay) {
-
-    // Range check nano per day as invariant.
-    if (nanoOfDay >= NANOSECONDS_PER_DAY || nanoOfDay <= -NANOSECONDS_PER_DAY) {
-      return false;
-    }
-
-    // Signs of epoch day and nano of day must match.
-    if (!(epochDay >= 0 && nanoOfDay >= 0 ||
-          epochDay <= 0 && nanoOfDay <= 0)) {
-      return false;
-    }
-
-    return true;
-  }
-
-  /**
-   * Set this PisaTimestamp from another PisaTimestamp.
-   * @param source
-   * @return this
-   */
-  public PisaTimestamp update(PisaTimestamp source) {
-    this.epochDay = source.epochDay;
-    this.nanoOfDay = source.nanoOfDay;
-    return this;
-  }
-
-  /**
-   * Set this PisaTimestamp from a epoch day and nano of day.
-   * @param epochDay
-   * @param nanoOfDay
-   * @return this
-   */
-  public PisaTimestamp update(long epochDay, long nanoOfDay) {
-
-    Preconditions.checkState(validateIntegrity(epochDay, nanoOfDay),
-        "epochDay " + epochDay + ", nanoOfDay " + nanoOfDay + " not valid");
-
-    this.epochDay = epochDay;
-    this.nanoOfDay = nanoOfDay;
-    return this;
-  }
-
-  /**
-   * Set the PisaTimestamp from a Timestamp object.
-   * @param timestamp
-   * @return this
-   */
-  public PisaTimestamp updateFromTimestamp(Timestamp timestamp) {
-
-    long timestampTime = timestamp.getTime();
-    int nanos = timestamp.getNanos();
-
-    /**
-     * Since the Timestamp class always stores nanos as a positive quantity (0 .. 999,999,999),
-     * we have to adjust back the time (subtract) by 1,000,000,000 to get right quantity for
-     * our calculations below.  One thing it ensures is nanoOfDay will be negative.
-     */
-    if (timestampTime < 0 && nanos > 0) {
-      timestampTime -= MILLISECONDS_PER_SECOND;
-    }
-
-    // The Timestamp class does not use the milliseconds part (always 0).  It is covered by nanos.
-    long epochSeconds = timestampTime / MILLISECONDS_PER_SECOND;
-
-    nanoOfDay = (epochSeconds % SECONDS_PER_DAY) * NANOSECONDS_PER_SECOND + nanos;
-    epochDay = epochSeconds / SECONDS_PER_DAY + (nanoOfDay / NANOSECONDS_PER_DAY);
-
-    Preconditions.checkState(validateIntegrity(epochDay, nanoOfDay));
-    return this;
-  }
-
-  /**
-   * Set this PisaTimestamp from a timestamp milliseconds.
-   * @param epochMilliseconds
-   * @return this
-   */
-  public PisaTimestamp updateFromTimestampMilliseconds(long timestampMilliseconds) {
-    /**
-     * The Timestamp class setTime sets both the time (seconds stored as milliseconds) and
-     * the nanos.
-     */
-    scratchTimestamp.setTime(timestampMilliseconds);
-    updateFromTimestamp(scratchTimestamp);
-    return this;
-  }
-
-  /**
-   * Set this PisaTimestamp from a timestamp seconds.
-   * @param epochMilliseconds
-   * @return this
-   */
-  public PisaTimestamp updateFromTimestampSeconds(long timestampSeconds) {
-    return updateFromTimestampMilliseconds(timestampSeconds * MILLISECONDS_PER_SECOND);
-  }
-
-  /**
-   * Set this PisaTimestamp from a timestamp seconds.
-   * @param epochMilliseconds
-   * @return this
-   */
-  public PisaTimestamp updateFromTimestampSecondsWithFractionalNanoseconds(
-      double timestampSecondsWithFractionalNanoseconds) {
-
-    // Otherwise, BigDecimal throws an exception.  (Support vector operations that sometimes
-    // do work on double Not-a-Number NaN values).
-    if (Double.isNaN(timestampSecondsWithFractionalNanoseconds)) {
-      timestampSecondsWithFractionalNanoseconds = 0;
-    }
-    // Algorithm used by TimestampWritable.doubleToTimestamp method.
-    // Allocates a BigDecimal object!
-
-    long seconds = (long) timestampSecondsWithFractionalNanoseconds;
-
-    // We must ensure the exactness of the double's fractional portion.
-    // 0.6 as the fraction part will be converted to 0.59999... and
-    // significantly reduce the savings from binary serialization.
-    BigDecimal bd;
-
-    bd = new BigDecimal(String.valueOf(timestampSecondsWithFractionalNanoseconds));
-    bd = bd.subtract(new BigDecimal(seconds));       // Get the nanos fraction.
-    bd = bd.multiply(BIG_NANOSECONDS_PER_SECOND);    // Make nanos an integer.
-
-    int nanos = bd.intValue();
-
-    // Convert to millis
-    long millis = seconds * 1000;
-    if (nanos < 0) {
-      millis -= 1000;
-      nanos += 1000000000;
-    }
-
-    scratchTimestamp.setTime(millis);
-    scratchTimestamp.setNanos(nanos);
-    updateFromTimestamp(scratchTimestamp);
-    return this;
-  }
-
-  /**
-   * Set this PisaTimestamp from a epoch seconds and signed nanos (-999999999 to 999999999).
-   * @param epochSeconds
-   * @param signedNanos
-   * @return this
-   */
-  public PisaTimestamp updateFromEpochSecondsAndSignedNanos(long epochSeconds, int signedNanos) {
-
-    long nanoOfDay = (epochSeconds % SECONDS_PER_DAY) * NANOSECONDS_PER_SECOND + signedNanos;
-    long epochDay = epochSeconds / SECONDS_PER_DAY + nanoOfDay / NANOSECONDS_PER_DAY;
-
-    Preconditions.checkState(validateIntegrity(epochDay, nanoOfDay));
-
-    this.epochDay = epochDay;
-    this.nanoOfDay = nanoOfDay;
-    return this;
-  }
-
-  /**
-   * Set a scratch PisaTimestamp with this PisaTimestamp's values and return the scratch object.
-   * @param epochDay
-   * @param nanoOfDay
-   */
-  public PisaTimestamp scratchCopy(PisaTimestamp scratch) {
-
-    scratch.epochDay = epochDay;
-    scratch.nanoOfDay = nanoOfDay;
-    return scratch;
-  }
-
-  /**
-   * Set a Timestamp object from this PisaTimestamp.
-   * @param timestamp
-   */
-  public void timestampUpdate(Timestamp timestamp) {
-
-    /*
-     * java.sql.Timestamp consists of a long variable to store milliseconds and an integer variable for nanoseconds.
-     * The long variable is used to store only the full seconds converted to millis. For example for 1234 milliseconds,
-     * 1000 is stored in the long variable, and 234000000 (234 converted to nanoseconds) is stored as nanoseconds.
-     * The negative timestamps are also supported, but nanoseconds must be positive therefore millisecond part is
-     * reduced by one second.
-     */
-
-    long epochSeconds = epochDay * SECONDS_PER_DAY + nanoOfDay / NANOSECONDS_PER_SECOND;
-    long integralSecInMillis;
-    int nanos = (int) (nanoOfDay % NANOSECONDS_PER_SECOND); // The nanoseconds.
-    if (nanos < 0) {
-      nanos = (int) NANOSECONDS_PER_SECOND + nanos; // The positive nano-part that will be added to milliseconds.
-      integralSecInMillis = (epochSeconds - 1) * MILLISECONDS_PER_SECOND; // Reduce by one second.
-    } else {
-      integralSecInMillis = epochSeconds * MILLISECONDS_PER_SECOND; // Full seconds converted to millis.
-    }
-
-    timestamp.setTime(integralSecInMillis);
-    timestamp.setNanos(nanos);
-  }
-
-  /**
-   * Return the scratch timestamp with values from Pisa timestamp.
-   * @return
-   */
-  public Timestamp asScratchTimestamp() {
-    timestampUpdate(scratchTimestamp);
-    return scratchTimestamp;
-  }
-
-  /**
-   * Return the scratch timestamp for use by the caller.
-   * @return
-   */
-  public Timestamp useScratchTimestamp() {
-    return scratchTimestamp;
-  }
-
-  public int compareTo(PisaTimestamp another) {
-
-    if (epochDay == another.epochDay) {
-      if (nanoOfDay == another.nanoOfDay){
-        return 0;
-      } else {
-        return (nanoOfDay < another.nanoOfDay ? -1 : 1);
-      }
-    } else {
-      return (epochDay < another.epochDay ? -1: 1);
-    }
-  }
-
-  public static int compareTo(long epochDay1, long nanoOfDay1, PisaTimestamp another) {
-
-    if (epochDay1 == another.epochDay) {
-      if (nanoOfDay1 == another.nanoOfDay){
-        return 0;
-      } else {
-        return (nanoOfDay1 < another.nanoOfDay ? -1 : 1);
-      }
-    } else {
-      return (epochDay1 < another.epochDay ? -1: 1);
-    }
-  }
-
-  public static int compareTo(PisaTimestamp pisaTimestamp1, long epochDay2, long nanoOfDay2) {
-
-    if (pisaTimestamp1.epochDay == epochDay2) {
-      if (pisaTimestamp1.nanoOfDay == nanoOfDay2){
-        return 0;
-      } else {
-        return (pisaTimestamp1.nanoOfDay < nanoOfDay2 ? -1 : 1);
-      }
-    } else {
-      return (pisaTimestamp1.epochDay < epochDay2 ? -1: 1);
-    }
-  }
-
-  public static int compareTo(long epochDay1, long nanoOfDay1, long epochDay2, long nanoOfDay2) {
-
-    if (epochDay1 == epochDay2) {
-      if (nanoOfDay1 == nanoOfDay2){
-        return 0;
-      } else {
-        return (nanoOfDay1 < nanoOfDay2 ? -1 : 1);
-      }
-    } else {
-      return (epochDay1 < epochDay2 ? -1: 1);
-    }
-  }
-
-
-  /**
-   * Standard equals method override.
-   */
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != getClass()) {
-      return false;
-    }
-    return equals((PisaTimestamp) obj);
-  }
-
-  public boolean equals(PisaTimestamp other) {
-
-    if (epochDay == other.epochDay) {
-      if  (nanoOfDay == other.nanoOfDay) {
-        return true;
-      } else {
-        return false;
-      }
-    } else {
-        return false;
-    }
-  }
-
-  public static void add(PisaTimestamp pisaTimestamp1, PisaTimestamp pisaTimestamp2,
-      PisaTimestamp result) {
-    add(pisaTimestamp1.epochDay, pisaTimestamp1.nanoOfDay,
-        pisaTimestamp2.epochDay, pisaTimestamp2.nanoOfDay,
-        result);
-  }
-
-  public static void add(long epochDay1, long nanoOfDay1,
-      long epochDay2, long nanoOfDay2,
-      PisaTimestamp result) {
-
-    // Validate integrity rules between epoch day and nano of day.
-    Preconditions.checkState(PisaTimestamp.validateIntegrity(epochDay1, nanoOfDay1));
-    Preconditions.checkState(PisaTimestamp.validateIntegrity(epochDay2, nanoOfDay2));
-
-    long intermediateEpochDay = epochDay1 + epochDay2;
-    long intermediateNanoOfDay = nanoOfDay1 + nanoOfDay2;
-
-    // Normalize so both are positive or both are negative.
-    long normalizedEpochDay;
-    long normalizedNanoOfDay;
-    if (intermediateEpochDay > 0 && intermediateNanoOfDay < 0) {
-      normalizedEpochDay = intermediateEpochDay - 1;
-      normalizedNanoOfDay = intermediateNanoOfDay + NANOSECONDS_PER_DAY;
-    } else if (intermediateEpochDay < 0 && intermediateNanoOfDay > 0) {
-      normalizedEpochDay = intermediateEpochDay + 1;
-      normalizedNanoOfDay = intermediateNanoOfDay - NANOSECONDS_PER_DAY;
-    } else {
-      normalizedEpochDay = intermediateEpochDay;
-      normalizedNanoOfDay = intermediateNanoOfDay;
-    }
-
-    long resultEpochDay;
-    long resultNanoOfDay;
-    if (normalizedNanoOfDay >= NANOSECONDS_PER_DAY || normalizedNanoOfDay <= -NANOSECONDS_PER_DAY) {
-      // Adjust for carry or overflow...
-
-      resultEpochDay = normalizedEpochDay + normalizedNanoOfDay / NANOSECONDS_PER_DAY;
-      resultNanoOfDay = normalizedNanoOfDay % NANOSECONDS_PER_DAY;
-
-    } else {
-      resultEpochDay = normalizedEpochDay;
-      resultNanoOfDay = normalizedNanoOfDay;
-    }
-
-    // The update method will validate integrity rules between epoch day and nano of day,
-    // but not overflow/underflow of epoch day.
-    result.update(resultEpochDay, resultNanoOfDay);
-  }
-
-  public static void addSeconds(PisaTimestamp timestamp1, long epochSeconds, PisaTimestamp result) {
-    long epochDay = epochSeconds / SECONDS_PER_DAY;
-    long nanoOfDay = (epochSeconds % SECONDS_PER_DAY) * NANOSECONDS_PER_SECOND;
-    add(timestamp1.epochDay, timestamp1.nanoOfDay, epochDay, nanoOfDay, result);
-  }
-
-  public static void subtract(PisaTimestamp timestamp1, PisaTimestamp timestamp2,
-      PisaTimestamp result) {
-
-    add(timestamp1.epochDay, timestamp1.nanoOfDay, -timestamp2.epochDay, -timestamp2.nanoOfDay,
-        result);
-  }
-
-  public static void subtract(long epochDay1, long nanoOfDay1,
-      long epochDay2, long nanoOfDay2,
-      PisaTimestamp result) {
-
-    add(epochDay1, nanoOfDay1, -epochDay2, -nanoOfDay2, result);
-  }
-
-  public static void subtractSeconds(PisaTimestamp timestamp1, long epochSeconds,
-      PisaTimestamp result) {
-    long epochDay = epochSeconds / SECONDS_PER_DAY;
-    long nanoOfDay = (epochSeconds % SECONDS_PER_DAY) * NANOSECONDS_PER_SECOND;
-    add(timestamp1.epochDay, timestamp1.nanoOfDay, -epochDay, -nanoOfDay, result);
-  }
-
-  /**
-   * Rounds the number of milliseconds relative to the epoch down to the nearest whole number of
-   * seconds. 500 would round to 0, -500 would round to -1.
-   */
-  public static long timestampMillisToSeconds(long millis) {
-    if (millis >= 0) {
-      return millis / 1000;
-    } else {
-      return (millis - 999) / 1000;
-    }
-  }
-
-  /**
-   * Return a double with the integer part as the seconds and the fractional part as
-   * the nanoseconds the way the Timestamp class does it.
-   * @return seconds.nanoseconds
-   */
-  public double getTimestampSecondsWithFractionalNanos() {
-    // Algorithm must be the same as TimestampWritable.getDouble method.
-    timestampUpdate(scratchTimestamp);
-    double seconds = timestampMillisToSeconds(scratchTimestamp.getTime());
-    double nanos = scratchTimestamp.getNanos();
-    BigDecimal bigSeconds = new BigDecimal(seconds);
-    BigDecimal bigNanos = new BigDecimal(nanos).divide(BIG_NANOSECONDS_PER_SECOND);
-    return bigSeconds.add(bigNanos).doubleValue();
-  }
-
-  /**
-   * Return an integer as the seconds the way the Timestamp class does it.
-   * @return seconds.nanoseconds
-   */
-  public long getTimestampSeconds() {
-    // Algorithm must be the same as TimestampWritable.getSeconds method.
-    timestampUpdate(scratchTimestamp);
-    return timestampMillisToSeconds(scratchTimestamp.getTime());
-  }
-
-  /**
-   * Return an integer as the milliseconds the way the Timestamp class does it.
-   * @return seconds.nanoseconds
-   */
-  public long getTimestampMilliseconds() {
-    timestampUpdate(scratchTimestamp);
-    return scratchTimestamp.getTime();
-  }
-
-  /**
-   * Return the epoch seconds.
-   * @return
-   */
-  public long getEpochSeconds() {
-    return epochDay * SECONDS_PER_DAY + nanoOfDay / NANOSECONDS_PER_SECOND;
-  }
-
-  /**
-   * Return the epoch seconds, given the epoch day and nano of day.
-   * @param epochDay
-   * @param nanoOfDay
-   * @return
-   */
-  public static long getEpochSecondsFromEpochDayAndNanoOfDay(long epochDay, long nanoOfDay) {
-    return epochDay * SECONDS_PER_DAY + nanoOfDay / NANOSECONDS_PER_SECOND;
-  }
-
-  /**
-   * Return the signed nanos (-999999999 to 999999999).
-   * NOTE: Not the same as Timestamp class nanos (which are always positive).
-   */
-  public int getSignedNanos() {
-    return (int) (nanoOfDay % NANOSECONDS_PER_SECOND);
-  }
-
-  /**
-   * Return the signed nanos (-999999999 to 999999999).
-   * NOTE: Not the same as Timestamp class nanos (which are always positive).
-   */
-  public static int getSignedNanos(long nanoOfDay) {
-    return (int) (nanoOfDay % NANOSECONDS_PER_SECOND);
-  }
-
-  /**
-   * Return the epoch milliseconds.
-   * @return
-   */
-  public long getEpochMilliseconds() {
-    return epochDay * MILLISECONDS_PER_DAY + nanoOfDay / NANOSECONDS_PER_MILLISECOND;
-  }
-
-  /**
-   * Return the epoch seconds, given the epoch day and nano of day.
-   * @param epochDay
-   * @param nanoOfDay
-   * @return
-   */
-  public static long getEpochMillisecondsFromEpochDayAndNanoOfDay(long epochDay, long nanoOfDay) {
-    return epochDay * MILLISECONDS_PER_DAY + nanoOfDay / NANOSECONDS_PER_MILLISECOND;
-  }
-
-  @Override
-  public int hashCode() {
-    // UNDONE: We don't want to box the longs just to get the hash codes...
-    return new Long(epochDay).hashCode() ^ new Long(nanoOfDay).hashCode();
-  }
-
-  @Override
-  public String toString() {
-    timestampUpdate(scratchTimestamp);
-    return scratchTimestamp.toString();
-  }
-}
\ No newline at end of file
diff --git a/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java b/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java
index 13baff4057..3fb0cfd434 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java
@@ -18,21 +18,67 @@
 package org.apache.hadoop.hive.common.type;
 
 import java.sql.Timestamp;
+import java.text.DateFormat;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
 import java.util.Random;
 import java.util.concurrent.TimeUnit;
 
 public class RandomTypeUtil {
 
   public static final long NANOSECONDS_PER_SECOND = TimeUnit.SECONDS.toNanos(1);
+  public static final long MILLISECONDS_PER_SECOND = TimeUnit.SECONDS.toMillis(1);
+  public static final long NANOSECONDS_PER_MILLISSECOND = TimeUnit.MILLISECONDS.toNanos(1);
+
+  private static ThreadLocal<DateFormat> DATE_FORMAT =
+      new ThreadLocal<DateFormat>() {
+        @Override
+        protected DateFormat initialValue() {
+          return new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
+        }
+      };
+
+  // We've switched to Joda/Java Calendar which has a more limited time range....
+  public static int MIN_YEAR = 1900;
+  public static int MAX_YEAR = 3000;
+  private static long MIN_FOUR_DIGIT_YEAR_MILLIS = parseToMillis("1900-01-01 00:00:00");
+  private static long MAX_FOUR_DIGIT_YEAR_MILLIS = parseToMillis("3000-01-01 00:00:00");
+
+  private static long parseToMillis(String s) {
+    try {
+      return DATE_FORMAT.get().parse(s).getTime();
+    } catch (ParseException ex) {
+      throw new RuntimeException(ex);
+    }
+  }
 
   public static Timestamp getRandTimestamp(Random r) {
+    return getRandTimestamp(r, MIN_YEAR, MAX_YEAR);
+  }
+
+  public static Timestamp getRandTimestamp(Random r, int minYear, int maxYear) {
     String optionalNanos = "";
-    if (r.nextInt(2) == 1) {
+    switch (r.nextInt(4)) {
+    case 0:
+      // No nanos.
+      break;
+    case 1:
+      optionalNanos = String.format(".%09d",
+          Integer.valueOf(r.nextInt((int) NANOSECONDS_PER_SECOND)));
+      break;
+    case 2:
+      // Limit to milliseconds only...
       optionalNanos = String.format(".%09d",
-          Integer.valueOf(0 + r.nextInt((int) NANOSECONDS_PER_SECOND)));
+          Integer.valueOf(r.nextInt((int) MILLISECONDS_PER_SECOND)) * NANOSECONDS_PER_MILLISSECOND);
+      break;
+    case 3:
+      // Limit to below milliseconds only...
+      optionalNanos = String.format(".%09d",
+          Integer.valueOf(r.nextInt((int) NANOSECONDS_PER_MILLISSECOND)));
+      break;
     }
     String timestampStr = String.format("%04d-%02d-%02d %02d:%02d:%02d%s",
-        Integer.valueOf(0 + r.nextInt(10000)),  // year
+        Integer.valueOf(minYear + r.nextInt(maxYear - minYear + 1)),  // year
         Integer.valueOf(1 + r.nextInt(12)),      // month
         Integer.valueOf(1 + r.nextInt(28)),      // day
         Integer.valueOf(0 + r.nextInt(24)),      // hour
@@ -48,4 +94,22 @@ public static Timestamp getRandTimestamp(Random r) {
     }
     return timestampVal;
   }
+
+  public static long randomMillis(long minMillis, long maxMillis, Random rand) {
+    return minMillis + (long) ((maxMillis - minMillis) * rand.nextDouble());
+  }
+
+  public static long randomMillis(Random rand) {
+    return randomMillis(MIN_FOUR_DIGIT_YEAR_MILLIS, MAX_FOUR_DIGIT_YEAR_MILLIS, rand);
+  }
+
+  public static int randomNanos(Random rand, int decimalDigits) {
+    // Only keep the most significant decimalDigits digits.
+    int nanos = rand.nextInt((int) NANOSECONDS_PER_SECOND);
+    return nanos - nanos % (int) Math.pow(10, 9 - decimalDigits);
+  }
+
+  public static int randomNanos(Random rand) {
+    return randomNanos(rand, 9);
+  }
 }
\ No newline at end of file
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
index 4ae9c47a71..c069a5ff29 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/ColumnVector.java
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.hive.ql.exec.vector;
 
-import java.io.IOException;
 import java.util.Arrays;
 
 /**
@@ -43,6 +42,7 @@ public static enum Type {
     BYTES,
     DECIMAL,
     TIMESTAMP,
+    INTERVAL_DAY_TIME,
     STRUCT,
     LIST,
     MAP,
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IntervalDayTimeColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IntervalDayTimeColumnVector.java
new file mode 100644
index 0000000000..39ccea8843
--- /dev/null
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/IntervalDayTimeColumnVector.java
@@ -0,0 +1,348 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hive.ql.exec.vector;
+
+import java.util.Arrays;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+import org.apache.hadoop.io.Writable;
+
+/**
+ * This class represents a nullable interval day time column vector capable of handing a
+ * wide range of interval day time values.
+ *
+ * We store the 2 (value) fields of a HiveIntervalDayTime class in primitive arrays.
+ *
+ * We do this to avoid an array of Java HiveIntervalDayTime objects which would have poor storage
+ * and memory access characteristics.
+ *
+ * Generally, the caller will fill in a scratch HiveIntervalDayTime object with values from a row,
+ * work using the scratch HiveIntervalDayTime, and then perhaps update the column vector row
+ * with a result.
+ */
+public class IntervalDayTimeColumnVector extends ColumnVector {
+
+  /*
+   * The storage arrays for this column vector corresponds to the storage of a HiveIntervalDayTime:
+   */
+  private long[] totalSeconds;
+      // The values from HiveIntervalDayTime.getTotalSeconds().
+
+  private int[] nanos;
+      // The values from HiveIntervalDayTime.getNanos().
+
+  /*
+   * Scratch objects.
+   */
+  private final HiveIntervalDayTime scratchIntervalDayTime;
+
+  private Writable scratchWritable;
+      // Supports keeping a HiveIntervalDayTimeWritable object without having to import
+      // that definition...
+
+  /**
+   * Use this constructor by default. All column vectors
+   * should normally be the default size.
+   */
+  public IntervalDayTimeColumnVector() {
+    this(VectorizedRowBatch.DEFAULT_SIZE);
+  }
+
+  /**
+   * Don't use this except for testing purposes.
+   *
+   * @param len the number of rows
+   */
+  public IntervalDayTimeColumnVector(int len) {
+    super(len);
+
+    totalSeconds = new long[len];
+    nanos = new int[len];
+
+    scratchIntervalDayTime = new HiveIntervalDayTime();
+
+    scratchWritable = null;     // Allocated by caller.
+  }
+
+  /**
+   * Return the number of rows.
+   * @return
+   */
+  public int getLength() {
+    return totalSeconds.length;
+  }
+
+  /**
+   * Return a row's HiveIntervalDayTime.getTotalSeconds() value.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param elementNum
+   * @return
+   */
+  public long getTotalSeconds(int elementNum) {
+    return totalSeconds[elementNum];
+  }
+
+  /**
+   * Return a row's HiveIntervalDayTime.getNanos() value.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param elementNum
+   * @return
+   */
+  public long getNanos(int elementNum) {
+    return nanos[elementNum];
+  }
+
+  /**
+   * Return a row's HiveIntervalDayTime.getDouble() value.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param elementNum
+   * @return
+   */
+  public double getDouble(int elementNum) {
+    return asScratchIntervalDayTime(elementNum).getDouble();
+  }
+
+  /**
+   * Set a HiveIntervalDayTime object from a row of the column.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param intervalDayTime
+   * @param elementNum
+   */
+  public void intervalDayTimeUpdate(HiveIntervalDayTime intervalDayTime, int elementNum) {
+    intervalDayTime.set(totalSeconds[elementNum], nanos[elementNum]);
+  }
+
+
+  /**
+   * Return the scratch HiveIntervalDayTime object set from a row.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param elementNum
+   * @return
+   */
+  public HiveIntervalDayTime asScratchIntervalDayTime(int elementNum) {
+    scratchIntervalDayTime.set(totalSeconds[elementNum], nanos[elementNum]);
+    return scratchIntervalDayTime;
+  }
+
+  /**
+   * Return the scratch HiveIntervalDayTime (contents undefined).
+   * @return
+   */
+  public HiveIntervalDayTime getScratchIntervalDayTime() {
+    return scratchIntervalDayTime;
+  }
+
+  /**
+   * Compare row to HiveIntervalDayTime.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param elementNum
+   * @param intervalDayTime
+   * @return -1, 0, 1 standard compareTo values.
+   */
+  public int compareTo(int elementNum, HiveIntervalDayTime intervalDayTime) {
+    return asScratchIntervalDayTime(elementNum).compareTo(intervalDayTime);
+  }
+
+  /**
+   * Compare HiveIntervalDayTime to row.
+   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param intervalDayTime
+   * @param elementNum
+   * @return -1, 0, 1 standard compareTo values.
+   */
+  public int compareTo(HiveIntervalDayTime intervalDayTime, int elementNum) {
+    return intervalDayTime.compareTo(asScratchIntervalDayTime(elementNum));
+  }
+
+  /**
+   * Compare a row to another TimestampColumnVector's row.
+   * @param elementNum1
+   * @param intervalDayTimeColVector2
+   * @param elementNum2
+   * @return
+   */
+  public int compareTo(int elementNum1, IntervalDayTimeColumnVector intervalDayTimeColVector2,
+      int elementNum2) {
+    return asScratchIntervalDayTime(elementNum1).compareTo(
+        intervalDayTimeColVector2.asScratchIntervalDayTime(elementNum2));
+  }
+
+  /**
+   * Compare another TimestampColumnVector's row to a row.
+   * @param intervalDayTimeColVector1
+   * @param elementNum1
+   * @param elementNum2
+   * @return
+   */
+  public int compareTo(IntervalDayTimeColumnVector intervalDayTimeColVector1, int elementNum1,
+      int elementNum2) {
+    return intervalDayTimeColVector1.asScratchIntervalDayTime(elementNum1).compareTo(
+        asScratchIntervalDayTime(elementNum2));
+  }
+
+  @Override
+  public void setElement(int outElementNum, int inputElementNum, ColumnVector inputVector) {
+
+    IntervalDayTimeColumnVector timestampColVector = (IntervalDayTimeColumnVector) inputVector;
+
+    totalSeconds[outElementNum] = timestampColVector.totalSeconds[inputElementNum];
+    nanos[outElementNum] = timestampColVector.nanos[inputElementNum];
+  }
+
+  // Simplify vector by brute-force flattening noNulls and isRepeating
+  // This can be used to reduce combinatorial explosion of code paths in VectorExpressions
+  // with many arguments.
+  public void flatten(boolean selectedInUse, int[] sel, int size) {
+    flattenPush();
+    if (isRepeating) {
+      isRepeating = false;
+      long repeatFastTime = totalSeconds[0];
+      int repeatNanos = nanos[0];
+      if (selectedInUse) {
+        for (int j = 0; j < size; j++) {
+          int i = sel[j];
+          totalSeconds[i] = repeatFastTime;
+          nanos[i] = repeatNanos;
+        }
+      } else {
+        Arrays.fill(totalSeconds, 0, size, repeatFastTime);
+        Arrays.fill(nanos, 0, size, repeatNanos);
+      }
+      flattenRepeatingNulls(selectedInUse, sel, size);
+    }
+    flattenNoNulls(selectedInUse, sel, size);
+  }
+
+  /**
+   * Set a row from a HiveIntervalDayTime.
+   * We assume the entry has already been isRepeated adjusted.
+   * @param elementNum
+   * @param intervalDayTime
+   */
+  public void set(int elementNum, HiveIntervalDayTime intervalDayTime) {
+    this.totalSeconds[elementNum] = intervalDayTime.getTotalSeconds();
+    this.nanos[elementNum] = intervalDayTime.getNanos();
+  }
+
+  /**
+   * Set a row from the current value in the scratch interval day time.
+   * @param elementNum
+   */
+  public void setFromScratchIntervalDayTime(int elementNum) {
+    this.totalSeconds[elementNum] = scratchIntervalDayTime.getTotalSeconds();
+    this.nanos[elementNum] = scratchIntervalDayTime.getNanos();
+  }
+
+  /**
+   * Set row to standard null value(s).
+   * We assume the entry has already been isRepeated adjusted.
+   * @param elementNum
+   */
+  public void setNullValue(int elementNum) {
+    totalSeconds[elementNum] = 0;
+    nanos[elementNum] = 1;
+  }
+
+  // Copy the current object contents into the output. Only copy selected entries,
+  // as indicated by selectedInUse and the sel array.
+  public void copySelected(
+      boolean selectedInUse, int[] sel, int size, IntervalDayTimeColumnVector output) {
+
+    // Output has nulls if and only if input has nulls.
+    output.noNulls = noNulls;
+    output.isRepeating = false;
+
+    // Handle repeating case
+    if (isRepeating) {
+      output.totalSeconds[0] = totalSeconds[0];
+      output.nanos[0] = nanos[0];
+      output.isNull[0] = isNull[0];
+      output.isRepeating = true;
+      return;
+    }
+
+    // Handle normal case
+
+    // Copy data values over
+    if (selectedInUse) {
+      for (int j = 0; j < size; j++) {
+        int i = sel[j];
+        output.totalSeconds[i] = totalSeconds[i];
+        output.nanos[i] = nanos[i];
+      }
+    }
+    else {
+      System.arraycopy(totalSeconds, 0, output.totalSeconds, 0, size);
+      System.arraycopy(nanos, 0, output.nanos, 0, size);
+    }
+
+    // Copy nulls over if needed
+    if (!noNulls) {
+      if (selectedInUse) {
+        for (int j = 0; j < size; j++) {
+          int i = sel[j];
+          output.isNull[i] = isNull[i];
+        }
+      }
+      else {
+        System.arraycopy(isNull, 0, output.isNull, 0, size);
+      }
+    }
+  }
+
+  /**
+   * Fill all the vector entries with a HiveIntervalDayTime.
+   * @param intervalDayTime
+   */
+  public void fill(HiveIntervalDayTime intervalDayTime) {
+    noNulls = true;
+    isRepeating = true;
+    totalSeconds[0] = intervalDayTime.getTotalSeconds();
+    nanos[0] = intervalDayTime.getNanos();
+  }
+
+  /**
+   * Return a convenience writable object stored by this column vector.
+   * Supports keeping a TimestampWritable object without having to import that definition...
+   * @return
+   */
+  public Writable getScratchWritable() {
+    return scratchWritable;
+  }
+
+  /**
+   * Set the convenience writable object stored by this column vector
+   * @param scratchWritable
+   */
+  public void setScratchWritable(Writable scratchWritable) {
+    this.scratchWritable = scratchWritable;
+  }
+
+  @Override
+  public void stringifyValue(StringBuilder buffer, int row) {
+    if (isRepeating) {
+      row = 0;
+    }
+    if (noNulls || !isNull[row]) {
+      scratchIntervalDayTime.set(totalSeconds[row], nanos[row]);
+      buffer.append(scratchIntervalDayTime.toString());
+    } else {
+      buffer.append("null");
+    }
+  }
+}
\ No newline at end of file
diff --git a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
index b73a0d29a8..c0dd5edc26 100644
--- a/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
+++ b/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/TimestampColumnVector.java
@@ -20,37 +20,35 @@
 import java.sql.Timestamp;
 import java.util.Arrays;
 
-import org.apache.hadoop.hive.common.type.PisaTimestamp;
-import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
-import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.io.Writable;
 
-import com.google.common.base.Preconditions;
-
 /**
  * This class represents a nullable timestamp column vector capable of handing a wide range of
  * timestamp values.
  *
- * We use the PisaTimestamp which is designed to be mutable and avoid the heavy memory allocation
- * and CPU data cache miss costs.
+ * We store the 2 (value) fields of a Timestamp class in primitive arrays.
+ *
+ * We do this to avoid an array of Java Timestamp objects which would have poor storage
+ * and memory access characteristics.
+ *
+ * Generally, the caller will fill in a scratch timestamp object with values from a row, work
+ * using the scratch timestamp, and then perhaps update the column vector row with a result.
  */
 public class TimestampColumnVector extends ColumnVector {
 
   /*
-   * The storage arrays for this column vector corresponds to the storage of a PisaTimestamp:
+   * The storage arrays for this column vector corresponds to the storage of a Timestamp:
    */
-  private long[] epochDay;
-      // An array of the number of days since 1970-01-01 (similar to Java 8 LocalDate).
+  public long[] time;
+      // The values from Timestamp.getTime().
 
-  private long[] nanoOfDay;
-      // An array of the number of nanoseconds within the day, with the range of
-      // 0 to 24 * 60 * 60 * 1,000,000,000 - 1 (similar to Java 8 LocalTime).
+  public int[] nanos;
+      // The values from Timestamp.getNanos().
 
   /*
    * Scratch objects.
    */
-  private PisaTimestamp scratchPisaTimestamp;
-      // Convenience scratch Pisa timestamp object.
+  private final Timestamp scratchTimestamp;
 
   private Writable scratchWritable;
       // Supports keeping a TimestampWritable object without having to import that definition...
@@ -71,10 +69,10 @@ public TimestampColumnVector() {
   public TimestampColumnVector(int len) {
     super(len);
 
-    epochDay = new long[len];
-    nanoOfDay = new long[len];
+    time = new long[len];
+    nanos = new int[len];
 
-    scratchPisaTimestamp = new PisaTimestamp();
+    scratchTimestamp = new Timestamp(0);
 
     scratchWritable = null;     // Allocated by caller.
   }
@@ -84,48 +82,27 @@ public TimestampColumnVector(int len) {
    * @return
    */
   public int getLength() {
-    return epochDay.length;
+    return time.length;
   }
 
   /**
-   * Returnt a row's epoch day.
+   * Return a row's Timestamp.getTime() value.
    * We assume the entry has already been NULL checked and isRepeated adjusted.
    * @param elementNum
    * @return
    */
-  public long getEpochDay(int elementNum) {
-    return epochDay[elementNum];
+  public long getTime(int elementNum) {
+    return time[elementNum];
   }
 
   /**
-   * Return a row's nano of day.
+   * Return a row's Timestamp.getNanos() value.
    * We assume the entry has already been NULL checked and isRepeated adjusted.
    * @param elementNum
    * @return
    */
-  public long getNanoOfDay(int elementNum) {
-    return nanoOfDay[elementNum];
-  }
-
-  /**
-   * Get a scratch PisaTimestamp object from a row of the column.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
-   * @param elementNum
-   * @return scratch
-   */
-  public PisaTimestamp asScratchPisaTimestamp(int elementNum) {
-    scratchPisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
-    return scratchPisaTimestamp;
-  }
-
-  /**
-   * Set a PisaTimestamp object from a row of the column.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
-   * @param pisaTimestamp
-   * @param elementNum
-   */
-  public void pisaTimestampUpdate(PisaTimestamp pisaTimestamp, int elementNum) {
-    pisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
+  public int getNanos(int elementNum) {
+    return nanos[elementNum];
   }
 
   /**
@@ -135,154 +112,133 @@ public void pisaTimestampUpdate(PisaTimestamp pisaTimestamp, int elementNum) {
    * @param elementNum
    */
   public void timestampUpdate(Timestamp timestamp, int elementNum) {
-    scratchPisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
-    scratchPisaTimestamp.timestampUpdate(timestamp);
+    timestamp.setTime(time[elementNum]);
+    timestamp.setNanos(nanos[elementNum]);
   }
 
   /**
-   * Compare row to PisaTimestamp.
+   * Return the scratch Timestamp object set from a row.
    * We assume the entry has already been NULL checked and isRepeated adjusted.
    * @param elementNum
-   * @param pisaTimestamp
-   * @return -1, 0, 1 standard compareTo values.
-   */
-  public int compareTo(int elementNum, PisaTimestamp pisaTimestamp) {
-    return PisaTimestamp.compareTo(epochDay[elementNum], nanoOfDay[elementNum], pisaTimestamp);
-  }
-
-  /**
-   * Compare PisaTimestamp to row.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
-   * @param pisaTimestamp
-   * @param elementNum
-   * @return -1, 0, 1 standard compareTo values.
+   * @return
    */
-  public int compareTo(PisaTimestamp pisaTimestamp, int elementNum) {
-    return PisaTimestamp.compareTo(pisaTimestamp, epochDay[elementNum], nanoOfDay[elementNum]);
+  public Timestamp asScratchTimestamp(int elementNum) {
+    scratchTimestamp.setTime(time[elementNum]);
+    scratchTimestamp.setNanos(nanos[elementNum]);
+    return scratchTimestamp;
   }
 
   /**
-   * Compare a row to another TimestampColumnVector's row.
-   * @param elementNum1
-   * @param timestampColVector2
-   * @param elementNum2
+   * Return the scratch timestamp (contents undefined).
    * @return
    */
-  public int compareTo(int elementNum1, TimestampColumnVector timestampColVector2,
-      int elementNum2) {
-    return PisaTimestamp.compareTo(
-        epochDay[elementNum1], nanoOfDay[elementNum1],
-        timestampColVector2.epochDay[elementNum2], timestampColVector2.nanoOfDay[elementNum2]);
+  public Timestamp getScratchTimestamp() {
+    return scratchTimestamp;
   }
 
   /**
-   * Compare another TimestampColumnVector's row to a row.
-   * @param timestampColVector1
-   * @param elementNum1
-   * @param elementNum2
+   * Return a long representation of a Timestamp.
+   * @param elementNum
    * @return
    */
-  public int compareTo(TimestampColumnVector timestampColVector1, int elementNum1,
-      int elementNum2) {
-    return PisaTimestamp.compareTo(
-        timestampColVector1.epochDay[elementNum1], timestampColVector1.nanoOfDay[elementNum1],
-        epochDay[elementNum2], nanoOfDay[elementNum2]);
-  }
-
-  public void add(PisaTimestamp timestamp1, PisaTimestamp timestamp2, int resultElementNum) {
-    PisaTimestamp.add(timestamp1, timestamp2, scratchPisaTimestamp);
-    epochDay[resultElementNum] = scratchPisaTimestamp.getEpochDay();
-    nanoOfDay[resultElementNum] = scratchPisaTimestamp.getNanoOfDay();
-  }
-
-  public void subtract(PisaTimestamp timestamp1, PisaTimestamp timestamp2, int resultElementNum) {
-    PisaTimestamp.subtract(timestamp1, timestamp2, scratchPisaTimestamp);
-    epochDay[resultElementNum] = scratchPisaTimestamp.getEpochDay();
-    nanoOfDay[resultElementNum] = scratchPisaTimestamp.getNanoOfDay();
+  public long getTimestampAsLong(int elementNum) {
+    scratchTimestamp.setTime(time[elementNum]);
+    scratchTimestamp.setNanos(nanos[elementNum]);
+    return getTimestampAsLong(scratchTimestamp);
   }
 
   /**
-   * Return row as a double with the integer part as the seconds and the fractional part as
-   * the nanoseconds the way the Timestamp class does it.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
-   * @param elementNum
-   * @return seconds.nanoseconds
+   * Return a long representation of a Timestamp.
+   * @param timestamp
+   * @return
    */
-  public double getTimestampSecondsWithFractionalNanos(int elementNum) {
-    scratchPisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
-    return scratchPisaTimestamp.getTimestampSecondsWithFractionalNanos();
+  public static long getTimestampAsLong(Timestamp timestamp) {
+    return millisToSeconds(timestamp.getTime());
   }
 
+  // Copy of TimestampWritable.millisToSeconds
   /**
-   * Return row as integer as the seconds the way the Timestamp class does it.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
-   * @param elementNum
-   * @return seconds
+   * Rounds the number of milliseconds relative to the epoch down to the nearest whole number of
+   * seconds. 500 would round to 0, -500 would round to -1.
    */
-  public long getTimestampSeconds(int elementNum) {
-    scratchPisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
-    return scratchPisaTimestamp.getTimestampSeconds();
+  private static long millisToSeconds(long millis) {
+    if (millis >= 0) {
+      return millis / 1000;
+    } else {
+      return (millis - 999) / 1000;
+    }
   }
 
-
   /**
-   * Return row as milliseconds the way the Timestamp class does it.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * Return a double representation of a Timestamp.
    * @param elementNum
    * @return
    */
-  public long getTimestampMilliseconds(int elementNum) {
-    scratchPisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
-    return scratchPisaTimestamp.getTimestampMilliseconds();
+  public double getDouble(int elementNum) {
+    scratchTimestamp.setTime(time[elementNum]);
+    scratchTimestamp.setNanos(nanos[elementNum]);
+    return getDouble(scratchTimestamp);
   }
 
   /**
-   * Return row as epoch seconds.
-   * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * Return a double representation of a Timestamp.
    * @param elementNum
    * @return
    */
-  public long getEpochSeconds(int elementNum) {
-    return PisaTimestamp.getEpochSecondsFromEpochDayAndNanoOfDay(epochDay[elementNum], nanoOfDay[elementNum]);
+  public static double getDouble(Timestamp timestamp) {
+    // Same algorithm as TimestampWritable (not currently import-able here).
+    double seconds, nanos;
+    seconds = millisToSeconds(timestamp.getTime());
+    nanos = timestamp.getNanos();
+    return seconds + nanos / 1000000000;
   }
 
   /**
-   * Return row as epoch milliseconds.
+   * Compare row to Timestamp.
    * We assume the entry has already been NULL checked and isRepeated adjusted.
    * @param elementNum
-   * @return
+   * @param timestamp
+   * @return -1, 0, 1 standard compareTo values.
    */
-  public long getEpochMilliseconds(int elementNum) {
-    return PisaTimestamp.getEpochMillisecondsFromEpochDayAndNanoOfDay(epochDay[elementNum], nanoOfDay[elementNum]);
+  public int compareTo(int elementNum, Timestamp timestamp) {
+    return asScratchTimestamp(elementNum).compareTo(timestamp);
   }
 
   /**
-   * Return row as signed nanos (-999999999 to 999999999).
-   * NOTE: This is not the same as the Timestamp class nanos (which is always positive).
+   * Compare Timestamp to row.
    * We assume the entry has already been NULL checked and isRepeated adjusted.
+   * @param timestamp
    * @param elementNum
-   * @return
+   * @return -1, 0, 1 standard compareTo values.
    */
-  public int getSignedNanos(int elementNum) {
-    return PisaTimestamp.getSignedNanos(nanoOfDay[elementNum]);
+  public int compareTo(Timestamp timestamp, int elementNum) {
+    return timestamp.compareTo(asScratchTimestamp(elementNum));
   }
 
   /**
-   * Get scratch timestamp with value of a row.
-   * @param elementNum
+   * Compare a row to another TimestampColumnVector's row.
+   * @param elementNum1
+   * @param timestampColVector2
+   * @param elementNum2
    * @return
    */
-  public Timestamp asScratchTimestamp(int elementNum) {
-    scratchPisaTimestamp.update(epochDay[elementNum], nanoOfDay[elementNum]);
-    return scratchPisaTimestamp.asScratchTimestamp();
+  public int compareTo(int elementNum1, TimestampColumnVector timestampColVector2,
+      int elementNum2) {
+    return asScratchTimestamp(elementNum1).compareTo(
+        timestampColVector2.asScratchTimestamp(elementNum2));
   }
 
   /**
-   * Get scratch Pisa timestamp for use by the caller.
+   * Compare another TimestampColumnVector's row to a row.
+   * @param timestampColVector1
+   * @param elementNum1
+   * @param elementNum2
    * @return
    */
-  public PisaTimestamp useScratchPisaTimestamp() {
-    return scratchPisaTimestamp;
+  public int compareTo(TimestampColumnVector timestampColVector1, int elementNum1,
+      int elementNum2) {
+    return timestampColVector1.asScratchTimestamp(elementNum1).compareTo(
+        asScratchTimestamp(elementNum2));
   }
 
   @Override
@@ -290,8 +246,8 @@ public void setElement(int outElementNum, int inputElementNum, ColumnVector inpu
 
     TimestampColumnVector timestampColVector = (TimestampColumnVector) inputVector;
 
-    epochDay[outElementNum] = timestampColVector.epochDay[inputElementNum];
-    nanoOfDay[outElementNum] = timestampColVector.nanoOfDay[inputElementNum];
+    time[outElementNum] = timestampColVector.time[inputElementNum];
+    nanos[outElementNum] = timestampColVector.nanos[inputElementNum];
   }
 
   // Simplify vector by brute-force flattening noNulls and isRepeating
@@ -301,34 +257,23 @@ public void flatten(boolean selectedInUse, int[] sel, int size) {
     flattenPush();
     if (isRepeating) {
       isRepeating = false;
-      long repeatEpochDay = epochDay[0];
-      long repeatNanoOfDay = nanoOfDay[0];
+      long repeatFastTime = time[0];
+      int repeatNanos = nanos[0];
       if (selectedInUse) {
         for (int j = 0; j < size; j++) {
           int i = sel[j];
-          epochDay[i] = repeatEpochDay;
-          nanoOfDay[i] = repeatNanoOfDay;
+          time[i] = repeatFastTime;
+          nanos[i] = repeatNanos;
         }
       } else {
-        Arrays.fill(epochDay, 0, size, repeatEpochDay);
-        Arrays.fill(nanoOfDay, 0, size, repeatNanoOfDay);
+        Arrays.fill(time, 0, size, repeatFastTime);
+        Arrays.fill(nanos, 0, size, repeatNanos);
       }
       flattenRepeatingNulls(selectedInUse, sel, size);
     }
     flattenNoNulls(selectedInUse, sel, size);
   }
 
-  /**
-   * Set a row from a PisaTimestamp.
-   * We assume the entry has already been isRepeated adjusted.
-   * @param elementNum
-   * @param pisaTimestamp
-   */
-  public void set(int elementNum, PisaTimestamp pisaTimestamp) {
-    this.epochDay[elementNum] = pisaTimestamp.getEpochDay();
-    this.nanoOfDay[elementNum] = pisaTimestamp.getNanoOfDay();
-  }
-
   /**
    * Set a row from a timestamp.
    * We assume the entry has already been isRepeated adjusted.
@@ -336,54 +281,17 @@ public void set(int elementNum, PisaTimestamp pisaTimestamp) {
    * @param timestamp
    */
   public void set(int elementNum, Timestamp timestamp) {
-    scratchPisaTimestamp.updateFromTimestamp(timestamp);
-    this.epochDay[elementNum] = scratchPisaTimestamp.getEpochDay();
-    this.nanoOfDay[elementNum] = scratchPisaTimestamp.getNanoOfDay();
+    this.time[elementNum] = timestamp.getTime();
+    this.nanos[elementNum] = timestamp.getNanos();
   }
 
   /**
-   * Set a row from a epoch seconds and signed nanos (-999999999 to 999999999).
+   * Set a row from the current value in the scratch timestamp.
    * @param elementNum
-   * @param epochSeconds
-   * @param signedNanos
    */
-  public void setEpochSecondsAndSignedNanos(int elementNum, long epochSeconds, int signedNanos) {
-    scratchPisaTimestamp.updateFromEpochSecondsAndSignedNanos(epochSeconds, signedNanos);
-    set(elementNum, scratchPisaTimestamp);
-  }
-
-  /**
-   * Set a row from timestamp milliseconds.
-   * We assume the entry has already been isRepeated adjusted.
-   * @param elementNum
-   * @param timestampMilliseconds
-   */
-  public void setTimestampMilliseconds(int elementNum, long timestampMilliseconds) {
-    scratchPisaTimestamp.updateFromTimestampMilliseconds(timestampMilliseconds);
-    set(elementNum, scratchPisaTimestamp.useScratchTimestamp());
-  }
-
-  /**
-   * Set a row from timestamp seconds.
-   * We assume the entry has already been isRepeated adjusted.
-   * @param elementNum
-   * @param timestamp
-   */
-  public void setTimestampSeconds(int elementNum, long timestampSeconds) {
-    scratchPisaTimestamp.updateFromTimestampSeconds(timestampSeconds);
-    set(elementNum, scratchPisaTimestamp);
-  }
-
-  /**
-   * Set a row from a double timestamp seconds with fractional nanoseconds.
-   * We assume the entry has already been isRepeated adjusted.
-   * @param elementNum
-   * @param timestamp
-   */
-  public void setTimestampSecondsWithFractionalNanoseconds(int elementNum,
-      double secondsWithFractionalNanoseconds) {
-    scratchPisaTimestamp.updateFromTimestampSecondsWithFractionalNanoseconds(secondsWithFractionalNanoseconds);
-    set(elementNum, scratchPisaTimestamp);
+  public void setFromScratchTimestamp(int elementNum) {
+    this.time[elementNum] = scratchTimestamp.getTime();
+    this.nanos[elementNum] = scratchTimestamp.getNanos();
   }
 
   /**
@@ -392,8 +300,8 @@ public void setTimestampSecondsWithFractionalNanoseconds(int elementNum,
    * @param elementNum
    */
   public void setNullValue(int elementNum) {
-    epochDay[elementNum] = 0;
-    nanoOfDay[elementNum] = 1;
+    time[elementNum] = 0;
+    nanos[elementNum] = 1;
   }
 
   // Copy the current object contents into the output. Only copy selected entries,
@@ -407,8 +315,8 @@ public void copySelected(
 
     // Handle repeating case
     if (isRepeating) {
-      output.epochDay[0] = epochDay[0];
-      output.nanoOfDay[0] = nanoOfDay[0];
+      output.time[0] = time[0];
+      output.nanos[0] = nanos[0];
       output.isNull[0] = isNull[0];
       output.isRepeating = true;
       return;
@@ -420,13 +328,13 @@ public void copySelected(
     if (selectedInUse) {
       for (int j = 0; j < size; j++) {
         int i = sel[j];
-        output.epochDay[i] = epochDay[i];
-        output.nanoOfDay[i] = nanoOfDay[i];
+        output.time[i] = time[i];
+        output.nanos[i] = nanos[i];
       }
     }
     else {
-      System.arraycopy(epochDay, 0, output.epochDay, 0, size);
-      System.arraycopy(nanoOfDay, 0, output.nanoOfDay, 0, size);
+      System.arraycopy(time, 0, output.time, 0, size);
+      System.arraycopy(nanos, 0, output.nanos, 0, size);
     }
 
     // Copy nulls over if needed
@@ -443,17 +351,6 @@ public void copySelected(
     }
   }
 
-  /**
-   * Fill all the vector entries with a PisaTimestamp.
-   * @param pisaTimestamp
-   */
-  public void fill(PisaTimestamp pisaTimestamp) {
-    noNulls = true;
-    isRepeating = true;
-    epochDay[0] = pisaTimestamp.getEpochDay();
-    nanoOfDay[0] = pisaTimestamp.getNanoOfDay();
-  }
-
   /**
    * Fill all the vector entries with a timestamp.
    * @param timestamp
@@ -461,9 +358,8 @@ public void fill(PisaTimestamp pisaTimestamp) {
   public void fill(Timestamp timestamp) {
     noNulls = true;
     isRepeating = true;
-    scratchPisaTimestamp.updateFromTimestamp(timestamp);
-    epochDay[0] = scratchPisaTimestamp.getEpochDay();
-    nanoOfDay[0] = scratchPisaTimestamp.getNanoOfDay();
+    time[0] = timestamp.getTime();
+    nanos[0] = timestamp.getNanos();
   }
 
   /**
@@ -489,8 +385,9 @@ public void stringifyValue(StringBuilder buffer, int row) {
       row = 0;
     }
     if (noNulls || !isNull[row]) {
-      scratchPisaTimestamp.update(epochDay[row], nanoOfDay[row]);
-      buffer.append(scratchPisaTimestamp.toString());
+      scratchTimestamp.setTime(time[row]);
+      scratchTimestamp.setNanos(nanos[row]);
+      buffer.append(scratchTimestamp.toString());
     } else {
       buffer.append("null");
     }
diff --git a/storage-api/src/java/org/apache/hive/common/util/IntervalDayTimeUtils.java b/storage-api/src/java/org/apache/hive/common/util/IntervalDayTimeUtils.java
new file mode 100644
index 0000000000..727c1e6699
--- /dev/null
+++ b/storage-api/src/java/org/apache/hive/common/util/IntervalDayTimeUtils.java
@@ -0,0 +1,77 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.common.util;
+
+import java.math.BigDecimal;
+import java.text.SimpleDateFormat;
+
+import org.apache.hadoop.hive.common.type.HiveIntervalDayTime;
+
+
+/**
+ * DateUtils. Thread-safe class
+ *
+ */
+public class IntervalDayTimeUtils {
+
+  private static final ThreadLocal<SimpleDateFormat> dateFormatLocal = new ThreadLocal<SimpleDateFormat>() {
+    @Override
+    protected SimpleDateFormat initialValue() {
+      return new SimpleDateFormat("yyyy-MM-dd");
+    }
+  };
+
+  public static SimpleDateFormat getDateFormat() {
+    return dateFormatLocal.get();
+  }
+
+  public static final int NANOS_PER_SEC = 1000000000;
+  public static final BigDecimal MAX_INT_BD = new BigDecimal(Integer.MAX_VALUE);
+  public static final BigDecimal NANOS_PER_SEC_BD = new BigDecimal(NANOS_PER_SEC);
+
+  public static int parseNumericValueWithRange(String fieldName,
+      String strVal, int minValue, int maxValue) throws IllegalArgumentException {
+    int result = 0;
+    if (strVal != null) {
+      result = Integer.parseInt(strVal);
+      if (result < minValue || result > maxValue) {
+        throw new IllegalArgumentException(String.format("%s value %d outside range [%d, %d]",
+            fieldName, result, minValue, maxValue));
+      }
+    }
+    return result;
+  }
+
+  public static long getIntervalDayTimeTotalNanos(HiveIntervalDayTime intervalDayTime) {
+    return intervalDayTime.getTotalSeconds() * NANOS_PER_SEC + intervalDayTime.getNanos();
+  }
+
+  public static void setIntervalDayTimeTotalNanos(HiveIntervalDayTime intervalDayTime,
+      long totalNanos) {
+    intervalDayTime.set(totalNanos / NANOS_PER_SEC, (int) (totalNanos % NANOS_PER_SEC));
+  }
+
+  public static long getIntervalDayTimeTotalSecondsFromTotalNanos(long totalNanos) {
+    return totalNanos / NANOS_PER_SEC;
+  }
+
+  public static int getIntervalDayTimeNanosFromTotalNanos(long totalNanos) {
+    return (int) (totalNanos % NANOS_PER_SEC);
+  }
+}
diff --git a/storage-api/src/test/org/apache/hadoop/hive/common/type/TestPisaTimestamp.java b/storage-api/src/test/org/apache/hadoop/hive/common/type/TestPisaTimestamp.java
deleted file mode 100644
index 8e7395c515..0000000000
--- a/storage-api/src/test/org/apache/hadoop/hive/common/type/TestPisaTimestamp.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.hive.common.type;
-
-import org.junit.Test;
-
-import java.math.BigDecimal;
-import java.math.RoundingMode;
-import java.sql.Timestamp;
-import java.util.Random;
-import org.apache.hadoop.hive.common.type.RandomTypeUtil;
-
-import static org.junit.Assert.*;
-
-/**
- * Test for ListColumnVector
- */
-public class TestPisaTimestamp {
-
-  private static int TEST_COUNT = 5000;
-
-  @Test
-  public void testPisaTimestampCreate() throws Exception {
-
-    Random r = new Random(1234);
-
-    for (int i = 0; i < TEST_COUNT; i++) {
-      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r);
-      PisaTimestamp pisaTimestamp = new PisaTimestamp(randTimestamp);
-      Timestamp reconstructedTimestamp = new Timestamp(0);
-      pisaTimestamp.timestampUpdate(reconstructedTimestamp);
-      if (!randTimestamp.equals(reconstructedTimestamp)) {
-        assertTrue(false);
-      }
-    }
-  }
-
-  static BigDecimal BIG_MAX_LONG = new BigDecimal(Long.MAX_VALUE);
-  static BigDecimal BIG_MIN_LONG = new BigDecimal(Long.MIN_VALUE);
-  static BigDecimal BIG_NANOSECONDS_PER_DAY = new BigDecimal(PisaTimestamp.NANOSECONDS_PER_DAY);
-
-  static boolean beyondLongRange = false;
-
-  private BigDecimal[] randomEpochDayAndNanoOfDay(Random r) {
-    double randDouble = (r.nextDouble() - 0.5D) * 2.0D;
-    randDouble *= PisaTimestamp.NANOSECONDS_PER_DAY;
-    randDouble *= 365 * 10000;
-    BigDecimal bigDecimal = new BigDecimal(randDouble);
-    bigDecimal = bigDecimal.setScale(0, RoundingMode.HALF_UP);
-
-    if (bigDecimal.compareTo(BIG_MAX_LONG) > 0 || bigDecimal.compareTo(BIG_MIN_LONG) < 0) {
-      beyondLongRange = true;
-    }
-
-    BigDecimal[] divideAndRemainder = bigDecimal.divideAndRemainder(BIG_NANOSECONDS_PER_DAY);
-
-    return new BigDecimal[] {divideAndRemainder[0], divideAndRemainder[1], bigDecimal};
-  }
-
-  private BigDecimal pisaTimestampToBig(PisaTimestamp pisaTimestamp) {
-    BigDecimal bigNanoOfDay = new BigDecimal(pisaTimestamp.getNanoOfDay());
-
-    BigDecimal bigEpochDay = new BigDecimal(pisaTimestamp.getEpochDay());
-    BigDecimal result = bigEpochDay.multiply(BIG_NANOSECONDS_PER_DAY);
-    result = result.add(bigNanoOfDay);
-    return result;
-  }
-
-  @Test
-  public void testPisaTimestampArithmetic() throws Exception {
-
-    Random r = new Random(1234);
-
-
-    for (int i = 0; i < TEST_COUNT; i++) {
-      BigDecimal[] random1 = randomEpochDayAndNanoOfDay(r);
-      long epochDay1 = random1[0].longValue();
-      long nanoOfDay1 = random1[1].longValue();
-      PisaTimestamp pisa1 = new PisaTimestamp(epochDay1, nanoOfDay1);
-      BigDecimal big1 = random1[2];
-
-      BigDecimal[] random2 = randomEpochDayAndNanoOfDay(r);
-      long epochDay2 = random2[0].longValue();
-      long nanoOfDay2 = random2[1].longValue();
-      PisaTimestamp pisa2 = new PisaTimestamp(epochDay2, nanoOfDay2);
-      BigDecimal big2 = random2[2];
-
-      BigDecimal expectedBig;
-      PisaTimestamp pisaResult = new PisaTimestamp();
-      if (i % 2 == 0) {
-        expectedBig = big1.add(big2);
-        PisaTimestamp.add(pisa1, pisa2, pisaResult);
-      } else {
-        expectedBig = big1.add(big2.negate());
-        PisaTimestamp.subtract(pisa1, pisa2, pisaResult);
-      }
-      BigDecimal resultBig = pisaTimestampToBig(pisaResult);
-      assertEquals(expectedBig, resultBig);
-
-    }
-  }
-}
diff --git a/storage-api/src/test/org/apache/hadoop/hive/ql/exec/vector/TestTimestampColumnVector.java b/storage-api/src/test/org/apache/hadoop/hive/ql/exec/vector/TestTimestampColumnVector.java
new file mode 100644
index 0000000000..6e5d5c8cfb
--- /dev/null
+++ b/storage-api/src/test/org/apache/hadoop/hive/ql/exec/vector/TestTimestampColumnVector.java
@@ -0,0 +1,117 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.exec.vector;
+
+import org.junit.Test;
+
+import java.io.PrintWriter;
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.sql.Timestamp;
+import java.util.Date;
+import java.util.Random;
+
+import org.apache.hadoop.hive.common.type.RandomTypeUtil;
+
+import static org.junit.Assert.*;
+
+/**
+ * Test for ListColumnVector
+ */
+public class TestTimestampColumnVector {
+
+  private static int TEST_COUNT = 5000;
+
+  private static int fake = 0;
+
+  @Test
+  public void testSaveAndRetrieve() throws Exception {
+
+    Random r = new Random(1234);
+    TimestampColumnVector timestampColVector = new TimestampColumnVector();
+    Timestamp[] randTimestamps = new Timestamp[VectorizedRowBatch.DEFAULT_SIZE];
+
+    for (int i = 0; i < VectorizedRowBatch.DEFAULT_SIZE; i++) {
+      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r);
+      randTimestamps[i] = randTimestamp;
+      timestampColVector.set(i, randTimestamp);
+    }
+    for (int i = 0; i < VectorizedRowBatch.DEFAULT_SIZE; i++) {
+      Timestamp retrievedTimestamp = timestampColVector.asScratchTimestamp(i);
+      Timestamp randTimestamp = randTimestamps[i];
+      if (!retrievedTimestamp.equals(randTimestamp)) {
+        assertTrue(false);
+      }
+    }
+  }
+
+  @Test
+  public void testTimestampCompare() throws Exception {
+    Random r = new Random(1234);
+    TimestampColumnVector timestampColVector = new TimestampColumnVector();
+    Timestamp[] randTimestamps = new Timestamp[VectorizedRowBatch.DEFAULT_SIZE];
+    Timestamp[] candTimestamps = new Timestamp[VectorizedRowBatch.DEFAULT_SIZE];
+    int[] compareToLeftRights = new int[VectorizedRowBatch.DEFAULT_SIZE];
+    int[] compareToRightLefts = new int[VectorizedRowBatch.DEFAULT_SIZE];
+
+    for (int i = 0; i < VectorizedRowBatch.DEFAULT_SIZE; i++) {
+      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r);
+      randTimestamps[i] = randTimestamp;
+      timestampColVector.set(i, randTimestamp);
+      Timestamp candTimestamp = RandomTypeUtil.getRandTimestamp(r);
+      candTimestamps[i] = candTimestamp;
+      compareToLeftRights[i] = candTimestamp.compareTo(randTimestamp);
+      compareToRightLefts[i] = randTimestamp.compareTo(candTimestamp);
+    }
+
+    for (int i = 0; i < VectorizedRowBatch.DEFAULT_SIZE; i++) {
+      Timestamp retrievedTimestamp = timestampColVector.asScratchTimestamp(i);
+      Timestamp randTimestamp = randTimestamps[i];
+      if (!retrievedTimestamp.equals(randTimestamp)) {
+        assertTrue(false);
+      }
+      Timestamp candTimestamp = candTimestamps[i];
+      int compareToLeftRight = timestampColVector.compareTo(candTimestamp, i);
+      if (compareToLeftRight != compareToLeftRights[i]) {
+        assertTrue(false);
+      }
+      int compareToRightLeft = timestampColVector.compareTo(i, candTimestamp);
+      if (compareToRightLeft != compareToRightLefts[i]) {
+        assertTrue(false);
+      }
+    }
+  }
+
+  /*
+  @Test
+  public void testGenerate() throws Exception {
+    PrintWriter writer = new PrintWriter("/Users/you/timestamps.txt");
+    Random r = new Random(18485);
+    for (int i = 0; i < 25; i++) {
+      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r);
+      writer.println(randTimestamp.toString());
+    }
+    for (int i = 0; i < 25; i++) {
+      Timestamp randTimestamp = RandomTypeUtil.getRandTimestamp(r, 1965, 2025);
+      writer.println(randTimestamp.toString());
+    }
+    writer.close();
+  }
+  */
+}
