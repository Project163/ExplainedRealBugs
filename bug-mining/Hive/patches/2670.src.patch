diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index b0cb98be16..3fe67b2298 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -1044,7 +1044,7 @@ public static enum ConfVars {
         "That means if reducer-num of the child RS is fixed (order by or forced bucketing) and small, it can make very slow, single MR.\n" +
         "The optimization will be automatically disabled if number of reducers would be less than specified value."),
 
-    HIVEOPTSORTDYNAMICPARTITION("hive.optimize.sort.dynamic.partition", true,
+    HIVEOPTSORTDYNAMICPARTITION("hive.optimize.sort.dynamic.partition", false,
         "When enabled dynamic partitioning column will be globally sorted.\n" +
         "This way we can keep only one record writer open for each partition value\n" +
         "in the reducer thereby reducing the memory pressure on reducers."),
diff --git a/data/files/dynpart_test.txt b/data/files/dynpart_test.txt
new file mode 100644
index 0000000000..ab6cd4a150
--- /dev/null
+++ b/data/files/dynpart_test.txt
@@ -0,0 +1,24 @@
+24526172.99-11.32
+245261710022.633952.8
+24526172.1-2026.3
+2452617552.96-1363.84
+24526171765.07-4648.8
+2452617879.07-2185.76
+24526177412.832071.68
+245261785.825.61
+2452617565.92196.48
+24526175362.01-600.28
+24526173423.95-3164.07
+24526384133.98-775.72
+245263810171.1660.48
+2452638317.87-3775.38
+2452638156.67-4626.56
+24526381327.0857.97
+24526381971.35-488.25
+2452638181.03-207.24
+2452638267.01-3266.36
+24526380.15-241.22
+24526381524.33494.37
+2452638150.39-162.12
+24526381413.19178.08
+24526384329.49-4000.51
diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index e5257988bc..b7de49a8e3 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -75,6 +75,7 @@ minitez.query.files.shared=alter_merge_2_orc.q,\
   disable_merge_for_bucketing.q,\
   dynpart_sort_opt_vectorization.q,\
   dynpart_sort_optimization.q,\
+  dynpart_sort_optimization2.q,\
   enforce_order.q,\
   filter_join_breaktask.q,\
   filter_join_breaktask2.q,\
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 3ff0782c74..e02071c6f8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -29,6 +29,8 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -38,13 +40,13 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.ErrorMsg;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
-import org.apache.hadoop.hive.ql.io.RecordUpdater;
-import org.apache.hadoop.hive.ql.io.StatsProvidingRecordWriter;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils;
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormat;
 import org.apache.hadoop.hive.ql.io.HivePartitioner;
 import org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat;
+import org.apache.hadoop.hive.ql.io.RecordUpdater;
+import org.apache.hadoop.hive.ql.io.StatsProvidingRecordWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveFatalException;
 import org.apache.hadoop.hive.ql.plan.DynamicPartitionCtx;
@@ -72,14 +74,16 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.util.ReflectionUtils;
 
-import com.google.common.collect.Lists;
-
 /**
  * File Sink operator implementation.
  **/
 public class FileSinkOperator extends TerminalOperator<FileSinkDesc> implements
     Serializable {
 
+  public static final Log LOG = LogFactory.getLog(FileSinkOperator.class);
+  private static final boolean isInfoEnabled = LOG.isInfoEnabled();
+  private static final boolean isDebugEnabled = LOG.isDebugEnabled();
+
   protected transient HashMap<String, FSPaths> valToPaths;
   protected transient int numDynParts;
   protected transient List<String> dpColNames;
@@ -101,10 +105,6 @@ public class FileSinkOperator extends TerminalOperator<FileSinkDesc> implements
   protected transient boolean isCollectRWStats;
   private transient FSPaths prevFsp;
   private transient FSPaths fpaths;
-  private transient ObjectInspector keyOI;
-  private transient List<Object> keyWritables;
-  private transient List<String> keys;
-  private transient int numKeyColToRead;
   private StructField recIdField; // field to find record identifier in
   private StructField bucketField; // field bucket is in in record id
   private StructObjectInspector recIdInspector; // OI for inspecting record id
@@ -131,9 +131,6 @@ public class FSPaths implements Cloneable {
     int acidLastBucket = -1;
     int acidFileOffset = -1;
 
-    public FSPaths() {
-    }
-
     public FSPaths(Path specPath) {
       tmpPath = Utilities.toTempPath(specPath);
       taskOutputTempPath = Utilities.toTaskTempPath(specPath);
@@ -141,7 +138,9 @@ public FSPaths(Path specPath) {
       finalPaths = new Path[numFiles];
       outWriters = new RecordWriter[numFiles];
       updaters = new RecordUpdater[numFiles];
-      LOG.debug("Created slots for  " + numFiles);
+      if (isDebugEnabled) {
+        LOG.debug("Created slots for  " + numFiles);
+      }
       stat = new Stat();
     }
 
@@ -326,7 +325,6 @@ protected void initializeOp(Configuration hconf) throws HiveException {
       parent = Utilities.toTempPath(conf.getDirName());
       statsCollectRawDataSize = conf.isStatsCollectRawDataSize();
       statsFromRecordWriter = new boolean[numFiles];
-
       serializer = (Serializer) conf.getTableInfo().getDeserializerClass().newInstance();
       serializer.initialize(null, conf.getTableInfo().getProperties());
       outputClass = serializer.getSerializedClass();
@@ -363,20 +361,6 @@ protected void initializeOp(Configuration hconf) throws HiveException {
         lbSetup();
       }
 
-      int numPart = 0;
-      int numBuck = 0;
-      if (conf.getPartitionCols() != null && !conf.getPartitionCols().isEmpty()) {
-        numPart = conf.getPartitionCols().size();
-      }
-
-      // bucket number will exists only in PARTITION_BUCKET_SORTED mode
-      if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {
-        numBuck = 1;
-      }
-      numKeyColToRead = numPart + numBuck;
-      keys = Lists.newArrayListWithCapacity(numKeyColToRead);
-      keyWritables = Lists.newArrayListWithCapacity(numKeyColToRead);
-
       if (!bDynParts) {
         fsp = new FSPaths(specPath);
 
@@ -423,7 +407,8 @@ private void dpSetup() {
     this.dpColNames = dpCtx.getDPColNames();
     this.maxPartitions = dpCtx.getMaxPartitionsPerNode();
 
-    assert numDynParts == dpColNames.size() : "number of dynamic paritions should be the same as the size of DP mapping";
+    assert numDynParts == dpColNames.size()
+        : "number of dynamic paritions should be the same as the size of DP mapping";
 
     if (dpColNames != null && dpColNames.size() > 0) {
       this.bDynParts = true;
@@ -441,6 +426,9 @@ private void dpSetup() {
           newFieldsOI.add(sf.getFieldObjectInspector());
           newFieldsName.add(sf.getFieldName());
           this.dpStartCol++;
+        } else {
+          // once we found the start column for partition column we are done
+          break;
         }
       }
       assert newFieldsOI.size() > 0 : "new Fields ObjectInspector is empty";
@@ -457,11 +445,15 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {
       Set<Integer> seenBuckets = new HashSet<Integer>();
       for (int idx = 0; idx < totalFiles; idx++) {
         if (this.getExecContext() != null && this.getExecContext().getFileId() != null) {
-          LOG.info("replace taskId from execContext ");
+          if (isInfoEnabled) {
+            LOG.info("replace taskId from execContext ");
+          }
 
           taskId = Utilities.replaceTaskIdFromFilename(taskId, this.getExecContext().getFileId());
 
-          LOG.info("new taskId: FS " + taskId);
+          if (isInfoEnabled) {
+            LOG.info("new taskId: FS " + taskId);
+          }
 
           assert !multiFileSpray;
           assert totalFiles == 1;
@@ -515,9 +507,13 @@ protected void createBucketForFileIdx(FSPaths fsp, int filesIdx)
     try {
       if (isNativeTable) {
         fsp.finalPaths[filesIdx] = fsp.getFinalPath(taskId, fsp.tmpPath, null);
-        LOG.info("Final Path: FS " + fsp.finalPaths[filesIdx]);
+        if (isInfoEnabled) {
+          LOG.info("Final Path: FS " + fsp.finalPaths[filesIdx]);
+        }
         fsp.outPaths[filesIdx] = fsp.getTaskOutPath(taskId);
-        LOG.info("Writing to temp file: FS " + fsp.outPaths[filesIdx]);
+        if (isInfoEnabled) {
+          LOG.info("Writing to temp file: FS " + fsp.outPaths[filesIdx]);
+        }
       } else {
         fsp.finalPaths[filesIdx] = fsp.outPaths[filesIdx] = specPath;
       }
@@ -532,7 +528,9 @@ protected void createBucketForFileIdx(FSPaths fsp, int filesIdx)
         fsp.finalPaths[filesIdx] = fsp.getFinalPath(taskId, fsp.tmpPath, extension);
       }
 
-      LOG.info("New Final Path: FS " + fsp.finalPaths[filesIdx]);
+      if (isInfoEnabled) {
+        LOG.info("New Final Path: FS " + fsp.finalPaths[filesIdx]);
+      }
 
       if (isNativeTable) {
         // in recent hadoop versions, use deleteOnExit to clean tmp files.
@@ -604,14 +602,22 @@ public void processOp(Object row, int tag) throws HiveException {
       updateProgress();
 
       // if DP is enabled, get the final output writers and prepare the real output row
-      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT : "input object inspector is not struct";
+      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT
+          : "input object inspector is not struct";
 
       if (bDynParts) {
+
+        // we need to read bucket number which is the last column in value (after partition columns)
+        if (conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {
+          numDynParts += 1;
+        }
+
         // copy the DP column values from the input row to dpVals
         dpVals.clear();
         dpWritables.clear();
-        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol, numDynParts,
-            (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);
+        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol,numDynParts,
+            (StructObjectInspector) inputObjInspectors[0],ObjectInspectorCopyOption.WRITABLE);
+
         // get a set of RecordWriter based on the DP column values
         // pass the null value along to the escaping process to determine what the dir should be
         for (Object o : dpWritables) {
@@ -621,16 +627,11 @@ public void processOp(Object row, int tag) throws HiveException {
             dpVals.add(o.toString());
           }
         }
-        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row
-        recordValue = serializer.serialize(row, subSetOI);
 
-        // when dynamic partition sorting is not used, the DPSortState will be NONE
-        // in which we will fall back to old method of file system path creation
-        // i.e, having as many record writers as distinct values in partition column
-        if (conf.getDpSortState().equals(DPSortState.NONE)) {
-          fpaths = getDynOutPaths(dpVals, lbDirName);
-        }
+        fpaths = getDynOutPaths(dpVals, lbDirName);
 
+        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row
+        recordValue = serializer.serialize(row, subSetOI);
       } else {
         if (lbDirName != null) {
           fpaths = lookupListBucketingPaths(lbDirName);
@@ -686,8 +687,10 @@ public void processOp(Object row, int tag) throws HiveException {
           fpaths.updaters[++fpaths.acidFileOffset] = HiveFileFormatUtils.getAcidRecordUpdater(
               jc, conf.getTableInfo(), bucketNum, conf, fpaths.outPaths[fpaths.acidFileOffset],
               rowInspector, reporter, 0);
-          LOG.debug("Created updater for bucket number " + bucketNum + " using file " +
-              fpaths.outPaths[fpaths.acidFileOffset]);
+          if (isDebugEnabled) {
+            LOG.debug("Created updater for bucket number " + bucketNum + " using file " +
+                fpaths.outPaths[fpaths.acidFileOffset]);
+          }
         }
 
         if (conf.getWriteType() == AcidUtils.Operation.UPDATE) {
@@ -834,10 +837,8 @@ protected FSPaths getDynOutPaths(List<String> row, String lbDirName) throws Hive
     if (dpDir != null) {
       dpDir = appendToSource(lbDirName, dpDir);
       pathKey = dpDir;
-      int numericBucketNum = 0;
       if(conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {
         String buckNum = row.get(row.size() - 1);
-        numericBucketNum = Integer.valueOf(buckNum);
         taskId = Utilities.replaceTaskIdFromFilename(Utilities.getTaskId(hconf), buckNum);
         pathKey = appendToSource(taskId, dpDir);
       }
@@ -917,26 +918,6 @@ private String getDynPartDirectory(List<String> row, List<String> dpColNames) {
     return FileUtils.makePartName(dpColNames, row);
   }
 
-  @Override
-  public void startGroup() throws HiveException {
-    if (!conf.getDpSortState().equals(DPSortState.NONE)) {
-      keyOI = getGroupKeyObjectInspector();
-      keys.clear();
-      keyWritables.clear();
-      ObjectInspectorUtils.partialCopyToStandardObject(keyWritables, getGroupKeyObject(), 0,
-          numKeyColToRead, (StructObjectInspector) keyOI, ObjectInspectorCopyOption.WRITABLE);
-
-      for (Object o : keyWritables) {
-        if (o == null || o.toString().length() == 0) {
-          keys.add(dpCtx.getDefaultPartitionName());
-        } else {
-          keys.add(o.toString());
-        }
-      }
-      fpaths = getDynOutPaths(keys, null);
-    }
-  }
-
   @Override
   public void closeOp(boolean abort) throws HiveException {
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
index 62b640249b..4632f08791 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java
@@ -18,7 +18,22 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
+import java.io.Serializable;
+import java.lang.management.ManagementFactory;
+import java.lang.management.MemoryMXBean;
+import java.lang.reflect.Field;
+import java.sql.Timestamp;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
 import javolution.util.FastBitSet;
+
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -54,20 +69,6 @@
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.Text;
 
-import java.io.Serializable;
-import java.lang.management.ManagementFactory;
-import java.lang.management.MemoryMXBean;
-import java.lang.reflect.Field;
-import java.sql.Timestamp;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
 /**
  * GroupBy operator implementation.
  */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
index a066f98718..3dc7c76578 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java
@@ -498,8 +498,6 @@ protected final void defaultStartGroup() throws HiveException {
 
     LOG.debug("Starting group for children:");
     for (Operator<? extends OperatorDesc> op : childOperators) {
-      op.setGroupKeyObjectInspector(groupKeyOI);
-      op.setGroupKeyObject(groupKeyObject);
       op.startGroup();
     }
 
@@ -970,7 +968,6 @@ protected static StructObjectInspector initEvaluatorsAndReturnStruct(
   }
 
   protected transient Object groupKeyObject;
-  protected transient ObjectInspector groupKeyOI;
 
   public String getOperatorId() {
     return operatorId;
@@ -1287,14 +1284,6 @@ public void setStatistics(Statistics stats) {
     }
   }
 
-  public void setGroupKeyObjectInspector(ObjectInspector keyObjectInspector) {
-    this.groupKeyOI = keyObjectInspector;
-  }
-
-  public ObjectInspector getGroupKeyObjectInspector() {
-    return groupKeyOI;
-  }
-
   public static Operator createDummy() {
     return new DummyOperator();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
index 9bbc4ecc44..d8698dab07 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java
@@ -50,7 +50,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;
 import org.apache.hadoop.io.BinaryComparable;
 import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.io.Writable;
 import org.apache.hadoop.mapred.OutputCollector;
@@ -67,6 +66,9 @@ public class ReduceSinkOperator extends TerminalOperator<ReduceSinkDesc>
   }
 
   private static final Log LOG = LogFactory.getLog(ReduceSinkOperator.class.getName());
+  private static final boolean isInfoEnabled = LOG.isInfoEnabled();
+  private static final boolean isDebugEnabled = LOG.isDebugEnabled();
+  private static final boolean isTraceEnabled = LOG.isTraceEnabled();
   private static final long serialVersionUID = 1L;
   private static final MurmurHash hash = (MurmurHash) MurmurHash.getInstance();
 
@@ -117,6 +119,8 @@ public class ReduceSinkOperator extends TerminalOperator<ReduceSinkDesc>
   protected transient Object[] cachedValues;
   protected transient List<List<Integer>> distinctColIndices;
   protected transient Random random;
+  protected transient int bucketNumber;
+
   /**
    * This two dimensional array holds key data and a corresponding Union object
    * which contains the tag identifying the aggregate expression for distinct columns.
@@ -144,8 +148,14 @@ public class ReduceSinkOperator extends TerminalOperator<ReduceSinkDesc>
   protected void initializeOp(Configuration hconf) throws HiveException {
     try {
       List<ExprNodeDesc> keys = conf.getKeyCols();
-      LOG.debug("keys size is " + keys.size());
-      for (ExprNodeDesc k : keys) LOG.debug("Key exprNodeDesc " + k.getExprString());
+
+      if (isDebugEnabled) {
+        LOG.debug("keys size is " + keys.size());
+        for (ExprNodeDesc k : keys) {
+          LOG.debug("Key exprNodeDesc " + k.getExprString());
+        }
+      }
+
       keyEval = new ExprNodeEvaluator[keys.size()];
       int i = 0;
       for (ExprNodeDesc e : keys) {
@@ -184,7 +194,9 @@ protected void initializeOp(Configuration hconf) throws HiveException {
       tag = conf.getTag();
       tagByte[0] = (byte) tag;
       skipTag = conf.getSkipTag();
-      LOG.info("Using tag = " + tag);
+      if (isInfoEnabled) {
+        LOG.info("Using tag = " + tag);
+      }
 
       TableDesc keyTableDesc = conf.getKeySerializeInfo();
       keySerializer = (Serializer) keyTableDesc.getDeserializerClass()
@@ -284,7 +296,10 @@ public void processOp(Object row, int tag) throws HiveException {
           bucketInspector = (IntObjectInspector)bucketField.getFieldObjectInspector();
         }
 
-        LOG.info("keys are " + conf.getOutputKeyColumnNames() + " num distributions: " + conf.getNumDistributionKeys());
+        if (isInfoEnabled) {
+          LOG.info("keys are " + conf.getOutputKeyColumnNames() + " num distributions: " +
+              conf.getNumDistributionKeys());
+        }
         keyObjectInspector = initEvaluatorsAndReturnStruct(keyEval,
             distinctColIndices,
             conf.getOutputKeyColumnNames(), numDistributionKeys, rowInspector);
@@ -304,15 +319,14 @@ public void processOp(Object row, int tag) throws HiveException {
       populateCachedDistributionKeys(row, 0);
 
       // replace bucketing columns with hashcode % numBuckets
-      int buckNum = -1;
       if (bucketEval != null) {
-        buckNum = computeBucketNumber(row, conf.getNumBuckets());
-        cachedKeys[0][buckColIdxInKey] = new IntWritable(buckNum);
+        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());
+        cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));
       } else if (conf.getWriteType() == AcidUtils.Operation.UPDATE ||
           conf.getWriteType() == AcidUtils.Operation.DELETE) {
         // In the non-partitioned case we still want to compute the bucket number for updates and
         // deletes.
-        buckNum = computeBucketNumber(row, conf.getNumBuckets());
+        bucketNumber = computeBucketNumber(row, conf.getNumBuckets());
       }
 
       HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);
@@ -328,7 +342,7 @@ public void processOp(Object row, int tag) throws HiveException {
       if (autoParallel && partitionEval.length > 0) {
         hashCode = computeMurmurHash(firstKey);
       } else {
-        hashCode = computeHashCode(row, buckNum);
+        hashCode = computeHashCode(row);
       }
 
       firstKey.setHashCode(hashCode);
@@ -377,7 +391,9 @@ private int computeBucketNumber(Object row, int numBuckets) throws HiveException
       // column directly.
       Object recIdValue = acidRowInspector.getStructFieldData(row, recIdField);
       buckNum = bucketInspector.get(recIdInspector.getStructFieldData(recIdValue, bucketField));
-      LOG.debug("Acid choosing bucket number " + buckNum);
+      if (isTraceEnabled) {
+        LOG.trace("Acid choosing bucket number " + buckNum);
+      }
     } else {
       for (int i = 0; i < bucketEval.length; i++) {
         Object o = bucketEval[i].evaluate(row);
@@ -422,7 +438,7 @@ protected final int computeMurmurHash(HiveKey firstKey) {
     return hash.hash(firstKey.getBytes(), firstKey.getDistKeyLength(), 0);
   }
 
-  private int computeHashCode(Object row, int buckNum) throws HiveException {
+  private int computeHashCode(Object row) throws HiveException {
     // Evaluate the HashCode
     int keyHashCode = 0;
     if (partitionEval.length == 0) {
@@ -446,8 +462,10 @@ private int computeHashCode(Object row, int buckNum) throws HiveException {
             + ObjectInspectorUtils.hashCode(o, partitionObjectInspectors[i]);
       }
     }
-    LOG.debug("Going to return hash code " + (keyHashCode * 31 + buckNum));
-    return buckNum < 0  ? keyHashCode : keyHashCode * 31 + buckNum;
+    if (isTraceEnabled) {
+      LOG.trace("Going to return hash code " + (keyHashCode * 31 + bucketNumber));
+    }
+    return bucketNumber < 0  ? keyHashCode : keyHashCode * 31 + bucketNumber;
   }
 
   private boolean partitionKeysAreNull(Object row) throws HiveException {
@@ -493,10 +511,19 @@ protected void collect(BytesWritable keyWritable, Writable valueWritable) throws
   }
 
   private BytesWritable makeValueWritable(Object row) throws Exception {
+    int length = valueEval.length;
+
+    // in case of bucketed table, insert the bucket number as the last column in value
+    if (bucketEval != null) {
+      length -= 1;
+      cachedValues[length] = new Text(String.valueOf(bucketNumber));
+    }
+
     // Evaluate the value
-    for (int i = 0; i < valueEval.length; i++) {
+    for (int i = 0; i < length; i++) {
       cachedValues[i] = valueEval[i].evaluate(row);
     }
+
     // Serialize the value
     return (BytesWritable) valueSerializer.serialize(cachedValues, valueObjectInspector);
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java
index c9e469c734..9cd8b569e7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java
@@ -66,6 +66,8 @@
 public class ExecReducer extends MapReduceBase implements Reducer {
 
   private static final Log LOG = LogFactory.getLog("ExecReducer");
+  private static final boolean isInfoEnabled = LOG.isInfoEnabled();
+  private static final boolean isTraceEnabled = LOG.isTraceEnabled();
   private static final String PLAN_KEY = "__REDUCE_PLAN__";
 
   // used to log memory usage periodically
@@ -75,7 +77,6 @@ public class ExecReducer extends MapReduceBase implements Reducer {
   private final Deserializer[] inputValueDeserializer = new Deserializer[Byte.MAX_VALUE];
   private final Object[] valueObject = new Object[Byte.MAX_VALUE];
   private final List<Object> row = new ArrayList<Object>(Utilities.reduceFieldNameList.size());
-  private final boolean isLogInfoEnabled = LOG.isInfoEnabled();
 
   // TODO: move to DynamicSerDe when it's ready
   private Deserializer inputKeyDeserializer;
@@ -101,16 +102,18 @@ public void configure(JobConf job) {
     ObjectInspector[] valueObjectInspector = new ObjectInspector[Byte.MAX_VALUE];
     ObjectInspector keyObjectInspector;
 
-    LOG.info("maximum memory = " + memoryMXBean.getHeapMemoryUsage().getMax());
+    if (isInfoEnabled) {
+      LOG.info("maximum memory = " + memoryMXBean.getHeapMemoryUsage().getMax());
 
-    try {
-      LOG.info("conf classpath = "
-          + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));
-      LOG.info("thread classpath = "
-          + Arrays.asList(((URLClassLoader) Thread.currentThread()
-          .getContextClassLoader()).getURLs()));
-    } catch (Exception e) {
-      LOG.info("cannot get classpath: " + e.getMessage());
+      try {
+        LOG.info("conf classpath = "
+            + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));
+        LOG.info("thread classpath = "
+            + Arrays.asList(((URLClassLoader) Thread.currentThread()
+            .getContextClassLoader()).getURLs()));
+      } catch (Exception e) {
+        LOG.info("cannot get classpath: " + e.getMessage());
+      }
     }
     jc = job;
 
@@ -147,7 +150,6 @@ public void configure(JobConf job) {
         ArrayList<ObjectInspector> ois = new ArrayList<ObjectInspector>();
         ois.add(keyObjectInspector);
         ois.add(valueObjectInspector[tag]);
-        reducer.setGroupKeyObjectInspector(keyObjectInspector);
         rowObjectInspector[tag] = ObjectInspectorFactory
             .getStandardStructObjectInspector(Utilities.reduceFieldNameList, ois);
       }
@@ -202,7 +204,9 @@ public void reduce(Object key, Iterator values, OutputCollector output,
           groupKey = new BytesWritable();
         } else {
           // If a operator wants to do some work at the end of a group
-          LOG.trace("End Group");
+          if (isTraceEnabled) {
+            LOG.trace("End Group");
+          }
           reducer.endGroup();
         }
 
@@ -217,9 +221,11 @@ public void reduce(Object key, Iterator values, OutputCollector output,
         }
 
         groupKey.set(keyWritable.get(), 0, keyWritable.getSize());
-        LOG.trace("Start Group");
-        reducer.setGroupKeyObject(keyObject);
+        if (isTraceEnabled) {
+          LOG.trace("Start Group");
+        }
         reducer.startGroup();
+        reducer.setGroupKeyObject(keyObject);
       }
       // System.err.print(keyObject.toString());
       while (values.hasNext()) {
@@ -239,12 +245,14 @@ public void reduce(Object key, Iterator values, OutputCollector output,
         row.clear();
         row.add(keyObject);
         row.add(valueObject[tag]);
-        if (isLogInfoEnabled) {
+        if (isInfoEnabled) {
           cntr++;
           if (cntr == nextCntr) {
             long used_memory = memoryMXBean.getHeapMemoryUsage().getUsed();
-            LOG.info("ExecReducer: processing " + cntr
-                + " rows: used memory = " + used_memory);
+            if (isInfoEnabled) {
+              LOG.info("ExecReducer: processing " + cntr
+                  + " rows: used memory = " + used_memory);
+            }
             nextCntr = getNextCntr(cntr);
           }
         }
@@ -290,17 +298,19 @@ private long getNextCntr(long cntr) {
   public void close() {
 
     // No row was processed
-    if (oc == null) {
+    if (oc == null && isTraceEnabled) {
       LOG.trace("Close called without any rows processed");
     }
 
     try {
       if (groupKey != null) {
         // If a operator wants to do some work at the end of a group
-        LOG.trace("End Group");
+        if (isTraceEnabled) {
+          LOG.trace("End Group");
+        }
         reducer.endGroup();
       }
-      if (isLogInfoEnabled) {
+      if (isInfoEnabled) {
         LOG.info("ExecReducer: processed " + cntr + " rows: used memory = "
             + memoryMXBean.getHeapMemoryUsage().getUsed());
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
index 8a54433044..72e2fe5c77 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java
@@ -131,7 +131,6 @@ void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyT
           .getDeserializerClass(), null);
       SerDeUtils.initializeSerDe(inputKeyDeserializer, null, keyTableDesc.getProperties(), null);
       keyObjectInspector = inputKeyDeserializer.getObjectInspector();
-      reducer.setGroupKeyObjectInspector(keyObjectInspector);
 
       if(vectorized) {
         keyStructInspector = (StructObjectInspector) keyObjectInspector;
@@ -240,8 +239,8 @@ public boolean pushRecord() throws HiveException {
         }
 
         groupKey.set(keyWritable.getBytes(), 0, keyWritable.getLength());
-        reducer.setGroupKeyObject(keyObject);
         reducer.startGroup();
+        reducer.setGroupKeyObject(keyObject);
       }
 
       /* this.keyObject passed via reference */
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorReduceSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorReduceSinkOperator.java
index c0964146f6..6c9d8e1b00 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorReduceSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorReduceSinkOperator.java
@@ -41,7 +41,6 @@
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
 import org.apache.hadoop.hive.serde2.objectinspector.StandardUnionObjectInspector.StandardUnion;
 import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.Text;
 // import org.apache.hadoop.util.StringUtils;
 
@@ -270,10 +269,9 @@ public void processOp(Object row, int tag) throws HiveException {
         populatedCachedDistributionKeys(vrg, rowIndex, 0);
 
         // replace bucketing columns with hashcode % numBuckets
-        int buckNum = -1;
         if (bucketEval != null) {
-          buckNum = computeBucketNumber(vrg, rowIndex, conf.getNumBuckets());
-          cachedKeys[0][buckColIdxInKey] = new IntWritable(buckNum);
+          bucketNumber = computeBucketNumber(vrg, rowIndex, conf.getNumBuckets());
+          cachedKeys[0][buckColIdxInKey] = new Text(String.valueOf(bucketNumber));
         }
         HiveKey firstKey = toHiveKey(cachedKeys[0], tag, null);
         int distKeyLength = firstKey.getDistKeyLength();
@@ -289,7 +287,7 @@ public void processOp(Object row, int tag) throws HiveException {
         if (autoParallel && partitionEval.length > 0) {
           hashCode = computeMurmurHash(firstKey);
         } else {
-          hashCode = computeHashCode(vrg, rowIndex, buckNum);
+          hashCode = computeHashCode(vrg, rowIndex);
         }
 
         firstKey.setHashCode(hashCode);
@@ -417,7 +415,15 @@ private void populateCachedDistinctKeys(
 
   private BytesWritable makeValueWritable(VectorizedRowBatch vrg, int rowIndex)
       throws HiveException, SerDeException {
-    for (int i = 0; i < valueEval.length; i++) {
+    int length = valueEval.length;
+
+    // in case of bucketed table, insert the bucket number as the last column in value
+    if (bucketEval != null) {
+      length -= 1;
+      cachedValues[length] = new Text(String.valueOf(bucketNumber));
+    }
+
+    for (int i = 0; i < length; i++) {
       int batchColumn = valueEval[i].getOutputColumn();
       ColumnVector vectorColumn = vrg.cols[batchColumn];
       cachedValues[i] = valueWriters[i].writeValue(vectorColumn, rowIndex);
@@ -426,7 +432,7 @@ private BytesWritable makeValueWritable(VectorizedRowBatch vrg, int rowIndex)
     return (BytesWritable)valueSerializer.serialize(cachedValues, valueObjectInspector);
   }
 
-  private int computeHashCode(VectorizedRowBatch vrg, int rowIndex, int buckNum) throws HiveException {
+  private int computeHashCode(VectorizedRowBatch vrg, int rowIndex) throws HiveException {
     // Evaluate the HashCode
     int keyHashCode = 0;
     if (partitionEval.length == 0) {
@@ -449,7 +455,7 @@ private int computeHashCode(VectorizedRowBatch vrg, int rowIndex, int buckNum) t
                 partitionWriters[p].getObjectInspector());
       }
     }
-    return buckNum < 0  ? keyHashCode : keyHashCode * 31 + buckNum;
+    return bucketNumber < 0  ? keyHashCode : keyHashCode * 31 + bucketNumber;
   }
 
   private boolean partitionKeysAreNull(VectorizedRowBatch vrg, int rowIndex)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java
index e3313e9f43..d79879cfcb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java
@@ -71,7 +71,6 @@
 import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
-import org.apache.hadoop.io.IntWritable;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -85,6 +84,7 @@
  */
 public class SortedDynPartitionOptimizer implements Transform {
 
+  private static final String BUCKET_NUMBER_COL_NAME = "_bucket_number";
   @Override
   public ParseContext transform(ParseContext pCtx) throws SemanticException {
 
@@ -216,6 +216,13 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,
       ReduceSinkDesc rsConf = getReduceSinkDesc(partitionPositions, sortPositions, sortOrder,
           newValueCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());
 
+      if (!bucketColumns.isEmpty()) {
+        String tableAlias = outRR.getColumnInfos().get(0).getTabAlias();
+        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo,
+            tableAlias, true, true);
+        outRR.put(tableAlias, BUCKET_NUMBER_COL_NAME, ci);
+      }
+
       // Create ReduceSink operator
       ReduceSinkOperator rsOp = (ReduceSinkOperator) putOpInsertMap(
           OperatorFactory.getAndMakeChild(rsConf, new RowSchema(outRR.getColumnInfos()), fsParent),
@@ -380,8 +387,11 @@ public ReduceSinkDesc getReduceSinkDesc(List<Integer> partitionPositions,
       // corresponding with bucket number and hence their OIs
       for (Integer idx : keyColsPosInVal) {
         if (idx < 0) {
-          newKeyCols.add(new ExprNodeConstantDesc(TypeInfoFactory
-              .getPrimitiveTypeInfoFromPrimitiveWritable(IntWritable.class), -1));
+          // add bucket number column to both key and value
+          ExprNodeConstantDesc encd = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo,
+              BUCKET_NUMBER_COL_NAME);
+          newKeyCols.add(encd);
+          newValueCols.add(encd);
         } else {
           newKeyCols.add(newValueCols.get(idx).clone());
         }
@@ -418,6 +428,9 @@ public ReduceSinkDesc getReduceSinkDesc(List<Integer> partitionPositions,
       List<String> outCols = Utilities.getInternalColumnNamesFromSignature(parent.getSchema()
           .getSignature());
       ArrayList<String> outValColNames = Lists.newArrayList(outCols);
+      if (!bucketColumns.isEmpty()) {
+        outValColNames.add(BUCKET_NUMBER_COL_NAME);
+      }
       List<FieldSchema> valFields = PlanUtils.getFieldSchemasFromColumnList(newValueCols,
           outValColNames, 0, "");
       TableDesc valueTable = PlanUtils.getReduceValueTableDesc(valFields);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 935887c51f..19110ce4a3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -6478,6 +6478,7 @@ Operator genConversionSelectOperator(String dest, QB qb, Operator input,
     int columnNumber = tableFields.size();
     ArrayList<ExprNodeDesc> expressions = new ArrayList<ExprNodeDesc>(
         columnNumber);
+
     // MetadataTypedColumnsetSerDe does not need type conversions because it
     // does the conversion to String by itself.
     boolean isMetaDataSerDe = table_desc.getDeserializerClass().equals(
@@ -6545,17 +6546,19 @@ Operator genConversionSelectOperator(String dest, QB qb, Operator input,
     if (converted) {
       // add the select operator
       RowResolver rowResolver = new RowResolver();
-      ArrayList<String> colName = new ArrayList<String>();
+      ArrayList<String> colNames = new ArrayList<String>();
+      Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();
       for (int i = 0; i < expressions.size(); i++) {
         String name = getColumnInternalName(i);
         rowResolver.put("", name, new ColumnInfo(name, expressions.get(i)
             .getTypeInfo(), "", false));
-        colName.add(name);
+        colNames.add(name);
+        colExprMap.put(name, expressions.get(i));
       }
       Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild(
-          new SelectDesc(expressions, colName), new RowSchema(rowResolver
+          new SelectDesc(expressions, colNames), new RowSchema(rowResolver
               .getColumnInfos()), input), rowResolver);
-
+      output.setColumnExprMap(colExprMap);
       return output;
     } else {
       // not converted
diff --git a/ql/src/test/queries/clientpositive/alter_partition_change_col.q b/ql/src/test/queries/clientpositive/alter_partition_change_col.q
index 64aafd104c..baabb9fa46 100644
--- a/ql/src/test/queries/clientpositive/alter_partition_change_col.q
+++ b/ql/src/test/queries/clientpositive/alter_partition_change_col.q
@@ -1,6 +1,8 @@
 SET hive.exec.dynamic.partition = true;
 SET hive.exec.dynamic.partition.mode = nonstrict;
 
+-- SORT_QUERY_RESULTS
+
 create table alter_partition_change_col0 (c1 string, c2 string);
 load data local inpath '../../data/files/dec.txt' overwrite into table alter_partition_change_col0;
 
diff --git a/ql/src/test/queries/clientpositive/dynpart_sort_optimization2.q b/ql/src/test/queries/clientpositive/dynpart_sort_optimization2.q
new file mode 100644
index 0000000000..70c795d8cf
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/dynpart_sort_optimization2.q
@@ -0,0 +1,246 @@
+set hive.optimize.sort.dynamic.partition=true;
+set hive.exec.dynamic.partition=true;
+set hive.exec.max.dynamic.partitions=1000;
+set hive.exec.max.dynamic.partitions.pernode=1000;
+set hive.exec.dynamic.partition.mode=nonstrict;
+set hive.enforce.bucketing=false;
+set hive.enforce.sorting=false;
+set hive.exec.submitviachild=true;
+set hive.exec.submit.local.task.via.child=true;
+
+drop table ss;
+drop table ss_orc;
+drop table ss_part;
+drop table ss_part_orc;
+
+create table ss (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float);
+
+create table ss_part (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int);
+
+load data local inpath '../../data/files/dynpart_test.txt' overwrite into table ss;
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk;
+
+insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452617);
+select * from ss_part where ss_sold_date_sk=2452617;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452638);
+select * from ss_part where ss_sold_date_sk=2452638;
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk;
+
+insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452617);
+select * from ss_part where ss_sold_date_sk=2452617;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452638);
+select * from ss_part where ss_sold_date_sk=2452638;
+
+set hive.optimize.sort.dynamic.partition=false;
+-- SORT DYNAMIC PARTITION DISABLED
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk;
+
+insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452617);
+select * from ss_part where ss_sold_date_sk=2452617;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452638);
+select * from ss_part where ss_sold_date_sk=2452638;
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk;
+
+insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452617);
+select * from ss_part where ss_sold_date_sk=2452617;
+
+desc formatted ss_part partition(ss_sold_date_sk=2452638);
+select * from ss_part where ss_sold_date_sk=2452638;
+
+set hive.vectorized.execution.enabled=true;
+-- VECTORIZATION IS ENABLED
+
+create table ss_orc (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float) stored as orc;
+
+create table ss_part_orc (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int) stored as orc;
+
+insert overwrite table ss_orc select * from ss;
+
+drop table ss;
+drop table ss_part;
+
+explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk;
+
+insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk;
+
+desc formatted ss_part_orc partition(ss_sold_date_sk=2452617);
+select * from ss_part_orc where ss_sold_date_sk=2452617;
+
+desc formatted ss_part_orc partition(ss_sold_date_sk=2452638);
+select * from ss_part_orc where ss_sold_date_sk=2452638;
+
+explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk;
+
+insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk;
+
+desc formatted ss_part_orc partition(ss_sold_date_sk=2452617);
+select * from ss_part_orc where ss_sold_date_sk=2452617;
+
+desc formatted ss_part_orc partition(ss_sold_date_sk=2452638);
+select * from ss_part_orc where ss_sold_date_sk=2452638;
+
+drop table ss_orc;
+drop table ss_part_orc;
+
+drop table if exists hive13_dp1;
+create table if not exists hive13_dp1 (
+    k1 int,
+    k2 int
+)
+PARTITIONED BY(`day` string)
+STORED AS ORC;
+
+set hive.optimize.sort.dynamic.partition=false;
+explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key;
+
+insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key;
+select * from hive13_dp1 limit 5;
+
+set hive.optimize.sort.dynamic.partition=true;
+explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key;
+
+insert overwrite table `hive13_dp1` partition(`day`)
+select 
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key;
+select * from hive13_dp1 limit 5;
+
+drop table hive13_dp1;
diff --git a/ql/src/test/results/clientpositive/alter_partition_change_col.q.out b/ql/src/test/results/clientpositive/alter_partition_change_col.q.out
index e48464c5b4..7123e40cde 100644
--- a/ql/src/test/results/clientpositive/alter_partition_change_col.q.out
+++ b/ql/src/test/results/clientpositive/alter_partition_change_col.q.out
@@ -1,8 +1,12 @@
-PREHOOK: query: create table alter_partition_change_col0 (c1 string, c2 string)
+PREHOOK: query: -- SORT_QUERY_RESULTS
+
+create table alter_partition_change_col0 (c1 string, c2 string)
 PREHOOK: type: CREATETABLE
 PREHOOK: Output: database:default
 PREHOOK: Output: default@alter_partition_change_col0
-POSTHOOK: query: create table alter_partition_change_col0 (c1 string, c2 string)
+POSTHOOK: query: -- SORT_QUERY_RESULTS
+
+create table alter_partition_change_col0 (c1 string, c2 string)
 POSTHOOK: type: CREATETABLE
 POSTHOOK: Output: database:default
 POSTHOOK: Output: default@alter_partition_change_col0
@@ -61,26 +65,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	234.79	__HIVE_DEFAULT_PARTITION__
-Cluck	5.96	__HIVE_DEFAULT_PARTITION__
-Tom	19.00	__HIVE_DEFAULT_PARTITION__
-Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Beck	0.0	__HIVE_DEFAULT_PARTITION__
-Snow	55.71	__HIVE_DEFAULT_PARTITION__
-Mary	33.33	__HIVE_DEFAULT_PARTITION__
+Beck	0.0	abc
 Beck	77.341	__HIVE_DEFAULT_PARTITION__
+Beck	77.341	abc
 Beck	79.9	__HIVE_DEFAULT_PARTITION__
-Tom	-12.25	__HIVE_DEFAULT_PARTITION__
 Beck	79.9	abc
-Beck	0.0	abc
-Tom	19.00	abc
+Cluck	5.96	__HIVE_DEFAULT_PARTITION__
+Cluck	5.96	abc
+Mary	33.33	__HIVE_DEFAULT_PARTITION__
 Mary	33.33	abc
-Tom	-12.25	abc
+Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Mary	4.329	abc
+Snow	55.71	__HIVE_DEFAULT_PARTITION__
 Snow	55.71	abc
-Beck	77.341	abc
+Tom	-12.25	__HIVE_DEFAULT_PARTITION__
+Tom	-12.25	abc
+Tom	19.00	__HIVE_DEFAULT_PARTITION__
+Tom	19.00	abc
+Tom	234.79	__HIVE_DEFAULT_PARTITION__
 Tom	234.79	abc
-Cluck	5.96	abc
 PREHOOK: query: -- Change c2 to decimal(10,0)
 alter table alter_partition_change_col1 change c2 c2 decimal(10,0)
 PREHOOK: type: ALTERTABLE_RENAMECOL
@@ -121,26 +125,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	235	__HIVE_DEFAULT_PARTITION__
-Cluck	6	__HIVE_DEFAULT_PARTITION__
-Tom	19	__HIVE_DEFAULT_PARTITION__
-Mary	4	__HIVE_DEFAULT_PARTITION__
 Beck	0	__HIVE_DEFAULT_PARTITION__
-Snow	56	__HIVE_DEFAULT_PARTITION__
-Mary	33	__HIVE_DEFAULT_PARTITION__
+Beck	0	abc
 Beck	77	__HIVE_DEFAULT_PARTITION__
+Beck	77	abc
 Beck	80	__HIVE_DEFAULT_PARTITION__
-Tom	-12	__HIVE_DEFAULT_PARTITION__
 Beck	80	abc
-Beck	0	abc
-Tom	19	abc
+Cluck	6	__HIVE_DEFAULT_PARTITION__
+Cluck	6	abc
+Mary	33	__HIVE_DEFAULT_PARTITION__
 Mary	33	abc
-Tom	-12	abc
+Mary	4	__HIVE_DEFAULT_PARTITION__
 Mary	4	abc
+Snow	56	__HIVE_DEFAULT_PARTITION__
 Snow	56	abc
-Beck	77	abc
+Tom	-12	__HIVE_DEFAULT_PARTITION__
+Tom	-12	abc
+Tom	19	__HIVE_DEFAULT_PARTITION__
+Tom	19	abc
+Tom	235	__HIVE_DEFAULT_PARTITION__
 Tom	235	abc
-Cluck	6	abc
 PREHOOK: query: -- Change the column type at the table level. Table-level describe shows the new type, but the existing partition does not.
 alter table alter_partition_change_col1 change c2 c2 decimal(14,4)
 PREHOOK: type: ALTERTABLE_RENAMECOL
@@ -191,26 +195,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	235	__HIVE_DEFAULT_PARTITION__
-Cluck	6	__HIVE_DEFAULT_PARTITION__
-Tom	19	__HIVE_DEFAULT_PARTITION__
-Mary	4	__HIVE_DEFAULT_PARTITION__
 Beck	0	__HIVE_DEFAULT_PARTITION__
-Snow	56	__HIVE_DEFAULT_PARTITION__
-Mary	33	__HIVE_DEFAULT_PARTITION__
+Beck	0	abc
 Beck	77	__HIVE_DEFAULT_PARTITION__
+Beck	77	abc
 Beck	80	__HIVE_DEFAULT_PARTITION__
-Tom	-12	__HIVE_DEFAULT_PARTITION__
 Beck	80	abc
-Beck	0	abc
-Tom	19	abc
+Cluck	6	__HIVE_DEFAULT_PARTITION__
+Cluck	6	abc
+Mary	33	__HIVE_DEFAULT_PARTITION__
 Mary	33	abc
-Tom	-12	abc
+Mary	4	__HIVE_DEFAULT_PARTITION__
 Mary	4	abc
+Snow	56	__HIVE_DEFAULT_PARTITION__
 Snow	56	abc
-Beck	77	abc
+Tom	-12	__HIVE_DEFAULT_PARTITION__
+Tom	-12	abc
+Tom	19	__HIVE_DEFAULT_PARTITION__
+Tom	19	abc
+Tom	235	__HIVE_DEFAULT_PARTITION__
 Tom	235	abc
-Cluck	6	abc
 PREHOOK: query: -- now change the column type of the existing partition
 alter table alter_partition_change_col1 partition (p1='abc') change c2 c2 decimal(14,4)
 PREHOOK: type: ALTERTABLE_RENAMECOL
@@ -248,26 +252,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	235	__HIVE_DEFAULT_PARTITION__
-Cluck	6	__HIVE_DEFAULT_PARTITION__
-Tom	19	__HIVE_DEFAULT_PARTITION__
-Mary	4	__HIVE_DEFAULT_PARTITION__
 Beck	0	__HIVE_DEFAULT_PARTITION__
-Snow	56	__HIVE_DEFAULT_PARTITION__
-Mary	33	__HIVE_DEFAULT_PARTITION__
+Beck	0.0	abc
 Beck	77	__HIVE_DEFAULT_PARTITION__
-Beck	80	__HIVE_DEFAULT_PARTITION__
-Tom	-12	__HIVE_DEFAULT_PARTITION__
+Beck	77.341	abc
 Beck	79.9	abc
-Beck	0.0	abc
-Tom	19.00	abc
+Beck	80	__HIVE_DEFAULT_PARTITION__
+Cluck	5.96	abc
+Cluck	6	__HIVE_DEFAULT_PARTITION__
+Mary	33	__HIVE_DEFAULT_PARTITION__
 Mary	33.33	abc
-Tom	-12.25	abc
+Mary	4	__HIVE_DEFAULT_PARTITION__
 Mary	4.329	abc
 Snow	55.71	abc
-Beck	77.341	abc
+Snow	56	__HIVE_DEFAULT_PARTITION__
+Tom	-12	__HIVE_DEFAULT_PARTITION__
+Tom	-12.25	abc
+Tom	19	__HIVE_DEFAULT_PARTITION__
+Tom	19.00	abc
 Tom	234.79	abc
-Cluck	5.96	abc
+Tom	235	__HIVE_DEFAULT_PARTITION__
 PREHOOK: query: -- change column for default partition value
 alter table alter_partition_change_col1 partition (p1='__HIVE_DEFAULT_PARTITION__') change c2 c2 decimal(14,4)
 PREHOOK: type: ALTERTABLE_RENAMECOL
@@ -305,26 +309,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	234.79	__HIVE_DEFAULT_PARTITION__
-Cluck	5.96	__HIVE_DEFAULT_PARTITION__
-Tom	19.00	__HIVE_DEFAULT_PARTITION__
-Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Beck	0.0	__HIVE_DEFAULT_PARTITION__
-Snow	55.71	__HIVE_DEFAULT_PARTITION__
-Mary	33.33	__HIVE_DEFAULT_PARTITION__
+Beck	0.0	abc
 Beck	77.341	__HIVE_DEFAULT_PARTITION__
+Beck	77.341	abc
 Beck	79.9	__HIVE_DEFAULT_PARTITION__
-Tom	-12.25	__HIVE_DEFAULT_PARTITION__
 Beck	79.9	abc
-Beck	0.0	abc
-Tom	19.00	abc
+Cluck	5.96	__HIVE_DEFAULT_PARTITION__
+Cluck	5.96	abc
+Mary	33.33	__HIVE_DEFAULT_PARTITION__
 Mary	33.33	abc
-Tom	-12.25	abc
+Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Mary	4.329	abc
+Snow	55.71	__HIVE_DEFAULT_PARTITION__
 Snow	55.71	abc
-Beck	77.341	abc
+Tom	-12.25	__HIVE_DEFAULT_PARTITION__
+Tom	-12.25	abc
+Tom	19.00	__HIVE_DEFAULT_PARTITION__
+Tom	19.00	abc
+Tom	234.79	__HIVE_DEFAULT_PARTITION__
 Tom	234.79	abc
-Cluck	5.96	abc
 PREHOOK: query: -- Try out replace columns
 alter table alter_partition_change_col1 partition (p1='abc') replace columns (c1 string)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
@@ -375,26 +379,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	234.79	__HIVE_DEFAULT_PARTITION__
-Cluck	5.96	__HIVE_DEFAULT_PARTITION__
-Tom	19.00	__HIVE_DEFAULT_PARTITION__
-Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Beck	0.0	__HIVE_DEFAULT_PARTITION__
-Snow	55.71	__HIVE_DEFAULT_PARTITION__
-Mary	33.33	__HIVE_DEFAULT_PARTITION__
 Beck	77.341	__HIVE_DEFAULT_PARTITION__
 Beck	79.9	__HIVE_DEFAULT_PARTITION__
-Tom	-12.25	__HIVE_DEFAULT_PARTITION__
 Beck	NULL	abc
 Beck	NULL	abc
-Tom	NULL	abc
+Beck	NULL	abc
+Cluck	5.96	__HIVE_DEFAULT_PARTITION__
+Cluck	NULL	abc
+Mary	33.33	__HIVE_DEFAULT_PARTITION__
+Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Mary	NULL	abc
-Tom	NULL	abc
 Mary	NULL	abc
+Snow	55.71	__HIVE_DEFAULT_PARTITION__
 Snow	NULL	abc
-Beck	NULL	abc
+Tom	-12.25	__HIVE_DEFAULT_PARTITION__
+Tom	19.00	__HIVE_DEFAULT_PARTITION__
+Tom	234.79	__HIVE_DEFAULT_PARTITION__
+Tom	NULL	abc
+Tom	NULL	abc
 Tom	NULL	abc
-Cluck	NULL	abc
 PREHOOK: query: alter table alter_partition_change_col1 replace columns (c1 string)
 PREHOOK: type: ALTERTABLE_REPLACECOLS
 PREHOOK: Input: default@alter_partition_change_col1
@@ -428,26 +432,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	__HIVE_DEFAULT_PARTITION__
-Cluck	__HIVE_DEFAULT_PARTITION__
-Tom	__HIVE_DEFAULT_PARTITION__
-Mary	__HIVE_DEFAULT_PARTITION__
 Beck	__HIVE_DEFAULT_PARTITION__
-Snow	__HIVE_DEFAULT_PARTITION__
-Mary	__HIVE_DEFAULT_PARTITION__
 Beck	__HIVE_DEFAULT_PARTITION__
 Beck	__HIVE_DEFAULT_PARTITION__
-Tom	__HIVE_DEFAULT_PARTITION__
 Beck	abc
 Beck	abc
-Tom	abc
+Beck	abc
+Cluck	__HIVE_DEFAULT_PARTITION__
+Cluck	abc
+Mary	__HIVE_DEFAULT_PARTITION__
+Mary	__HIVE_DEFAULT_PARTITION__
 Mary	abc
-Tom	abc
 Mary	abc
+Snow	__HIVE_DEFAULT_PARTITION__
 Snow	abc
-Beck	abc
+Tom	__HIVE_DEFAULT_PARTITION__
+Tom	__HIVE_DEFAULT_PARTITION__
+Tom	__HIVE_DEFAULT_PARTITION__
+Tom	abc
+Tom	abc
 Tom	abc
-Cluck	abc
 PREHOOK: query: -- Try add columns
 alter table alter_partition_change_col1 add columns (c2 decimal(14,4))
 PREHOOK: type: ALTERTABLE_ADDCOLS
@@ -497,26 +501,26 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	234.79	__HIVE_DEFAULT_PARTITION__
-Cluck	5.96	__HIVE_DEFAULT_PARTITION__
-Tom	19.00	__HIVE_DEFAULT_PARTITION__
-Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Beck	0.0	__HIVE_DEFAULT_PARTITION__
-Snow	55.71	__HIVE_DEFAULT_PARTITION__
-Mary	33.33	__HIVE_DEFAULT_PARTITION__
 Beck	77.341	__HIVE_DEFAULT_PARTITION__
 Beck	79.9	__HIVE_DEFAULT_PARTITION__
-Tom	-12.25	__HIVE_DEFAULT_PARTITION__
 Beck	NULL	abc
 Beck	NULL	abc
-Tom	NULL	abc
+Beck	NULL	abc
+Cluck	5.96	__HIVE_DEFAULT_PARTITION__
+Cluck	NULL	abc
+Mary	33.33	__HIVE_DEFAULT_PARTITION__
+Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Mary	NULL	abc
-Tom	NULL	abc
 Mary	NULL	abc
+Snow	55.71	__HIVE_DEFAULT_PARTITION__
 Snow	NULL	abc
-Beck	NULL	abc
+Tom	-12.25	__HIVE_DEFAULT_PARTITION__
+Tom	19.00	__HIVE_DEFAULT_PARTITION__
+Tom	234.79	__HIVE_DEFAULT_PARTITION__
+Tom	NULL	abc
+Tom	NULL	abc
 Tom	NULL	abc
-Cluck	NULL	abc
 PREHOOK: query: alter table alter_partition_change_col1 partition (p1='abc') add columns (c2 decimal(14,4))
 PREHOOK: type: ALTERTABLE_ADDCOLS
 PREHOOK: Input: default@alter_partition_change_col1
@@ -552,23 +556,23 @@ POSTHOOK: Input: default@alter_partition_change_col1
 POSTHOOK: Input: default@alter_partition_change_col1@p1=__HIVE_DEFAULT_PARTITION__
 POSTHOOK: Input: default@alter_partition_change_col1@p1=abc
 #### A masked pattern was here ####
-Tom	234.79	__HIVE_DEFAULT_PARTITION__
-Cluck	5.96	__HIVE_DEFAULT_PARTITION__
-Tom	19.00	__HIVE_DEFAULT_PARTITION__
-Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Beck	0.0	__HIVE_DEFAULT_PARTITION__
-Snow	55.71	__HIVE_DEFAULT_PARTITION__
-Mary	33.33	__HIVE_DEFAULT_PARTITION__
+Beck	0.0	abc
 Beck	77.341	__HIVE_DEFAULT_PARTITION__
+Beck	77.341	abc
 Beck	79.9	__HIVE_DEFAULT_PARTITION__
-Tom	-12.25	__HIVE_DEFAULT_PARTITION__
 Beck	79.9	abc
-Beck	0.0	abc
-Tom	19.00	abc
+Cluck	5.96	__HIVE_DEFAULT_PARTITION__
+Cluck	5.96	abc
+Mary	33.33	__HIVE_DEFAULT_PARTITION__
 Mary	33.33	abc
-Tom	-12.25	abc
+Mary	4.329	__HIVE_DEFAULT_PARTITION__
 Mary	4.329	abc
+Snow	55.71	__HIVE_DEFAULT_PARTITION__
 Snow	55.71	abc
-Beck	77.341	abc
+Tom	-12.25	__HIVE_DEFAULT_PARTITION__
+Tom	-12.25	abc
+Tom	19.00	__HIVE_DEFAULT_PARTITION__
+Tom	19.00	abc
+Tom	234.79	__HIVE_DEFAULT_PARTITION__
 Tom	234.79	abc
-Cluck	5.96	abc
diff --git a/ql/src/test/results/clientpositive/annotate_stats_part.q.out b/ql/src/test/results/clientpositive/annotate_stats_part.q.out
index d2e9ed1ddc..b952fa6be8 100644
--- a/ql/src/test/results/clientpositive/annotate_stats_part.q.out
+++ b/ql/src/test/results/clientpositive/annotate_stats_part.q.out
@@ -98,11 +98,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 5 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 5 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int), zip (type: bigint), year (type: string)
             outputColumnNames: _col0, _col1, _col2, _col3
-            Statistics: Num rows: 5 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+            Statistics: Num rows: 5 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
             ListSink
 
 PREHOOK: query: -- partition level analyze statistics for specific parition
@@ -158,11 +158,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 9 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 9 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int), zip (type: bigint), year (type: string)
             outputColumnNames: _col0, _col1, _col2, _col3
-            Statistics: Num rows: 9 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+            Statistics: Num rows: 9 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
             ListSink
 
 PREHOOK: query: -- basicStatState: COMPLETE colStatState: NONE
@@ -181,11 +181,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 7 Data size: 399 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 7 Data size: 400 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int), zip (type: bigint), year (type: string)
             outputColumnNames: _col0, _col1, _col2, _col3
-            Statistics: Num rows: 7 Data size: 399 Basic stats: COMPLETE Column stats: PARTIAL
+            Statistics: Num rows: 7 Data size: 400 Basic stats: COMPLETE Column stats: PARTIAL
             ListSink
 
 PREHOOK: query: -- partition level analyze statistics for all partitions
@@ -245,11 +245,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int), zip (type: bigint), year (type: string)
             outputColumnNames: _col0, _col1, _col2, _col3
-            Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+            Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
             ListSink
 
 PREHOOK: query: -- basicStatState: COMPLETE colStatState: NONE
@@ -268,11 +268,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int), zip (type: bigint), year (type: string)
             outputColumnNames: _col0, _col1, _col2, _col3
-            Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+            Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
             ListSink
 
 PREHOOK: query: -- both partitions will be pruned
@@ -331,11 +331,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: NONE
           Select Operator
             expressions: zip (type: bigint)
             outputColumnNames: _col0
-            Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: NONE
             ListSink
 
 PREHOOK: query: -- basicStatState: COMPLETE colStatState: PARTIAL
@@ -354,7 +354,7 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string)
             outputColumnNames: _col0
@@ -377,7 +377,7 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: COMPLETE
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: COMPLETE
           Select Operator
             expressions: year (type: string)
             outputColumnNames: _col0
@@ -402,7 +402,7 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int)
             outputColumnNames: _col0, _col1
@@ -425,7 +425,7 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 7 Data size: 399 Basic stats: COMPLETE Column stats: COMPLETE
+          Statistics: Num rows: 7 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
           Select Operator
             expressions: state (type: string), locid (type: int)
             outputColumnNames: _col0, _col1
@@ -471,11 +471,11 @@ STAGE PLANS:
       Processor Tree:
         TableScan
           alias: loc_orc
-          Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+          Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
           Select Operator
             expressions: state (type: string), locid (type: int), zip (type: bigint), year (type: string)
             outputColumnNames: _col0, _col1, _col2, _col3
-            Statistics: Num rows: 8 Data size: 722 Basic stats: COMPLETE Column stats: PARTIAL
+            Statistics: Num rows: 8 Data size: 723 Basic stats: COMPLETE Column stats: PARTIAL
             ListSink
 
 PREHOOK: query: -- This is to test filter expression evaluation on partition column
@@ -496,7 +496,7 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: loc_orc
-            Statistics: Num rows: 7 Data size: 399 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 7 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
             Filter Operator
               predicate: (locid > 0) (type: boolean)
               Statistics: Num rows: 2 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
@@ -532,7 +532,7 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: loc_orc
-            Statistics: Num rows: 7 Data size: 399 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 7 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
             Filter Operator
               predicate: (locid > 0) (type: boolean)
               Statistics: Num rows: 2 Data size: 176 Basic stats: COMPLETE Column stats: COMPLETE
@@ -568,7 +568,7 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: loc_orc
-            Statistics: Num rows: 7 Data size: 399 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 7 Data size: 400 Basic stats: COMPLETE Column stats: COMPLETE
             Filter Operator
               predicate: (locid > 0) (type: boolean)
               Statistics: Num rows: 2 Data size: 176 Basic stats: COMPLETE Column stats: COMPLETE
diff --git a/ql/src/test/results/clientpositive/auto_sortmerge_join_16.q.out b/ql/src/test/results/clientpositive/auto_sortmerge_join_16.q.out
index 03507dd063..b32ac060d3 100644
--- a/ql/src/test/results/clientpositive/auto_sortmerge_join_16.q.out
+++ b/ql/src/test/results/clientpositive/auto_sortmerge_join_16.q.out
@@ -232,6 +232,16 @@ POSTHOOK: Input: default@bucket_small@pri=2
 0	val_0	val_0	day1	1
 0	val_0	val_0	day1	1
 0	val_0	val_0	day1	1
+103	val_103	val_103	day1	1
+103	val_103	val_103	day1	1
+103	val_103	val_103	day1	1
+103	val_103	val_103	day1	1
+374	val_374	val_374	day1	1
+374	val_374	val_374	day1	1
+172	val_172	val_172	day1	1
+172	val_172	val_172	day1	1
+172	val_172	val_172	day1	1
+172	val_172	val_172	day1	1
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
@@ -240,13 +250,3 @@ POSTHOOK: Input: default@bucket_small@pri=2
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
-374	val_374	val_374	day1	1
-374	val_374	val_374	day1	1
-172	val_172	val_172	day1	1
-172	val_172	val_172	day1	1
-172	val_172	val_172	day1	1
-172	val_172	val_172	day1	1
-103	val_103	val_103	day1	1
-103	val_103	val_103	day1	1
-103	val_103	val_103	day1	1
-103	val_103	val_103	day1	1
diff --git a/ql/src/test/results/clientpositive/combine2.q.out b/ql/src/test/results/clientpositive/combine2.q.out
index 9da953c1a4..921dd908ca 100644
--- a/ql/src/test/results/clientpositive/combine2.q.out
+++ b/ql/src/test/results/clientpositive/combine2.q.out
@@ -263,7 +263,7 @@ STAGE PLANS:
               columns.types string
 #### A masked pattern was here ####
               name default.combine2
-              numFiles 1
+              numFiles 3
               numRows 3
               partition_columns value
               partition_columns.types string
@@ -398,7 +398,7 @@ STAGE PLANS:
               columns.types string
 #### A masked pattern was here ####
               name default.combine2
-              numFiles 1
+              numFiles 3
               numRows 3
               partition_columns value
               partition_columns.types string
diff --git a/ql/src/test/results/clientpositive/constprog_dp.q.out b/ql/src/test/results/clientpositive/constprog_dp.q.out
index e8764676bd..98af017c39 100644
--- a/ql/src/test/results/clientpositive/constprog_dp.q.out
+++ b/ql/src/test/results/clientpositive/constprog_dp.q.out
@@ -16,8 +16,13 @@ insert overwrite table dest partition (ds) select key, value, ds where ds='2008-
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5
+  Stage-4
+  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6
   Stage-2 depends on stages: Stage-0
+  Stage-3
+  Stage-5
+  Stage-6 depends on stages: Stage-5
 
 STAGE PLANS:
   Stage: Stage-1
@@ -30,23 +35,23 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.dest
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.dest
+
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-4
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -63,6 +68,36 @@ STAGE PLANS:
   Stage: Stage-2
     Stats-Aggr Operator
 
+  Stage: Stage-3
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.dest
+
+  Stage: Stage-5
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.dest
+
+  Stage: Stage-6
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
 PREHOOK: query: from srcpart
 insert overwrite table dest partition (ds) select key, value, ds where ds='2008-04-08'
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/delete_all_partitioned.q.out b/ql/src/test/results/clientpositive/delete_all_partitioned.q.out
index 90f8753687..c5149b28b4 100644
--- a/ql/src/test/results/clientpositive/delete_all_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/delete_all_partitioned.q.out
@@ -84,3 +84,5 @@ POSTHOOK: Input: default@acid_dap
 POSTHOOK: Input: default@acid_dap@ds=today
 POSTHOOK: Input: default@acid_dap@ds=tomorrow
 #### A masked pattern was here ####
+-1071480828	aw724t8c5558x2xneC624	today
+-1072076362	2uLyD28144vklju213J1mr	today
diff --git a/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out b/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out
index f2c42ae77e..df8ad447f5 100644
--- a/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out
+++ b/ql/src/test/results/clientpositive/dynpart_sort_opt_vectorization.q.out
@@ -342,11 +342,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                   sort order: ++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Execution mode: vectorized
       Reduce Operator Tree:
         Extract
@@ -399,11 +399,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                   sort order: +++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Execution mode: vectorized
       Reduce Operator Tree:
         Extract
@@ -691,11 +691,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                   sort order: ++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Execution mode: vectorized
       Reduce Operator Tree:
         Extract
@@ -748,11 +748,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                   sort order: +++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Execution mode: vectorized
       Reduce Operator Tree:
         Extract
@@ -2063,11 +2063,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                   sort order: +++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Execution mode: vectorized
       Reduce Operator Tree:
         Extract
diff --git a/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out b/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out
index 549169f68b..87948840a4 100644
--- a/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out
+++ b/ql/src/test/results/clientpositive/dynpart_sort_optimization.q.out
@@ -275,11 +275,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                   sort order: ++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Reduce Operator Tree:
         Extract
           Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
@@ -331,11 +331,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                   sort order: +++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Reduce Operator Tree:
         Extract
           Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
@@ -598,11 +598,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                   sort order: ++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Reduce Operator Tree:
         Extract
           Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
@@ -654,11 +654,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                   sort order: +++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Reduce Operator Tree:
         Extract
           Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
@@ -1962,11 +1962,11 @@ STAGE PLANS:
                 outputColumnNames: _col0, _col1, _col2, _col3, _col4
                 Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
-                  key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                  key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                   sort order: +++
                   Map-reduce partition columns: _col4 (type: tinyint)
                   Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                  value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
       Reduce Operator Tree:
         Extract
           Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
diff --git a/ql/src/test/results/clientpositive/dynpart_sort_optimization2.q.out b/ql/src/test/results/clientpositive/dynpart_sort_optimization2.q.out
new file mode 100644
index 0000000000..9b57bbbc3e
--- /dev/null
+++ b/ql/src/test/results/clientpositive/dynpart_sort_optimization2.q.out
@@ -0,0 +1,1782 @@
+PREHOOK: query: drop table ss
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table ss_orc
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss_orc
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table ss_part
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss_part
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table ss_part_orc
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss_part_orc
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table ss (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss
+POSTHOOK: query: create table ss (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss
+PREHOOK: query: create table ss_part (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: create table ss_part (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss_part
+PREHOOK: query: load data local inpath '../../data/files/dynpart_test.txt' overwrite into table ss
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@ss
+POSTHOOK: query: load data local inpath '../../data/files/dynpart_test.txt' overwrite into table ss
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@ss
+PREHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: ss
+            Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+              Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                outputColumnNames: ss_sold_date_sk, ss_net_paid_inc_tax, ss_net_profit
+                Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  keys: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int), _col1 (type: float), _col2 (type: float)
+                    sort order: +++
+                    Map-reduce partition columns: _col0 (type: int)
+                    Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+      Reduce Operator Tree:
+        Group By Operator
+          keys: KEY._col0 (type: int), KEY._col1 (type: float), KEY._col2 (type: float)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col1 (type: float), _col2 (type: float), _col0 (type: int)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.ss_part
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+2.1	-2026.3	2452617
+2.99	-11.32	2452617
+85.8	25.61	2452617
+552.96	-1363.84	2452617
+565.92	196.48	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+7412.83	2071.68	2452617
+10022.63	3952.8	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+0.15	-241.22	2452638
+150.39	-162.12	2452638
+156.67	-4626.56	2452638
+181.03	-207.24	2452638
+267.01	-3266.36	2452638
+317.87	-3775.38	2452638
+1327.08	57.97	2452638
+1413.19	178.08	2452638
+1524.33	494.37	2452638
+1971.35	-488.25	2452638
+4133.98	-775.72	2452638
+4329.49	-4000.51	2452638
+10171.1	660.48	2452638
+PREHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: ss
+            Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+              Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_net_paid_inc_tax (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col2 (type: int)
+                  sort order: +
+                  Map-reduce partition columns: _col2 (type: int)
+                  Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: int)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: VALUE._col0 (type: float), VALUE._col1 (type: float), VALUE._col2 (type: int)
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                name: default.ss_part
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+10022.63	3952.8	2452617
+2.99	-11.32	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+565.92	196.48	2452617
+85.8	25.61	2452617
+7412.83	2071.68	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+552.96	-1363.84	2452617
+2.1	-2026.3	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+4329.49	-4000.51	2452638
+1413.19	178.08	2452638
+150.39	-162.12	2452638
+1524.33	494.37	2452638
+0.15	-241.22	2452638
+267.01	-3266.36	2452638
+181.03	-207.24	2452638
+1971.35	-488.25	2452638
+1327.08	57.97	2452638
+156.67	-4626.56	2452638
+317.87	-3775.38	2452638
+10171.1	660.48	2452638
+4133.98	-775.72	2452638
+PREHOOK: query: -- SORT DYNAMIC PARTITION DISABLED
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: -- SORT DYNAMIC PARTITION DISABLED
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: ss
+            Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+              Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                outputColumnNames: ss_sold_date_sk, ss_net_paid_inc_tax, ss_net_profit
+                Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  keys: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int), _col1 (type: float), _col2 (type: float)
+                    sort order: +++
+                    Map-reduce partition columns: _col0 (type: int)
+                    Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+      Reduce Operator Tree:
+        Group By Operator
+          keys: KEY._col0 (type: int), KEY._col1 (type: float), KEY._col2 (type: float)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col1 (type: float), _col2 (type: float), _col0 (type: int)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.ss_part
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+2.1	-2026.3	2452617
+2.99	-11.32	2452617
+85.8	25.61	2452617
+552.96	-1363.84	2452617
+565.92	196.48	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+7412.83	2071.68	2452617
+10022.63	3952.8	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+0.15	-241.22	2452638
+150.39	-162.12	2452638
+156.67	-4626.56	2452638
+181.03	-207.24	2452638
+267.01	-3266.36	2452638
+317.87	-3775.38	2452638
+1327.08	57.97	2452638
+1413.19	178.08	2452638
+1524.33	494.37	2452638
+1971.35	-488.25	2452638
+4133.98	-775.72	2452638
+4329.49	-4000.51	2452638
+10171.1	660.48	2452638
+PREHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: ss
+            Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+              Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_net_paid_inc_tax (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  sort order: 
+                  Map-reduce partition columns: _col2 (type: int)
+                  Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: int)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: VALUE._col0 (type: float), VALUE._col1 (type: float), VALUE._col2 (type: int)
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                name: default.ss_part
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+565.92	196.48	2452617
+85.8	25.61	2452617
+7412.83	2071.68	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+552.96	-1363.84	2452617
+2.1	-2026.3	2452617
+10022.63	3952.8	2452617
+2.99	-11.32	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+4329.49	-4000.51	2452638
+1413.19	178.08	2452638
+150.39	-162.12	2452638
+1524.33	494.37	2452638
+0.15	-241.22	2452638
+267.01	-3266.36	2452638
+181.03	-207.24	2452638
+1971.35	-488.25	2452638
+1327.08	57.97	2452638
+156.67	-4626.56	2452638
+317.87	-3775.38	2452638
+10171.1	660.48	2452638
+4133.98	-775.72	2452638
+PREHOOK: query: -- VECTORIZATION IS ENABLED
+
+create table ss_orc (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss_orc
+POSTHOOK: query: -- VECTORIZATION IS ENABLED
+
+create table ss_orc (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss_orc
+PREHOOK: query: create table ss_part_orc (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: create table ss_part_orc (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss_part_orc
+PREHOOK: query: insert overwrite table ss_orc select * from ss
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_orc
+POSTHOOK: query: insert overwrite table ss_orc select * from ss
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_orc
+POSTHOOK: Lineage: ss_orc.ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_orc.ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_orc.ss_sold_date_sk SIMPLE [(ss)ss.FieldSchema(name:ss_sold_date_sk, type:int, comment:null), ]
+PREHOOK: query: drop table ss
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss
+POSTHOOK: query: drop table ss
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss
+PREHOOK: query: drop table ss_part
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss_part
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: drop table ss_part
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Output: default@ss_part
+PREHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: ss_orc
+            Statistics: Num rows: 24 Data size: 288 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+              Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                outputColumnNames: ss_sold_date_sk, ss_net_paid_inc_tax, ss_net_profit
+                Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                Group By Operator
+                  keys: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                  mode: hash
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int), _col1 (type: float), _col2 (type: float)
+                    sort order: +++
+                    Map-reduce partition columns: _col0 (type: int)
+                    Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Group By Operator
+          keys: KEY._col0 (type: int), KEY._col1 (type: float), KEY._col2 (type: float)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col1 (type: float), _col2 (type: float), _col0 (type: int)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                  name: default.ss_part_orc
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.ss_part_orc
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_orc
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_orc
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	88                  
+	totalSize           	417                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+2.1	-2026.3	2452617
+2.99	-11.32	2452617
+85.8	25.61	2452617
+552.96	-1363.84	2452617
+565.92	196.48	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+7412.83	2071.68	2452617
+10022.63	3952.8	2452617
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	104                 
+	totalSize           	440                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+0.15	-241.22	2452638
+150.39	-162.12	2452638
+156.67	-4626.56	2452638
+181.03	-207.24	2452638
+267.01	-3266.36	2452638
+317.87	-3775.38	2452638
+1327.08	57.97	2452638
+1413.19	178.08	2452638
+1524.33	494.37	2452638
+1971.35	-488.25	2452638
+4133.98	-775.72	2452638
+4329.49	-4000.51	2452638
+10171.1	660.48	2452638
+PREHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: ss_orc
+            Statistics: Num rows: 24 Data size: 288 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+              Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: ss_net_paid_inc_tax (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  sort order: 
+                  Map-reduce partition columns: _col2 (type: int)
+                  Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: int)
+      Execution mode: vectorized
+      Reduce Operator Tree:
+        Select Operator
+          expressions: VALUE._col0 (type: float), VALUE._col1 (type: float), VALUE._col2 (type: int)
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                name: default.ss_part_orc
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.ss_part_orc
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_orc
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_orc
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	88                  
+	totalSize           	417                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+565.92	196.48	2452617
+85.8	25.61	2452617
+7412.83	2071.68	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+552.96	-1363.84	2452617
+2.1	-2026.3	2452617
+10022.63	3952.8	2452617
+2.99	-11.32	2452617
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	104                 
+	totalSize           	440                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+4329.49	-4000.51	2452638
+1413.19	178.08	2452638
+150.39	-162.12	2452638
+1524.33	494.37	2452638
+0.15	-241.22	2452638
+267.01	-3266.36	2452638
+181.03	-207.24	2452638
+1971.35	-488.25	2452638
+1327.08	57.97	2452638
+156.67	-4626.56	2452638
+317.87	-3775.38	2452638
+10171.1	660.48	2452638
+4133.98	-775.72	2452638
+PREHOOK: query: drop table ss_orc
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss_orc
+PREHOOK: Output: default@ss_orc
+POSTHOOK: query: drop table ss_orc
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss_orc
+POSTHOOK: Output: default@ss_orc
+PREHOOK: query: drop table ss_part_orc
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: drop table ss_part_orc
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Output: default@ss_part_orc
+PREHOOK: query: drop table if exists hive13_dp1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists hive13_dp1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table if not exists hive13_dp1 (
+    k1 int,
+    k2 int
+)
+PARTITIONED BY(`day` string)
+STORED AS ORC
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: create table if not exists hive13_dp1 (
+    k1 int,
+    k2 int
+)
+PARTITIONED BY(`day` string)
+STORED AS ORC
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@hive13_dp1
+PREHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: src
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: string), value (type: string)
+              outputColumnNames: key, value
+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: count(value)
+                keys: 'day' (type: string), key (type: string)
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string), _col1 (type: string)
+                  sort order: ++
+                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col2 (type: bigint)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: count(VALUE._col0)
+          keys: KEY._col0 (type: string), KEY._col1 (type: string)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int), _col0 (type: string)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                  name: default.hive13_dp1
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            day 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.hive13_dp1
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hive13_dp1@day=day
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from hive13_dp1 limit 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hive13_dp1
+PREHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hive13_dp1 limit 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hive13_dp1
+POSTHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+0	3	day
+10	1	day
+100	2	day
+103	2	day
+104	2	day
+PREHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: src
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: string), value (type: string)
+              outputColumnNames: key, value
+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: count(value)
+                keys: 'day' (type: string), key (type: string)
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: string), _col1 (type: string)
+                  sort order: ++
+                  Map-reduce partition columns: _col0 (type: string)
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col2 (type: bigint)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: count(VALUE._col0)
+          keys: KEY._col0 (type: string), KEY._col1 (type: string)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int), _col0 (type: string)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                  serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                  name: default.hive13_dp1
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            day 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.hive13_dp1
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select 
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select 
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hive13_dp1@day=day
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from hive13_dp1 limit 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hive13_dp1
+PREHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hive13_dp1 limit 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hive13_dp1
+POSTHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+0	3	day
+10	1	day
+100	2	day
+103	2	day
+104	2	day
+PREHOOK: query: drop table hive13_dp1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hive13_dp1
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: drop table hive13_dp1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hive13_dp1
+POSTHOOK: Output: default@hive13_dp1
diff --git a/ql/src/test/results/clientpositive/extrapolate_part_stats_partial.q.out b/ql/src/test/results/clientpositive/extrapolate_part_stats_partial.q.out
index 3789c092c0..db5c77ca73 100644
--- a/ql/src/test/results/clientpositive/extrapolate_part_stats_partial.q.out
+++ b/ql/src/test/results/clientpositive/extrapolate_part_stats_partial.q.out
@@ -239,7 +239,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 383
+              totalSize 386
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -282,7 +282,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 390
+              totalSize 388
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -458,7 +458,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 383
+              totalSize 386
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -501,7 +501,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 390
+              totalSize 388
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -690,7 +690,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 383
+              totalSize 386
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -733,7 +733,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 390
+              totalSize 388
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -905,7 +905,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 383
+              totalSize 386
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
@@ -948,7 +948,7 @@ STAGE PLANS:
               serialization.ddl struct loc_orc_1d { string state, i32 locid, i32 zip}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.ql.io.orc.OrcSerde
-              totalSize 390
+              totalSize 388
 #### A masked pattern was here ####
             serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
           
diff --git a/ql/src/test/results/clientpositive/groupby4_map.q.out b/ql/src/test/results/clientpositive/groupby4_map.q.out
index bbd63ad12c..89a0778c65 100644
--- a/ql/src/test/results/clientpositive/groupby4_map.q.out
+++ b/ql/src/test/results/clientpositive/groupby4_map.q.out
@@ -44,10 +44,10 @@ STAGE PLANS:
           Select Operator
             expressions: UDFToInteger(_col0) (type: int)
             outputColumnNames: _col0
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
             File Output Operator
               compressed: false
-              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/groupby4_map_skew.q.out b/ql/src/test/results/clientpositive/groupby4_map_skew.q.out
index 0c15ea64bc..fc9e7d1995 100644
--- a/ql/src/test/results/clientpositive/groupby4_map_skew.q.out
+++ b/ql/src/test/results/clientpositive/groupby4_map_skew.q.out
@@ -44,10 +44,10 @@ STAGE PLANS:
           Select Operator
             expressions: UDFToInteger(_col0) (type: int)
             outputColumnNames: _col0
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
             File Output Operator
               compressed: false
-              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/infer_bucket_sort_dyn_part.q.out b/ql/src/test/results/clientpositive/infer_bucket_sort_dyn_part.q.out
index 2b1ec58615..8f83ada20d 100644
--- a/ql/src/test/results/clientpositive/infer_bucket_sort_dyn_part.q.out
+++ b/ql/src/test/results/clientpositive/infer_bucket_sort_dyn_part.q.out
@@ -231,7 +231,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	0                   
 	rawDataSize         	0                   
-	totalSize           	1381                
+	totalSize           	1342                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -302,7 +302,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	0                   
 	rawDataSize         	0                   
-	totalSize           	722                 
+	totalSize           	719                 
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -310,9 +310,9 @@ SerDe Library:      	org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
 InputFormat:        	org.apache.hadoop.hive.ql.io.RCFileInputFormat	 
 OutputFormat:       	org.apache.hadoop.hive.ql.io.RCFileOutputFormat	 
 Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
+Num Buckets:        	1                   	 
+Bucket Columns:     	[key]               	 
+Sort Columns:       	[Order(col:key, order:1)]	 
 Storage Desc Params:	 	 
 	serialization.format	1                   
 PREHOOK: query: DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='12')
@@ -344,7 +344,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	0                   
 	rawDataSize         	0                   
-	totalSize           	741                 
+	totalSize           	722                 
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -352,9 +352,9 @@ SerDe Library:      	org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
 InputFormat:        	org.apache.hadoop.hive.ql.io.RCFileInputFormat	 
 OutputFormat:       	org.apache.hadoop.hive.ql.io.RCFileOutputFormat	 
 Compressed:         	No                  	 
-Num Buckets:        	-1                  	 
-Bucket Columns:     	[]                  	 
-Sort Columns:       	[]                  	 
+Num Buckets:        	1                   	 
+Bucket Columns:     	[key]               	 
+Sort Columns:       	[Order(col:key, order:1)]	 
 Storage Desc Params:	 	 
 	serialization.format	1                   
 PREHOOK: query: CREATE TABLE srcpart_merge_dp LIKE srcpart
@@ -468,14 +468,13 @@ GROUP BY key) a
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
-  Stage-2 depends on stages: Stage-1
-  Stage-8 depends on stages: Stage-2 , consists of Stage-5, Stage-4, Stage-6
-  Stage-5
-  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7
-  Stage-3 depends on stages: Stage-0
+  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5
   Stage-4
-  Stage-6
-  Stage-7 depends on stages: Stage-6
+  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6
+  Stage-2 depends on stages: Stage-0
+  Stage-3
+  Stage-5
+  Stage-6 depends on stages: Stage-5
 
 STAGE PLANS:
   Stage: Stage-1
@@ -513,37 +512,17 @@ STAGE PLANS:
             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
               compressed: false
-              table:
-                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-
-  Stage: Stage-2
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            Reduce Output Operator
-              key expressions: _col2 (type: string)
-              sort order: +
-              Map-reduce partition columns: _col2 (type: string)
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                name: default.test_table
+              table:
+                  input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                  name: default.test_table
 
-  Stage: Stage-8
+  Stage: Stage-7
     Conditional Operator
 
-  Stage: Stage-5
+  Stage: Stage-4
     Move Operator
       files:
           hdfs directory: true
@@ -562,24 +541,24 @@ STAGE PLANS:
               serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
               name: default.test_table
 
-  Stage: Stage-3
+  Stage: Stage-2
     Stats-Aggr Operator
 
-  Stage: Stage-4
+  Stage: Stage-3
     Merge File Operator
       Map Operator Tree:
           RCFile Merge Operator
       merge level: block
       input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
 
-  Stage: Stage-6
+  Stage: Stage-5
     Merge File Operator
       Map Operator Tree:
           RCFile Merge Operator
       merge level: block
       input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
 
-  Stage: Stage-7
+  Stage: Stage-6
     Move Operator
       files:
           hdfs directory: true
@@ -639,7 +618,7 @@ Partition Parameters:
 	numFiles            	1                   
 	numRows             	0                   
 	rawDataSize         	0                   
-	totalSize           	94                  
+	totalSize           	115                 
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -678,10 +657,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	1                   
+	numFiles            	2                   
 	numRows             	0                   
 	rawDataSize         	0                   
-	totalSize           	1346                
+	totalSize           	1427                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
diff --git a/ql/src/test/results/clientpositive/input30.q.out b/ql/src/test/results/clientpositive/input30.q.out
index e5b67377ed..ae89ccbe0d 100644
--- a/ql/src/test/results/clientpositive/input30.q.out
+++ b/ql/src/test/results/clientpositive/input30.q.out
@@ -57,10 +57,10 @@ STAGE PLANS:
           Select Operator
             expressions: UDFToInteger(_col0) (type: int)
             outputColumnNames: _col0
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
             File Output Operator
               compressed: false
-              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/input32.q.out b/ql/src/test/results/clientpositive/input32.q.out
index 1cfc1892bb..eba95189ec 100644
--- a/ql/src/test/results/clientpositive/input32.q.out
+++ b/ql/src/test/results/clientpositive/input32.q.out
@@ -54,10 +54,10 @@ STAGE PLANS:
           Select Operator
             expressions: UDFToInteger(_col0) (type: int)
             outputColumnNames: _col0
-            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
             File Output Operator
               compressed: false
-              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+              Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
                   output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/insert_into6.q.out b/ql/src/test/results/clientpositive/insert_into6.q.out
index 53f8bac0bf..388b200560 100644
--- a/ql/src/test/results/clientpositive/insert_into6.q.out
+++ b/ql/src/test/results/clientpositive/insert_into6.q.out
@@ -132,8 +132,13 @@ POSTHOOK: query: EXPLAIN INSERT INTO TABLE insert_into6b PARTITION (ds)
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5
+  Stage-4
+  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6
   Stage-2 depends on stages: Stage-0
+  Stage-3
+  Stage-5
+  Stage-6 depends on stages: Stage-5
 
 STAGE PLANS:
   Stage: Stage-1
@@ -146,23 +151,23 @@ STAGE PLANS:
               expressions: key (type: int), value (type: string), ds (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 250 Data size: 2680 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 250 Data size: 2680 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 250 Data size: 2680 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 250 Data size: 2680 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.insert_into6b
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.insert_into6b
+
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-4
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -179,6 +184,36 @@ STAGE PLANS:
   Stage: Stage-2
     Stats-Aggr Operator
 
+  Stage: Stage-3
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.insert_into6b
+
+  Stage: Stage-5
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.insert_into6b
+
+  Stage: Stage-6
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
 PREHOOK: query: INSERT INTO TABLE insert_into6b PARTITION (ds) SELECT * FROM insert_into6a
 PREHOOK: type: QUERY
 PREHOOK: Input: default@insert_into6a
diff --git a/ql/src/test/results/clientpositive/load_dyn_part1.q.out b/ql/src/test/results/clientpositive/load_dyn_part1.q.out
index 4c9ee05d32..e7dace5e91 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part1.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part1.q.out
@@ -58,11 +58,20 @@ insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, v
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-2 is a root stage
-  Stage-0 depends on stages: Stage-2
+  Stage-8 depends on stages: Stage-2 , consists of Stage-5, Stage-4, Stage-6
+  Stage-5
+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7
   Stage-3 depends on stages: Stage-0
-  Stage-4 depends on stages: Stage-2
-  Stage-1 depends on stages: Stage-4
-  Stage-5 depends on stages: Stage-1
+  Stage-4
+  Stage-6
+  Stage-7 depends on stages: Stage-6
+  Stage-14 depends on stages: Stage-2 , consists of Stage-11, Stage-10, Stage-12
+  Stage-11
+  Stage-1 depends on stages: Stage-11, Stage-10, Stage-13
+  Stage-9 depends on stages: Stage-1
+  Stage-10
+  Stage-12
+  Stage-13 depends on stages: Stage-12
 
 STAGE PLANS:
   Stage: Stage-2
@@ -78,12 +87,14 @@ STAGE PLANS:
                 expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  key expressions: _col2 (type: string), _col3 (type: string)
-                  sort order: ++
-                  Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part1
             Filter Operator
               predicate: (ds > '2008-04-08') (type: boolean)
               Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
@@ -93,21 +104,21 @@ STAGE PLANS:
                 Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
                   table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part1
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part2
+
+  Stage: Stage-8
+    Conditional Operator
+
+  Stage: Stage-5
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -129,23 +140,40 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            Reduce Output Operator
-              key expressions: _col2 (type: string)
-              sort order: +
-              Map-reduce partition columns: _col2 (type: string)
-              Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part2
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part1
+
+  Stage: Stage-6
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part1
+
+  Stage: Stage-7
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
+  Stage: Stage-14
+    Conditional Operator
+
+  Stage: Stage-11
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-1
     Move Operator
@@ -160,9 +188,39 @@ STAGE PLANS:
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.nzhang_part2
 
-  Stage: Stage-5
+  Stage: Stage-9
     Stats-Aggr Operator
 
+  Stage: Stage-10
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part2
+
+  Stage: Stage-12
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part2
+
+  Stage: Stage-13
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
 PREHOOK: query: from srcpart
 insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
 insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08'
diff --git a/ql/src/test/results/clientpositive/load_dyn_part10.q.out b/ql/src/test/results/clientpositive/load_dyn_part10.q.out
index d05e4f165d..ca388b97f8 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part10.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part10.q.out
@@ -62,23 +62,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part10
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.nzhang_part10
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/load_dyn_part14.q.out b/ql/src/test/results/clientpositive/load_dyn_part14.q.out
index 37ae8ded9b..118d198fa4 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part14.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part14.q.out
@@ -55,11 +55,16 @@ select key, value from (
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
-  Stage-2 depends on stages: Stage-1, Stage-4, Stage-5
-  Stage-0 depends on stages: Stage-2
+  Stage-2 depends on stages: Stage-1, Stage-9, Stage-10
+  Stage-8 depends on stages: Stage-2 , consists of Stage-5, Stage-4, Stage-6
+  Stage-5
+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7
   Stage-3 depends on stages: Stage-0
-  Stage-4 is a root stage
-  Stage-5 is a root stage
+  Stage-4
+  Stage-6
+  Stage-7 depends on stages: Stage-6
+  Stage-9 is a root stage
+  Stage-10 is a root stage
 
 STAGE PLANS:
   Stage: Stage-1
@@ -104,12 +109,14 @@ STAGE PLANS:
                 expressions: _col0 (type: string), _col1 (type: string)
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  key expressions: _col1 (type: string)
-                  sort order: +
-                  Map-reduce partition columns: _col1 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: string), _col1 (type: string)
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part14
           TableScan
             Union
               Statistics: Num rows: 6 Data size: 1022 Basic stats: COMPLETE Column stats: COMPLETE
@@ -117,12 +124,14 @@ STAGE PLANS:
                 expressions: _col0 (type: string), _col1 (type: string)
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  key expressions: _col1 (type: string)
-                  sort order: +
-                  Map-reduce partition columns: _col1 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: string), _col1 (type: string)
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part14
           TableScan
             Union
               Statistics: Num rows: 6 Data size: 1022 Basic stats: COMPLETE Column stats: COMPLETE
@@ -130,23 +139,23 @@ STAGE PLANS:
                 expressions: _col0 (type: string), _col1 (type: string)
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  key expressions: _col1 (type: string)
-                  sort order: +
-                  Map-reduce partition columns: _col1 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: string), _col1 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 6 Data size: 1026 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part14
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part14
+
+  Stage: Stage-8
+    Conditional Operator
+
+  Stage: Stage-5
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -164,6 +173,36 @@ STAGE PLANS:
     Stats-Aggr Operator
 
   Stage: Stage-4
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part14
+
+  Stage: Stage-6
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part14
+
+  Stage: Stage-7
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
+  Stage: Stage-9
     Map Reduce
       Map Operator Tree:
           TableScan
@@ -195,7 +234,7 @@ STAGE PLANS:
                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                   serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
 
-  Stage: Stage-5
+  Stage: Stage-10
     Map Reduce
       Map Operator Tree:
           TableScan
diff --git a/ql/src/test/results/clientpositive/load_dyn_part3.q.out b/ql/src/test/results/clientpositive/load_dyn_part3.q.out
index 77ba8aa241..3242c3d432 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part3.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part3.q.out
@@ -60,23 +60,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string), _col3 (type: string)
-                sort order: ++
-                Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part3
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.nzhang_part3
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/load_dyn_part4.q.out b/ql/src/test/results/clientpositive/load_dyn_part4.q.out
index 80955e36f8..d24875f000 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part4.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part4.q.out
@@ -70,23 +70,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string), _col3 (type: string)
-                sort order: ++
-                Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part4
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.nzhang_part4
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/load_dyn_part5.q.out b/ql/src/test/results/clientpositive/load_dyn_part5.q.out
index 0bcc4326f6..e4bc742624 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part5.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part5.q.out
@@ -43,23 +43,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string)
               outputColumnNames: _col0, _col1
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col1 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col1 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part5
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.nzhang_part5
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/load_dyn_part8.q.out b/ql/src/test/results/clientpositive/load_dyn_part8.q.out
index a542fa8a8d..dc55eec3be 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part8.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part8.q.out
@@ -115,9 +115,8 @@ STAGE DEPENDENCIES:
   Stage-2 is a root stage
   Stage-0 depends on stages: Stage-2
   Stage-3 depends on stages: Stage-0
-  Stage-4 depends on stages: Stage-2
-  Stage-1 depends on stages: Stage-4
-  Stage-5 depends on stages: Stage-1
+  Stage-1 depends on stages: Stage-2
+  Stage-4 depends on stages: Stage-1
 
 STAGE PLANS:
   Stage: Stage-2
@@ -135,14 +134,34 @@ STAGE PLANS:
                 expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  key expressions: _col2 (type: string), _col3 (type: string)
-                  sort order: ++
-                  Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 1
+#### A masked pattern was here ####
+                  NumFilesPerFileSink: 1
                   Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                  tag: -1
-                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-                  auto parallelism: false
+#### A masked pattern was here ####
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      properties:
+                        bucket_count -1
+                        columns key,value
+                        columns.comments default default
+                        columns.types string:string
+#### A masked pattern was here ####
+                        name default.nzhang_part8
+                        partition_columns ds/hr
+                        partition_columns.types string:string
+                        serialization.ddl struct nzhang_part8 { string key, string value}
+                        serialization.format 1
+                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part8
+                  TotalFiles: 1
+                  GatherStats: true
+                  MultiFileSpray: false
             Filter Operator
               isSamplingPred: false
               predicate: (ds > '2008-04-08') (type: boolean)
@@ -153,20 +172,32 @@ STAGE PLANS:
                 Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
-                  GlobalTableId: 0
+                  GlobalTableId: 2
 #### A masked pattern was here ####
                   NumFilesPerFileSink: 1
+                  Static Partition Specification: ds=2008-12-31/
+                  Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
+#### A masked pattern was here ####
                   table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                       properties:
-                        columns _col0,_col1,_col2
-                        columns.types string,string,string
-                        escape.delim \
-                        serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+                        bucket_count -1
+                        columns key,value
+                        columns.comments default default
+                        columns.types string:string
+#### A masked pattern was here ####
+                        name default.nzhang_part8
+                        partition_columns ds/hr
+                        partition_columns.types string:string
+                        serialization.ddl struct nzhang_part8 { string key, string value}
+                        serialization.format 1
+                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part8
                   TotalFiles: 1
-                  GatherStats: false
+                  GatherStats: true
                   MultiFileSpray: false
       Path -> Alias:
 #### A masked pattern was here ####
@@ -360,38 +391,6 @@ STAGE PLANS:
         /srcpart/ds=2008-04-08/hr=12 [srcpart]
         /srcpart/ds=2008-04-09/hr=11 [srcpart]
         /srcpart/ds=2008-04-09/hr=12 [srcpart]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 1
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                properties:
-                  bucket_count -1
-                  columns key,value
-                  columns.comments default default
-                  columns.types string:string
-#### A masked pattern was here ####
-                  name default.nzhang_part8
-                  partition_columns ds/hr
-                  partition_columns.types string:string
-                  serialization.ddl struct nzhang_part8 { string key, string value}
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part8
-            TotalFiles: 1
-            GatherStats: true
-            MultiFileSpray: false
 
   Stage: Stage-0
     Move Operator
@@ -424,78 +423,6 @@ STAGE PLANS:
     Stats-Aggr Operator
 #### A masked pattern was here ####
 
-  Stage: Stage-4
-    Map Reduce
-      Map Operator Tree:
-          TableScan
-            GatherStats: false
-            Reduce Output Operator
-              key expressions: _col2 (type: string)
-              sort order: +
-              Map-reduce partition columns: _col2 (type: string)
-              Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-              tag: -1
-              value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-              auto parallelism: false
-      Path -> Alias:
-#### A masked pattern was here ####
-      Path -> Partition:
-#### A masked pattern was here ####
-          Partition
-            base file name: -mr-10002
-            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-            properties:
-              columns _col0,_col1,_col2
-              columns.types string,string,string
-              escape.delim \
-              serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-            serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-          
-              input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-              output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-              properties:
-                columns _col0,_col1,_col2
-                columns.types string,string,string
-                escape.delim \
-                serialization.lib org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-              serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-      Truncated Path -> Alias:
-#### A masked pattern was here ####
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 2
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Static Partition Specification: ds=2008-12-31/
-            Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                properties:
-                  bucket_count -1
-                  columns key,value
-                  columns.comments default default
-                  columns.types string:string
-#### A masked pattern was here ####
-                  name default.nzhang_part8
-                  partition_columns ds/hr
-                  partition_columns.types string:string
-                  serialization.ddl struct nzhang_part8 { string key, string value}
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part8
-            TotalFiles: 1
-            GatherStats: true
-            MultiFileSpray: false
-
   Stage: Stage-1
     Move Operator
       tables:
@@ -523,7 +450,7 @@ STAGE PLANS:
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.nzhang_part8
 
-  Stage: Stage-5
+  Stage: Stage-4
     Stats-Aggr Operator
 #### A masked pattern was here ####
 
diff --git a/ql/src/test/results/clientpositive/load_dyn_part9.q.out b/ql/src/test/results/clientpositive/load_dyn_part9.q.out
index d782880565..300f41e22e 100644
--- a/ql/src/test/results/clientpositive/load_dyn_part9.q.out
+++ b/ql/src/test/results/clientpositive/load_dyn_part9.q.out
@@ -62,23 +62,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string), _col3 (type: string)
-                sort order: ++
-                Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part9
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.nzhang_part9
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/merge3.q.out b/ql/src/test/results/clientpositive/merge3.q.out
index 1701be10b2..a0dc3a8fe7 100644
--- a/ql/src/test/results/clientpositive/merge3.q.out
+++ b/ql/src/test/results/clientpositive/merge3.q.out
@@ -2447,14 +2447,34 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
+                GlobalTableId: 1
+#### A masked pattern was here ####
+                NumFilesPerFileSink: 1
                 Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                tag: -1
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-                auto parallelism: false
+#### A masked pattern was here ####
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    properties:
+                      bucket_count -1
+                      columns key,value
+                      columns.comments  
+                      columns.types string:string
+#### A masked pattern was here ####
+                      name default.merge_src_part2
+                      partition_columns ds
+                      partition_columns.types string
+                      serialization.ddl struct merge_src_part2 { string key, string value}
+                      serialization.format 1
+                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.merge_src_part2
+                TotalFiles: 1
+                GatherStats: true
+                MultiFileSpray: false
       Path -> Alias:
 #### A masked pattern was here ####
       Path -> Partition:
@@ -2473,7 +2493,7 @@ STAGE PLANS:
               columns.types string:string
 #### A masked pattern was here ####
               name default.merge_src_part
-              numFiles 1
+              numFiles 2
               numRows 1000
               partition_columns ds
               partition_columns.types string
@@ -2518,7 +2538,7 @@ STAGE PLANS:
               columns.types string:string
 #### A masked pattern was here ####
               name default.merge_src_part
-              numFiles 1
+              numFiles 2
               numRows 1000
               partition_columns ds
               partition_columns.types string
@@ -2551,38 +2571,6 @@ STAGE PLANS:
       Truncated Path -> Alias:
         /merge_src_part/ds=2008-04-08 [merge_src_part]
         /merge_src_part/ds=2008-04-09 [merge_src_part]
-      Needs Tagging: false
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            GlobalTableId: 1
-#### A masked pattern was here ####
-            NumFilesPerFileSink: 1
-            Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-#### A masked pattern was here ####
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                properties:
-                  bucket_count -1
-                  columns key,value
-                  columns.comments  
-                  columns.types string:string
-#### A masked pattern was here ####
-                  name default.merge_src_part2
-                  partition_columns ds
-                  partition_columns.types string
-                  serialization.ddl struct merge_src_part2 { string key, string value}
-                  serialization.format 1
-                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-#### A masked pattern was here ####
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.merge_src_part2
-            TotalFiles: 1
-            GatherStats: true
-            MultiFileSpray: false
 
   Stage: Stage-7
     Conditional Operator
@@ -4915,8 +4903,7 @@ STAGE PLANS:
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
               Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
+                sort order: 
                 Map-reduce partition columns: _col2 (type: string)
                 Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
                 tag: -1
@@ -4940,7 +4927,7 @@ STAGE PLANS:
               columns.types string:string
 #### A masked pattern was here ####
               name default.merge_src_part
-              numFiles 1
+              numFiles 2
               numRows 1000
               partition_columns ds
               partition_columns.types string
@@ -4985,7 +4972,7 @@ STAGE PLANS:
               columns.types string:string
 #### A masked pattern was here ####
               name default.merge_src_part
-              numFiles 1
+              numFiles 2
               numRows 1000
               partition_columns ds
               partition_columns.types string
diff --git a/ql/src/test/results/clientpositive/merge4.q.out b/ql/src/test/results/clientpositive/merge4.q.out
index 515395f9b9..f86c21c001 100644
--- a/ql/src/test/results/clientpositive/merge4.q.out
+++ b/ql/src/test/results/clientpositive/merge4.q.out
@@ -37,23 +37,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.nzhang_part
 
   Stage: Stage-7
     Conditional Operator
@@ -2830,12 +2821,14 @@ STAGE PLANS:
                   expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                   outputColumnNames: _col0, _col1, _col2
                   Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
-                  Reduce Output Operator
-                    key expressions: _col2 (type: string)
-                    sort order: +
-                    Map-reduce partition columns: _col2 (type: string)
+                  File Output Operator
+                    compressed: false
                     Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
-                    value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                        name: default.nzhang_part
           TableScan
             Union
               Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
@@ -2843,23 +2836,14 @@ STAGE PLANS:
                 expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
                 outputColumnNames: _col0, _col1, _col2
                 Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
-                Reduce Output Operator
-                  key expressions: _col2 (type: string)
-                  sort order: +
-                  Map-reduce partition columns: _col2 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
-                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1001 Data size: 10883 Basic stats: COMPLETE Column stats: COMPLETE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part
 
   Stage: Stage-8
     Conditional Operator
diff --git a/ql/src/test/results/clientpositive/merge_dynamic_partition3.q.out b/ql/src/test/results/clientpositive/merge_dynamic_partition3.q.out
index 06b11dc43c..86978f3f47 100644
--- a/ql/src/test/results/clientpositive/merge_dynamic_partition3.q.out
+++ b/ql/src/test/results/clientpositive/merge_dynamic_partition3.q.out
@@ -155,23 +155,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 174 Data size: 34830 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string), _col3 (type: string)
-                sort order: ++
-                Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 174 Data size: 34830 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 174 Data size: 34830 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 174 Data size: 34830 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.merge_dynamic_part
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.merge_dynamic_part
 
   Stage: Stage-7
     Conditional Operator
@@ -286,9 +277,9 @@ outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
 columns:struct columns { string key, string value}
 partitioned:true
 partitionColumns:struct partition_columns { string ds, string hr}
-totalNumberFiles:4
+totalNumberFiles:6
 totalFileSize:34830
-maxFileSize:11603
-minFileSize:5812
+maxFileSize:5812
+minFileSize:5791
 #### A masked pattern was here ####
 
diff --git a/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out b/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
index f14d6ca49d..7fd0bfc8bf 100644
--- a/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
+++ b/ql/src/test/results/clientpositive/merge_dynamic_partition4.q.out
@@ -158,23 +158,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), if(((key % 2) = 0), 'a1', 'b1') (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 1000 Data size: 9624 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 1000 Data size: 9624 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 1000 Data size: 9624 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 1000 Data size: 9624 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                name: default.merge_dynamic_part
+                table:
+                    input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                    name: default.merge_dynamic_part
 
   Stage: Stage-7
     Conditional Operator
diff --git a/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out b/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
index 8a786c8463..044b76f919 100644
--- a/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
+++ b/ql/src/test/results/clientpositive/merge_dynamic_partition5.q.out
@@ -132,23 +132,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), if(((key % 100) = 0), 'a1', 'b1') (type: string)
               outputColumnNames: _col0, _col1, _col2
               Statistics: Num rows: 618 Data size: 5934 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string)
-                sort order: +
-                Map-reduce partition columns: _col2 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 618 Data size: 5934 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 618 Data size: 5934 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 618 Data size: 5934 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                name: default.merge_dynamic_part
+                table:
+                    input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                    name: default.merge_dynamic_part
 
   Stage: Stage-7
     Conditional Operator
diff --git a/ql/src/test/results/clientpositive/orc_analyze.q.out b/ql/src/test/results/clientpositive/orc_analyze.q.out
index af79e265f2..e718b297ce 100644
--- a/ql/src/test/results/clientpositive/orc_analyze.q.out
+++ b/ql/src/test/results/clientpositive/orc_analyze.q.out
@@ -624,10 +624,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	21980               
-	totalSize           	4959                
+	rawDataSize         	21950               
+	totalSize           	2024                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -669,10 +669,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	22048               
-	totalSize           	5044                
+	rawDataSize         	22050               
+	totalSize           	2043                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -777,10 +777,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	21980               
-	totalSize           	4959                
+	rawDataSize         	21950               
+	totalSize           	2024                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -822,10 +822,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	22048               
-	totalSize           	5044                
+	rawDataSize         	22050               
+	totalSize           	2043                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
diff --git a/ql/src/test/results/clientpositive/orc_merge2.q.out b/ql/src/test/results/clientpositive/orc_merge2.q.out
index 8f26ef0b9a..b927b75ecc 100644
--- a/ql/src/test/results/clientpositive/orc_merge2.q.out
+++ b/ql/src/test/results/clientpositive/orc_merge2.q.out
@@ -26,8 +26,13 @@ POSTHOOK: query: EXPLAIN INSERT OVERWRITE TABLE orcfile_merge2a PARTITION (one='
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5
+  Stage-4
+  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6
   Stage-2 depends on stages: Stage-0
+  Stage-3
+  Stage-5
+  Stage-6 depends on stages: Stage-5
 
 STAGE PLANS:
   Stage: Stage-1
@@ -40,23 +45,23 @@ STAGE PLANS:
               expressions: UDFToInteger(key) (type: int), value (type: string), (hash(key) pmod 10) (type: int), (hash(value) pmod 10) (type: int)
               outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: int), _col3 (type: int)
-                sort order: ++
-                Map-reduce partition columns: _col2 (type: int), _col3 (type: int)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: int), _col3 (type: int)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
-                serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
-                name: default.orcfile_merge2a
+                table:
+                    input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                    serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                    name: default.orcfile_merge2a
+
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-4
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -75,6 +80,26 @@ STAGE PLANS:
   Stage: Stage-2
     Stats-Aggr Operator
 
+  Stage: Stage-3
+    Merge File Operator
+      Map Operator Tree:
+          ORC File Merge Operator
+      merge level: stripe
+      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+
+  Stage: Stage-5
+    Merge File Operator
+      Map Operator Tree:
+          ORC File Merge Operator
+      merge level: stripe
+      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+
+  Stage: Stage-6
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
 PREHOOK: query: INSERT OVERWRITE TABLE orcfile_merge2a PARTITION (one='1', two, three)
     SELECT key, value, PMOD(HASH(key), 10) as two, 
         PMOD(HASH(value), 10) as three
diff --git a/ql/src/test/results/clientpositive/stats2.q.out b/ql/src/test/results/clientpositive/stats2.q.out
index 963f8d4ad8..694c1a2f68 100644
--- a/ql/src/test/results/clientpositive/stats2.q.out
+++ b/ql/src/test/results/clientpositive/stats2.q.out
@@ -27,23 +27,14 @@ STAGE PLANS:
               expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
               outputColumnNames: _col0, _col1, _col2, _col3
               Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-              Reduce Output Operator
-                key expressions: _col2 (type: string), _col3 (type: string)
-                sort order: ++
-                Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+              File Output Operator
+                compressed: false
                 Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.analyze_t1
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                    name: default.analyze_t1
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/stats4.q.out b/ql/src/test/results/clientpositive/stats4.q.out
index 7327b54860..39d5413c92 100644
--- a/ql/src/test/results/clientpositive/stats4.q.out
+++ b/ql/src/test/results/clientpositive/stats4.q.out
@@ -48,11 +48,20 @@ insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, v
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-2 is a root stage
-  Stage-0 depends on stages: Stage-2
+  Stage-8 depends on stages: Stage-2 , consists of Stage-5, Stage-4, Stage-6
+  Stage-5
+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7
   Stage-3 depends on stages: Stage-0
-  Stage-4 depends on stages: Stage-2
-  Stage-1 depends on stages: Stage-4
-  Stage-5 depends on stages: Stage-1
+  Stage-4
+  Stage-6
+  Stage-7 depends on stages: Stage-6
+  Stage-14 depends on stages: Stage-2 , consists of Stage-11, Stage-10, Stage-12
+  Stage-11
+  Stage-1 depends on stages: Stage-11, Stage-10, Stage-13
+  Stage-9 depends on stages: Stage-1
+  Stage-10
+  Stage-12
+  Stage-13 depends on stages: Stage-12
 
 STAGE PLANS:
   Stage: Stage-2
@@ -68,12 +77,14 @@ STAGE PLANS:
                 expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
                 outputColumnNames: _col0, _col1, _col2, _col3
                 Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  key expressions: _col2 (type: string), _col3 (type: string)
-                  sort order: ++
-                  Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part1
             Filter Operator
               predicate: (ds > '2008-04-08') (type: boolean)
               Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
@@ -83,21 +94,21 @@ STAGE PLANS:
                 Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
+                  Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
                   table:
-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part1
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.nzhang_part2
+
+  Stage: Stage-8
+    Conditional Operator
+
+  Stage: Stage-5
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -119,23 +130,40 @@ STAGE PLANS:
     Map Reduce
       Map Operator Tree:
           TableScan
-            Reduce Output Operator
-              key expressions: _col2 (type: string)
-              sort order: +
-              Map-reduce partition columns: _col2 (type: string)
-              Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-              value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.nzhang_part2
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part1
+
+  Stage: Stage-6
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part1
+
+  Stage: Stage-7
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
+  Stage: Stage-14
+    Conditional Operator
+
+  Stage: Stage-11
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-1
     Move Operator
@@ -150,9 +178,39 @@ STAGE PLANS:
               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
               name: default.nzhang_part2
 
-  Stage: Stage-5
+  Stage: Stage-9
     Stats-Aggr Operator
 
+  Stage: Stage-10
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part2
+
+  Stage: Stage-12
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.nzhang_part2
+
+  Stage: Stage-13
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
 PREHOOK: query: from srcpart
 insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
 insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08'
diff --git a/ql/src/test/results/clientpositive/stats_empty_dyn_part.q.out b/ql/src/test/results/clientpositive/stats_empty_dyn_part.q.out
index 27a3742a38..982baab705 100644
--- a/ql/src/test/results/clientpositive/stats_empty_dyn_part.q.out
+++ b/ql/src/test/results/clientpositive/stats_empty_dyn_part.q.out
@@ -20,8 +20,13 @@ POSTHOOK: query: explain insert overwrite table tmptable partition (part) select
 POSTHOOK: type: QUERY
 STAGE DEPENDENCIES:
   Stage-1 is a root stage
-  Stage-0 depends on stages: Stage-1
+  Stage-7 depends on stages: Stage-1 , consists of Stage-4, Stage-3, Stage-5
+  Stage-4
+  Stage-0 depends on stages: Stage-4, Stage-3, Stage-6
   Stage-2 depends on stages: Stage-0
+  Stage-3
+  Stage-5
+  Stage-6 depends on stages: Stage-5
 
 STAGE PLANS:
   Stage: Stage-1
@@ -37,23 +42,23 @@ STAGE PLANS:
                 expressions: 'no_such_value' (type: string), value (type: string)
                 outputColumnNames: _col0, _col1
                 Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                Reduce Output Operator
-                  key expressions: _col1 (type: string)
-                  sort order: +
-                  Map-reduce partition columns: _col1 (type: string)
+                File Output Operator
+                  compressed: false
                   Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-                  value expressions: _col0 (type: string), _col1 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
-            table:
-                input format: org.apache.hadoop.mapred.TextInputFormat
-                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                name: default.tmptable
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.tmptable
+
+  Stage: Stage-7
+    Conditional Operator
+
+  Stage: Stage-4
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
 
   Stage: Stage-0
     Move Operator
@@ -70,6 +75,36 @@ STAGE PLANS:
   Stage: Stage-2
     Stats-Aggr Operator
 
+  Stage: Stage-3
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.tmptable
+
+  Stage: Stage-5
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.tmptable
+
+  Stage: Stage-6
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
 PREHOOK: query: insert overwrite table tmptable partition (part) select key, value from src where key = 'no_such_value'
 PREHOOK: type: QUERY
 PREHOOK: Input: default@src
diff --git a/ql/src/test/results/clientpositive/tez/auto_sortmerge_join_16.q.out b/ql/src/test/results/clientpositive/tez/auto_sortmerge_join_16.q.out
index 03507dd063..b32ac060d3 100644
--- a/ql/src/test/results/clientpositive/tez/auto_sortmerge_join_16.q.out
+++ b/ql/src/test/results/clientpositive/tez/auto_sortmerge_join_16.q.out
@@ -232,6 +232,16 @@ POSTHOOK: Input: default@bucket_small@pri=2
 0	val_0	val_0	day1	1
 0	val_0	val_0	day1	1
 0	val_0	val_0	day1	1
+103	val_103	val_103	day1	1
+103	val_103	val_103	day1	1
+103	val_103	val_103	day1	1
+103	val_103	val_103	day1	1
+374	val_374	val_374	day1	1
+374	val_374	val_374	day1	1
+172	val_172	val_172	day1	1
+172	val_172	val_172	day1	1
+172	val_172	val_172	day1	1
+172	val_172	val_172	day1	1
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
@@ -240,13 +250,3 @@ POSTHOOK: Input: default@bucket_small@pri=2
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
 169	val_169	val_169	day1	1
-374	val_374	val_374	day1	1
-374	val_374	val_374	day1	1
-172	val_172	val_172	day1	1
-172	val_172	val_172	day1	1
-172	val_172	val_172	day1	1
-172	val_172	val_172	day1	1
-103	val_103	val_103	day1	1
-103	val_103	val_103	day1	1
-103	val_103	val_103	day1	1
-103	val_103	val_103	day1	1
diff --git a/ql/src/test/results/clientpositive/tez/delete_all_partitioned.q.out b/ql/src/test/results/clientpositive/tez/delete_all_partitioned.q.out
index 90f8753687..c5149b28b4 100644
--- a/ql/src/test/results/clientpositive/tez/delete_all_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/tez/delete_all_partitioned.q.out
@@ -84,3 +84,5 @@ POSTHOOK: Input: default@acid_dap
 POSTHOOK: Input: default@acid_dap@ds=today
 POSTHOOK: Input: default@acid_dap@ds=tomorrow
 #### A masked pattern was here ####
+-1071480828	aw724t8c5558x2xneC624	today
+-1072076362	2uLyD28144vklju213J1mr	today
diff --git a/ql/src/test/results/clientpositive/tez/dynpart_sort_opt_vectorization.q.out b/ql/src/test/results/clientpositive/tez/dynpart_sort_opt_vectorization.q.out
index ed42e670d4..d5202172bb 100644
--- a/ql/src/test/results/clientpositive/tez/dynpart_sort_opt_vectorization.q.out
+++ b/ql/src/test/results/clientpositive/tez/dynpart_sort_opt_vectorization.q.out
@@ -352,11 +352,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                         sort order: ++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
             Execution mode: vectorized
         Reducer 2 
             Reduce Operator Tree:
@@ -420,11 +420,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                         sort order: +++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
             Execution mode: vectorized
         Reducer 2 
             Reduce Operator Tree:
@@ -727,11 +727,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                         sort order: ++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
             Execution mode: vectorized
         Reducer 2 
             Reduce Operator Tree:
@@ -795,11 +795,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                         sort order: +++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
             Execution mode: vectorized
         Reducer 2 
             Reduce Operator Tree:
@@ -2160,11 +2160,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                         sort order: +++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 1048 Data size: 310873 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
             Execution mode: vectorized
         Reducer 2 
             Reduce Operator Tree:
diff --git a/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization.q.out b/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization.q.out
index 5ff3e49a4e..8c930950ab 100644
--- a/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization.q.out
+++ b/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization.q.out
@@ -291,11 +291,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                         sort order: ++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
         Reducer 2 
             Reduce Operator Tree:
               Extract
@@ -357,11 +357,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                         sort order: +++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
         Reducer 2 
             Reduce Operator Tree:
               Extract
@@ -644,11 +644,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string)
                         sort order: ++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
         Reducer 2 
             Reduce Operator Tree:
               Extract
@@ -710,11 +710,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                         sort order: +++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
         Reducer 2 
             Reduce Operator Tree:
               Extract
@@ -2058,11 +2058,11 @@ STAGE PLANS:
                       outputColumnNames: _col0, _col1, _col2, _col3, _col4
                       Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
-                        key expressions: _col4 (type: tinyint), -1 (type: int), _col3 (type: float)
+                        key expressions: _col4 (type: tinyint), '_bucket_number' (type: string), _col3 (type: float)
                         sort order: +++
                         Map-reduce partition columns: _col4 (type: tinyint)
                         Statistics: Num rows: 4442 Data size: 106611 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint)
+                        value expressions: _col0 (type: smallint), _col1 (type: int), _col2 (type: bigint), _col3 (type: float), _col4 (type: tinyint), '_bucket_number' (type: string)
         Reducer 2 
             Reduce Operator Tree:
               Extract
diff --git a/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization2.q.out b/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization2.q.out
new file mode 100644
index 0000000000..52bef6f225
--- /dev/null
+++ b/ql/src/test/results/clientpositive/tez/dynpart_sort_optimization2.q.out
@@ -0,0 +1,1866 @@
+PREHOOK: query: drop table ss
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table ss_orc
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss_orc
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table ss_part
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss_part
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table ss_part_orc
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table ss_part_orc
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table ss (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss
+POSTHOOK: query: create table ss (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss
+PREHOOK: query: create table ss_part (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: create table ss_part (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss_part
+PREHOOK: query: load data local inpath '../../data/files/dynpart_test.txt' overwrite into table ss
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@ss
+POSTHOOK: query: load data local inpath '../../data/files/dynpart_test.txt' overwrite into table ss
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@ss
+PREHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: ss
+                  Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+                    Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                      outputColumnNames: ss_sold_date_sk, ss_net_paid_inc_tax, ss_net_profit
+                      Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        keys: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2
+                        Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int), _col1 (type: float), _col2 (type: float)
+                          sort order: +++
+                          Map-reduce partition columns: _col0 (type: int)
+                          Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: int), KEY._col1 (type: float), KEY._col2 (type: float)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: float), _col2 (type: float), _col0 (type: int)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                        name: default.ss_part
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+2.1	-2026.3	2452617
+2.99	-11.32	2452617
+85.8	25.61	2452617
+552.96	-1363.84	2452617
+565.92	196.48	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+7412.83	2071.68	2452617
+10022.63	3952.8	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+0.15	-241.22	2452638
+150.39	-162.12	2452638
+156.67	-4626.56	2452638
+181.03	-207.24	2452638
+267.01	-3266.36	2452638
+317.87	-3775.38	2452638
+1327.08	57.97	2452638
+1413.19	178.08	2452638
+1524.33	494.37	2452638
+1971.35	-488.25	2452638
+4133.98	-775.72	2452638
+4329.49	-4000.51	2452638
+10171.1	660.48	2452638
+PREHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: ss
+                  Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+                    Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_net_paid_inc_tax (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col2 (type: int)
+                        sort order: +
+                        Map-reduce partition columns: _col2 (type: int)
+                        Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: int)
+        Reducer 2 
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: float), VALUE._col1 (type: float), VALUE._col2 (type: int)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.ss_part
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+10022.63	3952.8	2452617
+2.99	-11.32	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+565.92	196.48	2452617
+85.8	25.61	2452617
+7412.83	2071.68	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+552.96	-1363.84	2452617
+2.1	-2026.3	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+4329.49	-4000.51	2452638
+1413.19	178.08	2452638
+150.39	-162.12	2452638
+1524.33	494.37	2452638
+0.15	-241.22	2452638
+267.01	-3266.36	2452638
+181.03	-207.24	2452638
+1971.35	-488.25	2452638
+1327.08	57.97	2452638
+156.67	-4626.56	2452638
+317.87	-3775.38	2452638
+10171.1	660.48	2452638
+4133.98	-775.72	2452638
+PREHOOK: query: -- SORT DYNAMIC PARTITION DISABLED
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: -- SORT DYNAMIC PARTITION DISABLED
+
+explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: ss
+                  Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+                    Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                      outputColumnNames: ss_sold_date_sk, ss_net_paid_inc_tax, ss_net_profit
+                      Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        keys: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2
+                        Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int), _col1 (type: float), _col2 (type: float)
+                          sort order: +++
+                          Map-reduce partition columns: _col0 (type: int)
+                          Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: int), KEY._col1 (type: float), KEY._col2 (type: float)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: float), _col2 (type: float), _col0 (type: int)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                        name: default.ss_part
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+2.1	-2026.3	2452617
+2.99	-11.32	2452617
+85.8	25.61	2452617
+552.96	-1363.84	2452617
+565.92	196.48	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+7412.83	2071.68	2452617
+10022.63	3952.8	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+0.15	-241.22	2452638
+150.39	-162.12	2452638
+156.67	-4626.56	2452638
+181.03	-207.24	2452638
+267.01	-3266.36	2452638
+317.87	-3775.38	2452638
+1327.08	57.97	2452638
+1413.19	178.08	2452638
+1524.33	494.37	2452638
+1971.35	-488.25	2452638
+4133.98	-775.72	2452638
+4329.49	-4000.51	2452638
+10171.1	660.48	2452638
+PREHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: ss
+                  Statistics: Num rows: 46 Data size: 553 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+                    Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_net_paid_inc_tax (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Map-reduce partition columns: _col2 (type: int)
+                        Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: int)
+        Reducer 2 
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: float), VALUE._col1 (type: float), VALUE._col2 (type: int)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 5 Data size: 60 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.ss_part
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.ss_part
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: insert overwrite table ss_part partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	151                 
+	totalSize           	162                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+565.92	196.48	2452617
+85.8	25.61	2452617
+7412.83	2071.68	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+552.96	-1363.84	2452617
+2.1	-2026.3	2452617
+10022.63	3952.8	2452617
+2.99	-11.32	2452617
+PREHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part
+POSTHOOK: query: desc formatted ss_part partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part             	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	186                 
+	totalSize           	199                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part
+PREHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Input: default@ss_part@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+4329.49	-4000.51	2452638
+1413.19	178.08	2452638
+150.39	-162.12	2452638
+1524.33	494.37	2452638
+0.15	-241.22	2452638
+267.01	-3266.36	2452638
+181.03	-207.24	2452638
+1971.35	-488.25	2452638
+1327.08	57.97	2452638
+156.67	-4626.56	2452638
+317.87	-3775.38	2452638
+10171.1	660.48	2452638
+4133.98	-775.72	2452638
+PREHOOK: query: -- VECTORIZATION IS ENABLED
+
+create table ss_orc (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss_orc
+POSTHOOK: query: -- VECTORIZATION IS ENABLED
+
+create table ss_orc (
+ss_sold_date_sk int,
+ss_net_paid_inc_tax float,
+ss_net_profit float) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss_orc
+PREHOOK: query: create table ss_part_orc (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int) stored as orc
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: create table ss_part_orc (
+ss_net_paid_inc_tax float,
+ss_net_profit float)
+partitioned by (ss_sold_date_sk int) stored as orc
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@ss_part_orc
+PREHOOK: query: insert overwrite table ss_orc select * from ss
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss_orc
+POSTHOOK: query: insert overwrite table ss_orc select * from ss
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss_orc
+POSTHOOK: Lineage: ss_orc.ss_net_paid_inc_tax SIMPLE [(ss)ss.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_orc.ss_net_profit SIMPLE [(ss)ss.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_orc.ss_sold_date_sk SIMPLE [(ss)ss.FieldSchema(name:ss_sold_date_sk, type:int, comment:null), ]
+PREHOOK: query: drop table ss
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss
+PREHOOK: Output: default@ss
+POSTHOOK: query: drop table ss
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss
+POSTHOOK: Output: default@ss
+PREHOOK: query: drop table ss_part
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss_part
+PREHOOK: Output: default@ss_part
+POSTHOOK: query: drop table ss_part
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss_part
+POSTHOOK: Output: default@ss_part
+PREHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: ss_orc
+                  Statistics: Num rows: 24 Data size: 288 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+                    Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                      outputColumnNames: ss_sold_date_sk, ss_net_paid_inc_tax, ss_net_profit
+                      Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                      Group By Operator
+                        keys: ss_sold_date_sk (type: int), ss_net_paid_inc_tax (type: float), ss_net_profit (type: float)
+                        mode: hash
+                        outputColumnNames: _col0, _col1, _col2
+                        Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                        Reduce Output Operator
+                          key expressions: _col0 (type: int), _col1 (type: float), _col2 (type: float)
+                          sort order: +++
+                          Map-reduce partition columns: _col0 (type: int)
+                          Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+            Execution mode: vectorized
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                keys: KEY._col0 (type: int), KEY._col1 (type: float), KEY._col2 (type: float)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col1 (type: float), _col2 (type: float), _col0 (type: int)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                        serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                        name: default.ss_part_orc
+            Execution mode: vectorized
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.ss_part_orc
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_orc
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+  group by ss_sold_date_sk,
+    ss_net_paid_inc_tax,
+    ss_net_profit
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_orc
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	88                  
+	totalSize           	417                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+2.1	-2026.3	2452617
+2.99	-11.32	2452617
+85.8	25.61	2452617
+552.96	-1363.84	2452617
+565.92	196.48	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+7412.83	2071.68	2452617
+10022.63	3952.8	2452617
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	104                 
+	totalSize           	440                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+0.15	-241.22	2452638
+150.39	-162.12	2452638
+156.67	-4626.56	2452638
+181.03	-207.24	2452638
+267.01	-3266.36	2452638
+317.87	-3775.38	2452638
+1327.08	57.97	2452638
+1413.19	178.08	2452638
+1524.33	494.37	2452638
+1971.35	-488.25	2452638
+4133.98	-775.72	2452638
+4329.49	-4000.51	2452638
+10171.1	660.48	2452638
+PREHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: ss_orc
+                  Statistics: Num rows: 24 Data size: 288 Basic stats: COMPLETE Column stats: NONE
+                  Filter Operator
+                    predicate: ((ss_sold_date_sk >= 2452617) and (ss_sold_date_sk <= 2452638)) (type: boolean)
+                    Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                    Select Operator
+                      expressions: ss_net_paid_inc_tax (type: float), ss_net_profit (type: float), ss_sold_date_sk (type: int)
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        sort order: 
+                        Map-reduce partition columns: _col2 (type: int)
+                        Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col0 (type: float), _col1 (type: float), _col2 (type: int)
+            Execution mode: vectorized
+        Reducer 2 
+            Reduce Operator Tree:
+              Select Operator
+                expressions: VALUE._col0 (type: float), VALUE._col1 (type: float), VALUE._col2 (type: int)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 2 Data size: 24 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                      name: default.ss_part_orc
+            Execution mode: vectorized
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ss_sold_date_sk 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.ss_part_orc
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_orc
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: insert overwrite table ss_part_orc partition (ss_sold_date_sk)
+select ss_net_paid_inc_tax,
+  ss_net_profit,
+  ss_sold_date_sk
+  from ss_orc
+  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
+    distribute by ss_sold_date_sk
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_orc
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452617
+POSTHOOK: Output: default@ss_part_orc@ss_sold_date_sk=2452638
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452617).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_paid_inc_tax SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_paid_inc_tax, type:float, comment:null), ]
+POSTHOOK: Lineage: ss_part_orc PARTITION(ss_sold_date_sk=2452638).ss_net_profit SIMPLE [(ss_orc)ss_orc.FieldSchema(name:ss_net_profit, type:float, comment:null), ]
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452617)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452617]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	11                  
+	rawDataSize         	88                  
+	totalSize           	417                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452617
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452617
+#### A masked pattern was here ####
+3423.95	-3164.07	2452617
+5362.01	-600.28	2452617
+565.92	196.48	2452617
+85.8	25.61	2452617
+7412.83	2071.68	2452617
+879.07	-2185.76	2452617
+1765.07	-4648.8	2452617
+552.96	-1363.84	2452617
+2.1	-2026.3	2452617
+10022.63	3952.8	2452617
+2.99	-11.32	2452617
+PREHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@ss_part_orc
+POSTHOOK: query: desc formatted ss_part_orc partition(ss_sold_date_sk=2452638)
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@ss_part_orc
+# col_name            	data_type           	comment             
+	 	 
+ss_net_paid_inc_tax 	float               	                    
+ss_net_profit       	float               	                    
+	 	 
+# Partition Information	 	 
+# col_name            	data_type           	comment             
+	 	 
+ss_sold_date_sk     	int                 	                    
+	 	 
+# Detailed Partition Information	 	 
+Partition Value:    	[2452638]           	 
+Database:           	default             	 
+Table:              	ss_part_orc         	 
+#### A masked pattern was here ####
+Protect Mode:       	None                	 
+#### A masked pattern was here ####
+Partition Parameters:	 	 
+	COLUMN_STATS_ACCURATE	true                
+	numFiles            	1                   
+	numRows             	13                  
+	rawDataSize         	104                 
+	totalSize           	440                 
+#### A masked pattern was here ####
+	 	 
+# Storage Information	 	 
+SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde	 
+InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 
+OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat	 
+Compressed:         	No                  	 
+Num Buckets:        	-1                  	 
+Bucket Columns:     	[]                  	 
+Sort Columns:       	[]                  	 
+Storage Desc Params:	 	 
+	serialization.format	1                   
+PREHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+PREHOOK: type: QUERY
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+POSTHOOK: query: select * from ss_part_orc where ss_sold_date_sk=2452638
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Input: default@ss_part_orc@ss_sold_date_sk=2452638
+#### A masked pattern was here ####
+4329.49	-4000.51	2452638
+1413.19	178.08	2452638
+150.39	-162.12	2452638
+1524.33	494.37	2452638
+0.15	-241.22	2452638
+267.01	-3266.36	2452638
+181.03	-207.24	2452638
+1971.35	-488.25	2452638
+1327.08	57.97	2452638
+156.67	-4626.56	2452638
+317.87	-3775.38	2452638
+10171.1	660.48	2452638
+4133.98	-775.72	2452638
+PREHOOK: query: drop table ss_orc
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss_orc
+PREHOOK: Output: default@ss_orc
+POSTHOOK: query: drop table ss_orc
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss_orc
+POSTHOOK: Output: default@ss_orc
+PREHOOK: query: drop table ss_part_orc
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@ss_part_orc
+PREHOOK: Output: default@ss_part_orc
+POSTHOOK: query: drop table ss_part_orc
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@ss_part_orc
+POSTHOOK: Output: default@ss_part_orc
+PREHOOK: query: drop table if exists hive13_dp1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists hive13_dp1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table if not exists hive13_dp1 (
+    k1 int,
+    k2 int
+)
+PARTITIONED BY(`day` string)
+STORED AS ORC
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: create table if not exists hive13_dp1 (
+    k1 int,
+    k2 int
+)
+PARTITIONED BY(`day` string)
+STORED AS ORC
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@hive13_dp1
+PREHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: src
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(value)
+                      keys: 'day' (type: string), key (type: string)
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string), _col1 (type: string)
+                        sort order: ++
+                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)
+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col2 (type: bigint)
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                keys: KEY._col0 (type: string), KEY._col1 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int), _col0 (type: string)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                        serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                        name: default.hive13_dp1
+            Execution mode: vectorized
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            day 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.hive13_dp1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hive13_dp1@day=day
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from hive13_dp1 limit 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hive13_dp1
+PREHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hive13_dp1 limit 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hive13_dp1
+POSTHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+0	3	day
+10	1	day
+100	2	day
+103	2	day
+104	2	day
+PREHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table `hive13_dp1` partition(`day`)
+select
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+  Stage-3 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Tez
+      Edges:
+        Reducer 2 <- Map 1 (SIMPLE_EDGE)
+#### A masked pattern was here ####
+      Vertices:
+        Map 1 
+            Map Operator Tree:
+                TableScan
+                  alias: src
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                  Select Operator
+                    expressions: key (type: string), value (type: string)
+                    outputColumnNames: key, value
+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                    Group By Operator
+                      aggregations: count(value)
+                      keys: 'day' (type: string), key (type: string)
+                      mode: hash
+                      outputColumnNames: _col0, _col1, _col2
+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                      Reduce Output Operator
+                        key expressions: _col0 (type: string), _col1 (type: string)
+                        sort order: ++
+                        Map-reduce partition columns: _col0 (type: string)
+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+                        value expressions: _col2 (type: bigint)
+        Reducer 2 
+            Reduce Operator Tree:
+              Group By Operator
+                aggregations: count(VALUE._col0)
+                keys: KEY._col0 (type: string), KEY._col1 (type: string)
+                mode: mergepartial
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: UDFToInteger(_col1) (type: int), UDFToInteger(_col2) (type: int), _col0 (type: string)
+                  outputColumnNames: _col0, _col1, _col2
+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                  File Output Operator
+                    compressed: false
+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
+                    table:
+                        input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                        serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                        name: default.hive13_dp1
+            Execution mode: vectorized
+
+  Stage: Stage-2
+    Dependency Collection
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            day 
+          replace: true
+          table:
+              input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+              output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+              serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+              name: default.hive13_dp1
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+PREHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select 
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: insert overwrite table `hive13_dp1` partition(`day`)
+select 
+    key k1,
+    count(value) k2,
+    "day" `day`
+from src
+group by "day", key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@hive13_dp1@day=day
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k1 EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: hive13_dp1 PARTITION(day=day).k2 EXPRESSION [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: select * from hive13_dp1 limit 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@hive13_dp1
+PREHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+POSTHOOK: query: select * from hive13_dp1 limit 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@hive13_dp1
+POSTHOOK: Input: default@hive13_dp1@day=day
+#### A masked pattern was here ####
+0	3	day
+10	1	day
+100	2	day
+103	2	day
+104	2	day
+PREHOOK: query: drop table hive13_dp1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@hive13_dp1
+PREHOOK: Output: default@hive13_dp1
+POSTHOOK: query: drop table hive13_dp1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@hive13_dp1
+POSTHOOK: Output: default@hive13_dp1
diff --git a/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out b/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out
index 8ff926bad1..dcf011e67e 100644
--- a/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out
+++ b/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out
@@ -67,9 +67,6 @@ STAGE DEPENDENCIES:
 STAGE PLANS:
   Stage: Stage-2
     Tez
-      Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
-        Reducer 3 <- Map 1 (SIMPLE_EDGE)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -84,12 +81,14 @@ STAGE PLANS:
                       expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
                       outputColumnNames: _col0, _col1, _col2, _col3
                       Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                      Reduce Output Operator
-                        key expressions: _col2 (type: string), _col3 (type: string)
-                        sort order: ++
-                        Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+                      File Output Operator
+                        compressed: false
                         Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                            name: default.nzhang_part1
                   Filter Operator
                     predicate: (ds > '2008-04-08') (type: boolean)
                     Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
@@ -97,36 +96,14 @@ STAGE PLANS:
                       expressions: key (type: string), value (type: string), hr (type: string)
                       outputColumnNames: _col0, _col1, _col2
                       Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                      Reduce Output Operator
-                        key expressions: _col2 (type: string)
-                        sort order: +
-                        Map-reduce partition columns: _col2 (type: string)
+                      File Output Operator
+                        compressed: false
                         Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                        value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)
-        Reducer 2 
-            Reduce Operator Tree:
-              Extract
-                Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                      name: default.nzhang_part1
-        Reducer 3 
-            Reduce Operator Tree:
-              Extract
-                Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 666 Data size: 7075 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                      name: default.nzhang_part2
+                        table:
+                            input format: org.apache.hadoop.mapred.TextInputFormat
+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                            name: default.nzhang_part2
 
   Stage: Stage-3
     Dependency Collection
diff --git a/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out b/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out
index 9745dea325..8f95a76262 100644
--- a/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out
+++ b/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out
@@ -53,8 +53,6 @@ STAGE DEPENDENCIES:
 STAGE PLANS:
   Stage: Stage-1
     Tez
-      Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -66,24 +64,14 @@ STAGE PLANS:
                     expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)
                     outputColumnNames: _col0, _col1, _col2, _col3
                     Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      key expressions: _col2 (type: string), _col3 (type: string)
-                      sort order: ++
-                      Map-reduce partition columns: _col2 (type: string), _col3 (type: string)
+                    File Output Operator
+                      compressed: false
                       Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)
-        Reducer 2 
-            Reduce Operator Tree:
-              Extract
-                Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                      name: default.nzhang_part3
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.nzhang_part3
 
   Stage: Stage-2
     Dependency Collection
diff --git a/ql/src/test/results/clientpositive/tez/orc_analyze.q.out b/ql/src/test/results/clientpositive/tez/orc_analyze.q.out
index af79e265f2..e718b297ce 100644
--- a/ql/src/test/results/clientpositive/tez/orc_analyze.q.out
+++ b/ql/src/test/results/clientpositive/tez/orc_analyze.q.out
@@ -624,10 +624,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	21980               
-	totalSize           	4959                
+	rawDataSize         	21950               
+	totalSize           	2024                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -669,10 +669,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	22048               
-	totalSize           	5044                
+	rawDataSize         	22050               
+	totalSize           	2043                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -777,10 +777,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	21980               
-	totalSize           	4959                
+	rawDataSize         	21950               
+	totalSize           	2024                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
@@ -822,10 +822,10 @@ Protect Mode:       	None
 #### A masked pattern was here ####
 Partition Parameters:	 	 
 	COLUMN_STATS_ACCURATE	true                
-	numFiles            	4                   
+	numFiles            	1                   
 	numRows             	50                  
-	rawDataSize         	22048               
-	totalSize           	5044                
+	rawDataSize         	22050               
+	totalSize           	2043                
 #### A masked pattern was here ####
 	 	 
 # Storage Information	 	 
diff --git a/ql/src/test/results/clientpositive/tez/orc_merge2.q.out b/ql/src/test/results/clientpositive/tez/orc_merge2.q.out
index e08e211dd0..0f1917f733 100644
--- a/ql/src/test/results/clientpositive/tez/orc_merge2.q.out
+++ b/ql/src/test/results/clientpositive/tez/orc_merge2.q.out
@@ -33,8 +33,6 @@ STAGE DEPENDENCIES:
 STAGE PLANS:
   Stage: Stage-1
     Tez
-      Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -46,24 +44,14 @@ STAGE PLANS:
                     expressions: UDFToInteger(key) (type: int), value (type: string), (hash(key) pmod 10) (type: int), (hash(value) pmod 10) (type: int)
                     outputColumnNames: _col0, _col1, _col2, _col3
                     Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      key expressions: _col2 (type: int), _col3 (type: int)
-                      sort order: ++
-                      Map-reduce partition columns: _col2 (type: int), _col3 (type: int)
+                    File Output Operator
+                      compressed: false
                       Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col0 (type: int), _col1 (type: string), _col2 (type: int), _col3 (type: int)
-        Reducer 2 
-            Reduce Operator Tree:
-              Extract
-                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
-                      serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
-                      name: default.orcfile_merge2a
+                      table:
+                          input format: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat
+                          serde: org.apache.hadoop.hive.ql.io.orc.OrcSerde
+                          name: default.orcfile_merge2a
 
   Stage: Stage-2
     Dependency Collection
diff --git a/ql/src/test/results/clientpositive/tez/tez_dml.q.out b/ql/src/test/results/clientpositive/tez/tez_dml.q.out
index a6c45d560b..9b7f56466a 100644
--- a/ql/src/test/results/clientpositive/tez/tez_dml.q.out
+++ b/ql/src/test/results/clientpositive/tez/tez_dml.q.out
@@ -443,8 +443,6 @@ STAGE DEPENDENCIES:
 STAGE PLANS:
   Stage: Stage-1
     Tez
-      Edges:
-        Reducer 2 <- Map 1 (SIMPLE_EDGE)
 #### A masked pattern was here ####
       Vertices:
         Map 1 
@@ -456,24 +454,14 @@ STAGE PLANS:
                     expressions: value (type: string), cnt (type: bigint)
                     outputColumnNames: _col0, _col1
                     Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE
-                    Reduce Output Operator
-                      key expressions: _col1 (type: bigint)
-                      sort order: +
-                      Map-reduce partition columns: _col1 (type: bigint)
+                    File Output Operator
+                      compressed: false
                       Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE
-                      value expressions: _col0 (type: string), _col1 (type: bigint)
-        Reducer 2 
-            Reduce Operator Tree:
-              Extract
-                Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE
-                File Output Operator
-                  compressed: false
-                  Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE
-                  table:
-                      input format: org.apache.hadoop.mapred.TextInputFormat
-                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
-                      name: default.tmp_src_part
+                      table:
+                          input format: org.apache.hadoop.mapred.TextInputFormat
+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                          name: default.tmp_src_part
 
   Stage: Stage-2
     Dependency Collection
@@ -529,314 +517,314 @@ POSTHOOK: Input: default@tmp_src_part@d=4
 POSTHOOK: Input: default@tmp_src_part@d=5
 #### A masked pattern was here ####
 val_490	1
-val_289	1
-val_291	1
-val_292	1
-val_296	1
-val_180	1
-val_30	1
-val_302	1
-val_305	1
-val_306	1
-val_491	1
-val_308	1
-val_96	1
-val_310	1
-val_178	1
-val_315	1
-val_177	1
-val_493	1
-val_170	1
-val_494	1
-val_495	1
-val_323	1
-val_496	1
-val_17	1
-val_33	1
-val_497	1
-val_332	1
-val_111	1
-val_335	1
-val_336	1
-val_338	1
-val_339	1
-val_34	1
-val_341	1
-val_11	1
-val_85	1
-val_345	1
-val_168	1
-val_166	1
-val_351	1
-val_53	1
-val_356	1
-val_360	1
-val_362	1
-val_364	1
-val_365	1
-val_366	1
-val_54	1
-val_368	1
-val_163	1
-val_57	1
-val_373	1
-val_374	1
-val_375	1
-val_377	1
-val_378	1
-val_379	1
-val_86	1
-val_162	1
-val_386	1
-val_389	1
-val_392	1
-val_393	1
-val_394	1
-val_64	1
-val_160	1
-val_65	1
-val_66	1
-val_4	1
-val_400	1
-val_158	1
-val_402	1
-val_157	1
-val_87	1
-val_156	1
-val_407	1
-val_155	1
-val_41	1
-val_411	1
-val_69	1
-val_105	1
-val_153	1
-val_418	1
-val_419	1
-val_9	1
-val_421	1
-val_74	1
-val_427	1
-val_10	1
-val_43	1
-val_150	1
-val_145	1
-val_432	1
-val_435	1
-val_436	1
-val_437	1
-val_143	1
-val_77	1
-val_44	1
-val_443	1
-val_444	1
-val_446	1
-val_448	1
-val_192	1
-val_190	1
-val_194	1
-val_449	1
-val_196	1
-val_452	1
-val_19	1
-val_2	1
-val_20	1
-val_453	1
-val_201	1
-val_202	1
-val_136	1
-val_455	1
-val_457	1
-val_189	1
-val_78	1
-val_8	1
-val_214	1
-val_460	1
-val_80	1
-val_218	1
-val_82	1
-val_133	1
-val_222	1
-val_467	1
-val_131	1
-val_226	1
-val_228	1
-val_126	1
-val_186	1
-val_47	1
-val_235	1
-val_470	1
-val_472	1
-val_475	1
-val_477	1
-val_241	1
-val_92	1
-val_244	1
-val_247	1
-val_248	1
-val_249	1
-val_252	1
-val_479	1
-val_116	1
-val_257	1
-val_258	1
-val_481	1
-val_260	1
-val_262	1
-val_263	1
-val_482	1
-val_266	1
-val_27	1
-val_483	1
-val_183	1
-val_274	1
-val_275	1
-val_181	1
-val_484	1
-val_28	1
-val_485	1
-val_487	1
-val_114	1
-val_283	1
-val_284	1
-val_285	1
-val_286	1
 val_287	1
-val_84	2
-val_95	2
-val_97	2
-val_98	2
-val_100	2
-val_103	2
-val_104	2
-val_113	2
-val_118	2
-val_12	2
-val_120	2
-val_125	2
-val_129	2
-val_134	2
-val_137	2
-val_146	2
-val_149	2
-val_15	2
-val_152	2
-val_164	2
-val_165	2
-val_172	2
-val_174	2
-val_175	2
-val_176	2
-val_179	2
-val_18	2
-val_191	2
-val_195	2
-val_197	2
-val_200	2
-val_203	2
-val_205	2
-val_207	2
-val_209	2
-val_213	2
-val_216	2
-val_217	2
-val_219	2
-val_221	2
-val_223	2
-val_224	2
-val_229	2
-val_233	2
-val_237	2
-val_238	2
-val_239	2
-val_24	2
-val_242	2
-val_255	2
-val_256	2
-val_26	2
-val_265	2
-val_272	2
-val_278	2
-val_280	2
-val_281	2
-val_282	2
-val_288	2
-val_307	2
-val_309	2
-val_317	2
-val_321	2
-val_322	2
-val_325	2
-val_331	2
-val_333	2
-val_342	2
-val_344	2
-val_353	2
-val_367	2
-val_37	2
-val_382	2
-val_395	2
-val_397	2
-val_399	2
-val_404	2
-val_413	2
-val_414	2
-val_42	2
-val_424	2
-val_429	2
-val_439	2
-val_458	2
-val_459	2
-val_462	2
-val_463	2
-val_478	2
-val_492	2
-val_51	2
-val_58	2
-val_67	2
-val_72	2
-val_76	2
+val_286	1
+val_285	1
+val_284	1
+val_283	1
+val_114	1
+val_487	1
+val_485	1
+val_28	1
+val_484	1
+val_181	1
+val_275	1
+val_274	1
+val_183	1
+val_483	1
+val_27	1
+val_266	1
+val_482	1
+val_263	1
+val_262	1
+val_260	1
+val_481	1
+val_258	1
+val_257	1
+val_116	1
+val_479	1
+val_252	1
+val_249	1
+val_248	1
+val_247	1
+val_244	1
+val_92	1
+val_241	1
+val_477	1
+val_475	1
+val_472	1
+val_470	1
+val_235	1
+val_47	1
+val_186	1
+val_126	1
+val_228	1
+val_226	1
+val_131	1
+val_467	1
+val_222	1
+val_133	1
+val_82	1
+val_218	1
+val_80	1
+val_460	1
+val_214	1
+val_8	1
+val_78	1
+val_189	1
+val_457	1
+val_455	1
+val_136	1
+val_202	1
+val_201	1
+val_453	1
+val_20	1
+val_2	1
+val_19	1
+val_452	1
+val_196	1
+val_449	1
+val_194	1
+val_190	1
+val_192	1
+val_448	1
+val_446	1
+val_444	1
+val_443	1
+val_44	1
+val_77	1
+val_143	1
+val_437	1
+val_436	1
+val_435	1
+val_432	1
+val_145	1
+val_150	1
+val_43	1
+val_10	1
+val_427	1
+val_74	1
+val_421	1
+val_9	1
+val_419	1
+val_418	1
+val_153	1
+val_105	1
+val_69	1
+val_411	1
+val_41	1
+val_155	1
+val_407	1
+val_156	1
+val_87	1
+val_157	1
+val_402	1
+val_158	1
+val_400	1
+val_4	1
+val_66	1
+val_65	1
+val_160	1
+val_64	1
+val_394	1
+val_393	1
+val_392	1
+val_389	1
+val_386	1
+val_162	1
+val_86	1
+val_379	1
+val_378	1
+val_377	1
+val_375	1
+val_374	1
+val_373	1
+val_57	1
+val_163	1
+val_368	1
+val_54	1
+val_366	1
+val_365	1
+val_364	1
+val_362	1
+val_360	1
+val_356	1
+val_53	1
+val_351	1
+val_166	1
+val_168	1
+val_345	1
+val_85	1
+val_11	1
+val_341	1
+val_34	1
+val_339	1
+val_338	1
+val_336	1
+val_335	1
+val_111	1
+val_332	1
+val_497	1
+val_33	1
+val_17	1
+val_496	1
+val_323	1
+val_495	1
+val_494	1
+val_170	1
+val_493	1
+val_177	1
+val_315	1
+val_178	1
+val_310	1
+val_96	1
+val_308	1
+val_491	1
+val_306	1
+val_305	1
+val_302	1
+val_30	1
+val_180	1
+val_296	1
+val_292	1
+val_291	1
+val_289	1
+val_98	2
+val_97	2
+val_95	2
+val_84	2
 val_83	2
-val_396	3
-val_384	3
-val_369	3
+val_76	2
+val_72	2
+val_67	2
+val_58	2
+val_51	2
+val_492	2
+val_478	2
+val_463	2
+val_462	2
+val_459	2
+val_458	2
+val_439	2
+val_429	2
+val_424	2
+val_42	2
+val_414	2
+val_413	2
+val_404	2
+val_399	2
+val_397	2
+val_395	2
+val_382	2
+val_37	2
+val_367	2
+val_353	2
+val_344	2
+val_342	2
+val_333	2
+val_331	2
+val_325	2
+val_322	2
+val_321	2
+val_317	2
+val_309	2
+val_307	2
+val_288	2
+val_282	2
+val_281	2
+val_280	2
+val_278	2
+val_272	2
+val_265	2
+val_26	2
+val_256	2
+val_255	2
+val_242	2
+val_24	2
+val_239	2
+val_238	2
+val_237	2
+val_233	2
+val_229	2
+val_224	2
+val_223	2
+val_221	2
+val_219	2
+val_217	2
+val_216	2
+val_213	2
+val_209	2
+val_207	2
+val_205	2
+val_203	2
+val_200	2
+val_197	2
+val_195	2
+val_191	2
+val_18	2
+val_179	2
+val_176	2
+val_175	2
+val_174	2
+val_172	2
+val_165	2
+val_164	2
+val_152	2
+val_15	2
+val_149	2
+val_146	2
+val_137	2
+val_134	2
+val_129	2
+val_125	2
+val_120	2
+val_12	2
+val_118	2
+val_113	2
+val_104	2
+val_103	2
+val_100	2
 val_498	3
-val_35	3
-val_167	3
-val_327	3
-val_318	3
-val_128	3
-val_90	3
-val_466	3
-val_316	3
-val_311	3
-val_454	3
-val_298	3
-val_273	3
-val_187	3
-val_208	3
-val_199	3
-val_193	3
-val_480	3
-val_438	3
-val_431	3
-val_0	3
-val_119	3
-val_70	3
-val_430	3
-val_5	3
-val_417	3
-val_409	3
+val_369	3
+val_384	3
+val_396	3
 val_403	3
-val_406	4
-val_489	4
+val_409	3
+val_417	3
+val_5	3
+val_430	3
+val_70	3
+val_119	3
+val_0	3
+val_431	3
+val_438	3
+val_480	3
+val_193	3
+val_199	3
+val_208	3
+val_187	3
+val_273	3
+val_298	3
+val_454	3
+val_311	3
+val_316	3
+val_466	3
+val_90	3
+val_128	3
+val_318	3
+val_327	3
+val_167	3
+val_35	3
 val_468	4
-val_277	4
-val_138	4
+val_489	4
+val_406	4
 val_169	4
-val_348	5
-val_230	5
-val_401	5
+val_138	4
+val_277	4
 val_469	5
+val_401	5
+val_230	5
+val_348	5
 PREHOOK: query: -- multi insert
 CREATE TABLE even (c int, d string)
 PREHOOK: type: CREATETABLE
diff --git a/ql/src/test/results/clientpositive/tez/update_all_partitioned.q.out b/ql/src/test/results/clientpositive/tez/update_all_partitioned.q.out
index 286e2ed527..533dd88aaa 100644
--- a/ql/src/test/results/clientpositive/tez/update_all_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/tez/update_all_partitioned.q.out
@@ -85,15 +85,21 @@ POSTHOOK: Input: default@acid_uap@ds=today
 POSTHOOK: Input: default@acid_uap@ds=tomorrow
 #### A masked pattern was here ####
 -1073279343	fred	today
+-1073279343	oj1YrV5Wa	today
 -1073051226	fred	today
 -1072910839	fred	today
+-1072081801	dPkN74F7	today
 -1072081801	fred	today
 -1072076362	fred	today
 -1071480828	fred	today
+-1071363017	Anj0oF	today
 -1071363017	fred	today
+-1070883071	0ruyd6Y50JpdGRf6HqD	today
 -1070883071	fred	today
 -1070551679	fred	today
+-1070551679	iUR3Q	today
 -1069736047	fred	today
+-1069736047	k17Am8uPHWk02cEf1jet	today
 762	fred	tomorrow
 762	fred	tomorrow
 762	fred	tomorrow
diff --git a/ql/src/test/results/clientpositive/union10.q.out b/ql/src/test/results/clientpositive/union10.q.out
index 961be518df..d261a75750 100644
--- a/ql/src/test/results/clientpositive/union10.q.out
+++ b/ql/src/test/results/clientpositive/union10.q.out
@@ -83,10 +83,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -98,10 +98,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -113,10 +113,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/union12.q.out b/ql/src/test/results/clientpositive/union12.q.out
index e63453985e..4df390e3b1 100644
--- a/ql/src/test/results/clientpositive/union12.q.out
+++ b/ql/src/test/results/clientpositive/union12.q.out
@@ -83,10 +83,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -98,10 +98,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -113,10 +113,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 288 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 3 Data size: 276 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/union4.q.out b/ql/src/test/results/clientpositive/union4.q.out
index bf9bcdcfbe..90daaad2cb 100644
--- a/ql/src/test/results/clientpositive/union4.q.out
+++ b/ql/src/test/results/clientpositive/union4.q.out
@@ -80,10 +80,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 2 Data size: 192 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 2 Data size: 192 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 2 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -95,10 +95,10 @@ STAGE PLANS:
               Select Operator
                 expressions: _col0 (type: string), UDFToInteger(_col1) (type: int)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 2 Data size: 192 Basic stats: COMPLETE Column stats: COMPLETE
+                Statistics: Num rows: 2 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 2 Data size: 192 Basic stats: COMPLETE Column stats: COMPLETE
+                  Statistics: Num rows: 2 Data size: 184 Basic stats: COMPLETE Column stats: COMPLETE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/union_remove_17.q.out b/ql/src/test/results/clientpositive/union_remove_17.q.out
index 593ed06369..5b466c6444 100644
--- a/ql/src/test/results/clientpositive/union_remove_17.q.out
+++ b/ql/src/test/results/clientpositive/union_remove_17.q.out
@@ -81,12 +81,14 @@ STAGE PLANS:
                   expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint), _col2 (type: string)
                   outputColumnNames: _col0, _col1, _col2
                   Statistics: Num rows: 0 Data size: 60 Basic stats: PARTIAL Column stats: NONE
-                  Reduce Output Operator
-                    key expressions: _col2 (type: string)
-                    sort order: +
-                    Map-reduce partition columns: _col2 (type: string)
+                  File Output Operator
+                    compressed: false
                     Statistics: Num rows: 0 Data size: 60 Basic stats: PARTIAL Column stats: NONE
-                    value expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string)
+                    table:
+                        input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                        name: default.outputtbl1
           TableScan
             alias: inputtbl1
             Statistics: Num rows: 0 Data size: 30 Basic stats: PARTIAL Column stats: NONE
@@ -100,23 +102,14 @@ STAGE PLANS:
                   expressions: _col0 (type: string), UDFToLong(_col1) (type: bigint), _col2 (type: string)
                   outputColumnNames: _col0, _col1, _col2
                   Statistics: Num rows: 0 Data size: 60 Basic stats: PARTIAL Column stats: NONE
-                  Reduce Output Operator
-                    key expressions: _col2 (type: string)
-                    sort order: +
-                    Map-reduce partition columns: _col2 (type: string)
+                  File Output Operator
+                    compressed: false
                     Statistics: Num rows: 0 Data size: 60 Basic stats: PARTIAL Column stats: NONE
-                    value expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string)
-      Reduce Operator Tree:
-        Extract
-          Statistics: Num rows: 0 Data size: 60 Basic stats: PARTIAL Column stats: NONE
-          File Output Operator
-            compressed: false
-            Statistics: Num rows: 0 Data size: 60 Basic stats: PARTIAL Column stats: NONE
-            table:
-                input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
-                output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
-                serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
-                name: default.outputtbl1
+                    table:
+                        input format: org.apache.hadoop.hive.ql.io.RCFileInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.RCFileOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe
+                        name: default.outputtbl1
 
   Stage: Stage-0
     Move Operator
diff --git a/ql/src/test/results/clientpositive/update_all_partitioned.q.out b/ql/src/test/results/clientpositive/update_all_partitioned.q.out
index 286e2ed527..533dd88aaa 100644
--- a/ql/src/test/results/clientpositive/update_all_partitioned.q.out
+++ b/ql/src/test/results/clientpositive/update_all_partitioned.q.out
@@ -85,15 +85,21 @@ POSTHOOK: Input: default@acid_uap@ds=today
 POSTHOOK: Input: default@acid_uap@ds=tomorrow
 #### A masked pattern was here ####
 -1073279343	fred	today
+-1073279343	oj1YrV5Wa	today
 -1073051226	fred	today
 -1072910839	fred	today
+-1072081801	dPkN74F7	today
 -1072081801	fred	today
 -1072076362	fred	today
 -1071480828	fred	today
+-1071363017	Anj0oF	today
 -1071363017	fred	today
+-1070883071	0ruyd6Y50JpdGRf6HqD	today
 -1070883071	fred	today
 -1070551679	fred	today
+-1070551679	iUR3Q	today
 -1069736047	fred	today
+-1069736047	k17Am8uPHWk02cEf1jet	today
 762	fred	tomorrow
 762	fred	tomorrow
 762	fred	tomorrow
