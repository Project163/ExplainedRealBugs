diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
index f6121193ac..0db2a71243 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java
@@ -116,26 +116,6 @@ public boolean requireLock() {
     return true;
   }
 
-  public static String getResourceFiles(Configuration conf, SessionState.ResourceType t) {
-    // fill in local files to be added to the task environment
-    SessionState ss = SessionState.get();
-    Set<String> files = (ss == null) ? null : ss.list_resource(t, null);
-    if (files != null) {
-      List<String> realFiles = new ArrayList<String>(files.size());
-      for (String one : files) {
-        try {
-          realFiles.add(Utilities.realFile(one, conf));
-        } catch (IOException e) {
-          throw new RuntimeException("Cannot validate file " + one + "due to exception: "
-              + e.getMessage(), e);
-        }
-      }
-      return StringUtils.join(realFiles, ",");
-    } else {
-      return "";
-    }
-  }
-
   private void initializeFiles(String prop, String files) {
     if (files != null && files.length() > 0) {
       job.set(prop, files);
@@ -159,15 +139,15 @@ public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverC
     //
     // "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
     // which will be called by both local and NON-local mode.
-    String addedFiles = getResourceFiles(job, SessionState.ResourceType.FILE);
+    String addedFiles = Utilities.getResourceFiles(job, SessionState.ResourceType.FILE);
     if (StringUtils.isNotBlank(addedFiles)) {
       HiveConf.setVar(job, ConfVars.HIVEADDEDFILES, addedFiles);
     }
-    String addedJars = getResourceFiles(job, SessionState.ResourceType.JAR);
+    String addedJars = Utilities.getResourceFiles(job, SessionState.ResourceType.JAR);
     if (StringUtils.isNotBlank(addedJars)) {
       HiveConf.setVar(job, ConfVars.HIVEADDEDJARS, addedJars);
     }
-    String addedArchives = getResourceFiles(job, SessionState.ResourceType.ARCHIVE);
+    String addedArchives = Utilities.getResourceFiles(job, SessionState.ResourceType.ARCHIVE);
     if (StringUtils.isNotBlank(addedArchives)) {
       HiveConf.setVar(job, ConfVars.HIVEADDEDARCHIVES, addedArchives);
     }
@@ -777,7 +757,7 @@ public boolean isMapRedTask() {
   public Collection<Operator<? extends Serializable>> getTopOperators() {
     return getWork().getAliasToWork().values();
   }
-  
+
   @Override
   public boolean hasReduce() {
     MapredWork w = getWork();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
index a3e40f77cc..78306d2279 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapRedTask.java
@@ -147,7 +147,7 @@ public int execute(DriverContext driverContext) {
       String hiveJar = conf.getJar();
 
       String libJarsOption;
-      String addedJars = getResourceFiles(conf, SessionState.ResourceType.JAR);
+      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
       conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);
       String auxJars = conf.getAuxJars();
       // Put auxjars and addedjars together into libjars
@@ -188,7 +188,7 @@ public int execute(DriverContext driverContext) {
           + planPath.toString() + " " + isSilent + " " + hiveConfArgs;
 
       String workDir = (new File(".")).getCanonicalPath();
-      String files = getResourceFiles(conf, SessionState.ResourceType.FILE);
+      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
       if (!files.isEmpty()) {
         cmdLine = cmdLine + " -files " + files;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
index d2b12a754f..2a88133763 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapredLocalTask.java
@@ -134,7 +134,7 @@ public int execute(DriverContext driverContext) {
           + " " + isSilent + " " + hiveConfArgs;
 
       String workDir = (new File(".")).getCanonicalPath();
-      String files = ExecDriver.getResourceFiles(conf, SessionState.ResourceType.FILE);
+      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
 
       if (!files.isEmpty()) {
         cmdLine = cmdLine + " -files " + files;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 7f0d3de40d..94924e82fa 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -144,6 +144,7 @@
 import org.apache.hadoop.mapred.SequenceFileInputFormat;
 import org.apache.hadoop.mapred.SequenceFileOutputFormat;
 import org.apache.hadoop.util.ReflectionUtils;
+import org.apache.hadoop.hive.ql.session.SessionState;
 
 /**
  * Utilities.
@@ -403,8 +404,11 @@ public static ExprNodeDesc deserializeExpression(String s, Configuration conf) {
     } catch (UnsupportedEncodingException ex) {
       throw new RuntimeException("UTF-8 support required", ex);
     }
+
     ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
-    XMLDecoder decoder = new XMLDecoder(bais, null, null, conf.getClassLoader());
+
+    XMLDecoder decoder = new XMLDecoder(bais, null, null,
+                                       addResourceFilesToClassPath(conf));
     try {
       ExprNodeDesc expr = (ExprNodeDesc) decoder.readObject();
       return expr;
@@ -477,7 +481,8 @@ public void exceptionThrown(Exception e) {
   public static QueryPlan deserializeQueryPlan(InputStream in, Configuration conf) {
     XMLDecoder d = null;
     try {
-      d = new XMLDecoder(in, null, null, conf.getClassLoader());
+      d = new XMLDecoder(in, null, null,
+                         addResourceFilesToClassPath(conf));
       QueryPlan ret = (QueryPlan) d.readObject();
       return (ret);
     } finally {
@@ -510,7 +515,8 @@ public static void serializeMapRedWork(MapredWork w, OutputStream out) {
   public static MapredWork deserializeMapRedWork(InputStream in, Configuration conf) {
     XMLDecoder d = null;
     try {
-      d = new XMLDecoder(in, null, null, conf.getClassLoader());
+      d = new XMLDecoder(in, null, null,
+                         addResourceFilesToClassPath(conf));
       MapredWork ret = (MapredWork) d.readObject();
       return (ret);
     } finally {
@@ -542,7 +548,8 @@ public static void serializeMapRedLocalWork(MapredLocalWork w, OutputStream out)
   public static MapredLocalWork deserializeMapRedLocalWork(InputStream in, Configuration conf) {
     XMLDecoder d = null;
     try {
-      d = new XMLDecoder(in, null, null, conf.getClassLoader());
+      d = new XMLDecoder(in, null, null,
+                         addResourceFilesToClassPath(conf));
       MapredLocalWork ret = (MapredLocalWork) d.readObject();
       return (ret);
     } finally {
@@ -1446,6 +1453,26 @@ public static String getNameMessage(Exception e) {
     return e.getClass().getName() + "(" + e.getMessage() + ")";
   }
 
+  public static String getResourceFiles(Configuration conf, SessionState.ResourceType t) {
+    // fill in local files to be added to the task environment
+    SessionState ss = SessionState.get();
+    Set<String> files = (ss == null) ? null : ss.list_resource(t, null);
+    if (files != null) {
+      List<String> realFiles = new ArrayList<String>(files.size());
+      for (String one : files) {
+        try {
+          realFiles.add(realFile(one, conf));
+        } catch (IOException e) {
+          throw new RuntimeException("Cannot validate file " + one + "due to exception: "
+              + e.getMessage(), e);
+        }
+      }
+      return StringUtils.join(realFiles, ",");
+    } else {
+      return "";
+    }
+  }
+
   /**
    * Add new elements to the classpath.
    *
@@ -1478,6 +1505,18 @@ public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths)
     return new URLClassLoader(curPath.toArray(new URL[0]), loader);
   }
 
+  public static ClassLoader addResourceFilesToClassPath(Configuration conf) {
+    try {
+      String addedJars = getResourceFiles(conf, SessionState.ResourceType.JAR);
+      if (!StringUtils.isNotBlank(addedJars)) {
+        return conf.getClassLoader();
+      }
+      return addToClassPath(conf.getClassLoader(), StringUtils.split(addedJars, ','));
+    } catch (Exception e) {
+      throw new RuntimeException("Error in adding jars to conf ", e);
+    }
+  }
+
   /**
    * remove elements from the classpath.
    *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java
index 6b177625cc..e28945fff5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java
@@ -192,8 +192,8 @@ public int execute(DriverContext driverContext) {
         HiveConf.setVar(job, HiveConf.ConfVars.METASTOREPWD, "HIVE");
       }
       JobClient jc = new JobClient(job);
-      
-      String addedJars = ExecDriver.getResourceFiles(job, SessionState.ResourceType.JAR);
+
+      String addedJars = Utilities.getResourceFiles(job, SessionState.ResourceType.JAR);
       if (!addedJars.isEmpty()) {
         job.set("tmpjars", addedJars);
       }
