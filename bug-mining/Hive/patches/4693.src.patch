diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index d75ab40ebf..ad467c5096 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -936,7 +936,8 @@ public static enum ConfVars {
 
     // reloadable jars
     HIVERELOADABLEJARS("hive.reloadable.aux.jars.path", "",
-        "Jars can be renewed by executing reload command. And these jars can be "
+        "The locations of the plugin jars, which can be a comma-separated folders or jars. Jars can be renewed\n"
+        + "by executing reload command. And these jars can be "
             + "used as the auxiliary classes like creating a UDF or SerDe."),
 
     // hive added files and jars
diff --git a/common/src/java/org/apache/hive/common/util/HiveStringUtils.java b/common/src/java/org/apache/hive/common/util/HiveStringUtils.java
index c2ff6357e8..72c3fa9140 100644
--- a/common/src/java/org/apache/hive/common/util/HiveStringUtils.java
+++ b/common/src/java/org/apache/hive/common/util/HiveStringUtils.java
@@ -43,6 +43,7 @@
 import com.google.common.collect.Interner;
 import com.google.common.collect.Interners;
 
+import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang3.text.translate.CharSequenceTranslator;
 import org.apache.commons.lang3.text.translate.EntityArrays;
 import org.apache.commons.lang3.text.translate.LookupTranslator;
@@ -900,6 +901,24 @@ public static String join(CharSequence separator, Iterable<?> strings) {
     return sb.toString();
   }
 
+  /**
+   * Concatenates strings, using a separator. Empty/blank string or null will be
+   * ignored.
+   *
+   * @param strings Strings to join.
+   * @param separator Separator to join with.
+   */
+  public static String joinIgnoringEmpty(String[] strings, char separator) {
+    ArrayList<String> list = new ArrayList<String>();
+    for(String str : strings) {
+      if (StringUtils.isNotBlank(str)) {
+        list.add(str);
+      }
+    }
+
+    return StringUtils.join(list, separator);
+  }
+
   /**
    * Convert SOME_STUFF to SomeStuff
    *
@@ -911,7 +930,7 @@ public static String camelize(String s) {
     String[] words = split(s.toLowerCase(Locale.US), ESCAPE_CHAR, '_');
 
     for (String word : words) {
-      sb.append(org.apache.commons.lang.StringUtils.capitalize(word));
+      sb.append(StringUtils.capitalize(word));
     }
 
     return sb.toString();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 3fab298896..12a929a93e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -36,6 +36,7 @@
 import java.io.OutputStream;
 import java.io.Serializable;
 import java.net.URI;
+import java.net.URISyntaxException;
 import java.net.URL;
 import java.net.URLClassLoader;
 import java.net.URLDecoder;
@@ -85,6 +86,7 @@
 import org.apache.hadoop.fs.ContentSummary;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.GlobFilter;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.fs.permission.FsPermission;
@@ -1735,31 +1737,58 @@ private static URL urlFromPathString(String onestr) {
     return oneurl;
   }
 
+  /**
+   * Get the URI of the path. Assume to be local file system if no scheme.
+   */
+  public static URI getURI(String path) throws URISyntaxException {
+    if (path == null) {
+      return null;
+    }
+
+    URI uri = new URI(path);
+    if (uri.getScheme() == null) {
+      // if no scheme in the path, we assume it's file on local fs.
+      uri = new File(path).toURI();
+    }
+
+    return uri;
+  }
+
     /**
-     * get the jar files from specified directory or get jar files by several jar names sperated by comma
-     * @param path
-     * @return
+     * Given a path string, get all the jars from the folder or the files themselves.
+     *
+     * @param pathString  the path string is the comma-separated path list
+     * @return            the list of the file names in the format of URI formats.
      */
-    public static Set<String> getJarFilesByPath(String path){
-        Set<String> result = new HashSet<String>();
-        if (path == null || path.isEmpty()) {
-            return result;
-        }
+    public static Set<String> getJarFilesByPath(String pathString, Configuration conf) {
+      Set<String> result = new HashSet<String>();
+      if (pathString == null || StringUtils.isBlank(pathString)) {
+          return result;
+      }
 
-        File paths = new File(path);
-        if (paths.exists() && paths.isDirectory()) {
-            // add all jar files under the reloadable auxiliary jar paths
-            Set<File> jarFiles = new HashSet<File>();
-            jarFiles.addAll(org.apache.commons.io.FileUtils.listFiles(
-                    paths, new String[]{"jar"}, true));
-            for (File f : jarFiles) {
-                result.add(f.getAbsolutePath());
+      String[] paths = pathString.split(",");
+      for(String path : paths) {
+        try {
+          Path p = new Path(getURI(path));
+          FileSystem fs = p.getFileSystem(conf);
+          if (!fs.exists(p)) {
+            LOG.error("The jar file path " + path + " doesn't exist");
+            continue;
+          }
+          if (fs.isDirectory(p)) {
+            // add all jar files under the folder
+            FileStatus[] files = fs.listStatus(p, new GlobFilter("*.jar"));
+            for(FileStatus file : files) {
+              result.add(file.getPath().toUri().toString());
             }
-        } else {
-            String[] files = path.split(",");
-            Collections.addAll(result, files);
+          } else {
+            result.add(p.toUri().toString());
+          }
+        } catch(URISyntaxException | IOException e) {
+          LOG.error("Invalid file path " + path, e);
         }
-        return result;
+      }
+      return result;
     }
 
   private static boolean useExistingClassLoader(ClassLoader cl) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
index 8a6499b3a4..4a642db8a5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
@@ -91,6 +91,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.logging.log4j.Level;
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.core.Appender;
@@ -139,6 +140,26 @@ private void initializeFiles(String prop, String files) {
     }
   }
 
+  /**
+   * Retrieve the resources from the current session and configuration for the given type.
+   * @return Comma-separated list of resources
+   */
+  protected static String getResource(HiveConf conf, SessionState.ResourceType resType) {
+    switch(resType) {
+    case JAR:
+      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
+      String auxJars = conf.getAuxJars();
+      String reloadableAuxJars = SessionState.get() == null ? null : SessionState.get().getReloadableAuxJars();
+      return HiveStringUtils.joinIgnoringEmpty(new String[]{addedJars, auxJars, reloadableAuxJars}, ',');
+    case FILE:
+      return Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
+    case ARCHIVE:
+      return Utilities.getResourceFiles(conf, SessionState.ResourceType.ARCHIVE);
+    }
+
+    return null;
+  }
+
   /**
    * Initialization when invoked from QL.
    */
@@ -149,25 +170,10 @@ public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext
 
     job = new JobConf(conf, ExecDriver.class);
 
-    // NOTE: initialize is only called if it is in non-local mode.
-    // In case it's in non-local mode, we need to move the SessionState files
-    // and jars to jobConf.
-    // In case it's in local mode, MapRedTask will set the jobConf.
-    //
-    // "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
-    // which will be called by both local and NON-local mode.
-    String addedFiles = Utilities.getResourceFiles(job, SessionState.ResourceType.FILE);
-    if (StringUtils.isNotBlank(addedFiles)) {
-      HiveConf.setVar(job, ConfVars.HIVEADDEDFILES, addedFiles);
-    }
-    String addedJars = Utilities.getResourceFiles(job, SessionState.ResourceType.JAR);
-    if (StringUtils.isNotBlank(addedJars)) {
-      HiveConf.setVar(job, ConfVars.HIVEADDEDJARS, addedJars);
-    }
-    String addedArchives = Utilities.getResourceFiles(job, SessionState.ResourceType.ARCHIVE);
-    if (StringUtils.isNotBlank(addedArchives)) {
-      HiveConf.setVar(job, ConfVars.HIVEADDEDARCHIVES, addedArchives);
-    }
+    initializeFiles("tmpjars", getResource(conf, SessionState.ResourceType.JAR));
+    initializeFiles("tmpfiles", getResource(conf, SessionState.ResourceType.FILE));
+    initializeFiles("tmparchives", getResource(conf, SessionState.ResourceType.ARCHIVE));
+
     conf.stripHiddenConfigurations(job);
     this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);
   }
@@ -296,29 +302,10 @@ public int execute(DriverContext driverContext) {
       throw new RuntimeException(e.getMessage(), e);
     }
 
-
     // No-Op - we don't really write anything here ..
     job.setOutputKeyClass(Text.class);
     job.setOutputValueClass(Text.class);
 
-    // Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands
-    // it
-    String auxJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEAUXJARS);
-    String addedJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDJARS);
-    if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
-      String allJars = StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars
-          + "," + auxJars
-          : auxJars)
-          : addedJars;
-      LOG.info("adding libjars: " + allJars);
-      initializeFiles("tmpjars", allJars);
-    }
-
-    // Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
-    String addedFiles = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDFILES);
-    if (StringUtils.isNotBlank(addedFiles)) {
-      initializeFiles("tmpfiles", addedFiles);
-    }
     int returnVal = 0;
     boolean noName = StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));
 
@@ -326,11 +313,6 @@ public int execute(DriverContext driverContext) {
       // This is for a special case to ensure unit tests pass
       job.set(MRJobConfig.JOB_NAME, "JOB" + Utilities.randGen.nextInt());
     }
-    String addedArchives = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDARCHIVES);
-    // Transfer HIVEADDEDARCHIVES to "tmparchives" so hadoop understands it
-    if (StringUtils.isNotBlank(addedArchives)) {
-      initializeFiles("tmparchives", addedArchives);
-    }
 
     try{
       MapredLocalWork localwork = mWork.getMapRedLocalWork();
@@ -634,6 +616,7 @@ public static void main(String[] args) throws IOException, HiveException {
     String jobConfFileName = null;
     boolean noLog = false;
     String files = null;
+    String libjars = null;
     boolean localtask = false;
     try {
       for (int i = 0; i < args.length; i++) {
@@ -645,7 +628,9 @@ public static void main(String[] args) throws IOException, HiveException {
           noLog = true;
         } else if (args[i].equals("-files")) {
           files = args[++i];
-        } else if (args[i].equals("-localtask")) {
+        } else if (args[i].equals("-libjars")) {
+          libjars = args[++i];
+        }else if (args[i].equals("-localtask")) {
           localtask = true;
         }
       }
@@ -665,10 +650,15 @@ public static void main(String[] args) throws IOException, HiveException {
       conf.addResource(new Path(jobConfFileName));
     }
 
+    // Initialize the resources from command line
     if (files != null) {
       conf.set("tmpfiles", files);
     }
 
+    if (libjars != null) {
+      conf.set("tmpjars", libjars);
+    }
+
     if(UserGroupInformation.isSecurityEnabled()){
       String hadoopAuthToken = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);
       if(hadoopAuthToken != null){
@@ -721,17 +711,11 @@ public static void main(String[] args) throws IOException, HiveException {
 
     // this is workaround for hadoop-17 - libjars are not added to classpath of the
     // child process. so we add it here explicitly
-
-    String auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);
-    String addedJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEADDEDJARS);
     try {
       // see also - code in CliDriver.java
       ClassLoader loader = conf.getClassLoader();
-      if (StringUtils.isNotBlank(auxJars)) {
-        loader = Utilities.addToClassPath(loader, StringUtils.split(auxJars, ","));
-      }
-      if (StringUtils.isNotBlank(addedJars)) {
-        loader = Utilities.addToClassPath(loader, StringUtils.split(addedJars, ","));
+      if (StringUtils.isNotBlank(libjars)) {
+        loader = Utilities.addToClassPath(loader, StringUtils.split(libjars, ","));
       }
       conf.setClassLoader(loader);
       // Also set this to the Thread ContextClassLoader, so new threads will
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
index a42c2e99fe..ce1106d91d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
@@ -45,8 +45,11 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.ResourceType;
 import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.hive.common.util.StreamPrinter;
+
 /**
  * Extension of ExecDriver:
  * - can optionally spawn a map-reduce task from a separate jvm
@@ -148,24 +151,8 @@ public int execute(DriverContext driverContext) {
       String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);
       String hiveJar = conf.getJar();
 
-      String libJarsOption;
-      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
-      conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);
-      String auxJars = conf.getAuxJars();
-      // Put auxjars and addedjars together into libjars
-      if (StringUtils.isEmpty(addedJars)) {
-        if (StringUtils.isEmpty(auxJars)) {
-          libJarsOption = " ";
-        } else {
-          libJarsOption = " -libjars " + auxJars + " ";
-        }
-      } else {
-        if (StringUtils.isEmpty(auxJars)) {
-          libJarsOption = " -libjars " + addedJars + " ";
-        } else {
-          libJarsOption = " -libjars " + addedJars + "," + auxJars + " ";
-        }
-      }
+      String libJars = super.getResource(conf, ResourceType.JAR);
+      String libJarsOption = StringUtils.isEmpty(libJars) ? " " : " -libjars " + libJars + " ";
 
       // Generate the hiveConfArgs after potentially adding the jars
       String hiveConfArgs = generateCmdLine(conf, ctx);
@@ -194,7 +181,8 @@ public int execute(DriverContext driverContext) {
           + planPath.toString() + " " + isSilent + " " + hiveConfArgs;
 
       String workDir = (new File(".")).getCanonicalPath();
-      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
+
+      String files = super.getResource(conf, ResourceType.FILE);
       if (!files.isEmpty()) {
         cmdLine = cmdLine + " -files " + files;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 96c826b6fb..d4051a1827 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -1109,7 +1109,7 @@ public void reloadAuxJars() throws IOException {
       return;
     }
 
-    Set<String> jarPaths = Utilities.getJarFilesByPath(renewableJarPath);
+    Set<String> jarPaths = Utilities.getJarFilesByPath(renewableJarPath, sessionConf);
 
     // load jars under the hive.reloadable.aux.jars.path
     if(!jarPaths.isEmpty()){
@@ -1659,6 +1659,14 @@ public void setForwardedAddresses(List<String> forwardedAddresses) {
   public List<String> getForwardedAddresses() {
     return forwardedAddresses;
   }
+
+  /**
+   * Gets the comma-separated reloadable aux jars
+   * @return the list of reloadable aux jars
+   */
+  public String getReloadableAuxJars() {
+    return StringUtils.join(preReloadableAuxJars, ',');
+  }
 }
 
 class ResourceMaps {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
index cc59f130fc..3ce4723104 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
@@ -24,8 +24,8 @@
 import java.io.IOException;
 import java.sql.Timestamp;
 import java.util.ArrayList;
-import java.util.HashSet;
 import java.util.List;
+import java.util.Set;
 
 import org.apache.commons.io.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -38,13 +38,13 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.mapred.JobConf;
+import org.junit.Assert;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import com.google.common.collect.Sets;
 import com.google.common.io.Files;
 
-import junit.framework.Assert;
 import junit.framework.TestCase;
 
 public class TestUtilities extends TestCase {
@@ -118,20 +118,23 @@ public void testgetDbTableName() throws HiveException{
   }
 
   public void testGetJarFilesByPath() {
+    HiveConf conf = new HiveConf(this.getClass());
     File f = Files.createTempDir();
     String jarFileName1 = f.getAbsolutePath() + File.separator + "a.jar";
     String jarFileName2 = f.getAbsolutePath() + File.separator + "b.jar";
     File jarFile = new File(jarFileName1);
     try {
       FileUtils.touch(jarFile);
-      HashSet<String> jars = (HashSet) Utilities.getJarFilesByPath(f.getAbsolutePath());
-      Assert.assertEquals(Sets.newHashSet(jarFile.getAbsolutePath()),jars);
+      Set<String> jars = Utilities.getJarFilesByPath(f.getAbsolutePath(), conf);
+      Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1),jars);
+
+      jars = Utilities.getJarFilesByPath("/folder/not/exist", conf);
+      Assert.assertTrue(jars.isEmpty());
 
       File jarFile2 = new File(jarFileName2);
       FileUtils.touch(jarFile2);
-      String newPath = "file://" + jarFileName1 + "," + "file://" + jarFileName2;
-      jars = (HashSet) Utilities.getJarFilesByPath(newPath);
-
+      String newPath = "file://" + jarFileName1 + "," + "file://" + jarFileName2 + ",/file/not/exist";
+      jars = Utilities.getJarFilesByPath(newPath, conf);
       Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1, "file://" + jarFileName2), jars);
     } catch (IOException e) {
       LOG.error("failed to copy file to reloading folder", e);
diff --git a/ql/src/test/queries/clientpositive/reloadJar.q b/ql/src/test/queries/clientpositive/reloadJar.q
new file mode 100644
index 0000000000..6768a4f32d
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/reloadJar.q
@@ -0,0 +1,17 @@
+dfs -mkdir  ${system:test.tmp.dir}/aux;
+dfs -cp ${system:hive.root}/data/files/identity_udf.jar ${system:test.tmp.dir}/aux/udfexample.jar;
+
+SET hive.reloadable.aux.jars.path=${system:test.tmp.dir}/aux;
+RELOAD;
+CREATE TEMPORARY FUNCTION example_iden AS 'IdentityStringUDF';
+
+EXPLAIN
+SELECT example_iden(key)
+FROM src LIMIT 1;
+
+SELECT example_iden(key)
+FROM src LIMIT 1;
+
+DROP TEMPORARY FUNCTION example_iden;
+
+dfs -rm -r ${system:test.tmp.dir}/aux;
diff --git a/ql/src/test/results/clientpositive/reloadJar.q.out b/ql/src/test/results/clientpositive/reloadJar.q.out
new file mode 100644
index 0000000000..6991660f59
--- /dev/null
+++ b/ql/src/test/results/clientpositive/reloadJar.q.out
@@ -0,0 +1,64 @@
+PREHOOK: query: CREATE TEMPORARY FUNCTION example_iden AS 'IdentityStringUDF'
+PREHOOK: type: CREATEFUNCTION
+PREHOOK: Output: example_iden
+POSTHOOK: query: CREATE TEMPORARY FUNCTION example_iden AS 'IdentityStringUDF'
+POSTHOOK: type: CREATEFUNCTION
+POSTHOOK: Output: example_iden
+PREHOOK: query: EXPLAIN
+SELECT example_iden(key)
+FROM src LIMIT 1
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT example_iden(key)
+FROM src LIMIT 1
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: src
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: example_iden(key) (type: string)
+              outputColumnNames: _col0
+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+              Limit
+                Number of rows: 1
+                Statistics: Num rows: 1 Data size: 10 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 10 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT example_iden(key)
+FROM src LIMIT 1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT example_iden(key)
+FROM src LIMIT 1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+238
+PREHOOK: query: DROP TEMPORARY FUNCTION example_iden
+PREHOOK: type: DROPFUNCTION
+PREHOOK: Output: example_iden
+POSTHOOK: query: DROP TEMPORARY FUNCTION example_iden
+POSTHOOK: type: DROPFUNCTION
+POSTHOOK: Output: example_iden
+#### A masked pattern was here ####
