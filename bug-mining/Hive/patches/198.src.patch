diff --git a/CHANGES.txt b/CHANGES.txt
index d0df932856..20a1967a6b 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -319,6 +319,9 @@ Trunk - Unreleased
     HIVE-616. Make hive work with symbolic linked sub directories
     (Zheng Shao via rmurthy)
 
+    HIVE-527. Inserting into a partitioned table without specifying the partition field should fail.
+    (He Yongqiang via namit)
+
 Release 0.3.1 - Unreleased
 
   INCOMPATIBLE CHANGES
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
index fb1b08870b..4aaa6c11e1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ErrorMsg.java
@@ -83,7 +83,8 @@ public enum ErrorMsg {
   NO_OUTER_MAPJOIN("Map Join cannot be performed with Outer join"),
   INVALID_MAPJOIN_HINT("neither table specified as map-table"),
   INVALID_MAPJOIN_TABLE("result of a union cannot be a map table"),
-  NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table");
+  NON_BUCKETED_TABLE("Sampling Expression Needed for Non-Bucketed Table"),
+  NEED_PARTITION_ERROR("need to specify partition columns because the destination table is partitioned.");
 
   private String mesg;
   ErrorMsg(String mesg) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
index 0e75f8f07c..5f74599bee 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/LoadSemanticAnalyzer.java
@@ -23,6 +23,7 @@
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.util.HashMap;
+import java.util.List;
 
 import org.antlr.runtime.tree.Tree;
 import org.apache.commons.lang.StringUtils;
@@ -30,6 +31,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -176,6 +178,12 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {
     tableSpec ts = new tableSpec(db, conf, (ASTNode) table_t);
     URI toURI = (ts.partHandle != null) ? ts.partHandle.getDataLocation() : ts.tableHandle.getDataLocation();
 
+    List<FieldSchema> parts = ts.tableHandle.getTTable().getPartitionKeys();
+    if (isOverWrite && (parts != null && parts.size() > 0)
+        && (ts.partSpec == null || ts.partSpec.size() == 0)) {
+      throw new SemanticException(ErrorMsg.NEED_PARTITION_ERROR.getMsg());
+    }
+    
     // make sure the arguments make sense
     applyConstraints(fromURI, toURI, from_t, isLocal);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 193825a497..a24183f702 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -2276,6 +2276,11 @@ private Operator genFileSinkPlan(String dest, QB qb,
     case QBMetaData.DEST_TABLE: {
 
         dest_tab = qbm.getDestTableForAlias(dest);
+        //check for partition
+        List<FieldSchema> parts = dest_tab.getTTable().getPartitionKeys();
+        if(parts != null && parts.size() > 0) {
+          throw new SemanticException(ErrorMsg.NEED_PARTITION_ERROR.getMsg());
+        }
         dest_path = dest_tab.getPath();
         queryTmpdir = ctx.getExternalTmpFileURI(dest_path.toUri());
         table_desc = Utilities.getTableDesc(dest_tab);
diff --git a/ql/src/test/queries/clientnegative/nopart_insert.q b/ql/src/test/queries/clientnegative/nopart_insert.q
new file mode 100644
index 0000000000..a8b3e0c8db
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/nopart_insert.q
@@ -0,0 +1,8 @@
+DROP TABLE nopart_insert;
+CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING);
+
+INSERT OVERWRITE TABLE nopart_insert 
+SELECT TRANSFORM(src.key, src.value) USING '../data/scripts/error_script' AS (tkey, tvalue)
+FROM src;
+
+DROP TABLE nopart_insert;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientnegative/nopart_load.q b/ql/src/test/queries/clientnegative/nopart_load.q
new file mode 100644
index 0000000000..89536277e7
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/nopart_load.q
@@ -0,0 +1,6 @@
+DROP TABLE nopart_load;
+CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING);
+
+load data local inpath '../data/files/kv1.txt' overwrite into table nopart_load ;
+
+DROP TABLE nopart_load;
\ No newline at end of file
diff --git a/ql/src/test/results/clientnegative/nopart_insert.q.out b/ql/src/test/results/clientnegative/nopart_insert.q.out
new file mode 100644
index 0000000000..9a0ae22369
--- /dev/null
+++ b/ql/src/test/results/clientnegative/nopart_insert.q.out
@@ -0,0 +1,3 @@
+query: DROP TABLE nopart_insert
+query: CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING)
+FAILED: Error in semantic analysis: need to specify partition columns because the destination table is partitioned.
diff --git a/ql/src/test/results/clientnegative/nopart_load.q.out b/ql/src/test/results/clientnegative/nopart_load.q.out
new file mode 100644
index 0000000000..fe435e856b
--- /dev/null
+++ b/ql/src/test/results/clientnegative/nopart_load.q.out
@@ -0,0 +1,3 @@
+query: DROP TABLE nopart_load
+query: CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING)
+FAILED: Error in semantic analysis: need to specify partition columns because the destination table is partitioned.
