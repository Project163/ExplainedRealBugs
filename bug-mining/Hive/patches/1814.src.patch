diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFPercentile.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFPercentile.java
index 4186a9dc6a..1757ed4409 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFPercentile.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDAFPercentile.java
@@ -30,6 +30,7 @@
 import org.apache.hadoop.hive.ql.exec.UDAF;
 import org.apache.hadoop.hive.ql.exec.UDAFEvaluator;
 import org.apache.hadoop.hive.serde2.io.DoubleWritable;
+import org.apache.hadoop.hive.shims.ShimLoader;
 import org.apache.hadoop.io.LongWritable;
 
 /**
@@ -44,6 +45,12 @@
       + "pc can be a double or double array")
 public class UDAFPercentile extends UDAF {
 
+  private static final Comparator<LongWritable> COMPARATOR;
+
+  static {
+    COMPARATOR = ShimLoader.getHadoopShims().getLongComparator();
+  }
+
   /**
    * A state class to store intermediate aggregation results.
    */
@@ -59,7 +66,7 @@ public static class MyComparator implements Comparator<Map.Entry<LongWritable, L
     @Override
     public int compare(Map.Entry<LongWritable, LongWritable> o1,
         Map.Entry<LongWritable, LongWritable> o2) {
-      return o1.getKey().compareTo(o2.getKey());
+      return COMPARATOR.compare(o1.getKey(), o2.getKey());
     }
   }
 
diff --git a/ql/src/test/queries/clientpositive/udaf_percentile.q b/ql/src/test/queries/clientpositive/udaf_percentile.q
new file mode 100644
index 0000000000..8ebf01dcec
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/udaf_percentile.q
@@ -0,0 +1 @@
+select percentile(cast(key as bigint), 0.3) from src;
diff --git a/ql/src/test/results/clientpositive/udaf_percentile.q.out b/ql/src/test/results/clientpositive/udaf_percentile.q.out
new file mode 100644
index 0000000000..1861a9f9b2
--- /dev/null
+++ b/ql/src/test/results/clientpositive/udaf_percentile.q.out
@@ -0,0 +1,9 @@
+PREHOOK: query: select percentile(cast(key as bigint), 0.3) from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: select percentile(cast(key as bigint), 0.3) from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+169.0
diff --git a/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java b/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
index 6ff1a8440b..f03790bcb7 100644
--- a/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
+++ b/shims/0.20/src/main/java/org/apache/hadoop/hive/shims/Hadoop20Shims.java
@@ -29,6 +29,7 @@
 import java.security.PrivilegedExceptionAction;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.Comparator;
 import java.util.HashSet;
 import java.util.Iterator;
 import java.util.List;
@@ -47,6 +48,7 @@
 import org.apache.hadoop.fs.Trash;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil;
+import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapred.ClusterStatus;
 import org.apache.hadoop.mapred.FileInputFormat;
@@ -160,6 +162,16 @@ public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
     TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
   }
 
+  @Override
+  public Comparator<LongWritable> getLongComparator() {
+    return new Comparator<LongWritable>() {
+      @Override
+      public int compare(LongWritable o1, LongWritable o2) {
+        return o1.compareTo(o2);
+      }
+    };
+  }
+
   public static class InputSplitShim extends CombineFileSplit implements HadoopShims.InputSplitShim {
     long shrinkedLength;
     boolean _isShrinked;
diff --git a/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java b/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
index 62f79843e1..f2f3dab625 100644
--- a/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
+++ b/shims/0.20S/src/main/java/org/apache/hadoop/hive/shims/Hadoop20SShims.java
@@ -21,6 +21,7 @@
 import java.net.InetSocketAddress;
 import java.net.MalformedURLException;
 import java.net.URL;
+import java.util.Comparator;
 import java.util.Iterator;
 import java.net.URI;
 
@@ -34,6 +35,7 @@
 import org.apache.hadoop.fs.ProxyFileSystem;
 import org.apache.hadoop.fs.Trash;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.JobTracker;
 import org.apache.hadoop.mapred.MiniMRCluster;
 import org.apache.hadoop.mapred.ClusterStatus;
@@ -150,6 +152,16 @@ public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
     TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
   }
 
+  @Override
+  public Comparator<LongWritable> getLongComparator() {
+    return new Comparator<LongWritable>() {
+      @Override
+      public int compare(LongWritable o1, LongWritable o2) {
+        return o1.compareTo(o2);
+      }
+    };
+  }
+
   /**
    * Returns a shim to wrap MiniMrCluster
    */
diff --git a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index cac1594f71..47591d2a67 100644
--- a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -22,6 +22,7 @@
 import java.net.InetSocketAddress;
 import java.net.MalformedURLException;
 import java.net.URL;
+import java.util.Comparator;
 import java.util.Iterator;
 import java.util.Map;
 import java.net.URI;
@@ -39,6 +40,7 @@
 import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.hadoop.fs.Trash;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
+import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.ClusterStatus;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.MiniMRCluster;
@@ -166,6 +168,16 @@ public void setTotalOrderPartitionFile(JobConf jobConf, Path partitionFile){
     TotalOrderPartitioner.setPartitionFile(jobConf, partitionFile);
   }
 
+  @Override
+  public Comparator<LongWritable> getLongComparator() {
+    return new Comparator<LongWritable>() {
+      @Override
+      public int compare(LongWritable o1, LongWritable o2) {
+        return o1.compareTo(o2);
+      }
+    };
+  }
+
   /**
    * Returns a shim to wrap MiniMrCluster
    */
diff --git a/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java b/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
index 0d5615cb9c..0e982ee493 100644
--- a/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
+++ b/shims/common/src/main/java/org/apache/hadoop/hive/shims/HadoopShims.java
@@ -25,6 +25,7 @@
 import java.net.URI;
 import java.net.URISyntaxException;
 import java.security.PrivilegedExceptionAction;
+import java.util.Comparator;
 import java.util.Iterator;
 import java.util.List;
 
@@ -38,17 +39,14 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.InputSplit;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.JobProfile;
 import org.apache.hadoop.mapred.JobStatus;
 import org.apache.hadoop.mapred.RecordReader;
 import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.TaskCompletionEvent;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.JobID;
@@ -354,6 +352,9 @@ public boolean moveToAppropriateTrash(FileSystem fs, Path path, Configuration co
    * @param partition
    */
   void setTotalOrderPartitionFile(JobConf jobConf, Path partition);
+
+  Comparator<LongWritable> getLongComparator();
+
   /**
    * InputSplitShim.
    *
