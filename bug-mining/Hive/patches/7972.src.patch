diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java
index 2adf43d121..f3a4773bc1 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java
@@ -31,6 +31,7 @@
 import org.apache.iceberg.PartitionKey;
 import org.apache.iceberg.PartitionSpec;
 import org.apache.iceberg.Schema;
+import org.apache.iceberg.data.InternalRecordWrapper;
 import org.apache.iceberg.data.Record;
 import org.apache.iceberg.io.FileAppenderFactory;
 import org.apache.iceberg.io.FileIO;
@@ -49,6 +50,7 @@ class HiveIcebergRecordWriter extends PartitionedFanoutWriter<Record>
   // The current key is reused at every write to avoid unnecessary object creation
   private final PartitionKey currentKey;
   private final FileIO io;
+  private final InternalRecordWrapper wrapper;
 
   // <TaskAttemptId, <TABLE_NAME, HiveIcebergRecordWriter>> map to store the active writers
   // Stored in concurrent map, since some executor engines can share containers
@@ -68,13 +70,14 @@ static Map<String, HiveIcebergRecordWriter> getWriters(TaskAttemptID taskAttempt
     super(spec, format, appenderFactory, fileFactory, io, targetFileSize);
     this.io = io;
     this.currentKey = new PartitionKey(spec, schema);
+    this.wrapper = new InternalRecordWrapper(schema.asStruct());
     writers.putIfAbsent(taskAttemptID, Maps.newConcurrentMap());
     writers.get(taskAttemptID).put(tableName, this);
   }
 
   @Override
   protected PartitionKey partition(Record row) {
-    currentKey.partition(row);
+    currentKey.partition(wrapper.wrap(row));
     return currentKey;
   }
 
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java
index 305b2575d0..23ab6669e7 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java
@@ -23,6 +23,7 @@
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;
 import org.apache.hive.iceberg.org.apache.orc.OrcConf;
 import org.apache.iceberg.FileFormat;
 import org.apache.iceberg.Schema;
@@ -103,11 +104,18 @@ static void init(TestHiveShell shell, TestTables testTables, TemporaryFolder tem
       shell.setHiveSessionValue("hive.vectorized.execution.enabled", "false");
     }
 
+    // Until HADOOP-16435 we have to manually remove the RpcMetrics for every run otherwise we might end up with OOM
+    // We have to initialize the metrics as TestMetrics, so shutdown will remove them
+    DefaultMetricsSystem.instance().init("TestMetrics");
   }
 
   static void close(TestHiveShell shell) throws Exception {
     shell.closeSession();
     shell.metastore().reset();
+
+    // Until HADOOP-16435 we have to manually remove the RpcMetrics for every run otherwise we might end up with OOM
+    DefaultMetricsSystem.shutdown();
+
     // HiveServer2 thread pools are using thread local Hive -> HMSClient objects. These are not cleaned up when the
     // HiveServer2 is stopped. Only Finalizer closes the HMS connections.
     System.gc();
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java
index b751af8ee9..3b1a111909 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java
@@ -40,6 +40,8 @@
 import java.util.stream.Collectors;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
+import org.apache.hadoop.hive.common.type.TimestampTZUtil;
+import org.apache.hadoop.hive.common.type.TimestampUtils;
 import org.apache.hadoop.hive.serde2.io.DateWritable;
 import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;
 import org.apache.hadoop.hive.serde2.io.TimestampWritable;
@@ -266,4 +268,35 @@ public static void validateFiles(Table table, Configuration conf, JobID jobId, i
     Assert.assertFalse(
         new File(HiveIcebergOutputCommitter.generateJobLocation(table.location(), conf, jobId)).exists());
   }
+
+  /**
+   * Validates whether the table contains the expected records. The records are retrieved by Hive query and compared as
+   * strings. The results should be sorted by a unique key so we do not end up with flaky tests.
+   * @param shell Shell to execute the query
+   * @param tableName The table to query
+   * @param expected The expected list of Records
+   * @param sortBy The column name by which we will sort
+   */
+  public static void validateDataWithSQL(TestHiveShell shell, String tableName, List<Record> expected, String sortBy) {
+    List<Object[]> rows = shell.executeStatement("SELECT * FROM " + tableName + " ORDER BY " + sortBy);
+
+    Assert.assertEquals(expected.size(), rows.size());
+    for (int i = 0; i < expected.size(); ++i) {
+      Object[] row = rows.get(i);
+      Record record = expected.get(i);
+      Assert.assertEquals(record.size(), row.length);
+      for (int j = 0; j < record.size(); ++j) {
+        Object field = record.get(j);
+        if (field instanceof LocalDateTime) {
+          Assert.assertEquals(((LocalDateTime) field).toInstant(ZoneOffset.UTC).toEpochMilli(),
+              TimestampUtils.stringToTimestamp((String) row[j]).toEpochMilli());
+        } else if (field instanceof OffsetDateTime) {
+          Assert.assertEquals(((OffsetDateTime) field).toInstant().toEpochMilli(),
+              TimestampTZUtil.parse((String) row[j]).toEpochMilli());
+        } else {
+          Assert.assertEquals(field.toString(), row[j].toString());
+        }
+      }
+    }
+  }
 }
diff --git a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
index a8ac3372d6..419ca3482f 100644
--- a/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
+++ b/iceberg/iceberg-handler/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandlerWithEngine.java
@@ -21,6 +21,10 @@
 
 import java.io.IOException;
 import java.math.BigDecimal;
+import java.time.LocalDate;
+import java.time.LocalDateTime;
+import java.time.OffsetDateTime;
+import java.time.ZoneOffset;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.HashMap;
@@ -812,6 +816,108 @@ public void testMultilevelIdentityPartitionedWrite() throws IOException {
     HiveIcebergTestUtils.validateData(table, records, 0);
   }
 
+  @Test
+  public void testYearTransform() throws IOException {
+    Schema schema = new Schema(
+        optional(1, "id", Types.LongType.get()),
+        optional(2, "part_field", Types.DateType.get()));
+    PartitionSpec spec = PartitionSpec.builderFor(schema).year("part_field").build();
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(schema)
+        .add(1L, LocalDate.of(2020, 1, 21))
+        .add(2L, LocalDate.of(2020, 1, 22))
+        .add(3L, LocalDate.of(2019, 1, 21))
+        .build();
+    Table table = testTables.createTable(shell, "part_test", schema, spec, fileFormat, records);
+    HiveIcebergTestUtils.validateData(table, records, 0);
+
+    HiveIcebergTestUtils.validateDataWithSQL(shell, "part_test", records, "id");
+  }
+
+  @Test
+  public void testMonthTransform() throws IOException {
+    Schema schema = new Schema(
+        optional(1, "id", Types.LongType.get()),
+        optional(2, "part_field", Types.TimestampType.withZone()));
+    PartitionSpec spec = PartitionSpec.builderFor(schema).month("part_field").build();
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(schema)
+        .add(1L, OffsetDateTime.of(2017, 11, 22, 11, 30, 7, 0, ZoneOffset.ofHours(1)))
+        .add(2L, OffsetDateTime.of(2017, 11, 22, 11, 30, 7, 0, ZoneOffset.ofHours(2)))
+        .add(3L, OffsetDateTime.of(2017, 11, 23, 11, 30, 7, 0, ZoneOffset.ofHours(3)))
+        .build();
+    Table table = testTables.createTable(shell, "part_test", schema, spec, fileFormat, records);
+    HiveIcebergTestUtils.validateData(table, records, 0);
+
+    HiveIcebergTestUtils.validateDataWithSQL(shell, "part_test", records, "id");
+  }
+
+  @Test
+  public void testDayTransform() throws IOException {
+    Schema schema = new Schema(
+        optional(1, "id", Types.LongType.get()),
+        optional(2, "part_field", Types.TimestampType.withoutZone()));
+    PartitionSpec spec = PartitionSpec.builderFor(schema).day("part_field").build();
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(schema)
+        .add(1L, LocalDateTime.of(2019, 2, 22, 9, 44, 54))
+        .add(2L, LocalDateTime.of(2019, 2, 22, 10, 44, 54))
+        .add(3L, LocalDateTime.of(2019, 2, 23, 9, 44, 54))
+        .build();
+    Table table = testTables.createTable(shell, "part_test", schema, spec, fileFormat, records);
+    HiveIcebergTestUtils.validateData(table, records, 0);
+
+    HiveIcebergTestUtils.validateDataWithSQL(shell, "part_test", records, "id");
+  }
+
+  @Test
+  public void testHourTransform() throws IOException {
+    Schema schema = new Schema(
+        optional(1, "id", Types.LongType.get()),
+        optional(2, "part_field", Types.TimestampType.withoutZone()));
+    PartitionSpec spec = PartitionSpec.builderFor(schema).hour("part_field").build();
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(schema)
+        .add(1L, LocalDateTime.of(2019, 2, 22, 9, 44, 54))
+        .add(2L, LocalDateTime.of(2019, 2, 22, 10, 44, 54))
+        .add(3L, LocalDateTime.of(2019, 2, 23, 9, 44, 54))
+        .build();
+    Table table = testTables.createTable(shell, "part_test", schema, spec, fileFormat, records);
+    HiveIcebergTestUtils.validateData(table, records, 0);
+
+    HiveIcebergTestUtils.validateDataWithSQL(shell, "part_test", records, "id");
+  }
+
+  @Test
+  public void testBucketTransform() throws IOException {
+    Schema schema = new Schema(
+        optional(1, "id", Types.LongType.get()),
+        optional(2, "part_field", Types.StringType.get()));
+    PartitionSpec spec = PartitionSpec.builderFor(schema).bucket("part_field", 2).build();
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(schema)
+        .add(1L, "Part1")
+        .add(2L, "Part2")
+        .add(3L, "Art3")
+        .build();
+    Table table = testTables.createTable(shell, "part_test", schema, spec, fileFormat, records);
+    HiveIcebergTestUtils.validateData(table, records, 0);
+
+    HiveIcebergTestUtils.validateDataWithSQL(shell, "part_test", records, "id");
+  }
+
+  @Test
+  public void testTruncateTransform() throws IOException {
+    Schema schema = new Schema(
+        optional(1, "id", Types.LongType.get()),
+        optional(2, "part_field", Types.StringType.get()));
+    PartitionSpec spec = PartitionSpec.builderFor(schema).truncate("part_field", 2).build();
+    List<Record> records = TestHelper.RecordsBuilder.newInstance(schema)
+        .add(1L, "Part1")
+        .add(2L, "Part2")
+        .add(3L, "Art3")
+        .build();
+    Table table = testTables.createTable(shell, "part_test", schema, spec, fileFormat, records);
+    HiveIcebergTestUtils.validateData(table, records, 0);
+
+    HiveIcebergTestUtils.validateDataWithSQL(shell, "part_test", records, "id");
+  }
+
   @Test
   public void testMultiTableInsert() throws IOException {
     testTables.createTable(shell, "customers", HiveIcebergStorageHandlerTestUtils.CUSTOMER_SCHEMA,
