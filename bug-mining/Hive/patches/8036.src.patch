diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
index 285b099f89..549e59a853 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/HiveVectorizedReader.java
@@ -45,6 +45,7 @@
 import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;
 import org.apache.iceberg.orc.VectorizedReadUtils;
 import org.apache.iceberg.relocated.com.google.common.collect.Lists;
+import org.apache.iceberg.types.Types;
 
 /**
  * Utility class to create vectorized readers for Hive.
@@ -60,7 +61,8 @@ private HiveVectorizedReader() {
 
   public static <D> CloseableIterable<D> reader(InputFile inputFile, FileScanTask task, Map<Integer, ?> idToConstant,
       TaskAttemptContext context) {
-    JobConf job = (JobConf) context.getConfiguration();
+    // Tweaks on jobConf here are relevant for this task only, so we need to copy it first as context's conf is reused..
+    JobConf job = new JobConf((JobConf) context.getConfiguration());
     Path path = new Path(inputFile.location());
     FileFormat format = task.file().format();
     Reporter reporter = ((MapredIcebergInputFormat.CompatibilityTaskAttemptContextImpl) context).getLegacyReporter();
@@ -79,15 +81,22 @@ public static <D> CloseableIterable<D> reader(InputFile inputFile, FileScanTask
       List<Integer> partitionColIndicesList = Lists.newLinkedList();
       List<Object> partitionValuesList = Lists.newLinkedList();
 
-      for (PartitionField field : fields) {
-        if (field.transform().isIdentity()) {
-          // Skip reading identity partition columns from source file...
-          int hiveColIndex = field.sourceId() - 1;
-          readColumnIds.remove((Integer) hiveColIndex);
-
-          // ...and use the corresponding constant value instead
-          partitionColIndicesList.add(hiveColIndex);
-          partitionValuesList.add(idToConstant.get(field.sourceId()));
+      for (PartitionField partitionField : fields) {
+        if (partitionField.transform().isIdentity()) {
+
+          // Get columns in read schema order (which matches those of readColumnIds) to find partition column indices
+          List<Types.NestedField> columns = task.spec().schema().columns();
+          for (int colIdx = 0; colIdx < columns.size(); ++colIdx) {
+            if (columns.get(colIdx).fieldId() == partitionField.sourceId()) {
+              // Skip reading identity partition columns from source file...
+              readColumnIds.remove((Integer) colIdx);
+
+              // ...and use the corresponding constant value instead
+              partitionColIndicesList.add(colIdx);
+              partitionValuesList.add(idToConstant.get(partitionField.sourceId()));
+              break;
+            }
+          }
         }
       }
 
diff --git a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/VectorizedRowBatchIterator.java b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/VectorizedRowBatchIterator.java
index 6497d5d26c..1ad5180e58 100644
--- a/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/VectorizedRowBatchIterator.java
+++ b/iceberg/iceberg-handler/src/main/java/org/apache/iceberg/mr/hive/vector/VectorizedRowBatchIterator.java
@@ -66,7 +66,10 @@ private void advance() {
         if (partitionColIndices != null) {
           for (int i = 0; i < partitionColIndices.length; ++i) {
             int colIdx = partitionColIndices[i];
-            vrbCtx.addPartitionColsToBatch(batch.cols[colIdx], partitionValues[i], partitionColIndices[i]);
+            // The partition column might not be part of the current projection - in which case no CV is inited
+            if (batch.cols[colIdx] != null) {
+              vrbCtx.addPartitionColsToBatch(batch.cols[colIdx], partitionValues[i], partitionColIndices[i]);
+            }
           }
         }
       } catch (IOException ioe) {
diff --git a/iceberg/iceberg-handler/src/test/queries/positive/vectorized_iceberg_read.q b/iceberg/iceberg-handler/src/test/queries/positive/vectorized_iceberg_read.q
index 8daa187cb3..6227b17511 100644
--- a/iceberg/iceberg-handler/src/test/queries/positive/vectorized_iceberg_read.q
+++ b/iceberg/iceberg-handler/src/test/queries/positive/vectorized_iceberg_read.q
@@ -8,8 +8,6 @@ analyze table tbl_ice_orc compute statistics for columns;
 explain select b, max(a) from tbl_ice_orc group by b;
 select b, max(a) from tbl_ice_orc group by b;
 
-
-
 create external table tbl_ice_orc_all_types (
     t_float FLOAT,
     t_double DOUBLE,
@@ -30,5 +28,42 @@ explain select max(t_float), t_double, t_boolean, t_int, t_bigint, t_binary, t_s
 select max(t_float), t_double, t_boolean, t_int, t_bigint, t_binary, t_string, t_timestamp, t_date, t_decimal from tbl_ice_orc_all_types
         group by t_double, t_boolean, t_int, t_bigint, t_binary, t_string, t_timestamp, t_date, t_decimal;
 
+create external table tbl_ice_orc_parted (
+    a int,
+    b string
+    ) partitioned by (p1 string, p2 string)
+    stored by iceberg stored as orc location 'file:/tmp/tbl_ice_orc_parted';
+
+insert into tbl_ice_orc_parted values
+                                      (1, 'aa', 'Europe', 'Hungary'),
+                                      (1, 'bb', 'Europe', 'Hungary'),
+                                      (2, 'aa', 'America', 'USA'),
+                                      (2, 'bb', 'America', 'Canada');
+-- query with projection of partition columns' subset
+select p1, a, min(b) from tbl_ice_orc_parted group by p1, a;
+
+-- required for reordering between differnt types
+set hive.metastore.disallow.incompatible.col.type.changes=false;
+
+-- move partition columns
+alter table tbl_ice_orc_parted change column p1 p1 string after a;
+
+-- should yield to the same result as previously
+select p1, a, min(b) from tbl_ice_orc_parted group by p1, a;
+
+-- move non-partition columns
+alter table tbl_ice_orc_parted change column a a int after b;
+
+describe tbl_ice_orc_parted;
+
+-- should yield to the same result as previously
+select p1, a, min(b) from tbl_ice_orc_parted group by p1, a;
+
+insert into tbl_ice_orc_parted values ('Europe', 3, 'cc', 'Austria');
+
+-- projecting all columns
+select p1, p2, a, min(b) from tbl_ice_orc_parted group by p1, p2, a;
+
 drop table tbl_ice_orc;
-drop table tbl_ice_orc_all_types;
\ No newline at end of file
+drop table tbl_ice_orc_all_types;
+drop table tbl_ice_orc_parted;
\ No newline at end of file
diff --git a/iceberg/iceberg-handler/src/test/results/positive/vectorized_iceberg_read.q.out b/iceberg/iceberg-handler/src/test/results/positive/vectorized_iceberg_read.q.out
index dfb7b2cb6d..b0d592328f 100644
--- a/iceberg/iceberg-handler/src/test/results/positive/vectorized_iceberg_read.q.out
+++ b/iceberg/iceberg-handler/src/test/results/positive/vectorized_iceberg_read.q.out
@@ -152,6 +152,121 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@tbl_ice_orc_all_types
 POSTHOOK: Output: hdfs://### HDFS PATH ###
 1.1	1.2	false	4	567890123456789	6	col7	2012-10-03 19:58:08	1234-09-02	10.01
+PREHOOK: query: create external table tbl_ice_orc_parted (
+    a int,
+    b string
+    ) partitioned by (p1 string, p2 string)
+#### A masked pattern was here ####
+PREHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+PREHOOK: Output: database:default
+PREHOOK: Output: default@tbl_ice_orc_parted
+POSTHOOK: query: create external table tbl_ice_orc_parted (
+    a int,
+    b string
+    ) partitioned by (p1 string, p2 string)
+#### A masked pattern was here ####
+POSTHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tbl_ice_orc_parted
+PREHOOK: query: insert into tbl_ice_orc_parted values
+                                      (1, 'aa', 'Europe', 'Hungary'),
+                                      (1, 'bb', 'Europe', 'Hungary'),
+                                      (2, 'aa', 'America', 'USA'),
+                                      (2, 'bb', 'America', 'Canada')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@tbl_ice_orc_parted
+POSTHOOK: query: insert into tbl_ice_orc_parted values
+                                      (1, 'aa', 'Europe', 'Hungary'),
+                                      (1, 'bb', 'Europe', 'Hungary'),
+                                      (2, 'aa', 'America', 'USA'),
+                                      (2, 'bb', 'America', 'Canada')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@tbl_ice_orc_parted
+PREHOOK: query: select p1, a, min(b) from tbl_ice_orc_parted group by p1, a
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select p1, a, min(b) from tbl_ice_orc_parted group by p1, a
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+Europe	1	aa
+America	2	aa
+PREHOOK: query: alter table tbl_ice_orc_parted change column p1 p1 string after a
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: default@tbl_ice_orc_parted
+POSTHOOK: query: alter table tbl_ice_orc_parted change column p1 p1 string after a
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: default@tbl_ice_orc_parted
+PREHOOK: query: select p1, a, min(b) from tbl_ice_orc_parted group by p1, a
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select p1, a, min(b) from tbl_ice_orc_parted group by p1, a
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+Europe	1	aa
+America	2	aa
+PREHOOK: query: alter table tbl_ice_orc_parted change column a a int after b
+PREHOOK: type: ALTERTABLE_RENAMECOL
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: default@tbl_ice_orc_parted
+POSTHOOK: query: alter table tbl_ice_orc_parted change column a a int after b
+POSTHOOK: type: ALTERTABLE_RENAMECOL
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: default@tbl_ice_orc_parted
+PREHOOK: query: describe tbl_ice_orc_parted
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: query: describe tbl_ice_orc_parted
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@tbl_ice_orc_parted
+p1                  	string              	from deserializer   
+a                   	int                 	from deserializer   
+b                   	string              	from deserializer   
+p2                  	string              	from deserializer   
+	 	 
+# Partition Transform Information	 	 
+# col_name            	transform_type      	 
+p1                  	IDENTITY            	 
+p2                  	IDENTITY            	 
+PREHOOK: query: select p1, a, min(b) from tbl_ice_orc_parted group by p1, a
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select p1, a, min(b) from tbl_ice_orc_parted group by p1, a
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+America	2	aa
+Europe	1	aa
+PREHOOK: query: insert into tbl_ice_orc_parted values ('Europe', 3, 'cc', 'Austria')
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+PREHOOK: Output: default@tbl_ice_orc_parted
+POSTHOOK: query: insert into tbl_ice_orc_parted values ('Europe', 3, 'cc', 'Austria')
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+POSTHOOK: Output: default@tbl_ice_orc_parted
+PREHOOK: query: select p1, p2, a, min(b) from tbl_ice_orc_parted group by p1, p2, a
+PREHOOK: type: QUERY
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: hdfs://### HDFS PATH ###
+POSTHOOK: query: select p1, p2, a, min(b) from tbl_ice_orc_parted group by p1, p2, a
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: hdfs://### HDFS PATH ###
+America	Canada	2	bb
+America	USA	2	aa
+Europe	Hungary	1	aa
+Europe	Austria	3	cc
 PREHOOK: query: drop table tbl_ice_orc
 PREHOOK: type: DROPTABLE
 PREHOOK: Input: default@tbl_ice_orc
@@ -168,3 +283,11 @@ POSTHOOK: query: drop table tbl_ice_orc_all_types
 POSTHOOK: type: DROPTABLE
 POSTHOOK: Input: default@tbl_ice_orc_all_types
 POSTHOOK: Output: default@tbl_ice_orc_all_types
+PREHOOK: query: drop table tbl_ice_orc_parted
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@tbl_ice_orc_parted
+PREHOOK: Output: default@tbl_ice_orc_parted
+POSTHOOK: query: drop table tbl_ice_orc_parted
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@tbl_ice_orc_parted
+POSTHOOK: Output: default@tbl_ice_orc_parted
