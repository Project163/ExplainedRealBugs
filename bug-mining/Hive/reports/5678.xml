<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:55:21 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-17019] Add support to download debugging information as an archive.</title>
                <link>https://issues.apache.org/jira/browse/HIVE-17019</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Given a queryId or dagId, get all information related to it: like, tez am, task logs, hive ats data, tez ats data, slider am status, etc. Package it into and archive.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13084504">HIVE-17019</key>
            <summary>Add support to download debugging information as an archive.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="10002" iconUrl="https://issues.apache.org/jira/images/icons/statuses/document.png" description="A patch for this issue has been uploaded to JIRA by a contributor.">Patch Available</status>
                    <statusCategory id="4" key="indeterminate" colorName="yellow"/>
                                    <resolution id="-1">Unresolved</resolution>
                                        <assignee username="harishjp">Harish JP</assignee>
                                    <reporter username="harishjp">Harish JP</reporter>
                        <labels>
                    </labels>
                <created>Tue, 4 Jul 2017 10:44:35 +0000</created>
                <updated>Mon, 24 Jul 2017 21:33:39 +0000</updated>
                                                                                <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16073564" author="hiveqa" created="Tue, 4 Jul 2017 12:17:47 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12875614/HIVE-17019.01.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12875614/HIVE-17019.01.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 3 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 40 failed/errored test(s), 10498 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=162)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=163)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=164)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=165)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=169)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=98)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=99)
org.apache.hadoop.hive.cli.TestMinimrCliDriver.org.apache.hadoop.hive.cli.TestMinimrCliDriver (batchId=86)
org.apache.hadoop.hive.cli.TestMinimrCliDriver.org.apache.hadoop.hive.cli.TestMinimrCliDriver (batchId=87)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver (batchId=91)
org.apache.hadoop.hive.cli.TestPerfCliDriver.org.apache.hadoop.hive.cli.TestPerfCliDriver (batchId=232)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=102)
org.apache.hive.beeline.TestBeeLineWithArgs.org.apache.hive.beeline.TestBeeLineWithArgs (batchId=220)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler (batchId=230)
org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat (batchId=187)
org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish (batchId=182)
org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat (batchId=187)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark (batchId=226)
org.apache.hive.jdbc.TestJdbcWithMiniHA.org.apache.hive.jdbc.TestJdbcWithMiniHA (batchId=225)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.org.apache.hive.jdbc.TestJdbcWithMiniLlap (batchId=226)
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark (batchId=226)
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerPrimaryQueueMapping (batchId=229)
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerQueueMapping (batchId=229)
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerSecondaryQueueMapping (batchId=229)
org.apache.hive.jdbc.TestSchedulerQueue.testQueueMappingCheckDisabled (batchId=229)
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS (batchId=222)
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez (batchId=221)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5880/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5880/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5880/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5880/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5880/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5880/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 40 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12875614 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16083111" author="sseth" created="Tue, 11 Jul 2017 22:37:29 +0000"  >&lt;p&gt;Thanks for posting the patch. Will be useful to get relevant data for a query.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Change the top level package from llap-debug to tez-debug? (Works with both I believe) &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ashutoshc&quot; class=&quot;user-hover&quot; rel=&quot;ashutoshc&quot;&gt;ashutoshc&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thejas&quot; class=&quot;user-hover&quot; rel=&quot;thejas&quot;&gt;thejas&lt;/a&gt; - any recommendations on whether the code gets a top level module, or goes under an existing module. This allows downloading of various debug artifacts for a tez job - logs, metrics for llap, hiveserver2 logs (soon), tez am logs, ATS data for the query (hive and tez).&lt;/li&gt;
	&lt;li&gt;In the new pom.xml, dependency on hive-llap-server. 1) Is it required?, 2) Will need to exclude some dependent artifacts. See service/pom.xml llap-server dependency handling&lt;/li&gt;
	&lt;li&gt;LogDownloadServlet - Should this throw an error as soon as the filename pattern validation fails?&lt;/li&gt;
	&lt;li&gt;LogDownloadServlet - change to dagId/queryId validation instead&lt;/li&gt;
	&lt;li&gt;LogDownloadServlet - thread being created inside of the request handler? This should be limited outside of the request? so that only a controlled number of parallel artifact downloads can run.&lt;/li&gt;
	&lt;li&gt;LogDownloadServlet - what happens in case of aggregator failure? Exception back to the user?&lt;/li&gt;
	&lt;li&gt;LogDownloadServlet - seems to be generating the file to disk and then streaming it over. Can this be streamed over directly instead. Otherwise there&apos;s the possibility of leaking files. (Artifact.downloadIntoStream or some such?) Guessing this is complicated further by the multi-threaded artifact downloader.&lt;br/&gt;
Alternately need to have a cleanup mechanism. &lt;/li&gt;
	&lt;li&gt;Timeout on the tests&lt;/li&gt;
	&lt;li&gt;Apache header needs to be added to files where it is missing.&lt;/li&gt;
	&lt;li&gt;Main - Please rename to something more indicative of what the tool does.&lt;/li&gt;
	&lt;li&gt;Main - Likely a follow up jira - parse using a standard library, instead of trying to parse the arguments to main directly.&lt;/li&gt;
	&lt;li&gt;Server - Enabling the artifact should be controlled via a config. Does not always need to be hosted in HS2 (Default disabled, at least till security can be sorted out)&lt;/li&gt;
	&lt;li&gt;Is it possible to support a timeout on the downloads? (Can be a follow up jira)&lt;/li&gt;
	&lt;li&gt;ArtifactAggregator - I believe this does 2 stages of dependent artifacts / downloads? Stage1 - download whatever it can. Information from this should should be adequate for stage2 downloads ?&lt;/li&gt;
	&lt;li&gt;For the ones not implemented yet (DummyArtifact) - think it&apos;s better to just comment out the code, instead of invoking the DummyArtifacts downloader&lt;/li&gt;
	&lt;li&gt;Security - ACL enforcement required on secure clusters to make sure users can only download what they have access to. This is a must fix before this can be enabled by default.&lt;/li&gt;
	&lt;li&gt;Security - this can work around yarn restrictions on log downloads, since the files are being accessed by the hive user.&lt;br/&gt;
Could you please add some details on cluster testing.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16083774" author="harishjp" created="Wed, 12 Jul 2017 10:32:04 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sseth&quot; class=&quot;user-hover&quot; rel=&quot;sseth&quot;&gt;sseth&lt;/a&gt;.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Change the top level package from llap-debug to tez-debug? (Works with both I believe) &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ashutoshc&quot; class=&quot;user-hover&quot; rel=&quot;ashutoshc&quot;&gt;ashutoshc&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thejas&quot; class=&quot;user-hover&quot; rel=&quot;thejas&quot;&gt;thejas&lt;/a&gt; - any recommendations on whether the code gets a top level module, or goes under an existing module. This allows downloading of various debug artifacts for a tez job - logs, metrics for llap, hiveserver2 logs (soon), tez am logs, ATS data for the query (hive and tez).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Will change the directory.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;In the new pom.xml, dependency on hive-llap-server. 1) Is it required?, 2) Will need to exclude some dependent artifacts. See service/pom.xml llap-server dependency handling&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The llap status is fetched using LlapStatusServiceDriver which is part of hive-llap-server.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogDownloadServlet - Should this throw an error as soon as the filename pattern validation fails?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The filename check is to prevent any injection attack into the file name/http header, not to validate the id.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogDownloadServlet - change to dagId/queryId validation instead&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Can do, but it will be sensitive to changes to the id format. Currently its passed down to ATS and nothing will be retrieved for it.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogDownloadServlet - thread being created inside of the request handler? This should be limited outside of the request? so that only a controlled number of parallel artifact downloads can run.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Creating a shared executor, does it make sense to use Guava&apos;s direct executor, which will schedule task in current thread.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogDownloadServlet - what happens in case of aggregator failure? Exception back to the user?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Jetty will handle the exception, returning 500 to the user. Not sure if exception trace is part of it. Will try and see.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LogDownloadServlet - seems to be generating the file to disk and then streaming it over. Can this be streamed over directly instead. Otherwise there&apos;s the possibility of leaking files. (Artifact.downloadIntoStream or some such?) Guessing this is complicated further by the multi-threaded artifact downloader.&lt;br/&gt;
Alternately need to have a cleanup mechanism.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For streaming directly, it would not be possible because of multithreading. If its single threaded then I can use a ZipOutputStream and add entry one at a time.&lt;/p&gt;

&lt;p&gt;Oops, sorry the finally got moved down since aggregator had to be closed before streaming the file. I&apos;ll handle it using a try finally to cleanup.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Timeout on the tests&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Setting timeouts on tests.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Apache header needs to be added to files where it is missing.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Sorry, will add the licence header to all files.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Main - Please rename to something more indicative of what the tool does.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I was planning to remove this and integrate with hive cli, --service &amp;lt;download_logs&amp;gt;. This does not work without lot of classpath fixes, or I&apos;ll have to create a script to add hive jars.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Main - Likely a follow up jira - parse using a standard library, instead of trying to parse the arguments to main directly.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Will check a few libs, apache commons OptionBuilder uses a static instance in its builder. Should be ok, for a cli based invoke once app, but will look at something better on lines of python argparse.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Server - Enabling the artifact should be controlled via a config. Does not always need to be hosted in HS2 (Default disabled, at least till security can be sorted out)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ll add a config.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Is it possible to support a timeout on the downloads? (Can be a follow up jira)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Sure, will do. Global or per download or both?&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ArtifactAggregator - I believe this does 2 stages of dependent artifacts / downloads? Stage1 - download whatever it can. Information from this should should be adequate for stage2 downloads ?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It could be more stages:&lt;br/&gt;
Ex: given dag_id&lt;br/&gt;
stage 1: will fetch tez ats info which is used to extract hive id, task container/node list.&lt;br/&gt;
stage 2: will fetch hive ats info, tez container log list.&lt;br/&gt;
stage 3: llap containers log list, tez task logs.&lt;br/&gt;
stage 4: llap container logs.&lt;/p&gt;

&lt;p&gt;aggregator iterates through the list of sources and finds those which can download using info in the params.&lt;br/&gt;
It schedules the sources and waits for them to complete everything and the repeats.&lt;br/&gt;
Stop if no new sources could download or all sources are exhausted.&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;For the ones not implemented yet (DummyArtifact) - think it&apos;s better to just comment out the code, instead of invoking the DummyArtifacts downloader&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Sorry, will do.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Security - ACL enforcement required on secure clusters to make sure users can only download what they have access to. This is a must fix before this can be enabled by default.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Working on this.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Security - this can work around yarn restrictions on log downloads, since the files are being accessed by the hive user.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Yes this should work.&lt;/p&gt;

&lt;p&gt;Could you please add some details on cluster testing.&lt;/p&gt;

&lt;p&gt;I&apos;ll add another comment with the details of testing.&lt;/p&gt;</comment>
                            <comment id="16088180" author="sseth" created="Fri, 14 Jul 2017 22:03:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;The llap status is fetched using LlapStatusServiceDriver which is part of hive-llap-server.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OK. llap-status should really be under it&apos;s own module. Anyway, that can be changed later. Another alternate is to have llap-status hosted as a webservice. That can happen in a follow up. Would be really good to skip the llap-server dependency, which in turn pulls in a lot of others.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Creating a shared executor, does it make sense to use Guava&apos;s direct executor, which will schedule task in current thread.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sure. It could just be done inline otherwise? The main question is how the total number of downloads are restricted. Restricting the number of handlers on the web interface does not help, since that serves out a lot more than download debug artifacts.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For streaming directly, it would not be possible because of multithreading. If its single threaded then I can use a ZipOutputStream and add entry one at a time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Think this is ok as long as files are cleaned up. May want to cap file sizes as well, since logs can get really large.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I was planning to remove this and integrate with hive cli, --service &amp;lt;download_logs&amp;gt;. This does not work without lot of classpath fixes, or I&apos;ll have to create a script to add hive jars.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sounds good.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Will check a few libs, apache commons OptionBuilder uses a static instance in its builder. Should be ok, for a cli based invoke once app, but will look at something better on lines of python argparse.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sounds good. Maybe in a follow up jira to get this in faster?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Sure, will do. Global or per download or both?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Global, defined by the server.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Stop if no new sources could download or all sources are exhausted.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Sounds good. I misread the code, sorry.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Jetty will handle the exception, returning 500 to the user. Not sure if exception trace is part of it. Will try and see.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Think this is ok as long as the error has enough information to let the user know what happened.&lt;/p&gt;


</comment>
                            <comment id="16091580" author="harishjp" created="Tue, 18 Jul 2017 14:02:18 +0000"  >&lt;ul&gt;
	&lt;li&gt;Addressing review comments from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sseth&quot; class=&quot;user-hover&quot; rel=&quot;sseth&quot;&gt;sseth&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Adding a new service to hive --service command.&lt;/li&gt;
	&lt;li&gt;Adding support to download and check acls.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16091723" author="hiveqa" created="Tue, 18 Jul 2017 15:37:27 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12877805/HIVE-17019.02.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12877805/HIVE-17019.02.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 3 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 41 failed/errored test(s), 10737 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestSSL - did not produce a TEST-*.xml file (likely timed out) (batchId=225)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=239)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=162)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=163)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=164)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=165)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver (batchId=166)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=139)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=141)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=142)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.org.apache.hadoop.hive.cli.TestMiniLlapCliDriver (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=169)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=170)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=98)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.org.apache.hadoop.hive.cli.TestMiniTezCliDriver (batchId=99)
org.apache.hadoop.hive.cli.TestMinimrCliDriver.org.apache.hadoop.hive.cli.TestMinimrCliDriver (batchId=86)
org.apache.hadoop.hive.cli.TestMinimrCliDriver.org.apache.hadoop.hive.cli.TestMinimrCliDriver (batchId=87)
org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver.org.apache.hadoop.hive.cli.TestNegativeMinimrCliDriver (batchId=91)
org.apache.hadoop.hive.cli.TestPerfCliDriver.org.apache.hadoop.hive.cli.TestPerfCliDriver (batchId=234)
org.apache.hive.beeline.TestBeeLineWithArgs.org.apache.hive.beeline.TestBeeLineWithArgs (batchId=222)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler.org.apache.hive.hcatalog.hbase.TestPigHBaseStorageHandler (batchId=232)
org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestHCatMultiOutputFormat (batchId=188)
org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish (batchId=183)
org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat.org.apache.hive.hcatalog.mapreduce.TestMultiOutputFormat (batchId=188)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark (batchId=228)
org.apache.hive.jdbc.TestJdbcWithMiniHA.org.apache.hive.jdbc.TestJdbcWithMiniHA (batchId=227)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.org.apache.hive.jdbc.TestJdbcWithMiniLlap (batchId=228)
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark (batchId=228)
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerPrimaryQueueMapping (batchId=231)
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerQueueMapping (batchId=231)
org.apache.hive.jdbc.TestSchedulerQueue.testFairSchedulerSecondaryQueueMapping (batchId=231)
org.apache.hive.jdbc.TestSchedulerQueue.testQueueMappingCheckDisabled (batchId=231)
org.apache.hive.service.TestHS2ImpersonationWithRemoteMS.org.apache.hive.service.TestHS2ImpersonationWithRemoteMS (batchId=224)
org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez.org.apache.hive.service.cli.operation.TestOperationLoggingAPIWithTez (batchId=223)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6073/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6073/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6073/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6073/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-6073/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-6073/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 41 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12877805 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16091794" author="harishjp" created="Tue, 18 Jul 2017 16:32:32 +0000"  >&lt;p&gt;Investigating test case failures, one of them seem to be because of guice version mismatch.&lt;/p&gt;</comment>
                            <comment id="16092916" author="harishjp" created="Wed, 19 Jul 2017 10:48:23 +0000"  >&lt;p&gt;Downgrading guice version to 3.0.&lt;/p&gt;</comment>
                            <comment id="16092968" author="hiveqa" created="Wed, 19 Jul 2017 11:59:34 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12877960/HIVE-17019.03.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12877960/HIVE-17019.03.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 3 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 17 failed/errored test(s), 11084 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=239)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=239)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[smb_mapjoin_1] (batchId=239)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_2] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_op_stats] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_ts_stats_for_mapjoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=234)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=234)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
org.apache.hive.service.cli.session.TestSessionManagerMetrics.testAbandonedSessionMetrics (batchId=194)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6086/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6086/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6086/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6086/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-6086/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-6086/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12877960 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16099157" author="sseth" created="Mon, 24 Jul 2017 21:33:39 +0000"  >&lt;p&gt;Re-looked at the patch. Mostly looks good. Some comments and questions.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;How is the context set up for LogDownloadServlet. e.g.CONF_LOG_DOWNLODER_NUM_EXECUTORS. The config should likely be set up in HiveConf in some way.&lt;/li&gt;
	&lt;li&gt;init for the servlet will happen once at startup? So if there&apos;s multiple requests to download, and the limit is hit, all webserver threads will block? Should we just return an error if there&apos;s too many parallel downloads, so that other parts of the UI continue to be functional.&lt;/li&gt;
	&lt;li&gt;In terms of the security - this becomes interesting. Essentially says that the feature will only work if authentication is enabled on secure clusters.&lt;/li&gt;
	&lt;li&gt;Timeout for the downloads as a separate jira?&lt;/li&gt;
	&lt;li&gt;Are any credentials required on the HttpClient created to download artifacts from various end points?&lt;/li&gt;
	&lt;li&gt;For Constants like TIMELINE_PATH_PREFIX - any chance YARN has a helper method? Otherwise we should file a jira to ask yarn to expose such utilities.&lt;/li&gt;
	&lt;li&gt;Both dagId and queryId cannot be specified at the same time?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12875614" name="HIVE-17019.01.patch" size="105674" author="harishjp" created="Tue, 4 Jul 2017 10:47:26 +0000"/>
                            <attachment id="12877805" name="HIVE-17019.02.patch" size="138034" author="harishjp" created="Tue, 18 Jul 2017 14:00:16 +0000"/>
                            <attachment id="12877960" name="HIVE-17019.03.patch" size="138032" author="harishjp" created="Wed, 19 Jul 2017 10:49:31 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 17 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3h2nj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>