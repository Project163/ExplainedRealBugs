<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:38:32 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-11540] Too many delta files during Compaction - OOM</title>
                <link>https://issues.apache.org/jira/browse/HIVE-11540</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am streaming weblogs to Kafka and then to Flume 1.6 using a Hive sink, with an average of 20 million records a day. I have 5 compactors running at various times (30m/5m/5s), no matter what time I give, the compactors seem to run out of memory cleaning up a couple thousand delta files and ultimately falls behind compacting/cleaning delta files. Any suggestions on what I can do to improve performance? Or can Hive streaming not handle this kind of load?&lt;/p&gt;

&lt;p&gt;I used this post as reference: &lt;a href=&quot;http://henning.kropponline.de/2015/05/19/hivesink-for-flume/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://henning.kropponline.de/2015/05/19/hivesink-for-flume/&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory

Max block location exceeded for split: CompactorInputSplit{base: hdfs://Dev01HWNameService/user/hive/warehouse/weblogs.db/dt=15-08-12/base_1056406, bucket: 0, length: 6493042, deltas: [delta_1056407_1056408, delta_1056409_1056410, delta_1056411_1056412, delta_1056413_1056414, delta_1056415_1056416, delta_1056417_1056418,&#8230;
, delta_1074039_1074040, delta_1074041_1074042, delta_1074043_1074044, delta_1074045_1074046, delta_1074047_1074048, delta_1074049_1074050, delta_1074051_1074052]} splitsize: 8772 maxsize: 10
2015-08-12 15:34:25,271 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:3
2015-08-12 15:34:25,367 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1439397150426_0068
2015-08-12 15:34:25,603 INFO  [upladevhwd04v.researchnow.com-18]: impl.YarnClientImpl (YarnClientImpl.java:submitApplication(274)) - Submitted application application_1439397150426_0068
2015-08-12 15:34:25,610 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://upladevhwd02v.researchnow.com:8088/proxy/application_1439397150426_0068/
2015-08-12 15:34:25,611 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1439397150426_0068
2015-08-12 15:34:30,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:33,756 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1439397150426_0068 running in uber mode : false
2015-08-12 15:34:33,757 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%
2015-08-12 15:34:35,147 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:40,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:45,184 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:50,201 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:34:55,256 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:00,205 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:02,975 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 33% reduce 0%
2015-08-12 15:35:02,982 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_0, Status : FAILED
2015-08-12 15:35:03,000 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_0, Status : FAILED
2015-08-12 15:35:04,008 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%
2015-08-12 15:35:05,132 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:10,206 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:15,228 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:20,207 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:25,148 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:28,154 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_1, Status : FAILED
2015-08-12 15:35:29,161 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_1, Status : FAILED
2015-08-12 15:35:30,142 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:35,140 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:40,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:45,153 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:50,150 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:35:52,268 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_2, Status : FAILED
2015-08-12 15:35:53,274 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_2, Status : FAILED
2015-08-12 15:35:55,149 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:00,160 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:05,145 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:10,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:15,158 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12
2015-08-12 15:36:17,397 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%
2015-08-12 15:36:18,409 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Job job_1439397150426_0068 failed with state FAILED due to: Task failed task_1439397150426_0068_m_000000
Job failed as tasks failed. failedMaps:1 failedReduces:0

2015-08-12 15:36:18,443 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 10
	Job Counters 
		Failed map tasks=7
		Killed map tasks=1
		Launched map tasks=8
		Other local map tasks=6
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=191960
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=191960
		Total vcore-seconds taken by all map tasks=191960
		Total megabyte-seconds taken by all map tasks=884551680
2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)
	at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)

2015-08-12 15:36:18,444 ERROR [upladevhwd04v.researchnow.com-18]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327)) - Expected to remove at least one row from completed_txn_components when marking compaction entry as clean!
^C
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;ngmathew@upladevhwd04v ~&amp;#93;&lt;/span&gt;$ tail -f /var/log/hive/hivemetastore.log&lt;br/&gt;
2015-08-12 15:36:18,443 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;upladevhwd04v.researchnow.com-18&amp;#93;&lt;/span&gt;: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!&lt;br/&gt;
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)&lt;/p&gt;



&lt;p&gt;Settings:&lt;br/&gt;
hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;br/&gt;
hive.compactor.initiator.on = true&lt;br/&gt;
hive.compactor.worker.threads = 5&lt;br/&gt;
Table stored as ORC&lt;br/&gt;
hive.vectorized.execution.enabled = false&lt;br/&gt;
hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;/p&gt;</description>
                <environment></environment>
        <key id="12855717">HIVE-11540</key>
            <summary>Too many delta files during Compaction - OOM</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ekoifman">Eugene Koifman</assignee>
                                    <reporter username="nivinm">Nivin Mathew</reporter>
                        <labels>
                            <label>TODOC1.3</label>
                    </labels>
                <created>Wed, 12 Aug 2015 23:12:18 +0000</created>
                <updated>Tue, 1 Oct 2019 22:07:40 +0000</updated>
                            <resolved>Sun, 25 Oct 2015 18:25:55 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.3.0</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>Transactions</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14694397" author="alangates" created="Wed, 12 Aug 2015 23:15:20 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=owen.omalley&quot; class=&quot;user-hover&quot; rel=&quot;owen.omalley&quot;&gt;owen.omalley&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ekoifman&quot; class=&quot;user-hover&quot; rel=&quot;ekoifman&quot;&gt;ekoifman&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14694598" author="nivinm" created="Thu, 13 Aug 2015 02:58:41 +0000"  >&lt;p&gt;To add, the compactor definitely is not running after 10 delta files. Possibly because of the volume of deltas coming in, but I would think right after the 11th delta, hivemetastore.log should kick off a minor compaction instead of the check interval time for minor compactions. &lt;/p&gt;</comment>
                            <comment id="14706691" author="nivinm" created="Fri, 21 Aug 2015 12:32:17 +0000"  >&lt;p&gt;Update from my side. I got the streaming working after changing the flume configs for transactions. So i dont get &quot;too many files&quot; now. &lt;/p&gt;</comment>
                            <comment id="14968442" author="ekoifman" created="Thu, 22 Oct 2015 03:27:10 +0000"  >&lt;p&gt;there are 2 things happening here:&lt;br/&gt;
OOM&lt;br/&gt;
and &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Max block location exceeded for split: CompactorInputSplit
{base: hdfs://Dev01HWNameService/user/hive/warehouse/weblogs.db/dt=15-08-12/base_1056406,
 bucket: 0, length: 6493042, deltas: [delta_1056407_1056408, delta_1056409_1056410, ... 
delta_1074047_1074048, delta_1074049_1074050, delta_1074051_1074052]}
splitsize: 8772 maxsize: 10
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The latter is discussed in &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/hadoop-user/201309.mbox/%3CCADCZBhGD5EE7d+bkTBUHvYi3Rq40JfY+KnT5Jbdv=aPJAaLBjA@mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/hadoop-user/201309.mbox/%3CCADCZBhGD5EE7d+bkTBUHvYi3Rq40JfY+KnT5Jbdv=aPJAaLBjA@mail.gmail.com%3E&lt;/a&gt;&lt;br/&gt;
and is not in itself an error but does mean the job is running less efficiently&lt;/p&gt;
</comment>
                            <comment id="14970142" author="hiveqa" created="Thu, 22 Oct 2015 23:59:19 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12767962/HIVE-11540.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12767962/HIVE-11540.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5741/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5741/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5741/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5741/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5741/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5741/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command &apos;bash /data/hive-ptest/working/scratch/source-prep.sh&apos; failed with exit status 1 and output &apos;+ [[ -n /usr/java/jdk1.7.0_45-cloudera ]]
+ export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera
+ export PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ PATH=/usr/java/jdk1.7.0_45-cloudera/bin/:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-maven-3.0.5/bin:/usr/local/apache-maven-3.0.5/bin:/usr/java/jdk1.7.0_45-cloudera/bin:/usr/local/apache-ant-1.9.1/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/hiveptest/bin
+ export &apos;ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m &apos;
+ ANT_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m &apos;
+ export &apos;M2_OPTS=-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128&apos;
+ M2_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128&apos;
+ cd /data/hive-ptest/working/
+ tee /data/hive-ptest/logs/PreCommit-HIVE-TRUNK-Build-5741/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   552bfbc..51a0c03  branch-1   -&amp;gt; origin/branch-1
   2fd619b..3f03d26  master     -&amp;gt; origin/master
+ git reset --hard HEAD
HEAD is now at 2fd619b HIVE-11895: CBO: Calcite Operator To Hive Operator (Calcite Return Path): fix udaf_percentile_approx_23.q (Pengcheng Xiong, reviewed by Ashutosh Chauhan)
+ git clean -f -d
Removing data/files/parquet_type_promotion.txt
Removing ql/src/test/queries/clientpositive/parquet_type_promotion.q
Removing ql/src/test/results/clientpositive/parquet_type_promotion.q.out
+ git checkout master
Already on &apos;master&apos;
Your branch is behind &apos;origin/master&apos; by 3 commits, and can be fast-forwarded.
+ git reset --hard origin/master
HEAD is now at 3f03d26 HIVE-11710: Beeline embedded mode doesn&apos;t output query progress after setting any session property (Aihua via Xuefu)
+ git merge --ff-only origin/master
Already up-to-date.
+ git gc
+ patchCommandPath=/data/hive-ptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hive-ptest/working/scratch/build.patch
+ [[ -f /data/hive-ptest/working/scratch/build.patch ]]
+ chmod +x /data/hive-ptest/working/scratch/smart-apply-patch.sh
+ /data/hive-ptest/working/scratch/smart-apply-patch.sh /data/hive-ptest/working/scratch/build.patch
The patch does not appear to apply with p0, p1, or p2
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12767962 - PreCommit-HIVE-TRUNK-Build&lt;/p&gt;</comment>
                            <comment id="14970389" author="alangates" created="Fri, 23 Oct 2015 03:58:16 +0000"  >&lt;p&gt;Changes look good to me, +1 assuming you can get the patch to run through the tests.&lt;/p&gt;</comment>
                            <comment id="14971983" author="hiveqa" created="Fri, 23 Oct 2015 22:13:14 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12768194/HIVE-11540.3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12768194/HIVE-11540.3.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5756/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5756/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5756/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5756/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5756/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5756/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;**** This message was trimmed, see log for full details ****
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[68,13] error: cannot find symbol
[ERROR] interface HBaseConnection
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[84,2] error: cannot find symbol
[ERROR] interface HBaseConnection
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[94,2] error: cannot find symbol
[ERROR] interface HBaseConnection
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[27,30] error: package org.apache.hadoop.hbase does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[28,30] error: package org.apache.hadoop.hbase does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[29,30] error: package org.apache.hadoop.hbase does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[30,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[31,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[32,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[33,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[34,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[35,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[36,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[37,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[38,37] error: package org.apache.hadoop.hbase.filter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[39,37] error: package org.apache.hadoop.hbase.filter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[40,37] error: package org.apache.hadoop.hbase.filter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[41,37] error: package org.apache.hadoop.hbase.filter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[30,37] error: package org.apache.hadoop.hbase.filter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[45,44] error: cannot find symbol
[ERROR] class ByteArrayComparable
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[22,37] error: package org.apache.hadoop.hbase.client does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[749,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[810,55] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2006,10] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2025,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2030,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2029,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2034,46] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2034,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2039,58] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[2038,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[68,13] error: cannot find symbol
[ERROR] interface HBaseConnection
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[84,2] error: cannot find symbol
[ERROR] interface HBaseConnection
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseConnection.java:[94,2] error: cannot find symbol
[ERROR] interface HBaseConnection
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[179,2] error: method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/PartitionKeyComparator.java:[223,2] error: method does not override or implement a method from a supertype
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[210,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[354,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[356,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[356,42] error: package CompareFilter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[356,64] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[358,13] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[362,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[420,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[422,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[422,42] error: package CompareFilter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[422,64] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[424,13] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[428,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[512,10] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[512,35] error: unexpected type
[ERROR] 
[ERROR] E extends Object declared in class ArrayList
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[515,7] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[515,21] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[519,5] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[520,5] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[580,9] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[580,34] error: unexpected type
[ERROR] 
[ERROR] E extends Object declared in class ArrayList
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[586,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[586,18] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[591,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[600,9] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[600,34] error: unexpected type
[ERROR] 
[ERROR] E extends Object declared in class ArrayList
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[614,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[614,18] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[624,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[729,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[731,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[731,42] error: package CompareFilter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[812,13] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[818,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[867,9] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[867,34] error: unexpected type
[ERROR] 
[ERROR] E extends Object declared in class ArrayList
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[868,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[871,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[871,18] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[876,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[916,13] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[918,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1055,9] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1055,34] error: unexpected type
[ERROR] 
[ERROR] E extends Object declared in class ArrayList
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1070,8] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1070,22] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1082,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1104,8] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1104,22] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1111,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1127,12] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1127,26] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1136,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1161,13] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1164,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1194,15] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1196,8] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1244,9] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1244,34] error: unexpected type
[ERROR] 
[ERROR] E extends Object declared in class ArrayList
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1245,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1249,6] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1249,18] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1253,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1286,4] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1288,19] error: cannot find symbol
[ERROR] class HBaseReadWrite
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1288,42] error: package CompareFilter does not exist
[ERROR] /data/hive-ptest/working/apache-github-source-source/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseReadWrite.java:[1288,64] error: cannot find symbol
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-metastore
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12768194 - PreCommit-HIVE-TRUNK-Build&lt;/p&gt;</comment>
                            <comment id="14972412" author="hiveqa" created="Sat, 24 Oct 2015 05:35:00 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12768469/HIVE-11540.4.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12768469/HIVE-11540.4.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 1 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 9705 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.testHasNull
org.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.testJsonDump
org.apache.hadoop.hive.ql.txn.compactor.TestWorker2.majorNoBaseLotsOfDeltas
org.apache.hadoop.hive.ql.txn.compactor.TestWorker2.minorNoBaseLotsOfDeltas
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.jdbc.TestSSL.testSSLVersion
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5772/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5772/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5772/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5772/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5772/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5772/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12768469 - PreCommit-HIVE-TRUNK-Build&lt;/p&gt;</comment>
                            <comment id="14972936" author="hiveqa" created="Sun, 25 Oct 2015 00:48:05 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12768544/HIVE-11540.6.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12768544/HIVE-11540.6.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 2 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 9708 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_index_bitmap3
org.apache.hadoop.hive.cli.TestMinimrCliDriver.testCliDriver_stats_counter_partitioned
org.apache.hadoop.hive.ql.io.orc.TestColumnStatistics.testHasNull
org.apache.hadoop.hive.ql.io.orc.TestJsonFileDump.testJsonDump
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation
org.apache.hive.jdbc.TestSSL.testSSLVersion
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5782/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5782/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5782/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/5782/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5782/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-5782/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12768544 - PreCommit-HIVE-TRUNK-Build&lt;/p&gt;</comment>
                            <comment id="14972970" author="lefty@hortonworks.com" created="Sun, 25 Oct 2015 02:59:06 +0000"  >&lt;p&gt;Doc note:  This adds configuration parameter &lt;b&gt;hive.compactor.max.num.delta&lt;/b&gt; to HiveConf.java, so it needs to be documented for release 2.0.0 in the Transactions and Compactor section of Configuration Properties.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-TransactionsandCompactor&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Configuration Properties &amp;#8211; Transactions and Compactor &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;(And by the way, this needs a status update.  But maybe it&apos;s also going into the 1.3.0 release, in which case the TODOC2.0 label should be changed to TODOC1.3.)&lt;/p&gt;</comment>
                            <comment id="14973361" author="ekoifman" created="Sun, 25 Oct 2015 18:25:37 +0000"  >&lt;p&gt;Committed to master: &lt;a href=&quot;https://github.com/apache/hive/commit/e3ef96f2b83ffa932dd59fc3df79dff8747309ba&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hive/commit/e3ef96f2b83ffa932dd59fc3df79dff8747309ba&lt;/a&gt; and branch-1 &lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/commit/e654efeb32c62fb5cd56214b823526173cb009bb&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hive/commit/e654efeb32c62fb5cd56214b823526173cb009bb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gates&quot; class=&quot;user-hover&quot; rel=&quot;gates&quot;&gt;gates&lt;/a&gt; for the review&lt;/p&gt;</comment>
                            <comment id="14973819" author="lefty@hortonworks.com" created="Mon, 26 Oct 2015 06:47:12 +0000"  >&lt;p&gt;Okay, changed TODOC2.0 to TODOC1.3.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12912709">HIVE-12403</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12768194" name="HIVE-11540.3.patch" size="18629" author="ekoifman" created="Fri, 23 Oct 2015 01:57:46 +0000"/>
                            <attachment id="12768469" name="HIVE-11540.4.patch" size="18629" author="ekoifman" created="Sat, 24 Oct 2015 00:59:58 +0000"/>
                            <attachment id="12768544" name="HIVE-11540.6.patch" size="21555" author="ekoifman" created="Sat, 24 Oct 2015 17:34:57 +0000"/>
                            <attachment id="12767962" name="HIVE-11540.patch" size="18503" author="ekoifman" created="Thu, 22 Oct 2015 04:45:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 4 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2iupr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>