<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:55:54 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-15767] Hive On Spark is not working on secure clusters from Oozie</title>
                <link>https://issues.apache.org/jira/browse/HIVE-15767</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;When a HiveAction is launched form Oozie with Hive On Spark enabled, we&apos;re getting errors:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.io.IOException: Exception reading file:/yarn/nm/usercache/yshi/appcache/application_1485271416004_0022/container_1485271416004_0022_01_000002/container_tokens
        at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:188)
        at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:155)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is caused by passing the &lt;tt&gt;mapreduce.job.credentials.binary&lt;/tt&gt; property to the Spark configuration in RemoteHiveSparkClient.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13039168">HIVE-15767</key>
            <summary>Hive On Spark is not working on secure clusters from Oozie</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gezapeti">G&#233;zapeti</assignee>
                                    <reporter username="gezapeti">G&#233;zapeti</reporter>
                        <labels>
                    </labels>
                <created>Tue, 31 Jan 2017 12:21:54 +0000</created>
                <updated>Tue, 22 May 2018 23:58:12 +0000</updated>
                            <resolved>Fri, 11 Aug 2017 16:52:19 +0000</resolved>
                                    <version>1.2.1</version>
                    <version>2.1.1</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>Spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="15846742" author="gezapeti" created="Tue, 31 Jan 2017 12:25:53 +0000"  >&lt;p&gt;Attaching fix&lt;/p&gt;</comment>
                            <comment id="16082308" author="pvary" created="Tue, 11 Jul 2017 14:46:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gezapeti&quot; class=&quot;user-hover&quot; rel=&quot;gezapeti&quot;&gt;gezapeti&lt;/a&gt;: There is a typo in the comment (missing &apos;n&apos; from locatioN)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;stakiar&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;: Could you please take a look at this?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Peter&lt;/p&gt;</comment>
                            <comment id="16082315" author="gezapeti" created="Tue, 11 Jul 2017 14:49:09 +0000"  >&lt;p&gt;Addressing typo&lt;/p&gt;</comment>
                            <comment id="16082411" author="stakiar" created="Tue, 11 Jul 2017 15:46:52 +0000"  >&lt;p&gt;Overall LGTM. Just a few questions:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Are these errors thrown by HiveServer2 or by the HoS Remote Driver?&lt;/li&gt;
	&lt;li&gt;Is that same thing required for Hive-on-MR?&lt;/li&gt;
	&lt;li&gt;Is it possible to add a test for this?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16082948" author="gezapeti" created="Tue, 11 Jul 2017 20:44:45 +0000"  >&lt;p&gt;This happens with HiveCLI, not with HS2. &lt;br/&gt;
The exception is coming from the spark driver.&lt;/p&gt;

&lt;p&gt;When the HiveCLI is executed from shell, the mapreduce.job.credentials.binary is empty in the configuration as spark-submit is called from the RemoteClient.&lt;br/&gt;
When it&apos;s executed from Oozie&apos;s LauncherMapper, Hive picks up this property from the Oozie launcher&apos;s configuration which is correct, but passes it to Spark. Spark runs in yarn-cluster mode so the Spark driver gets it&apos;s own container (which may be on an other machine). It look for the credential files in the folder where the Oozie Launcher ran. That&apos;s on a different machine, so it can&apos;t pick up the conatiner_tokens file which leaves the spark driver with no tokens so it fails.&lt;/p&gt;

&lt;p&gt;I don&apos;t know how Hive-on-MR works in this regards, but we had no similar issues with the HiveAction before, so I assume it works differently.&lt;/p&gt;

&lt;p&gt;I don&apos;t think it&apos;s possible to reproduce it using MiniClusters as the local folders will be available in the test so the Spark driver will be able to access it. &lt;/p&gt;</comment>
                            <comment id="16083374" author="yibing" created="Wed, 12 Jul 2017 03:33:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=peterceluch&quot; class=&quot;user-hover&quot; rel=&quot;peterceluch&quot;&gt;peterceluch&lt;/a&gt;, can the tokens in Oozie launcher application still be passed to Spark job when property &lt;tt&gt;mapreduce.job.credentials.binary&lt;/tt&gt; is unset? For example, in an environment where HDFS transparent encryption is enabled, is Spark job still able to connect to KMS servers?&lt;/p&gt;

&lt;p&gt;(The change is in &lt;tt&gt;RemoteHiveSparkClient&lt;/tt&gt;. Hive on MR shouldn&apos;t be affected. Oozie actions have already make sure the tokens are added to action configuration, which then should be passed to MR jobs).&lt;/p&gt;</comment>
                            <comment id="16085448" author="gezapeti" created="Thu, 13 Jul 2017 09:36:12 +0000"  >&lt;p&gt;I think the Spark driver will get the tokens afterwards, this property is pointing to an invalid location.&lt;/p&gt;</comment>
                            <comment id="16085612" author="yibing" created="Thu, 13 Jul 2017 12:07:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think the Spark driver will get the tokens afterwards&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I really doubt that Spark driver can do this. In Oozie environment, it is Oozie server that obtains all the tokens &lt;b&gt;on behalf of the end user&lt;/b&gt;. When the Hive actions starts a Spark job, the Spark driver has no access to end user ticket or keytab file. I don&apos;t think it can obtain necessary tokens. &lt;br/&gt;
I believe we should somehow extract all the tokens from existing toke file, and pass it on to the Spark driver.&lt;/p&gt;</comment>
                            <comment id="16085625" author="gezapeti" created="Thu, 13 Jul 2017 12:20:49 +0000"  >&lt;p&gt;The Spark driver will get the correct tokens from the parent application - it&apos;s in the local folder created for it&apos;s container. I&apos;m not sure how it get&apos;s them, but they are there. &lt;br/&gt;
The driver will pick it up from the correct container_tokens file using the HADOOP_TOKEN_FILE_LOCATION env variable or something like that. The issue is that Hadoop&apos;s TokenCache is looking for the mapreduce.job.credentials.binary property as well, while it&apos;s not needed and this invalid reference causes the job to fail.&lt;/p&gt;</comment>
                            <comment id="16085634" author="yibing" created="Thu, 13 Jul 2017 12:28:49 +0000"  >&lt;p&gt;Thanks for the explanation!&lt;br/&gt;
This may be done by YARN instead of Spark. &lt;/p&gt;</comment>
                            <comment id="16087605" author="stakiar" created="Fri, 14 Jul 2017 17:10:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;aihuaxu&lt;/a&gt; may have some input, he knows more about security.&lt;/p&gt;

&lt;p&gt;Other than that, LGTM.&lt;/p&gt;</comment>
                            <comment id="16087611" author="aihuaxu" created="Fri, 14 Jul 2017 17:14:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gezapeti&quot; class=&quot;user-hover&quot; rel=&quot;gezapeti&quot;&gt;gezapeti&lt;/a&gt; I will take a look. Can you rename your patch to the format of &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15767&quot; title=&quot;Hive On Spark is not working on secure clusters from Oozie&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-15767&quot;&gt;&lt;del&gt;HIVE-15767&lt;/del&gt;&lt;/a&gt;.1.patch to kick off the build? Looks like that&apos;s the reason why the build is not run.&lt;/p&gt;</comment>
                            <comment id="16087957" author="gezapeti" created="Fri, 14 Jul 2017 20:08:20 +0000"  >&lt;p&gt;Thanks for the comments and reviews!&lt;/p&gt;

&lt;p&gt;Renaming patch -002 to .1 to kick off pre-commit job. &lt;/p&gt;</comment>
                            <comment id="16088177" author="hiveqa" created="Fri, 14 Jul 2017 22:02:23 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12877383/HIVE-15767.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12877383/HIVE-15767.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 14 failed/errored test(s), 10907 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=238)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=238)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_2] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_op_stats] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_ts_stats_for_mapjoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=233)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testHttpRetryOnServerIdleTimeout (batchId=227)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6041/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6041/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6041/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6041/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-6041/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-6041/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12877383 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16090171" author="aihuaxu" created="Mon, 17 Jul 2017 17:52:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gezapeti&quot; class=&quot;user-hover&quot; rel=&quot;gezapeti&quot;&gt;gezapeti&lt;/a&gt; Logically seems it&apos;s correct to set proper mapreduce.job.credentials.binary and pass to Spark. And MR is also doing the same thing. Can you find out why it makes the difference when oozie calls HiveCLI MR vs. Spark actions? &lt;/p&gt;
</comment>
                            <comment id="16103843" author="gezapeti" created="Thu, 27 Jul 2017 20:25:27 +0000"  >&lt;p&gt;The problem is that we&apos;re not setting the &lt;em&gt;proper&lt;/em&gt; mapreduce.job.credentials.binary, but &lt;a href=&quot;https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java#L235&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;, were passing every property from the HiveConf conf to the configuration for Spark.&lt;br/&gt;
If HiveCLI is called from the Oozie LauncherMapper, that HiveConf will contain the &quot;mapreduce.job.credentials.binary&quot; property for the LauncherMapper. e.g /yarn/nm/usercache/systest/appcache/application_1501079366372_0045/container_1501079366372_0045_01_000001/container_tokens&lt;br/&gt;
This property have to be there so HiveCLI can access the tokens properly.&lt;/p&gt;

&lt;p&gt;Passing this folder to the Spark driver is problematic as the driver often will be executed on an other machine in the cluster where it won&apos;t be able to read this file as it&apos;s not there. There are a couple ways to define the location of the container_tokens file and Yarn takes care of Spark getting the correct location on the node the driver will be executed on.&lt;/p&gt;</comment>
                            <comment id="16117614" author="xuefuz" created="Tue, 8 Aug 2017 00:28:18 +0000"  >&lt;p&gt;From what I see, the patch seems logical, harmless at least. What I don&apos;t understand is that why Spark would attempt reading this file. As a side note, I didn&apos;t find the source in Spark code base that does this.&lt;/p&gt;</comment>
                            <comment id="16117956" author="gezapeti" created="Tue, 8 Aug 2017 07:12:01 +0000"  >&lt;p&gt;I don&apos;t remember all the details, but here is a longer stack trace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.RuntimeException: java.io.IOException: Exception reading file:/yarn/nm/usercache/yshi/appcache/application_1485271416004_0001/container_1485271416004_0001_01_000002/container_tokens
	at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:160)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:138)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:243)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Spark does not refer the &lt;tt&gt;mapreduce.job.credentials.binary&lt;/tt&gt; directly, it is hard-coded in Hadoop&apos;s TokenCache &lt;a href=&quot;https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java#L148&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. I think this TokenCache is used by Hadoop&apos;s FileSystem implementation too so when Spark talks to HDFS it does through this class.&lt;/p&gt;</comment>
                            <comment id="16118589" author="xuefuz" created="Tue, 8 Aug 2017 16:35:10 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="16123621" author="pvary" created="Fri, 11 Aug 2017 16:52:19 +0000"  >&lt;p&gt;Pushed to master.&lt;br/&gt;
Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gezapeti&quot; class=&quot;user-hover&quot; rel=&quot;gezapeti&quot;&gt;gezapeti&lt;/a&gt; for your contribution!&lt;/p&gt;</comment>
                            <comment id="16449392" author="linwukang" created="Tue, 24 Apr 2018 06:44:26 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gezapeti&quot; class=&quot;user-hover&quot; rel=&quot;gezapeti&quot;&gt;gezapeti&lt;/a&gt;&#160;, after apply this patch, i find that the Hive On Spark worked with yarn, all tasks is finished successfully. but there&apos;s another error throws at the end of the progress:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2018-04-24T14:28:46,409 INFO [116dbf89-2982-407d-9b64-4206b3bbe105 main] lockmgr.DbTxnManager: Stopped heartbeat &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; query: flowagent_20180424142839_be68e2b9-aca9-4023-89f8-6a18d53dd0c5
2018-04-24T14:28:46,409 INFO [116dbf89-2982-407d-9b64-4206b3bbe105 main] lockmgr.DbLockManager: releaseLocks: [lockid:438 queryId=flowagent_20180424142839_be68e2b9-aca9-4023-89f8-6a18d53dd0c5 txnid:0]
2018-04-24T14:28:46,422 ERROR [116dbf89-2982-407d-9b64-4206b3bbe105 main] CliDriver: Failed with exception java.io.IOException:org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6635)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:563)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:988)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)

java.io.IOException: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:6635)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:563)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:988)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1727)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)

at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:521)
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:428)
at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:147)
at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2208)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:253)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16450073" author="gezapeti" created="Tue, 24 Apr 2018 15:38:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=linwukang&quot; class=&quot;user-hover&quot; rel=&quot;linwukang&quot;&gt;linwukang&lt;/a&gt;, we haven&apos;t seen this exception after the fix.&lt;br/&gt;
This might have to do something in regards the command you&apos;re executing or the time frame where the job runs. Without knowing those it&apos;s hard to give meaningful suggestions.&lt;br/&gt;
Can you check the expiry date for the HDFS token in the job? &lt;/p&gt;</comment>
                            <comment id="16485906" author="vgarg" created="Tue, 22 May 2018 23:58:12 +0000"  >&lt;p&gt;Hive 3.0.0 has been released so closing this jira.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12850187" name="HIVE-15767-001.patch" size="1328" author="gezapeti" created="Tue, 31 Jan 2017 12:25:53 +0000"/>
                            <attachment id="12876635" name="HIVE-15767-002.patch" size="1329" author="gezapeti" created="Tue, 11 Jul 2017 14:48:51 +0000"/>
                            <attachment id="12877383" name="HIVE-15767.1.patch" size="1329" author="gezapeti" created="Fri, 14 Jul 2017 20:07:25 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 26 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i39eyn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>