<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:01:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-2372] java.io.IOException: error=7, Argument list too long</title>
                <link>https://issues.apache.org/jira/browse/HIVE-2372</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;I execute a huge query on a table with a lot of 2-level partitions. There is a perl reducer in my query. Maps worked ok, but every reducer fails with the following exception:&lt;/p&gt;

&lt;p&gt;2011-08-11 04:58:29,865 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: Executing &lt;span class=&quot;error&quot;&gt;&amp;#91;/usr/bin/perl, &amp;lt;reducer.pl&amp;gt;, &amp;lt;my_argument&amp;gt;&amp;#93;&lt;/span&gt;&lt;br/&gt;
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: tablename=null&lt;br/&gt;
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: partname=null&lt;br/&gt;
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: alias=null&lt;br/&gt;
2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {&quot;key&quot;:&lt;/p&gt;
{&quot;reducesinkkey0&quot;:129390185139228,&quot;reducesinkkey1&quot;:&quot;00008AF10000000063CA6F&quot;}
&lt;p&gt;,&quot;value&quot;:&lt;/p&gt;
{&quot;_col0&quot;:&quot;00008AF10000000063CA6F&quot;,&quot;_col1&quot;:&quot;2011-07-27 22:48:52&quot;,&quot;_col2&quot;:129390185139228,&quot;_col3&quot;:2006,&quot;_col4&quot;:4100,&quot;_col5&quot;:&quot;10017388=6&quot;,&quot;_col6&quot;:1063,&quot;_col7&quot;:&quot;NULL&quot;,&quot;_col8&quot;:&quot;address.com&quot;,&quot;_col9&quot;:&quot;NULL&quot;,&quot;_col10&quot;:&quot;NULL&quot;}
&lt;p&gt;,&quot;alias&quot;:0}&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)&lt;br/&gt;
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)&lt;br/&gt;
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)&lt;br/&gt;
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:396)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)&lt;br/&gt;
	at org.apache.hadoop.mapred.Child.main(Child.java:262)&lt;br/&gt;
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)&lt;br/&gt;
	... 7 more&lt;br/&gt;
Caused by: java.io.IOException: Cannot run program &quot;/usr/bin/perl&quot;: java.io.IOException: error=7, Argument list too long&lt;br/&gt;
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)&lt;br/&gt;
	... 15 more&lt;br/&gt;
Caused by: java.io.IOException: java.io.IOException: error=7, Argument list too long&lt;br/&gt;
	at java.lang.UNIXProcess.&amp;lt;init&amp;gt;(UNIXProcess.java:148)&lt;br/&gt;
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)&lt;br/&gt;
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)&lt;br/&gt;
	... 16 more&lt;/p&gt;

&lt;p&gt;It seems to me, I found the cause. ScriptOperator.java puts a lot of configs as environment variables to the child reduce process. One of variables is mapred.input.dir, which in my case more than 150KB. There are a huge amount of input directories in this variable. In short, the problem is that Linux (up to 2.6.23 kernel version) limits summary size of environment variables for child processes to 132KB. This problem could be solved by upgrading the kernel. But strings limitations still be 132KB per string in environment variable. So such huge variable doesn&apos;t work even on my home computer (2.6.32). You can read more information on (&lt;a href=&quot;http://www.kernel.org/doc/man-pages/online/pages/man2/execve.2.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.kernel.org/doc/man-pages/online/pages/man2/execve.2.html&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For now all our work has been stopped because of this problem and I can&apos;t find the solution. The only solution, which seems to me more reasonable is to get rid of this variable in reducers.&lt;/p&gt;

</description>
                <environment></environment>
        <key id="12518679">HIVE-2372</key>
            <summary>java.io.IOException: error=7, Argument list too long</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="sergeant">Sergey Tryuber</reporter>
                        <labels>
                    </labels>
                <created>Fri, 12 Aug 2011 09:07:34 +0000</created>
                <updated>Wed, 11 Jun 2014 19:53:40 +0000</updated>
                            <resolved>Fri, 25 May 2012 23:55:02 +0000</resolved>
                                    <version>0.12.0</version>
                                    <fixVersion>0.10.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="13089459" author="sergeant" created="Tue, 23 Aug 2011 13:33:13 +0000"  >&lt;p&gt;I&apos;ve added a quick fix on our cluster. The fix just erases mapred_input_dir from environment of child process.&lt;br/&gt;
ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java:&lt;br/&gt;
         ProcessBuilder pb = new ProcessBuilder(wrappedCmdArgs);&lt;br/&gt;
         Map&amp;lt;String, String&amp;gt; env = pb.environment();&lt;br/&gt;
         addJobConfToEnvironment(hconf, env);&lt;br/&gt;
+&lt;br/&gt;
+        LOG.info(&quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-2372&quot; title=&quot;java.io.IOException: error=7, Argument list too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-2372&quot;&gt;&lt;del&gt;HIVE-2372&lt;/del&gt;&lt;/a&gt;. HOTFIX. Removing mapred_input_dir from environment variables&quot;);&lt;br/&gt;
+        env.remove(&quot;mapred_input_dir&quot;);&lt;br/&gt;
+&lt;br/&gt;
         env.put(safeEnvVarName(HiveConf.ConfVars.HIVEALIAS.varname), String&lt;br/&gt;
             .valueOf(alias));&lt;/p&gt;

&lt;p&gt;All queries are work fine now. If anyone proposes more correct decision (add a property which will disable this property), I&apos;ll try to make code changes and prepare a patch.&lt;/p&gt;</comment>
                            <comment id="13129442" author="he yongqiang" created="Tue, 18 Oct 2011 02:56:50 +0000"  >&lt;p&gt;Sergey Tryuber, can you put a patch?&lt;/p&gt;</comment>
                            <comment id="13129560" author="sdong" created="Tue, 18 Oct 2011 07:18:03 +0000"  >&lt;p&gt;Instead of simply removing the key, we should truncate the value to a shorter one if it is too long.&lt;/p&gt;</comment>
                            <comment id="13130793" author="sergeant" created="Wed, 19 Oct 2011 17:24:29 +0000"  >&lt;p&gt;Siying, what the limit we should set on this property? Limiting it to 132KB won&apos;t work on kernels prior to 2.6.23, because there are other properties exist and sum limit will exceed 132KB threshold for those kernels. May be we could introduce on more property that (if set to true) will remove the key?&lt;/p&gt;</comment>
                            <comment id="13134710" author="sdong" created="Tue, 25 Oct 2011 03:05:08 +0000"  >&lt;p&gt;I&apos;m thinking a very short limit, like 1024 chars or so. A longer chars won&apos;t get user any information.&lt;/p&gt;</comment>
                            <comment id="13135111" author="appodictic" created="Tue, 25 Oct 2011 14:56:29 +0000"  >&lt;p&gt;I am surprised no one has ran into this sooner. I had assumed the limit was much shorter. Streaming is &apos;a neat hack&apos; but with the UDF/UDAF frameworks I have never been convinced it is needed. Most often I see use it improperly to make hacky map side joins etc. Maybe your real problem is having to many input files and you should merge your input first, or use bucketing instead of two level partitioning.   &lt;/p&gt;</comment>
                            <comment id="13139745" author="sergeant" created="Sun, 30 Oct 2011 20:32:41 +0000"  >&lt;p&gt;Edward, firstly, I also was the same problems/questions on other forums, unsolved. The issue is that we need hourly based access to our data in HDFS. Sometimes, we just need to process couple of ours quickly, and, sometimes, we need to process several monthes of data. We don&apos;t use &quot;hacky map side joins&quot;. Our custom reducer performs only aggregations (quite complicated), nothing more.&lt;br/&gt;
Hive manages all our workflow quite well. Actually, this still be our only problem and we hope to use Hive later on.&lt;br/&gt;
Ok, what&apos;s the final decision: cut this option&apos;s lenght to 1,5,10KB? Or imply another option which enables this option removing from environment variables?&lt;/p&gt;</comment>
                            <comment id="13173554" author="lramos85" created="Tue, 20 Dec 2011 22:11:37 +0000"  >&lt;p&gt;We are running into the same problem here, my list of input files is even larger. In my case we are using a TRANSFORM() python script to convert the output of the reducers to csv format. I confirmed that lowering the number of input files works by specifying a limited partition or merging the inputs. Either way, I don&apos;t think it should pass that entire list through environment variables. I can go with an option to remove that.&lt;/p&gt;</comment>
                            <comment id="13173795" author="lramos85" created="Wed, 21 Dec 2011 03:00:39 +0000"  >&lt;p&gt;I just wanted to note, that in my case where I use a stream script for TRANSFORM, which only happens at the very end, a workaround was to simply force it to have 2 stages so that by the last stage, the input list has been merged. I understand that REDUCE scripts will work differently. &lt;/p&gt;

&lt;p&gt;Also +1 on Sergey Triuber quick fix. But might not be a solid solution. Thanks.&lt;/p&gt;</comment>
                            <comment id="13188263" author="sdong" created="Wed, 18 Jan 2012 03:59:03 +0000"  >&lt;p&gt;is there anyone still working on this?&lt;/p&gt;</comment>
                            <comment id="13188326" author="lramos85" created="Wed, 18 Jan 2012 08:05:01 +0000"  >&lt;p&gt;That&apos;s a good question, if anyone official wants to vote on a patch or Sergey Tryuber HOTFIX. &lt;/p&gt;

&lt;p&gt;After applying the HOTFIX and running &quot;ant test&quot; I didn&apos;t see any issues. And this definitely fixed our current problems with the limitation. +1 Thanks.&lt;/p&gt;</comment>
                            <comment id="13194602" author="sergeant" created="Fri, 27 Jan 2012 10:46:27 +0000"  >&lt;p&gt;Well, guys, give me a week to prepare a patch (just have no time right now) that will truncate this variable to 20KB if string length is too large (I&apos;m going to hardcode this value) and prints WARN to log if such thing happend. &lt;/p&gt;</comment>
                            <comment id="13197887" author="sergeant" created="Wed, 1 Feb 2012 15:00:41 +0000"  >&lt;p&gt;Patch, 1st version&lt;/p&gt;</comment>
                            <comment id="13197895" author="sergeant" created="Wed, 1 Feb 2012 15:09:10 +0000"  >&lt;p&gt;I&apos;ve attached a patch (as an attachment, not by &quot;submit patch&quot;, as described on wiki HowToContribute). When I cloned trunk and run tests without any changes, for about 5 hours, there was several test errors((( Build and testing with my changes showed the same errors count. So, please, review this patch and make remarks.&lt;/p&gt;</comment>
                            <comment id="13199411" author="sdong" created="Fri, 3 Feb 2012 00:34:11 +0000"  >&lt;p&gt;Is 2048 per value too long?&lt;/p&gt;</comment>
                            <comment id="13199587" author="sergeant" created="Fri, 3 Feb 2012 08:25:25 +0000"  >&lt;p&gt;Limitation in my patch is 20KB, I don&apos;t think, that smaller limit is good idea, because in some cases user might be really want to set long values. Actually, in some cases 2KB (as you proposed) is even not enough for for &quot;hive.query.string&quot;. But, of course, if you insist, I&apos;ll set limitation to 1KB, just let me know about it.&lt;/p&gt;</comment>
                            <comment id="13203105" author="sdong" created="Wed, 8 Feb 2012 01:30:27 +0000"  >&lt;p&gt;How about make it configurable? (though I hate to add more and more parameters.)&lt;/p&gt;</comment>
                            <comment id="13203684" author="sergeant" created="Wed, 8 Feb 2012 15:54:54 +0000"  >&lt;p&gt;That&apos;s was my initial idea (you even can see that in my comments). But after that I&apos;ve changed my mind because I also &quot;hate to add more and more parameters&quot;. This bug affects may be 0.001% of Hive users and I can&apos;t figure out the case of those 0.001% users when 20KB is not enough. Make this limit less then 20KB also has no sense, because that&apos;s not a bottleneck on modern environment. So, introducing one more useless option, prepare documentation for it and compel new Hive users to read/understand it... &lt;/p&gt;</comment>
                            <comment id="13204037" author="lramos85" created="Wed, 8 Feb 2012 21:54:40 +0000"  >&lt;p&gt;As a hive user affected by this bug, I would have no problem with truncating the variable to 20KB. That should be enough to give you an idea of the input list. 2KB sounds low to me.&lt;/p&gt;</comment>
                            <comment id="13214997" author="appodictic" created="Thu, 23 Feb 2012 20:06:23 +0000"  >&lt;p&gt;Lets get this committed. I think we should introduce the variables. In the normal case we want someone to get all the input they would expect. If they run into an exception they can turn the variable on to get out of the problem. &lt;/p&gt;

&lt;p&gt;Lets do this. Rebase your patch and add hive.scriptoperator.truncate.env=true|false to control this feature. Default it to false which would be how hive works now. I will review and commit. &lt;/p&gt;</comment>
                            <comment id="13217938" author="sergeant" created="Tue, 28 Feb 2012 06:08:22 +0000"  >&lt;p&gt;Ok, got your point. Edward, could you answer some questions:&lt;br/&gt;
1. As I understood from sources, there is no hive-default.xml (it is deprecated)? &lt;br/&gt;
2. I&apos;m going to add one more entry to HiveConf#ConfVars. Is that right way? &lt;br/&gt;
3. Where is place (in code base) for inserting comments for properties, or it just hive wiki? &lt;/p&gt;</comment>
                            <comment id="13218339" author="appodictic" created="Tue, 28 Feb 2012 16:40:16 +0000"  >&lt;p&gt;1. There is a hive-default.xml.template you should add it there along with a short description&lt;br/&gt;
2. Yes adding a ConfVar is the right way&lt;br/&gt;
3. You can add some inline comments if you wish although generally if the property name is chosen correctly it usually explains itself.&lt;/p&gt;</comment>
                            <comment id="13219942" author="sergeant" created="Thu, 1 Mar 2012 09:35:54 +0000"  >&lt;p&gt;Patch that has hive.script.operator.truncate.env option that enables truncation.&lt;/p&gt;</comment>
                            <comment id="13219946" author="sergeant" created="Thu, 1 Mar 2012 09:45:53 +0000"  >&lt;p&gt;Edward, I&apos;ve named my option hive.script.operator.truncate.env (instead of hive.scriptoperator.truncate.env as you proposed), to be analogous of hive.script.operator.id.env.var. Actually, I have a question, why do we have hive-default.xml.template if all options (their names and default values) are hardcoded in ConfVar?&lt;/p&gt;

&lt;p&gt;Also, as I wrote in previous posts, there was some Hive tests failures in my &quot;ant test&quot; before. I just want to share this information with other developers: this problem was solved by changing locale in my system from Russian to EN. So, I have all tests successfully completed.&lt;/p&gt;

&lt;p&gt;And the last notice. Edward, could you add *.iml files to gitignore. I use Idea as IDE and it creates all those files.&lt;/p&gt;</comment>
                            <comment id="13262330" author="appodictic" created="Thu, 26 Apr 2012 02:36:11 +0000"  >&lt;p&gt;I will take take a look at this. You should mark as match available when you are ready for review.&lt;/p&gt;</comment>
                            <comment id="13283808" author="appodictic" created="Fri, 25 May 2012 23:17:02 +0000"  >&lt;p&gt;+1 tests pass will commit&lt;/p&gt;</comment>
                            <comment id="13283924" author="hudson" created="Sat, 26 May 2012 06:49:15 +0000"  >&lt;p&gt;Integrated in Hive-trunk-h0.21 #1450 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-h0.21/1450/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hive-trunk-h0.21/1450/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-2372&quot; title=&quot;java.io.IOException: error=7, Argument list too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-2372&quot;&gt;&lt;del&gt;HIVE-2372&lt;/del&gt;&lt;/a&gt; Argument list too long when streaming (Sergey Tryuber via egc) (Revision 1342841)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
ecapriolo : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1342841&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1342841&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/conf/hive-default.xml.template&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13547980" author="hudson" created="Wed, 9 Jan 2013 10:23:50 +0000"  >&lt;p&gt;Integrated in Hive-trunk-hadoop2 #54 (See &lt;a href=&quot;https://builds.apache.org/job/Hive-trunk-hadoop2/54/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hive-trunk-hadoop2/54/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-2372&quot; title=&quot;java.io.IOException: error=7, Argument list too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-2372&quot;&gt;&lt;del&gt;HIVE-2372&lt;/del&gt;&lt;/a&gt; Argument list too long when streaming (Sergey Tryuber via egc) (Revision 1342841)&lt;/p&gt;

&lt;p&gt;     Result = ABORTED&lt;br/&gt;
ecapriolo : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1342841&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1342841&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hive/trunk/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/conf/hive-default.xml.template&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/java/org/apache/hadoop/hive/ql/exec/ScriptOperator.java&lt;/li&gt;
	&lt;li&gt;/hive/trunk/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13550088" author="ashutoshc" created="Thu, 10 Jan 2013 19:53:20 +0000"  >&lt;p&gt;This issue is fixed and released as part of 0.10.0 release. If you find an issue which seems to be related to this one, please create a new jira and link this one with new jira.&lt;/p&gt;</comment>
                            <comment id="14002734" author="rharris" created="Tue, 20 May 2014 03:05:58 +0000"  >&lt;p&gt;While creating a table with a large number of columns, a large hive variable is temporarily created using SET, the variable contains the columns and column descriptions.&lt;/p&gt;

&lt;p&gt;A CREATE TABLE statement then successfully uses that large variable.&lt;/p&gt;

&lt;p&gt;After successfully creating the table the hive script attempts to load data into the table using a TRANSFORM script, triggering the error:&lt;br/&gt;
java.io.IOException: error=7, Argument list too long&lt;/p&gt;

&lt;p&gt;Since the variable is no longer used after the table is created, the hive script was updated to SET the large variable to empty.&lt;br/&gt;
After setting the variable empty the second statement in the hive script ran fine.&lt;/p&gt;</comment>
                            <comment id="14027993" author="sergeant" created="Wed, 11 Jun 2014 16:42:51 +0000"  >&lt;p&gt;Hi Ryan,&lt;/p&gt;

&lt;p&gt;Yes, your issue is very related. Hive passes properties to TRANSFORM script via environment variables. In the scope of this ticket I&apos;ve shortened only environment variable which stores information about partitions.&lt;/p&gt;

&lt;p&gt;In case of user-defined variables (via SET statement), I&apos;m not even sure that approach to shorten them is correct. May be it would be better just to fail with error before map-reduce job execution and ask user to unset the variable (as you did). But it is quite hard to judge what is length limitation, because it depends on OS (even in my patch, as I remember, I hardcoded the length and now it seems to be not the best choice). As an alternative, Hive can print a warning, but continue the execution. &lt;/p&gt;

&lt;p&gt;Anyway, this issue had been closed so much time ago (and applied patch really solves the problem in issue description) that I think it would be better to create a new one and lead all the discussion there. Don&apos;t you mind to do it?&lt;/p&gt;</comment>
                            <comment id="14028312" author="rharris" created="Wed, 11 Jun 2014 19:53:40 +0000"  >&lt;p&gt;Thanks Sergey, &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-7218&quot; title=&quot;java.io.IOException: error=7, Argument list too long&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-7218&quot;&gt;HIVE-7218&lt;/a&gt; created for continued tracking&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12512758" name="HIVE-2372.1.patch.txt" size="4595" author="sergeant" created="Wed, 1 Feb 2012 15:00:41 +0000"/>
                            <attachment id="12516666" name="HIVE-2372.2.patch.txt" size="7180" author="sergeant" created="Thu, 1 Mar 2012 09:35:53 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>42146</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 23 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i02uhz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>14542</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Committed thanks</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>