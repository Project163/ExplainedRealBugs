<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:51:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-15671] RPCServer.registerClient() erroneously uses server/client handshake timeout for connection timeout</title>
                <link>https://issues.apache.org/jira/browse/HIVE-15671</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  /**
   * Tells the RPC server to expect a connection from a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; client.
   * ...
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Future&amp;lt;Rpc&amp;gt; registerClient(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; clientId, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; secret,
      RpcDispatcher serverDispatcher) {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; registerClient(clientId, secret, serverDispatcher, config.getServerConnectTimeoutMs());
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;tt&gt;config.getServerConnectTimeoutMs()&lt;/tt&gt; returns value for &lt;b&gt;hive.spark.client.server.connect.timeout&lt;/b&gt;, which is meant for timeout for handshake between Hive client and remote Spark driver. Instead, the timeout should be &lt;b&gt;hive.spark.client.connect.timeout&lt;/b&gt;, which is for timeout for remote Spark driver in connecting back to Hive client.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13036412">HIVE-15671</key>
            <summary>RPCServer.registerClient() erroneously uses server/client handshake timeout for connection timeout</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="xuefuz">Xuefu Zhang</assignee>
                                    <reporter username="xuefuz">Xuefu Zhang</reporter>
                        <labels>
                    </labels>
                <created>Fri, 20 Jan 2017 02:49:55 +0000</created>
                <updated>Fri, 21 Jul 2017 18:36:02 +0000</updated>
                            <resolved>Mon, 13 Feb 2017 19:08:33 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15831072" author="xuefuz" created="Fri, 20 Jan 2017 02:55:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;/&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, could you please review?&lt;br/&gt;
cc: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15831115" author="vanzin" created="Fri, 20 Jan 2017 03:30:05 +0000"  >&lt;p&gt;Hmm, the options are poorly named (my fault, and they always confuse me when I look at them now), but the current use looks correct.&lt;/p&gt;

&lt;p&gt;&quot;client.connect.timeout&quot; is for the connection that the Spark driver opens to HS2.&lt;br/&gt;
&quot;server.connect.timeout&quot; is actually used in two places, but is basically the time allowed between HS2 starting the Spark driver, and the SASL handshake to finish.&lt;/p&gt;</comment>
                            <comment id="15831131" author="xuefuz" created="Fri, 20 Jan 2017 03:45:51 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;. To confirm, when you say &quot;the current use looks correct&quot;, do you mean with or without the patch? &lt;tt&gt;registerClient()&lt;/tt&gt; is called before the remote driver connecting back to HS2. For that, I think it should use client.connect.timeout. However, &lt;tt&gt;config.getServerConnectTimeoutMs()&lt;/tt&gt; returns the value for &lt;b&gt;server.connect.timeout&lt;/b&gt;. That&apos;s why I think the patch here is needed.&lt;/p&gt;</comment>
                            <comment id="15831138" author="vanzin" created="Fri, 20 Jan 2017 03:50:20 +0000"  >&lt;p&gt;I mean the current code without the patch.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;registerClient()&lt;/tt&gt; is called on the server side; so it basically starts the countdown for the &quot;SASL handshake&quot; timeout (which is what &quot;getServerConnectTimeoutMs()&quot; and is probably a better name for that method). The client should connect back and authenticate within that timeout.&lt;/p&gt;</comment>
                            <comment id="15831178" author="xuefuz" created="Fri, 20 Jan 2017 04:31:00 +0000"  >&lt;p&gt;Actually my understanding is a little different. Checking the code, I see:&lt;br/&gt;
1. On server side (RpcServer constructor), saslHandler is set a timeout using &lt;tt&gt;getServerConnectTimeoutMs()&lt;/tt&gt;.&lt;br/&gt;
2. On client side, in &lt;tt&gt;Rpc.createClient()&lt;/tt&gt;, saslHandler is also set a timeout using  &lt;tt&gt;getServerConnectTimeoutMs()&lt;/tt&gt;.&lt;br/&gt;
These two are consistent, which I don&apos;t see any issue.&lt;/p&gt;

&lt;p&gt;On the other hand, &lt;br/&gt;
3. On server side, in &lt;tt&gt;Repc.registerClient()&lt;/tt&gt;, ClientInfo stores &lt;tt&gt;getServerConnectTimeoutMs()&lt;/tt&gt;. And, the timeout happens, the exception is TimeoutException(&quot;Timed out waiting for client connection.&quot;).&lt;br/&gt;
4. On client side, in &lt;tt&gt;Rpc.createClient()&lt;/tt&gt;, the channel is initialized with &lt;tt&gt;getConnectTimeoutMs()&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;To me, it seems there is mismatch between 3 and 4. In 3, the timeout message implies &quot;connection timeout&quot;, while the value is what is supposed to be that for saslHandler handshake. This is why I think 3 should use &lt;tt&gt;getConnectTimeoutMs()&lt;/tt&gt; instead.&lt;/p&gt;

&lt;p&gt;Could you take another look?&lt;/p&gt;

&lt;p&gt;I actually ran into issues with this. Our cluster is constantly busy, and it takes minutes for the Hive to get a YARN container to launch the remote driver. In that case, the query fails with a failure of creating a spark session. For such a scenario, I supposed we should increase &lt;b&gt;client.connect.timeout&lt;/b&gt;. However, that&apos;s not effective. On the other hand, if I increase &lt;b&gt;server.connect.timeout&lt;/b&gt;, Hive waits longer  for the driver to come up, which is good. However, doing that has a bad consequence that Hive will wait as long to declare a failure if for any reason the remote driver becomes dead.&lt;/p&gt;

&lt;p&gt;With the patch in place, the problem is solved in both cases. I only need to increase &lt;b&gt;client.connect.timeout&lt;/b&gt; and keep &lt;b&gt;server.connect.timeout&lt;/b&gt; unchanged.&lt;/p&gt;</comment>
                            <comment id="15831452" author="lirui" created="Fri, 20 Jan 2017 09:36:02 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, I tried your patch locally. &lt;tt&gt;hive.spark.client.connect.timeout&lt;/tt&gt; defaults to 1000ms. But starting the RemoteDriver can easily take longer than that. Therefore my job just failed with &quot;Timed out waiting for client connection&quot;.&lt;br/&gt;
I&apos;m quite ignorant about the Rpc code. What I see is we have two timeout configs with different default value. And the specific code here needs the one with the bigger default value. I&apos;d really appreciate it if &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; could give more detailed explanations about the purposes of the two configs, and whether they&apos;re used inconsistently as Xuefu pointed out.&lt;/p&gt;

&lt;p&gt;Besides, the naming is really confusing to me, like SparkClient is the RpcServer, and we pass ClientProtocol to serverDispatcher etc. I understand the client/server concepts are probably reversed for HS2/RemoteDriver and Rpc. Wondering if it&apos;s better to make it somehow consistent.&lt;/p&gt;</comment>
                            <comment id="15832159" author="vanzin" created="Fri, 20 Jan 2017 17:41:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt; I see what you mean, but I think your analysis is slightly off.&lt;/p&gt;

&lt;p&gt;1 and 2 are actually where the problem, if any, is; 2 should use &lt;tt&gt;getConnectTimeoutMs()&lt;/tt&gt; instead of the server version. As Rui said, the &quot;server timeout&quot; here, which is actually the &quot;authentication timeout&quot;, needs to be much longer than the client timeout since it involves the time to start the driver.&lt;/p&gt;

&lt;p&gt;So basically: all calls made on the client side (= Spark driver) should use &lt;tt&gt;getConnectTimeoutMs()&lt;/tt&gt;, all calls made on the server side (= HS2) should use &lt;tt&gt;getServerConnectTimeoutMs()&lt;/tt&gt; (although, if I remember the code correct, the one timeout set up in &lt;tt&gt;registerClient()&lt;/tt&gt; ends up taking precedence over all others on the server path).&lt;/p&gt;

&lt;p&gt;&amp;gt; doing that has a bad consequence that Hive will wait as long to declare a failure if for any reason the remote driver becomes dead&lt;/p&gt;

&lt;p&gt;That&apos;s kinda hard to solve, because the server doesn&apos;t know which client connected until two things happen: first the driver has started, second the driver completed the SASL handshake to identify itself. A lot of things can go wrong in that time. There&apos;s already some code, IIRC, that fails the session if the spark-submit job dies with an error, but aside from that, it&apos;s kinda hard to do more.&lt;/p&gt;</comment>
                            <comment id="15832245" author="xuefuz" created="Fri, 20 Jan 2017 18:48:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;, thanks for your insight. I think we are approaching to something. I&apos;m going to change #2 to use &lt;tt&gt;getConnectTimeoutMs()&lt;/tt&gt; and try it out. Naming is one thing, but yes, the server-side timeout should be bigger. When I tested with my patch, I actually made &lt;b&gt;client.connect.timeout&lt;/b&gt; much bigger than &lt;b&gt;server.connect.timeout&lt;/b&gt; and that&apos;s why I didn&apos;t have the problem that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; got. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That&apos;s kinda hard to solve, because the server doesn&apos;t know which client connected until...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;My original problem (with no patch so ever) was about a busy cluster where it took longer time (up to 10m) to get a container to run the driver. To overcome that, I increased &lt;b&gt;server.connect.timeout&lt;/b&gt; to 10m which worked. With that, however, I got a different problem when the driver suddenly dies (due to OOM, for instance), at which point the driver had already connected back to Hive and the job was running. In such a case, Hive wouldn&apos;t detect the driver was gone until 10m later. My patch here was to solve this problem.&lt;/p&gt;

&lt;p&gt;With the new understanding, I&apos;d like to make sure that both the problems are solved: 1. user should be able to increase &lt;b&gt;server.connect.timeout&lt;/b&gt; to handler longer startup of the driver. 2. Hive should be able to immediately detect the death of the driver (after connection has been made).&lt;/p&gt;

&lt;p&gt;Any additional thoughts?&lt;/p&gt;</comment>
                            <comment id="15832257" author="vanzin" created="Fri, 20 Jan 2017 18:58:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;I got a different problem when the driver suddenly dies (due to OOM, for instance) ... Hive wouldn&apos;t detect the driver was gone until 10m later.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you mean it dies before the SASL handshake is complete, then in that case maybe my understanding that the server timeout applies to the whole connection + handshake is wrong and that should be fixed. i.e. the timeout set up in &lt;tt&gt;registerClient&lt;/tt&gt; should apply to the whole handshake and not only until there&apos;s a connection.&lt;/p&gt;

&lt;p&gt;But if it dies after the SASL handshake, then it seems like the problem is somewhere else and shouldn&apos;t really be related to either of these timeouts.&lt;/p&gt;</comment>
                            <comment id="15832582" author="xuefuz" created="Fri, 20 Jan 2017 23:11:13 +0000"  >&lt;p&gt;Patch #1 followed what &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; suggested. With it, I observed the following behavior:&lt;/p&gt;

&lt;p&gt;1. Increasing &lt;b&gt;server.connect.timeout&lt;/b&gt; will make hive wait longer for the driver to connect back, which solves the busy cluster problem.&lt;br/&gt;
2. Killing driver while the job is running immediately fails the query on Hive side with the following error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2017-01-20 22:01:08,235	Stage-2_0: 7(+3)/685	Stage-3_0: 0/1	
2017-01-20 22:01:09,237	Stage-2_0: 16(+6)/685	Stage-3_0: 0/1	
Failed to monitor Job[ 1] with exception &lt;span class=&quot;code-quote&quot;&gt;&apos;java.lang.IllegalStateException(RPC channel is closed.)&apos;&lt;/span&gt;
FAILED: Execution Error, &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This meets my expectation.&lt;/p&gt;

&lt;p&gt;However, I didn&apos;t test the case of driver death before connecting back to Hive. (It&apos;s also hard to construct such a test case.) In that case, I assume that Hive will wait for &lt;b&gt;server.connect.timeout&lt;/b&gt; before declaring a failure. I guess there isn&apos;t much we can do for this case. I don&apos;t think the change here has any implication on this.&lt;/p&gt;</comment>
                            <comment id="15832749" author="xuefuz" created="Sat, 21 Jan 2017 02:35:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;, could you please review the patch? Thanks.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, Could you also try and review the patch? Thanks.&lt;/p&gt;</comment>
                            <comment id="15832821" author="hiveqa" created="Sat, 21 Jan 2017 05:31:25 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12848640/HIVE-15671.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12848640/HIVE-15671.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 9 failed/errored test(s), 10974 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.testCliDriver[cascade_dbdrop] (batchId=226)
org.apache.hadoop.hive.cli.TestHBaseNegativeCliDriver.testCliDriver[generatehfiles_require_family_path] (batchId=226)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=135)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_schema_evol_3a] (batchId=136)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[limit_pushdown3] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part] (batchId=149)
org.apache.hive.service.cli.TestEmbeddedThriftBinaryCLIService.testTaskStatus (batchId=213)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3085/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3085/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3085/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3085/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-3085/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-3085/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12848640 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15834115" author="lirui" created="Mon, 23 Jan 2017 09:22:27 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, I tried your case but didn&apos;t reproduce your issue. Here&apos;s my findings (w/o patch):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;I set &lt;tt&gt;hive.spark.client.server.connect.timeout&lt;/tt&gt; to 10min and kill the driver during execution of the job. (Hive CLI + yarn-cluster mode)&lt;/li&gt;
	&lt;li&gt;Hive can detect the job failure instantly. But whether the CLI can return instantly (blocking on &lt;tt&gt;RemoteSparkJobMonitor.startMonitor&lt;/tt&gt;) depends on whether we&apos;re in the middle of retrieving job progress from the driver. If we&apos;re, CLI needs to wait for &lt;tt&gt;hive.spark.client.future.timeout&lt;/tt&gt;, default to 1min. If not, CLI returns instantly.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="15834977" author="vanzin" created="Mon, 23 Jan 2017 18:13:37 +0000"  >&lt;p&gt;The patch looks ok but I don&apos;t know how it relates to the issue you described. This is shortening the timeout on the client side, and you seemed to be concerned about some long timeout on the server side. This patch will just make sessions fail more quickly when the server is in a weird state and is taking long to reply to client messages.&lt;/p&gt;</comment>
                            <comment id="15835626" author="xuefuz" created="Tue, 24 Jan 2017 04:27:14 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I retried the case that Rui described. Yes, Hive detects the disconnection right after the driver process is killed. I think killing the process will close the socket, so the other end detects the problem right way.&lt;/p&gt;

&lt;p&gt;I guess my original problem is not about killing the driver process. Rather, if the connection between Hive and the driver is congested but not broken before the driver exists abnormally, Hive will not detect a broken connection, so it will not time out until &lt;b&gt;server.connect.timeout&lt;/b&gt; has elapsed. I agree the patch doesn&apos;t help this case. I also agree with Marcelo that the patch only makes the driver be more willing to exit if Hive happens to be busy.&lt;/p&gt;

&lt;p&gt;Let me step back and recap what I really need. 1. I want Hive to wait longer for the driver to connect back in case of a busy cluster. Increasing &lt;b&gt;server.connect.timeout&lt;/b&gt; solves the problem. 2. I also want Hive to detect a nonreachable driver early in case of network congestion or abnormal exit. In reality, #2 is also decided by &lt;b&gt;server.connect.timeout&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Now, I&apos;m wondering if it&apos;s possible to define a timeout for the initial connection (driver connecting back to Hive), and another timeout for subsequent communications between Hive and the driver. Is this possible at all?&lt;/p&gt;</comment>
                            <comment id="15835669" author="xuefuz" created="Tue, 24 Jan 2017 05:21:28 +0000"  >&lt;p&gt;As a side note, the patch here might be still needed. If Hive is busy or the connection between Hive and the driver is congested, we want the driver to go away quicker too. When this happen in one case, I saw that the driver, not knowing Hive was gone, was still allocating executors and killing them after 60s because they idled out. (This probably doesn&apos;t happen that much in normal conditions. I got this because apparently we are having networking issues at the moment.)&lt;/p&gt;</comment>
                            <comment id="15857738" author="kaixu" created="Wed, 8 Feb 2017 09:47:41 +0000"  >&lt;p&gt;I may encounter this situation you mentioned. I run a query, Hive on Spark, failed with error:&lt;br/&gt;
2017-02-08 09:50:59,331 Stage-2_0: 1039(+2)/1041        Stage-3_0: 796(+456)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1&lt;br/&gt;
2017-02-08 09:51:00,335 Stage-2_0: 1040(+1)/1041        Stage-3_0: 914(+398)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1&lt;br/&gt;
2017-02-08 09:51:01,338 Stage-2_0: 1041/1041 Finished   Stage-3_0: 961(+383)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1&lt;br/&gt;
Failed to monitor Job[ 2] with exception &apos;java.lang.IllegalStateException(RPC channel is closed.)&apos;&lt;br/&gt;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask&lt;/p&gt;

&lt;p&gt;the driver was indeed failed with some unknown reason:&lt;/p&gt;

&lt;p&gt;17/02/08 09:51:04 INFO exec.Utilities: PLAN PATH = hdfs://hsx-node1:8020/tmp/hive/root/b723c85d-2a7b-469e-bab1-9c165b25e656/hive_2017-02-08_09-49-37_890_6267025825539539056-1/-mr-10006/71a9dacb-a463-40ef-9e86-78d3b8e3738d/map.xml&lt;br/&gt;
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1169.0 in stage 3.0 (TID 2519)&lt;br/&gt;
17/02/08 09:51:04 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown&lt;br/&gt;
17/02/08 09:51:04 INFO storage.MemoryStore: MemoryStore cleared&lt;br/&gt;
17/02/08 09:51:04 INFO storage.BlockManager: BlockManager stopped&lt;br/&gt;
17/02/08 09:51:04 INFO exec.Utilities: PLAN PATH = hdfs://hsx-node1:8020/tmp/hive/root/b723c85d-2a7b-469e-bab1-9c165b25e656/hive_2017-02-08_09-49-37_890_6267025825539539056-1/-mr-10006/71a9dacb-a463-40ef-9e86-78d3b8e3738d/map.xml&lt;br/&gt;
17/02/08 09:51:04 WARN executor.CoarseGrainedExecutorBackend: An unknown (hsx-node1:42777) driver disconnected.&lt;br/&gt;
17/02/08 09:51:04 ERROR executor.CoarseGrainedExecutorBackend: Driver 192.168.1.1:42777 disassociated! Shutting down.&lt;br/&gt;
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1105.0 in stage 3.0 (TID 2511)&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called&lt;br/&gt;
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk6/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-71da1dfc-99bd-4687-bc2f-33452db8de3d&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk2/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-7f134d81-e77e-4b92-bd99-0a51d0962c14&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk5/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-77a90d63-fb05-4bc6-8d5e-1562cc502e6c&lt;br/&gt;
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk4/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-91f8b91a-114d-4340-8560-d3cd085c1cd4&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk1/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-a3c24f9e-8609-48f0-9d37-0de7ae06682a&lt;br/&gt;
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk7/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-f6120a43-2158-4780-927c-c5786b78f53e&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk3/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-e17931ad-9e8a-45da-86f8-9a0fdca0fad1&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk8/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-4de34175-f871-4c28-8ec0-d2fc0020c5c3&lt;br/&gt;
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1137.0 in stage 3.0 (TID 2515)&lt;br/&gt;
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 897.0 in stage 3.0 (TID 2417)&lt;br/&gt;
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 1225.0 in stage 3.0 (TID 2526)&lt;br/&gt;
17/02/08 09:51:04 INFO executor.Executor: Executor killed task 905.0 in stage 3.0 (TID 2423)&lt;/p&gt;

&lt;p&gt;in hive&apos;s log, &lt;/p&gt;

&lt;p&gt;2017-02-08T09:51:04,327  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 INFO scheduler.TaskSetManager: Finished task 971.0 in stage 3.0 (TID 2218) in 5948 ms on hsx-node8 (1338/1520)&lt;br/&gt;
2017-02-08T09:51:04,346  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: &lt;span class=&quot;error&quot;&gt;&amp;#91;DriverProtocol&amp;#93;&lt;/span&gt; Closing channel due to exception in pipeline (org.apache.hive.spark.client.RemoteDriver$DriverProtocol.handle(io.netty.channel.ChannelHandlerContext, org.apache.hive.spark.client.rpc.Rpc$MessageHeader)).&lt;br/&gt;
2017-02-08T09:51:04,346  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.RpcDispatcher: &lt;span class=&quot;error&quot;&gt;&amp;#91;DriverProtocol&amp;#93;&lt;/span&gt; Expected RPC header, got org.apache.hive.spark.client.rpc.Rpc$NullMessage instead.&lt;br/&gt;
2017-02-08T09:51:04,347  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: &lt;span class=&quot;error&quot;&gt;&amp;#91;DriverProtocol&amp;#93;&lt;/span&gt; Closing channel due to exception in pipeline (null).&lt;br/&gt;
2017-02-08T09:51:04,347  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;RPC-Handler-3&amp;#93;&lt;/span&gt; rpc.RpcDispatcher: &lt;span class=&quot;error&quot;&gt;&amp;#91;ClientProtocol&amp;#93;&lt;/span&gt; Closing channel due to exception in pipeline (Connection reset by peer).&lt;br/&gt;
2017-02-08T09:51:04,347  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception&lt;br/&gt;
2017-02-08T09:51:04,347  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.&lt;br/&gt;
2017-02-08T09:51:04,347  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at com.google.common.base.Preconditions.checkState(Preconditions.java:149)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.Rpc: Failed to send RPC, closing connection.&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: java.nio.channels.ClosedChannelException&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 WARN client.RemoteDriver: Shutting down driver because RPC channel was closed.&lt;br/&gt;
2017-02-08T09:51:04,348  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 INFO client.RemoteDriver: Shutting down remote driver.&lt;/p&gt;

&lt;p&gt;2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at com.google.common.base.Preconditions.checkState(Preconditions.java:149)&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)&lt;br/&gt;
2017-02-08T09:51:04,349  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 INFO scheduler.DAGScheduler: Asked to cancel job 2&lt;br/&gt;
2017-02-08T09:51:04,350  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception&lt;/p&gt;

&lt;p&gt;2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: java.lang.InterruptedException&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at java.lang.Object.wait(Native Method)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at java.lang.Object.wait(Object.java:502)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.JobWaiter.awaitResult(JobWaiter.scala:73)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.SimpleFutureAction.org$apache$spark$SimpleFutureAction$$awaitResult(FutureAction.scala:165)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:120)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.SimpleFutureAction.ready(FutureAction.scala:108)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.concurrent.Await$.ready(package.scala:86)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:303)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:362)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 ERROR scheduler.LiveListenerBus: Listener ClientListener threw an exception&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: java.lang.IllegalStateException: RPC channel is closed.&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at com.google.common.base.Preconditions.checkState(Preconditions.java:149)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:259)&lt;br/&gt;
2017-02-08T09:51:04,351  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$DriverProtocol.sendMetrics(RemoteDriver.java:270)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.hive.spark.client.RemoteDriver$ClientListener.onTaskEnd(RemoteDriver.java:490)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:42)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:55)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AsynchronousListenerBus.scala:80)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(AsynchronousListenerBus.scala:65)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)&lt;br/&gt;
2017-02-08T09:51:04,352  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl:  at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:64)&lt;/p&gt;

&lt;p&gt;2017-02-08T09:51:04,654  INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;stderr-redir-1&amp;#93;&lt;/span&gt; client.SparkClientImpl: 17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-65f40590-d87f-4701-b374-6b3b2a11538c&lt;br/&gt;
2017-02-08T09:52:04,346  WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;b723c85d-2a7b-469e-bab1-9c165b25e656 main&amp;#93;&lt;/span&gt; impl.RemoteSparkJobStatus: Error getting stage info&lt;br/&gt;
java.util.concurrent.TimeoutException&lt;br/&gt;
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;netty-all-4.0.23.Final.jar:4.0.23.Final&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageInfo(RemoteSparkJobStatus.java:161) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.getSparkStageProgress(RemoteSparkJobStatus.java:96) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.startMonitor(RemoteSparkJobMonitor.java:82) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobRef.monitorJob(RemoteSparkJobRef.java:60) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.spark.SparkTask.execute(SparkTask.java:101) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1997) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1688) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1419) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:430) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.8.0_60&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.8.0_60&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.8.0_60&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:497) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;?:1.8.0_60&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.6.2-hadoop2.6.0.jar:1.6.2&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;spark-assembly-1.6.2-hadoop2.6.0.jar:1.6.2&amp;#93;&lt;/span&gt;&lt;br/&gt;
2017-02-08T09:52:04,346 ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;b723c85d-2a7b-469e-bab1-9c165b25e656 main&amp;#93;&lt;/span&gt; status.SparkJobMonitor: Failed to monitor Job[ 2] with exception &apos;java.lang.IllegalStateException(RPC channel is closed.)&apos;&lt;br/&gt;
java.lang.IllegalStateException: RPC channel is closed.&lt;br/&gt;
        at com.google.common.base.Preconditions.checkState(Preconditions.java:149) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;guava-14.0.1.jar:?&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.hive.spark.client.rpc.Rpc.call(Rpc.java:276) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;also in container&apos;s log, I find Driver still request for executors:&lt;/p&gt;

&lt;p&gt;17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 77 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 2 executor containers&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 76 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 75 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 74 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 73 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 71 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 2 executor containers&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Driver requested a total number of 70 executor(s).&lt;br/&gt;
17/02/08 09:51:00 INFO yarn.YarnAllocator: Canceling requests for 1 executor containers&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.YarnAllocator: Driver requested a total number of 50 executor(s).&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.YarnAllocator: Canceling requests for 0 executor containers&lt;br/&gt;
17/02/08 09:51:04 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.YarnAllocator: Canceling requests for 0 executor containers&lt;br/&gt;
17/02/08 09:51:04 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. 192.168.1.1:42777&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. hsx-node1:42777&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED&lt;br/&gt;
17/02/08 09:51:04 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.&lt;br/&gt;
17/02/08 09:51:04 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1486453422616_0150&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called&lt;/p&gt;


&lt;p&gt;So, is this situation the client to server timeout? I only set hive.spark.job.monitor.timeout=3600s;&lt;/p&gt;</comment>
                            <comment id="15857739" author="kaixu" created="Wed, 8 Feb 2017 09:49:51 +0000"  >&lt;p&gt;I am also very confused about these timeouts.&lt;/p&gt;</comment>
                            <comment id="15857950" author="xuefuz" created="Wed, 8 Feb 2017 12:55:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=KaiXu&quot; class=&quot;user-hover&quot; rel=&quot;KaiXu&quot;&gt;KaiXu&lt;/a&gt;, your case might be different. The connection is explicitly closed due to &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2017-02-08T09:51:04,346 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 WARN rpc.RpcDispatcher: [DriverProtocol] Expected RPC header, got org.apache.hive.spark.client.rpc.Rpc$NullMessage instead.
2017-02-08T09:51:04,347 INFO [stderr-redir-1] client.SparkClientImpl: 17/02/08 09:51:04 INFO rpc.RpcDispatcher: [DriverProtocol] Closing channel due to exception in pipeline (&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;).
2017-02-08T09:51:04,347 INFO [RPC-Handler-3] rpc.RpcDispatcher: [ClientProtocol] Closing channel due to exception in pipeline (Connection reset by peer).
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I haven&apos;t seen this kind of exception, so am not sure how it happens. If this can be reproduced, complete logs (driver, hive, yarn) would be helpful.&lt;/p&gt;</comment>
                            <comment id="15857975" author="kaixu" created="Wed, 8 Feb 2017 13:22:58 +0000"  >&lt;p&gt;this error occurs when several queries run at the same time with large data scale, in fact it would not occur when running the query separately, but it can frequently occur when running together again.&lt;/p&gt;

&lt;p&gt;the connection is closed suddenly, seems to be killed manually.  &lt;br/&gt;
2017-02-08 09:51:01,338 Stage-2_0: 1041/1041 Finished   Stage-3_0: 961(+383)/1520       Stage-4_0: 0/2021       Stage-5_0: 0/1009       Stage-6_0: 0/1&lt;br/&gt;
Failed to monitor Job[ 2] with exception &apos;java.lang.IllegalStateException(RPC channel is closed.)&apos;&lt;br/&gt;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask&lt;/p&gt;

&lt;p&gt;found only one ERROR in yarn application log, it seems the driver was closed but not know what caused it close, above comment is hive&apos;s log, any suggestions shall be appreciated!&lt;/p&gt;

&lt;p&gt;17/02/08 09:51:00 INFO executor.Executor: Finished task 1492.0 in stage 3.0 (TID 2168). 3294 bytes result sent to driver&lt;br/&gt;
17/02/08 09:51:00 INFO executor.Executor: Finished task 556.0 in stage 3.0 (TID 1587). 3312 bytes result sent to driver&lt;br/&gt;
17/02/08 09:51:00 INFO executor.Executor: Finished task 1412.0 in stage 3.0 (TID 2136). 3294 bytes result sent to driver&lt;br/&gt;
17/02/08 09:51:00 INFO executor.Executor: Finished task 1236.0 in stage 3.0 (TID 2007). 3294 bytes result sent to driver&lt;br/&gt;
17/02/08 09:51:04 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown&lt;br/&gt;
17/02/08 09:51:04 INFO storage.MemoryStore: MemoryStore cleared&lt;br/&gt;
17/02/08 09:51:04 INFO storage.BlockManager: BlockManager stopped&lt;br/&gt;
17/02/08 09:51:04 WARN executor.CoarseGrainedExecutorBackend: An unknown (hsx-node1:42777) driver disconnected.&lt;br/&gt;
17/02/08 09:51:04 ERROR executor.CoarseGrainedExecutorBackend: Driver 192.168.1.1:42777 disassociated! Shutting down.&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Shutdown hook called&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk8/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-a8167f0b-f3c3-458f-ad51-8a0f4bcda4f3&lt;br/&gt;
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.&lt;br/&gt;
17/02/08 09:51:04 INFO util.ShutdownHookManager: Deleting directory /mnt/disk1/yarn/nm/usercache/root/appcache/application_1486453422616_0150/spark-26cba445-66d2-4b78-a428-17881c92f0f6&lt;br/&gt;
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.&lt;br/&gt;
17/02/08 09:51:04 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.&lt;/p&gt;</comment>
                            <comment id="15859267" author="lirui" created="Thu, 9 Feb 2017 09:46:27 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, can you get a thread dump of HS2/CLI when you hit the 2nd problem? Then we can find out how the JVM is hanging.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=KaiXu&quot; class=&quot;user-hover&quot; rel=&quot;KaiXu&quot;&gt;KaiXu&lt;/a&gt;, I also saw some similar issue before. Please feel free to open a JIRA for it and continue the investigation there. Thanks.&lt;/p&gt;</comment>
                            <comment id="15861113" author="xuefuz" created="Fri, 10 Feb 2017 11:10:25 +0000"  >&lt;p&gt;Thanks, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, I will try to reproduce, though it might be hard to do so.&lt;/p&gt;</comment>
                            <comment id="15861114" author="xuefuz" created="Fri, 10 Feb 2017 11:10:36 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;, to backtrack a little bit, I have a followup question about your comment.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;That&apos;s kinda hard to solve, because the server doesn&apos;t know which client connected until two things happen: first the driver has started, second the driver completed the SASL handshake to identify itself. A lot of things can go wrong in that time. There&apos;s already some code, IIRC, that fails the session if the spark-submit job dies with an error, but aside from that, it&apos;s kinda hard to do more.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was talking about server detecting a driver problem after it has connected back to the server. I&apos;m wondering which timeout applies in case of a problem on the driver side, such as long GC, stall connection between the server and the driver, etc. It&apos;s kind of long if this timeout is also server.connect.timeout, which is increased to 10m in our case to accommodate for the busy cluster. To me it doesn&apos;t seem that such a timeout exist, in absence of a heartbeat mechanism.&lt;/p&gt;</comment>
                            <comment id="15861241" author="kaixu" created="Fri, 10 Feb 2017 13:18:15 +0000"  >&lt;p&gt;I created &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15859&quot; title=&quot;HoS: Write RPC messages in event loop&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-15859&quot;&gt;&lt;del&gt;HIVE-15859&lt;/del&gt;&lt;/a&gt; for the issue, comments or suggestions are welcomed. Thanks!&lt;/p&gt;</comment>
                            <comment id="15861611" author="vanzin" created="Fri, 10 Feb 2017 18:06:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;I was talking about server detecting a driver problem after it has connected back to the server.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm. That is definitely not any of the &quot;connect&quot; timeouts, which probably means it isn&apos;t configured and is just using netty&apos;s default (which is probably no timeout?). Would probably need something using &lt;tt&gt;io.netty.handler.timeout.IdleStateHandler&lt;/tt&gt;, and also some periodic &quot;ping&quot; so that the connection isn&apos;t torn down without reason.&lt;/p&gt;</comment>
                            <comment id="15863989" author="xuefuz" created="Mon, 13 Feb 2017 17:06:23 +0000"  >&lt;p&gt;Thanks for the input, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;! For what you suggested, I think it is deemed for more investigation and development. Since Hive will monitor job after the driver connected back, hopefully, the monitoring thread will detect any network/driver issue. With &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15860&quot; title=&quot;RemoteSparkJobMonitor may hang when RemoteDriver exits abnormally&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-15860&quot;&gt;&lt;del&gt;HIVE-15860&lt;/del&gt;&lt;/a&gt;, the issue might have been resolved.&lt;/p&gt;

&lt;p&gt;I will create a separate JIRA for your proposal. In the mean time, I think Patch #1 is still needed as we also like the driver to detect any issue with Hive sooner. What do you think? Thanks.&lt;/p&gt;</comment>
                            <comment id="15864066" author="vanzin" created="Mon, 13 Feb 2017 17:46:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;In the mean time, I think Patch #1 is still needed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds fine to me.&lt;/p&gt;</comment>
                            <comment id="15864186" author="jxiang" created="Mon, 13 Feb 2017 18:45:39 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="15864231" author="xuefuz" created="Mon, 13 Feb 2017 19:08:33 +0000"  >&lt;p&gt;Committed to master. Thanks to Marchelo, Jimmy, and Rui for the review.&lt;br/&gt;
Created &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15893&quot; title=&quot;Followup on HIVE-15671&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-15893&quot;&gt;&lt;del&gt;HIVE-15893&lt;/del&gt;&lt;/a&gt; as a followup.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13047177">HIVE-16071</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13042651">HIVE-15893</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12848640" name="HIVE-15671.1.patch" size="716" author="xuefuz" created="Fri, 20 Jan 2017 22:38:22 +0000"/>
                            <attachment id="12848453" name="HIVE-15671.patch" size="785" author="xuefuz" created="Fri, 20 Jan 2017 02:53:00 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 40 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i38ytj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>