<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:22:08 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-7540] NotSerializableException encountered when using sortByKey transformation</title>
                <link>https://issues.apache.org/jira/browse/HIVE-7540</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;This exception is thrown when sortByKey is used as the shuffle transformation between MapWork and ReduceWork:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.hadoop.io.BytesWritable&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1049)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1033)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1031)&lt;br/&gt;
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1031)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:772)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:715)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:719)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:718)&lt;br/&gt;
    at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:718)&lt;br/&gt;
    at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:699)&lt;br/&gt;
&#8230;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt; The root cause is that the RangePartitioner used by sortByKey contains rangeBounds: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;BytesWritable&amp;#93;&lt;/span&gt;, which is considered not serializable in spark.&lt;br/&gt;
A workaround to this issue is to set the number of partitions to 1 when calling sortByKey, in which case the rangeBounds will be just an empty array.&lt;/p&gt;

&lt;p&gt;NO PRECOMMIT TESTS. This is for spark branch only.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Spark-1.0.1&lt;/p&gt;</environment>
        <key id="12730450">HIVE-7540</key>
            <summary>NotSerializableException encountered when using sortByKey transformation</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lirui">Rui Li</assignee>
                                    <reporter username="lirui">Rui Li</reporter>
                        <labels>
                    </labels>
                <created>Tue, 29 Jul 2014 10:14:09 +0000</created>
                <updated>Fri, 29 May 2015 02:31:33 +0000</updated>
                            <resolved>Mon, 11 Aug 2014 05:11:05 +0000</resolved>
                                                    <fixVersion>1.1.0</fixVersion>
                                    <component>Spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14085539" author="brocknoland" created="Tue, 5 Aug 2014 00:13:55 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;It seems this related to serializing a closure as opposed to data. I talked with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sandyr&quot; class=&quot;user-hover&quot; rel=&quot;sandyr&quot;&gt;sandyr&lt;/a&gt; and it seems that using Writable serialization is probably difficult/impossible. It looks like the Spark folks have also tried using Kryo to &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/spark-dev/201405.mbox/%3CCAPh_B=Z3mNZp4A=B3M9TnQqe6n+foB6fQSmLEkVwB+mUbb=Akg@mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;serialize closures&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Can you try creating a class HiveBytesWritable which extends from BytesWritable and implements Serializable and then transform the objects into that class before the soryByKey?&lt;/p&gt;</comment>
                            <comment id="14085662" author="lirui" created="Tue, 5 Aug 2014 02:00:03 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brocknoland&quot; class=&quot;user-hover&quot; rel=&quot;brocknoland&quot;&gt;brocknoland&lt;/a&gt;, we&apos;re using BytesWritable because the key of the RDD we create is of that type. Do you mean we should apply some map function to the RDD before (and after?) the sortByKey transformation?&lt;/p&gt;</comment>
                            <comment id="14085668" author="brocknoland" created="Tue, 5 Aug 2014 02:07:22 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I am hoping we don&apos;t have to transform back since it will be a subclass of BytesWritable. If it works and we don&apos;t have a better solution in the long term we might have to transform to our special type directly after reading from hdfs. We can deal with that later though.&lt;/p&gt;</comment>
                            <comment id="14085670" author="lirui" created="Tue, 5 Aug 2014 02:10:08 +0000"  >&lt;p&gt;OK got it. Let me try it out.&lt;/p&gt;</comment>
                            <comment id="14085695" author="xuefuz" created="Tue, 5 Aug 2014 02:33:47 +0000"  >&lt;p&gt;It&apos;s good to try this out. However, I still think the right solution is in Spark. Any additional processing at per row level will make performance suffer. If there is absolutely impossible in Spark, it&apos;s helpful to provide a clear reason for that. &lt;/p&gt;</comment>
                            <comment id="14085717" author="brocknoland" created="Tue, 5 Aug 2014 03:02:38 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;I agree it will cause performance to suffer. However, this will get us past a blocker and doesn&apos;t introduce fundamental performance issues in the design so I think we should optimize this later, once we have the functionality working.&lt;/p&gt;</comment>
                            <comment id="14085755" author="sandyr" created="Tue, 5 Aug 2014 03:59:55 +0000"  >&lt;p&gt;There are two main cases where serialization occurs in Spark&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Serializing each row for shuffling or caching&lt;/li&gt;
	&lt;li&gt;Serializing the functions and data required to execute a task to send from the driver to the executors&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;My understanding is that the error here is caused by serialization in the latter, which has minimal performance impact.  For per-row serialization, I agree that using Writable serialization is a worthy goal.  Serializing Writables in task closures is both much more difficult and will have minimal performance benefit.&lt;/p&gt;</comment>
                            <comment id="14085766" author="brocknoland" created="Tue, 5 Aug 2014 04:11:40 +0000"  >&lt;p&gt;Thx Sandy. The performance discussion here relates to the workaround...a row by roe transform to a serializable type.&lt;/p&gt;

&lt;p&gt;Can you speak to the challenges with using writable serialization for closures.&lt;/p&gt;</comment>
                            <comment id="14085786" author="sandyr" created="Tue, 5 Aug 2014 04:33:25 +0000"  >&lt;p&gt;In general a Spark closure is a Serializable object.  The proposal would be to allow objects underneath this object to be Writable, but not Serializable.  I&apos;m not aware of a way to tell Java serialization to listen for objects that implement a certain type as it navigates the object graph and use a custom serialization for them.&lt;/p&gt;

&lt;p&gt;I&apos;m looking at the RangePartitioner code now and it might be possible to use custom serialization for the range bounds.  I just don&apos;t see a way to do it in the general case.&lt;/p&gt;</comment>
                            <comment id="14085821" author="brocknoland" created="Tue, 5 Aug 2014 05:22:23 +0000"  >&lt;p&gt;Thanks Sandy. For the specific RangePartitioner case, I think we could use writeObject/readObject which are &lt;a href=&quot;http://docs.oracle.com/javase/7/docs/api/java/io/Serializable.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;optional methods&lt;/a&gt; Serializable&apos;s can implement.&lt;/p&gt;

&lt;p&gt;If the number of cases where we need to serialize a writable via java serialization is small, then providing point solutions using writeObject/readObject in RangePartitioner might be a reasonable fix. If anyone has a feeling for the number of times we will end up hitting this, please speak up. In the absence of more information, I would suggest we work around this issue on the Hive side and then once we&apos;ve gathered more information (how many times we need to serialize a writable in a closure, performance impact, etc) we can decide on how to proceed.&lt;/p&gt;</comment>
                            <comment id="14085875" author="xuefuz" created="Tue, 5 Aug 2014 06:40:39 +0000"  >&lt;p&gt;Here is what I have in mind:&lt;br/&gt;
1. per-row serialization should be using Writable interface. Right now Hive (on Spark) is using Kryo, which is a workaround rather than a solution. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hshreedharan&quot; class=&quot;user-hover&quot; rel=&quot;hshreedharan&quot;&gt;hshreedharan&lt;/a&gt; has some proposal for this, and in my opinion we should push for it.&lt;/p&gt;

&lt;p&gt;2. it&apos;s acceptable to use Kryo to serialize non-per-row objects. For this particular case, RangePartitioner should allow kryo if there is a problem with Writable. Especially, Hive already sets serializer=kryo.&lt;/p&gt;

&lt;p&gt;3. java serialization should be avoided by all means. Obviously, Spark is tied with this very much, but Hive should not rely on that.&lt;/p&gt;

&lt;p&gt;4. the proposed workaround mentioned above seems solving the problem of #2 while putting a per-row penalty. I&apos;m concerned about this.&lt;/p&gt;

&lt;p&gt;In general, I like the idea of putting in workaround to allow the project to proceed, but I&apos;d also like the idea of not hiding the underneath real problem just because of the existence of an unacceptable workaround.&lt;/p&gt;
</comment>
                            <comment id="14085889" author="brocknoland" created="Tue, 5 Aug 2014 06:51:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;the proposed workaround mentioned above seems solving the problem of #2 while putting a per-row penalty. I&apos;m concerned about this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I too am concerned about this as it&apos;s potentially an expensive workaround. Implementing the &quot;expensive&quot; workaround here simply unblocks us, it doesn&apos;t &quot;hide&quot; the problem. We can open another JIRA and discuss the final solution while being unblocked here and giving us more time to gather information. That&apos;s valuable IMO.&lt;/p&gt;</comment>
                            <comment id="14085898" author="xuefuz" created="Tue, 5 Aug 2014 06:59:53 +0000"  >&lt;p&gt;I&apos;d be happy as long as we&apos;re not taking the &quot;workaround&quot; as the final solution. Thanks, Brock. Let&apos;s try to have the real solution from Spark.&lt;/p&gt;</comment>
                            <comment id="14085910" author="xuefuz" created="Tue, 5 Aug 2014 07:10:28 +0000"  >&lt;p&gt;BTW, if we are just looking for a workaround that moves the project forward, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; proposed to set the partition number to one, which seems simpler and quicker.&lt;/p&gt;</comment>
                            <comment id="14085912" author="brocknoland" created="Tue, 5 Aug 2014 07:12:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;d be happy as long as we&apos;re not taking the &quot;workaround&quot; as the final solution. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed 100%. In my original statement I said, we&apos;d only use this solution if we had no better option.&lt;/p&gt;

&lt;p&gt;Let&apos;s move the medium/long term discussion over to &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-7614&quot; title=&quot;Find solution for closures containing writables [Spark Branch]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-7614&quot;&gt;&lt;del&gt;HIVE-7614&lt;/del&gt;&lt;/a&gt; and use this JIRA to unblock the project.&lt;/p&gt;</comment>
                            <comment id="14085921" author="brocknoland" created="Tue, 5 Aug 2014 07:18:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;BTW, if we are just looking for a workaround that moves the project forward, Rui Li proposed to set the partition number to one, which seems simpler and quicker.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s not clear to me how long &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-7614&quot; title=&quot;Find solution for closures containing writables [Spark Branch]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-7614&quot;&gt;&lt;del&gt;HIVE-7614&lt;/del&gt;&lt;/a&gt; will take to resolve. I don&apos;t see any simple answers. Thus I would suggest we move the functionality of the project forward while giving us time to think through the long term solution to this problem.&lt;/p&gt;</comment>
                            <comment id="14086005" author="sandyr" created="Tue, 5 Aug 2014 08:39:24 +0000"  >&lt;p&gt;Just to make sure, was Kryo serialization turned on when you ran into this exception? On closer look, it appears that the Spark code is already trying to handle this situation.&lt;/p&gt;</comment>
                            <comment id="14086009" author="sandyr" created="Tue, 5 Aug 2014 08:45:32 +0000"  >&lt;p&gt;Actually, this handling hasn&apos;t made it into a release yet - &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2104&quot; title=&quot;RangePartitioner should use user specified serializer to serialize range bounds&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2104&quot;&gt;&lt;del&gt;SPARK-2104&lt;/del&gt;&lt;/a&gt;, but will be in 1.1.&lt;/p&gt;</comment>
                            <comment id="14086011" author="brocknoland" created="Tue, 5 Aug 2014 08:47:26 +0000"  >&lt;p&gt;Nice, the fix might already be in place. Let&apos;s retry with with 1.1?&lt;/p&gt;</comment>
                            <comment id="14086012" author="lirui" created="Tue, 5 Aug 2014 08:47:48 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sandyr&quot; class=&quot;user-hover&quot; rel=&quot;sandyr&quot;&gt;sandyr&lt;/a&gt;, I set spark.serializer to KryoSerializer when I met this problem.&lt;br/&gt;
I noted that spark.closure.serializer is used when DAGScheduler tries to serialize a task. But spark.closure.serializer currently only supports java serializer.&lt;/p&gt;</comment>
                            <comment id="14086035" author="brocknoland" created="Tue, 5 Aug 2014 09:22:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, it seems in Spark 1.1 RangePartitioner uses spark.serializer (&lt;a href=&quot;https://github.com/apache/spark/commit/66135a341d9f8baecc149d13ae5511f14578c395&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/66135a341d9f8baecc149d13ae5511f14578c395&lt;/a&gt;). Can you retry this on the spark 1.1 branch?&lt;/p&gt;</comment>
                            <comment id="14086059" author="lirui" created="Tue, 5 Aug 2014 10:07:17 +0000"  >&lt;p&gt;Oh sure. It&apos;s great if that&apos;s already fixed.&lt;/p&gt;</comment>
                            <comment id="14086121" author="lirui" created="Tue, 5 Aug 2014 11:16:56 +0000"  >&lt;p&gt;I&apos;ve tested with spark 1.1 branch, and verified the NotSerializableException is gone.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brocknoland&quot; class=&quot;user-hover&quot; rel=&quot;brocknoland&quot;&gt;brocknoland&lt;/a&gt; should we make hive use spark-1.1.0-SNAPSHOT to have this fix?&lt;/p&gt;</comment>
                            <comment id="14086326" author="brocknoland" created="Tue, 5 Aug 2014 14:40:59 +0000"  >&lt;p&gt;Yes, since Spark has already implemented the &quot;point&quot; solution let&apos;s go with that! I resolved &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-7540&quot; title=&quot;NotSerializableException encountered when using sortByKey transformation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-7540&quot;&gt;&lt;del&gt;HIVE-7540&lt;/del&gt;&lt;/a&gt; as a dup due to this new information.&lt;/p&gt;</comment>
                            <comment id="14087166" author="lirui" created="Wed, 6 Aug 2014 02:52:18 +0000"  >&lt;p&gt;Use spark-1.1.0-SNAPSHOT to solve this issue.&lt;br/&gt;
SortByShuffler is changed accordingly since previously we have to set numPartitions to 1 to workaround this issue.&lt;/p&gt;

&lt;p&gt;Please note since spark-1.1 is not released yet, we&apos;ll have to build it locally and install to the local maven repo.&lt;/p&gt;</comment>
                            <comment id="14087169" author="lirui" created="Wed, 6 Aug 2014 02:54:58 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brocknoland&quot; class=&quot;user-hover&quot; rel=&quot;brocknoland&quot;&gt;brocknoland&lt;/a&gt;, I&apos;ve uploaded a patch. But I&apos;m afraid it will break the build since spark-1.1 is not available yet.&lt;/p&gt;</comment>
                            <comment id="14087191" author="brocknoland" created="Wed, 6 Aug 2014 03:15:03 +0000"  >&lt;p&gt;Thank you Rui! I think we have some options such as deploying spark 1.1 to a webserver we control. I will look into these..&lt;/p&gt;</comment>
                            <comment id="14087199" author="lirui" created="Wed, 6 Aug 2014 03:38:33 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brocknoland&quot; class=&quot;user-hover&quot; rel=&quot;brocknoland&quot;&gt;brocknoland&lt;/a&gt; I updated the patch to remove some unused code. Sorry about this.&lt;/p&gt;</comment>
                            <comment id="14092442" author="brocknoland" created="Mon, 11 Aug 2014 05:08:37 +0000"  >&lt;p&gt;The following patch adds a repository which I control. I can push spark 1.1-SNAPSHOT whenever we require.&lt;/p&gt;</comment>
                            <comment id="14092444" author="brocknoland" created="Mon, 11 Aug 2014 05:11:05 +0000"  >&lt;p&gt;Thank you very much for your contribution! I have committed the patch to trunk.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12731949">HIVE-7614</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310010">
                    <name>Incorporates</name>
                                                                <inwardlinks description="is part of">
                                        <issuelink>
            <issuekey id="12723734">HIVE-7292</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12726417">SPARK-2421</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12660031" name="HIVE-7540-spark.patch" size="1814" author="lirui" created="Wed, 6 Aug 2014 02:52:18 +0000"/>
                            <attachment id="12660038" name="HIVE-7540.2-spark.patch" size="2347" author="lirui" created="Wed, 6 Aug 2014 03:38:33 +0000"/>
                            <attachment id="12660922" name="HIVE-7540.3-spark.patch" size="3153" author="brocknoland" created="Mon, 11 Aug 2014 05:08:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>408523</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 15 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1yajr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>408521</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>