<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:55:01 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-16958] Setting hive.merge.sparkfiles=true will retrun an error when generating parquet databases </title>
                <link>https://issues.apache.org/jira/browse/HIVE-16958</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;The process will return &lt;br/&gt;
Job failed with java.lang.NullPointerException&lt;br/&gt;
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. java.util.concurrent.ExecutionException: Exception thrown by job&lt;br/&gt;
at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:272)&lt;br/&gt;
at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:277)&lt;br/&gt;
at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:362)&lt;br/&gt;
at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:323)&lt;br/&gt;
at java.util.concurrent.FutureTask.run(FutureTask.java:266)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 31, bdpe822n1): java.io.IOException: java.lang.reflect.InvocationTargetException&lt;br/&gt;
at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)&lt;br/&gt;
at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:271)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&amp;lt;init&amp;gt;(HadoopShimsSecure.java:217)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:345)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:695)&lt;br/&gt;
at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:246)&lt;br/&gt;
at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:209)&lt;br/&gt;
at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)&lt;br/&gt;
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&lt;br/&gt;
at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.lang.reflect.InvocationTargetException&lt;br/&gt;
at sun.reflect.GeneratedConstructorAccessor26.newInstance(Unknown Source)&lt;br/&gt;
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:257)&lt;br/&gt;
... 17 more&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
at java.util.AbstractCollection.addAll(AbstractCollection.java:343)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushProjectionsAndFilters(ProjectionPusher.java:118)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushProjectionsAndFilters(ProjectionPusher.java:189)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.getSplit(ParquetRecordReaderBase.java:84)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:74)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:59)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:75)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&amp;lt;init&amp;gt;(CombineHiveRecordReader.java:99)&lt;br/&gt;
... 21 more&lt;/p&gt;

&lt;p&gt;Driver stacktrace:&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)&lt;br/&gt;
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
at scala.Option.foreach(Option.scala:257)&lt;br/&gt;
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)&lt;br/&gt;
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)&lt;br/&gt;
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)&lt;br/&gt;
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)&lt;br/&gt;
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&lt;br/&gt;
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException&lt;br/&gt;
at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)&lt;br/&gt;
at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:271)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&amp;lt;init&amp;gt;(HadoopShimsSecure.java:217)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:345)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:695)&lt;br/&gt;
at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:246)&lt;br/&gt;
at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:209)&lt;br/&gt;
at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:102)&lt;br/&gt;
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&lt;br/&gt;
at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.lang.reflect.InvocationTargetException&lt;br/&gt;
at sun.reflect.GeneratedConstructorAccessor26.newInstance(Unknown Source)&lt;br/&gt;
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&lt;br/&gt;
at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:257)&lt;br/&gt;
... 17 more&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
at java.util.AbstractCollection.addAll(AbstractCollection.java:343)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushProjectionsAndFilters(ProjectionPusher.java:118)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.pushProjectionsAndFilters(ProjectionPusher.java:189)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.getSplit(ParquetRecordReaderBase.java:84)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:74)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:59)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:75)&lt;br/&gt;
at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&amp;lt;init&amp;gt;(CombineHiveRecordReader.java:99)&lt;br/&gt;
... 21 more&lt;/p&gt;

&lt;p&gt;when generating data for a database stored as parquet by setting hive.merge.sparkfiles=true. The detailed log and error messages can be seen at attach files.&lt;br/&gt;
The enviroment I use was hadoop2.7.3 hive release-2.3.0-rc0 spark2.0.0.&lt;br/&gt;
SQL file is showed below:&lt;br/&gt;
CREATE DATABASE IF NOT EXISTS sale;&lt;br/&gt;
use sale;&lt;/p&gt;

&lt;p&gt;CREATE EXTERNAL TABLE IF NOT EXISTS sale_record_temporary&lt;br/&gt;
( record_id DECIMAL(23,0)&lt;br/&gt;
, product_name STRING&lt;br/&gt;
, amount DECIMAL(6,0)&lt;br/&gt;
, prize DECIMAL(11,0)&lt;br/&gt;
, customer_name STRING&lt;br/&gt;
, doc_date TIMESTAMP&lt;br/&gt;
, due_date TIMESTAMP&lt;br/&gt;
)&lt;br/&gt;
ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;|&apos;&lt;br/&gt;
STORED AS TEXTFILE LOCATION &apos;/user/hive/warehouse/sale.db/sale_record_temporary&apos;&lt;br/&gt;
;&lt;/p&gt;

&lt;p&gt;DROP TABLE IF EXISTS sale_record;&lt;br/&gt;
CREATE TABLE sale_record&lt;br/&gt;
( record_id DECIMAL(23,0), product_name STRING, amount DECIMAL(6,0), prize DECIMAL(11,0), customer_name STRING, due_date TIMESTAMP) partitioned by (doc_date TIMESTAMP)&lt;br/&gt;
STORED AS parquet;&lt;br/&gt;
insert overwrite table sale_record PARTITION(doc_date) select record_id,product_name,amount,prize,customer_name,due_date,doc_date from sale_record_temporary;&lt;br/&gt;
External data has aleady been put in hdfs path: &apos;/user/hive/warehouse/sale.db/sale_record_temporary&apos;&lt;/p&gt;</description>
                <environment>&lt;p&gt;centos7 hadoop2.7.3 spark2.0.0&lt;/p&gt;</environment>
        <key id="13082436">HIVE-16958</key>
            <summary>Setting hive.merge.sparkfiles=true will retrun an error when generating parquet databases </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kellyzly">liyunzhang</assignee>
                                    <reporter username="Liu765940375">Liu Chunxiao</reporter>
                        <labels>
                    </labels>
                <created>Mon, 26 Jun 2017 08:24:47 +0000</created>
                <updated>Sun, 27 Dec 2020 13:35:51 +0000</updated>
                            <resolved>Mon, 3 Jul 2017 01:39:14 +0000</resolved>
                                    <version>2.2.0</version>
                    <version>2.3.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                        <due>Mon, 26 Jun 2017 00:00:00 +0000</due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="16062960" author="lirui" created="Mon, 26 Jun 2017 11:33:58 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Liu765940375&quot; class=&quot;user-hover&quot; rel=&quot;Liu765940375&quot;&gt;Liu765940375&lt;/a&gt;, does the issue happen for MR too? I.e. when hive.merge.mapfiles=true and hive.merge.mapredfiles=true.&lt;br/&gt;
And could you share your data to reproduce it?&lt;/p&gt;</comment>
                            <comment id="16064070" author="kellyzly" created="Tue, 27 Jun 2017 01:13:06 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;: this also happened in MR.&lt;/p&gt;</comment>
                            <comment id="16064419" author="liu765940375" created="Tue, 27 Jun 2017 07:51:35 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, I have added the DataGen mvn project file(Main.java, RowGenerator.java, pom.xml) and hive-site.xml in attach files. It is easy to generate data on hdfs by the code(Default textfile is 500M. Parquet file is about 200M, and it generates 10 partitions dynamically. One file in partition is about 10M, so you need at least 2 files in one partition to merge them).&lt;/p&gt;</comment>
                            <comment id="16064513" author="lirui" created="Tue, 27 Jun 2017 09:11:35 +0000"  >&lt;p&gt;I think the cause is we need to set neededNestedColumnPaths in &lt;tt&gt;GenMapRedUtils::createTemporaryTableScanOperator&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="16066104" author="kellyzly" created="Wed, 28 Jun 2017 08:05:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;: nestedColumnPaths is to solve the problem of &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-13873&quot; title=&quot;Support column pruning for struct fields in select statement&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-13873&quot;&gt;&lt;del&gt;HIVE-13873&lt;/del&gt;&lt;/a&gt; which relates the struct type. In current case, there is no struct type and I can not set neededNestedColumnPaths in GenMapRedUtils::createTemporaryTableScanOperator.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;CREATE EXTERNAL TABLE IF NOT EXISTS sale_record_temporary
( record_id DECIMAL(23,0)
, product_name STRING
, amount DECIMAL(6,0)
, prize DECIMAL(11,0)
, customer_name STRING
, doc_date TIMESTAMP
, due_date TIMESTAMP
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;|&apos;&lt;/span&gt;
STORED AS TEXTFILE LOCATION &lt;span class=&quot;code-quote&quot;&gt;&apos;/user/hive/warehouse/sale.db/sale_record_temporary&apos;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I directly add a not null judgement before the code.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;-          neededNestedColumnPaths.addAll(ts.getNeededNestedColumnPaths());
+          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ts.getNeededNestedColumnPaths() != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
+            neededNestedColumnPaths.addAll(ts.getNeededNestedColumnPaths());
+          }
         }

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Liu765940375&quot; class=&quot;user-hover&quot; rel=&quot;Liu765940375&quot;&gt;Liu765940375&lt;/a&gt;: can you verify whether &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-16958&quot; title=&quot;Setting hive.merge.sparkfiles=true will retrun an error when generating parquet databases &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-16958&quot;&gt;&lt;del&gt;HIVE-16958&lt;/del&gt;&lt;/a&gt;.patch can solve your problem?&lt;/p&gt;</comment>
                            <comment id="16066170" author="lirui" created="Wed, 28 Jun 2017 09:03:35 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="16066497" author="hiveqa" created="Wed, 28 Jun 2017 13:44:20 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12874823/HIVE-16958.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12874823/HIVE-16958.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 13 failed/errored test(s), 10850 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=146)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=233)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query16] (batchId=233)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=233)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query94] (batchId=233)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testBootstrapFunctionReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionIncrementalReplication (batchId=217)
org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.testCreateFunctionWithFunctionBinaryJarsOnHDFS (batchId=217)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5807/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5807/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5807/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5807/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5807/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5807/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12874823 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16067696" author="kellyzly" created="Thu, 29 Jun 2017 04:01:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;ferd&lt;/a&gt;: as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; finished review, can you help commit it to upstream?&lt;/p&gt;</comment>
                            <comment id="16071840" author="ferd" created="Mon, 3 Jul 2017 01:39:14 +0000"  >&lt;p&gt;Committed to the upstream. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kellyzly&quot; class=&quot;user-hover&quot; rel=&quot;kellyzly&quot;&gt;kellyzly&lt;/a&gt; for the patch and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; for the review.&lt;/p&gt;</comment>
                            <comment id="16486167" author="vgarg" created="Tue, 22 May 2018 23:59:25 +0000"  >&lt;p&gt;Hive 3.0.0 has been released so closing this jira.&lt;/p&gt;</comment>
                            <comment id="17255231" author="laixiong" created="Sun, 27 Dec 2020 13:35:51 +0000"  >&lt;p&gt;Why this patch not applied into branch 2.3.0 , is there any historical reasons or rules?&lt;/p&gt;

&lt;p&gt;As a new user of hive, I download the source code tar ball 2.3.7 from &lt;a href=&quot;https://mirror.bit.edu.cn/apache/hive/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://mirror.bit.edu.cn/apache/hive/&lt;/a&gt;&#160;and build it and then met this bug , so I&apos;m confused.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12874823" name="HIVE-16958.patch" size="812" author="kellyzly" created="Wed, 28 Jun 2017 08:05:30 +0000"/>
                            <attachment id="12874633" name="Main.java" size="1078" author="Liu765940375" created="Tue, 27 Jun 2017 07:37:54 +0000"/>
                            <attachment id="12874632" name="RowGenerator.java" size="1851" author="Liu765940375" created="Tue, 27 Jun 2017 07:38:04 +0000"/>
                            <attachment id="12874634" name="hive-site.xml" size="2546" author="Liu765940375" created="Tue, 27 Jun 2017 07:41:16 +0000"/>
                            <attachment id="12874449" name="parquet-hivemergesparkfiles.txt" size="8475" author="Liu765940375" created="Mon, 26 Jun 2017 08:27:06 +0000"/>
                            <attachment id="12874631" name="pom.xml" size="835" author="Liu765940375" created="Tue, 27 Jun 2017 07:38:10 +0000"/>
                            <attachment id="12874448" name="sale.sql" size="910" author="Liu765940375" created="Mon, 26 Jun 2017 08:28:23 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 46 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3gpxj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12335837">2.2.0</customfieldvalue>
    <customfieldvalue id="12340269">2.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>