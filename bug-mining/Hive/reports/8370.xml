<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 19:23:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-26612] INT64 Parquet timestamps cannot be read into BIGINT Hive type</title>
                <link>https://issues.apache.org/jira/browse/HIVE-26612</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;If a parquet file has a Type of &quot;int64 eventtime (TIMESTAMP(MILLIS,true))&quot;, the following error is produced:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.RuntimeException: java.io.IOException: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/xxxx/hive/itests/qtest/target/tmp/parquet_format_ts_as_bigint/part-00000/timestamp_as_bigint.parquet
	at org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:213)
	at org.apache.hadoop.hive.ql.exec.FetchTask.execute(FetchTask.java:98)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:212)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:149)
Caused by: java.io.IOException: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/xxxx/hive/itests/qtest/target/tmp/parquet_format_ts_as_bigint/part-00000/timestamp_as_bigint.parquet
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:624)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:531)
	at org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:197)
	... 55 more
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file file:/home/stamatis/Projects/Apache/hive/itests/qtest/target/tmp/parquet_format_ts_as_bigint/part-00000/timestamp_as_bigint.parquet
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:255)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:87)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:89)
	at org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:771)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:335)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:562)
	... 57 more
Caused by: java.lang.UnsupportedOperationException: org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter$10$1
	at org.apache.parquet.io.api.PrimitiveConverter.addLong(PrimitiveConverter.java:105)
	at org.apache.parquet.column.impl.ColumnReaderBase$2$4.writeValue(ColumnReaderBase.java:301)
	at org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:410)
	at org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)
	at org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:230)
	... 63 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The parquet file can be created with the following steps (through spark):&lt;/p&gt;

&lt;p&gt;spark.conf.set(&quot;spark.sql.parquet.outputTimestampType&quot;, &quot;TIMESTAMP_MILLIS&quot;)&lt;br/&gt;
spark.conf.set(&quot;spark.sql.legacy.parquet.int96RebaseModeInWrite&quot;, &quot;LEGACY&quot;)&lt;br/&gt;
spark.conf.set(&quot;spark.sql.legacy.parquet.datetimeRebaseModeInWrite&quot;, &quot;LEGACY&quot;)&lt;br/&gt;
spark.conf.set(&quot;spark.sql.legacy.parquet.int96RebaseModeInRead&quot;, &quot;LEGACY&quot;)&lt;br/&gt;
spark.conf.set(&quot;spark.sql.legacy.parquet.datetimeRebaseModeInRead&quot;, &quot;LEGACY&quot;)&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;br/&gt;
val df = Seq(&lt;br/&gt;
(1, Timestamp.valueOf(&quot;2014-01-01 23:00:01&quot;)),&lt;br/&gt;
(1, Timestamp.valueOf(&quot;2014-11-30 12:40:32&quot;)),&lt;br/&gt;
(2, Timestamp.valueOf(&quot;2016-12-29 09:54:00&quot;)),&lt;br/&gt;
(2, Timestamp.valueOf(&quot;2016-05-09 10:12:43&quot;))&lt;br/&gt;
).toDF(&quot;typeid&quot;,&quot;eventtime&quot;)&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;root@c4839-node3 test_parquet2&amp;#93;&lt;/span&gt;# parquet-tools schema part-00001-6c90b794-90b9-4cc0-afc5-2e49a4e96bad-c000.snappy.parquet&lt;br/&gt;
message spark_schema &lt;/p&gt;
{
required int32 typeid;
optional int64 eventtime (TIMESTAMP(MILLIS,true));
}

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;root@c4839-node3 test_parquet1&amp;#93;&lt;/span&gt;# parquet-tools schema part-00001-cb1aeebb-ec87-4273-82ec-911c4fb605b6-c000.snappy.parquet&lt;br/&gt;
message spark_schema &lt;/p&gt;
{
required int32 typeid;
optional int96 eventtime;
}</description>
                <environment></environment>
        <key id="13485212">HIVE-26612</key>
            <summary>INT64 Parquet timestamps cannot be read into BIGINT Hive type</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="scarlin">Steve Carlin</assignee>
                                    <reporter username="scarlin">Steve Carlin</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Fri, 7 Oct 2022 23:03:35 +0000</created>
                <updated>Wed, 16 Nov 2022 13:50:37 +0000</updated>
                            <resolved>Fri, 21 Oct 2022 09:56:40 +0000</resolved>
                                                    <fixVersion>4.0.0-alpha-2</fixVersion>
                                    <component>Database/Schema</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="4200">1h 10m</timespent>
                                <comments>
                            <comment id="17615005" author="zabetak" created="Mon, 10 Oct 2022 09:55:41 +0000"  >&lt;p&gt;The description does not contain all the steps to reproduce the problem in the Hive side; I think it makes sense to include the DDL + SQL statement that lead to the error above.&lt;/p&gt;

&lt;p&gt;Moreover, I don&apos;t understand what steps &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; and &lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; want to show and how the respective schema was produced in each case. What is &lt;tt&gt;test_parquet1&lt;/tt&gt; and &lt;tt&gt;test_parquet2&lt;/tt&gt;? Can you please add few more details?&lt;/p&gt;</comment>
                            <comment id="17615977" author="scarlin" created="Tue, 11 Oct 2022 17:08:32 +0000"  >&lt;p&gt;Whoops, sorry, I left out how to reproduce it.&#160;&lt;/p&gt;

&lt;p&gt;Once the parquet file is created via the above technique through Spark, we do the following on the Hive side&lt;/p&gt;

&lt;p&gt;CREATE EXTERNAL TABLE ts_as_bigint_pq (dummy int, ts2 BIGINT)&lt;br/&gt;
STORED AS PARQUET&lt;br/&gt;
LOCATION &apos;${system:test.tmp.dir}/parquet_format_ts_as_bigint&apos;;&lt;/p&gt;

&lt;p&gt;This create statement should point to the location of the parquet file as created in the description by Spark.&#160; The parquet file is for a timestamp datatype but the native type is a legacy INT64 format.&lt;/p&gt;

&lt;p&gt;Then a simple&lt;/p&gt;

&lt;p&gt;SELECT * from ts_as_bigint_pq&lt;/p&gt;

&lt;p&gt;will cause the exception in the description.&lt;/p&gt;</comment>
                            <comment id="17616460" author="zabetak" created="Wed, 12 Oct 2022 14:21:01 +0000"  >&lt;p&gt;There are two Parquet schemas in the description of the ticket, &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; and &lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;. Can you clarify which of the two causes the problem?&lt;/p&gt;</comment>
                            <comment id="17616490" author="zabetak" created="Wed, 12 Oct 2022 14:48:51 +0000"  >&lt;p&gt;Based on the summary and description, I assume the problematic schema is &#160;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;{ required int32 typeid; optional int64 eventtime (TIMESTAMP(MILLIS,true)); }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="17616502" author="zabetak" created="Wed, 12 Oct 2022 15:11:40 +0000"  >&lt;p&gt;It seems that this ticket is closely related to &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Basically the problem in both cases (here and &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt;) is that we are storing something as a timestamp and we want to read it back as a bigint.&lt;/p&gt;

&lt;p&gt;When we create the Parquet file we are saying that the column holds timestamps. It seems that the writer makes use of the &lt;a href=&quot;https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/LogicalTypes.md?plain=1#L337&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LogicalType.Timestamp&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Parquet &lt;a href=&quot;https://github.com/apache/parquet-format/blob/54e53e5d7794d383529dd30746378f19a12afd58/LogicalTypes.md?plain=1#L23&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;type specification&lt;/a&gt; writes the following:&lt;br/&gt;
_Logical types are used to extend the types that parquet can be used to store,&lt;br/&gt;
by specifying how the primitive types should be interpreted_&lt;/p&gt;

&lt;p&gt;From my perspective the fact that we are trying to interpret a timestamp as a bigint is more like a user problem rather than a Hive problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; tried to support this kind of conversion although the motivation behind that change remains unknown.&lt;/p&gt;

&lt;p&gt;The motivation for the change here is also somewhat questionable. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;If we are storing timestamps then why not read it back as timestamps?&lt;/li&gt;
	&lt;li&gt;If further we want to transform timestamps to bigints can&apos;t we use a cast in the query?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17617169" author="scarlin" created="Thu, 13 Oct 2022 16:19:24 +0000"  >&lt;p&gt;So it looks like &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; did some support for this, but it actually broke the functionality in the legacy case.&#160; The data is stored as INT64 in the legacy case, so if the HiveTypeInfo is a BIGINT, it used the INT64 -&amp;gt; INT64 ETypeConverter.&#160; But after &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt;, the wrong ETypeConverter is being called.&lt;/p&gt;

&lt;p&gt;Since we can&apos;t really roll back &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; without breaking someone, I think we should move forward with this fix and support the BIGINT Hive datatype binding to the legacy INT64 Timestamp parquet datatype.&lt;/p&gt;</comment>
                            <comment id="17618928" author="zabetak" created="Mon, 17 Oct 2022 14:19:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=scarlin&quot; class=&quot;user-hover&quot; rel=&quot;scarlin&quot;&gt;scarlin&lt;/a&gt; I reverted &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; locally and tried to run the test case you have in the PR but it seems that there are still errors at a different level:&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.TimestampWritableV2 cannot be cast to org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:213)
	at org.apache.hadoop.hive.ql.exec.FetchTask.execute(FetchTask.java:98)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:212)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:149)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:185)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:228)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd1(CliDriver.java:200)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:126)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:421)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:352)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:727)
	at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:697)
	at org.apache.hadoop.hive.cli.control.CoreCliDriver.runTest(CoreCliDriver.java:114)
	at org.apache.hadoop.hive.cli.control.CliAdapter.runTest(CliAdapter.java:157)
	at org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver(TestMiniLlapLocalCliDriver.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.hadoop.hive.cli.control.CliAdapter$2$1.evaluate(CliAdapter.java:135)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.apache.hadoop.hive.cli.control.CliAdapter$1$1.evaluate(CliAdapter.java:95)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:377)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:138)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:465)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:451)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.TimestampWritableV2 cannot be cast to org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.hive.ql.exec.ListSinkOperator.process(ListSinkOperator.java:98)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:888)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:94)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:888)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:173)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:541)
	at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:533)
	at org.apache.hadoop.hive.ql.exec.FetchTask.executeInner(FetchTask.java:197)
	... 55 more
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.TimestampWritableV2 cannot be cast to org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.get(WritableLongObjectInspector.java:36)
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:258)
	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:308)
	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:263)
	at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:72)
	at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:247)
	at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:53)
	at org.apache.hadoop.hive.serde2.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:67)
	at org.apache.hadoop.hive.serde2.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:36)
	at org.apache.hadoop.hive.ql.exec.ListSinkOperator.process(ListSinkOperator.java:94)
	... 62 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Due to this I am skeptical about if &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; broke this functionality. Did this ever work?&lt;/p&gt;</comment>
                            <comment id="17618991" author="scarlin" created="Mon, 17 Oct 2022 16:22:51 +0000"  >&lt;p&gt;The customer is claiming that it was working in their environment.&lt;/p&gt;

&lt;p&gt;It seems like a clean bind without any conversion needed&#160; if it was stored as a BIGINT in parquet and is bound as a BIGINT in Hive.&#160; So I have reason to believe that this was working for the customer and should be something that can be easily supported.&lt;/p&gt;</comment>
                            <comment id="17619000" author="scarlin" created="Mon, 17 Oct 2022 16:44:20 +0000"  >&lt;p&gt;Fuller explanation from my last comment:&lt;/p&gt;

&lt;p&gt;When I initially saw the problem, I tried deleting the line where it went to the TimeStamp converter.&#160; This caused it to go directly to the BIGINT-&amp;gt;BIGINT ETypeConverter and this fixed the issue.&lt;/p&gt;

&lt;p&gt;I had improperly assumed that &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; was the issue that broke the functionality.&#160; I didn&apos;t do the research on that because I didn&apos;t think it was important enough.&#160; I had knowledge that the customer claimed it was working and I had knowledge that the correct ETypeConverter exists to get the customer the right information.&#160; My apologies for not doing the research, but I still don&apos;t think it&apos;s important enough to go down the path to see what caused this since I will take the customer at their word here.&lt;/p&gt;

&lt;p&gt;The fix involved is basically routing the code to use the correct ETypeConverter.&#160; If we had to add a new ETypeConverter or change any code in that file, I&apos;d be a bit more wary about the solution.&#160; But given that this is a very small fix to use the proper ETypeConverter, presumably the same ETypeConverter that was being used in the customer environment makes me comfortable with the idea of giving the customer this fix.&lt;/p&gt;</comment>
                            <comment id="17620440" author="zabetak" created="Wed, 19 Oct 2022 16:17:55 +0000"  >&lt;p&gt;It is not my intention to prove that the customer is right or wrong but rather clarify if there is a bug and where it is. When there are multiple projects involved in a problem (in this case Spark vs Hive) it is important to understand which side is causing the problem. If there is a change in the way Spark writes the Parquet file then this could also be causing the exceptions mentioned here.&lt;/p&gt;

&lt;p&gt;The Hive Parquet documentation (&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Parquet&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Parquet&lt;/a&gt;) is very sketchy leaving a lot of open questions on what exactly is supported and how things are supposed to work. This ticket as well as &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt; present the fact that Hive cannot read a Parquet TIMESTAMP into a Hive BIGINT as a Hive bug but there were no tests and no documentation implying that is possible. In these cases, there is a fine line between bug and feature request.&lt;/p&gt;

&lt;p&gt;Another reason why I wanted to know the commit which caused the breaking change in Hive is to understand if it was intentional or not.&lt;/p&gt;

&lt;p&gt;Running git bisect with the test case in the PR shows that the Hive commit which broke this use-case is &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-21215&quot; title=&quot;Read Parquet INT64 timestamp&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-21215&quot;&gt;&lt;del&gt;HIVE-21215&lt;/del&gt;&lt;/a&gt;. Note, that if the Logical type was missing from the metadata then things would work as before without problems.&lt;/p&gt;

&lt;p&gt;Now I have a better picture of what is happening and it seems reasonable to fix this; I will try to have a look in the PR in the next few days.&lt;/p&gt;</comment>
                            <comment id="17622161" author="zabetak" created="Fri, 21 Oct 2022 09:50:59 +0000"  >&lt;p&gt;I changed the summary to reflect exactly what&apos;s the actual problem here. &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt;, claims to have fixed the exact same problem but that&apos;s not true. &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-23345&quot; title=&quot;INT64 Parquet timestamps cannot be read into bigint Hive type&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-23345&quot;&gt;&lt;del&gt;HIVE-23345&lt;/del&gt;&lt;/a&gt;, was sufficient to fix the conversion of INT96 Parquet timestamp to BIGINT but not for INT64.&lt;/p&gt;</comment>
                            <comment id="17622164" author="zabetak" created="Fri, 21 Oct 2022 09:56:40 +0000"  >&lt;p&gt;Fixed in &lt;a href=&quot;https://github.com/apache/hive/commit/f3e3c91b882c442ffd872de998f8db40f7bff162.&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hive/commit/f3e3c91b882c442ffd872de998f8db40f7bff162.&lt;/a&gt; Thanks for the PR &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=scarlin&quot; class=&quot;user-hover&quot; rel=&quot;scarlin&quot;&gt;scarlin&lt;/a&gt; !&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13213925">HIVE-21215</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310040">
                    <name>Required</name>
                                                                <inwardlinks description="is required by">
                                        <issuelink>
            <issuekey id="13302111">HIVE-23345</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310760">
                    <name>Testing</name>
                                            <outwardlinks description="Testing discovered">
                                        <issuelink>
            <issuekey id="13488652">HIVE-26658</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 3 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z196mw:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>