<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:50:37 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-15485] Investigate the DoAs failure in HoS</title>
                <link>https://issues.apache.org/jira/browse/HIVE-15485</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;With DoAs enabled, HoS failed with following errors:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.hadoop.security.AccessControlException: systest tries to renew a token with renewer hive
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.renewToken(AbstractDelegationTokenSecretManager.java:484)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:7543)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:555)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.renewDelegationToken(AuthorizationProviderProxyClientProtocol.java:674)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:999)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2141)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2137)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1783)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2135)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It is related to the change from &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14383&quot; title=&quot;SparkClientImpl should pass principal and keytab to spark-submit instead of calling kinit explicitely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14383&quot;&gt;&lt;del&gt;HIVE-14383&lt;/del&gt;&lt;/a&gt;. It looks like that SparkSubmit logs in Kerberos with passed in hive principal/keytab and then tries to create a hdfs delegation token for user systest with renewer hive.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13029740">HIVE-15485</key>
            <summary>Investigate the DoAs failure in HoS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ctang">Chaoyu Tang</assignee>
                                    <reporter username="ctang">Chaoyu Tang</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Dec 2016 13:22:09 +0000</created>
                <updated>Mon, 27 Nov 2017 01:11:09 +0000</updated>
                            <resolved>Mon, 30 Jan 2017 16:31:21 +0000</resolved>
                                                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15771202" author="ctang.ma" created="Thu, 22 Dec 2016 21:56:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14383&quot; title=&quot;SparkClientImpl should pass principal and keytab to spark-submit instead of calling kinit explicitely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-14383&quot;&gt;&lt;del&gt;HIVE-14383&lt;/del&gt;&lt;/a&gt; is the right way to renew the delegation token for a long running HoS session. Spark needs the principal/keytab passed in via --principal and --keytab options, and does the renewal by copying the keytab to the cluster and handling login to kerberos inside the application. &lt;br/&gt;
But the option --principal, --keytab could not work with --proxy-user in spark-submit.sh as suggested by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;, so at this moment we could support either the token renewal or the impersonation, but not both.&lt;/p&gt;</comment>
                            <comment id="15837148" author="ctang.ma" created="Wed, 25 Jan 2017 04:02:36 +0000"  >&lt;p&gt;Given that we are not able to support both doAs and delegation token renewal in Spark at this moment (see comments in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5493&quot; title=&quot;Support proxy users under kerberos&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-5493&quot;&gt;&lt;del&gt;SPARK-5493&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19143&quot; title=&quot;API in Spark for distributing new delegation tokens (Improve delegation token handling in secure clusters)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-19143&quot;&gt;&lt;del&gt;SPARK-19143&lt;/del&gt;&lt;/a&gt; etc), and doAs is more common case in Hive, so when doAs is enabled, we will use kinit instead of passing the principal/keytab to Spark. I could not thought of other ways to make both work. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, do you have any thought? If you agree on that, could you help review the patch? Thanks. &lt;/p&gt;</comment>
                            <comment id="15837578" author="hiveqa" created="Wed, 25 Jan 2017 11:12:21 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12849217/HIVE-15485.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12849217/HIVE-15485.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10983 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=98)
	[ptf_general_queries.q,auto_join_reordering_values.q,sample2.q,join1.q,decimal_join.q,mapjoin_subquery2.q,join32_lessSize.q,mapjoin1.q,order2.q,skewjoinopt18.q,union_remove_18.q,join25.q,groupby9.q,bucketsortoptimize_insert_6.q,ctas.q]
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[offset_limit_ppd_optimizer] (batchId=151)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_part] (batchId=149)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=93)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3169/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3169/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3169/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3169/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-3169/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-3169/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12849217 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15839755" author="ctang.ma" created="Thu, 26 Jan 2017 14:34:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;jxiang&lt;/a&gt;, could you review the patch to see if it makes sense, so that we can at lease unblock the doAs issue? Thanks&lt;/p&gt;</comment>
                            <comment id="15840049" author="jxiang" created="Thu, 26 Jan 2017 17:03:58 +0000"  >&lt;p&gt;Could you put the two changes in your patch in the same place to make it a little easier to understand?&lt;/p&gt;</comment>
                            <comment id="15840222" author="ctang.ma" created="Thu, 26 Jan 2017 18:58:17 +0000"  >&lt;p&gt;Thanks, Jimmy, for looking into this. When doAs is enabled, we use kinit to login Kerberos and this kinit command need be put before the spark-submit.sh, but when doAs is disabled, the principal/keytab should be after spark-submit.sh as its parameters. I was also wondering how to combine these two changes into one, but have not found a good way. Any suggestion?&lt;/p&gt;</comment>
                            <comment id="15840465" author="jxiang" created="Thu, 26 Jan 2017 21:10:17 +0000"  >&lt;p&gt;For doAs, add kinit etc to the beginning of the list; for the other add principal etc at the end. If you are concerned with performance, will LinkedList be better than ArrayList here?&lt;/p&gt;

&lt;p&gt;By the way, should keyTabFile + &quot;;&quot; be two argvs?&lt;/p&gt;</comment>
                            <comment id="15840922" author="ctang.ma" created="Fri, 27 Jan 2017 04:07:44 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;jxiang&lt;/a&gt;. Please take a look to see if it is you suggested. The keytab + &quot;;&quot; do not have to be in two argv. Thanks&lt;/p&gt;</comment>
                            <comment id="15843082" author="jxiang" created="Fri, 27 Jan 2017 16:34:03 +0000"  >&lt;p&gt;Thanks for making the change. Looks good to me. +1&lt;/p&gt;</comment>
                            <comment id="15843696" author="hiveqa" created="Sat, 28 Jan 2017 00:22:18 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12849642/HIVE-15485.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12849642/HIVE-15485.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 43 failed/errored test(s), 10419 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=100)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=101)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=102)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=103)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=104)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=105)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=106)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=107)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=108)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=109)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=111)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=112)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=113)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=114)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=116)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=118)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=121)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=122)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=123)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=124)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=125)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=126)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=127)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=129)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=130)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=131)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=132)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=95)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=96)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=97)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=98)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=99)
org.apache.hive.spark.client.TestSparkClient.testRemoteClient (batchId=278)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3226/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3226/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3226/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3226/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-3226/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-3226/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 43 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12849642 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15843793" author="ctang.ma" created="Sat, 28 Jan 2017 03:06:37 +0000"  >&lt;p&gt;Fixed the test failures.&lt;/p&gt;</comment>
                            <comment id="15844057" author="hiveqa" created="Sat, 28 Jan 2017 13:33:27 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12849796/HIVE-15485.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12849796/HIVE-15485.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 8 failed/errored test(s), 10973 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=109)
	[union_remove_1.q,ppd_outer_join2.q,date_udf.q,groupby1_noskew.q,join20.q,smb_mapjoin_13.q,groupby_rollup1.q,temp_table_gb1.q,vector_string_concat.q,smb_mapjoin_6.q,metadata_only_queries.q,auto_sortmerge_join_12.q,groupby_bigdata.q,groupby3_map_multi_distinct.q,innerjoin.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=125)
	[table_access_keys_stats.q,bucketmapjoin11.q,auto_join4.q,mapjoin_decimal.q,join34.q,nullgroup.q,mergejoins_mixed.q,sort.q,stats8.q,auto_join28.q,join17.q,union17.q,skewjoinopt11.q,groupby1_map.q,load_dyn_part11.q]
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_varchar_simple] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=93)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3240/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3240/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3240/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3240/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-3240/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-3240/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12849796 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15844102" author="ctang.ma" created="Sat, 28 Jan 2017 16:07:35 +0000"  >&lt;p&gt;The test failures are not related to this patch.&lt;/p&gt;</comment>
                            <comment id="15844215" author="xuefuz" created="Sat, 28 Jan 2017 22:40:45 +0000"  >&lt;p&gt;Sorry for my late reply. (I&apos;m currently OOO.) The patch looks good to me too. While these test failures are caused by something else, the fact that some Spark tests didn&apos;t actually run is a little concern. Is there a way to validate these tests locally?&lt;/p&gt;</comment>
                            <comment id="15844487" author="ctang.ma" created="Sun, 29 Jan 2017 15:48:07 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt; for looking into this patch. I have attached a new patch to trigger the build test. The patch has been verified in my local environment. The TestSparkCliDriver tests ran successfully. I also manually validated cases (e.g. kerberos w/ or w/o doAs) in my local cluster, they all passed.&lt;/p&gt;</comment>
                            <comment id="15844510" author="hiveqa" created="Sun, 29 Jan 2017 16:29:59 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12849876/HIVE-15485.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12849876/HIVE-15485.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 11003 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_char_simple] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_3] (batchId=93)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=93)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3249/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3249/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/3249/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/3249/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-3249/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-3249/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12849876 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15844526" author="ctang.ma" created="Sun, 29 Jan 2017 16:49:06 +0000"  >&lt;p&gt;The failed tests are not related to this patch.&lt;/p&gt;</comment>
                            <comment id="15844657" author="xuefuz" created="Sun, 29 Jan 2017 21:10:04 +0000"  >&lt;p&gt;+1. Thanks for looking into this, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ctang.ma&quot; class=&quot;user-hover&quot; rel=&quot;ctang.ma&quot;&gt;ctang.ma&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15845408" author="ctang.ma" created="Mon, 30 Jan 2017 16:31:21 +0000"  >&lt;p&gt;Committed to 2.2.0. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jxiang&quot; class=&quot;user-hover&quot; rel=&quot;jxiang&quot;&gt;jxiang&lt;/a&gt; for review.&lt;/p&gt;</comment>
                            <comment id="16265585" author="linzhangbing" created="Sat, 25 Nov 2017 03:13:46 +0000"  >&lt;p&gt;Hi,&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ctang.ma&quot; class=&quot;user-hover&quot; rel=&quot;ctang.ma&quot;&gt;ctang.ma&lt;/a&gt;,when I use you patch in my cluster,the cluster use hive2.2.0 and spark-assembly-1.6.0,I use beeline to commit spark task occuring some error:&lt;br/&gt;
SLF4J: Actual binding is of type &lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.logging.slf4j.Log4jLoggerFactory&amp;#93;&lt;/span&gt;&lt;br/&gt;
Exception in thread &quot;main&quot; java.lang.reflect.UndeclaredThrowableException&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1643)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:161)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:232)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.&amp;lt;init&amp;gt;(Hive.java:384)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:328)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:308)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:284)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil.obtainTokenForHiveMetastoreInner(YarnSparkHadoopUtil.scala:204)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil.obtainTokenForHiveMetastore(YarnSparkHadoopUtil.scala:159)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$obtainTokenForHiveMetastore(Client.scala:1365)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:350)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:722)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1016)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1076)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:161)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)&lt;br/&gt;
	... 4 more&lt;br/&gt;
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1654)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:83)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3496)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3548)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3528)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3790)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:244)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:227)&lt;br/&gt;
	... 31 more&lt;br/&gt;
Caused by: java.lang.reflect.InvocationTargetException&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)&lt;br/&gt;
	... 40 more&lt;br/&gt;
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: GSS initiate failed&lt;br/&gt;
	at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)&lt;br/&gt;
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:316)&lt;br/&gt;
	at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)&lt;br/&gt;
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)&lt;br/&gt;
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)&lt;br/&gt;
	at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:484)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;(HiveMetaStoreClient.java:292)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&amp;lt;init&amp;gt;(SessionHiveMetaStoreClient.java:70)&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&amp;lt;init&amp;gt;(RetryingMetaStoreClient.java:83)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3496)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3548)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3528)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3790)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:244)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:227)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.&amp;lt;init&amp;gt;(Hive.java:384)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:328)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:308)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:284)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil.obtainTokenForHiveMetastoreInner(YarnSparkHadoopUtil.scala:204)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.YarnSparkHadoopUtil.obtainTokenForHiveMetastore(YarnSparkHadoopUtil.scala:159)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$obtainTokenForHiveMetastore(Client.scala:1365)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:350)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:722)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1016)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1076)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.Client.main(Client.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:161)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at javax.security.auth.Subject.doAs(Subject.java:415)&lt;br/&gt;
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:161)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:531)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&amp;lt;init&amp;gt;(HiveMetaStoreClient.java:292)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&amp;lt;init&amp;gt;(SessionHiveMetaStoreClient.java:70)&lt;br/&gt;
	... 45 more&lt;/p&gt;

&lt;p&gt;	at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:37)&lt;br/&gt;
	at org.apache.hive.spark.client.SparkClientImpl.&amp;lt;init&amp;gt;(SparkClientImpl.java:106)&lt;br/&gt;
	... 27 more&lt;/p&gt;</comment>
                            <comment id="16265873" author="ctang.ma" created="Sat, 25 Nov 2017 22:45:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=linzhangbing&quot; class=&quot;user-hover&quot; rel=&quot;linzhangbing&quot;&gt;linzhangbing&lt;/a&gt; I assume that you used beeline via HoS. Please try this Spark property spark.yarn.security.tokens.hive.enabled=true to see if it helps.&lt;/p&gt;</comment>
                            <comment id="16266282" author="linzhangbing" created="Mon, 27 Nov 2017 01:11:09 +0000"  >&lt;p&gt;Thank you,&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ctang.ma&quot; class=&quot;user-hover&quot; rel=&quot;ctang.ma&quot;&gt;ctang.ma&lt;/a&gt;,i will try it with your suggest.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12993590">HIVE-14383</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13081326">HIVE-16930</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12849642" name="HIVE-15485.1.patch" size="3375" author="ctang" created="Fri, 27 Jan 2017 04:07:44 +0000"/>
                            <attachment id="12849876" name="HIVE-15485.2.patch" size="3289" author="ctang" created="Sun, 29 Jan 2017 15:28:49 +0000"/>
                            <attachment id="12849217" name="HIVE-15485.patch" size="3031" author="ctang" created="Wed, 25 Jan 2017 04:02:36 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 51 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i37vgv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>