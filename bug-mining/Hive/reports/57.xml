<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 17:51:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-217] Stream closed exception</title>
                <link>https://issues.apache.org/jira/browse/HIVE-217</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;When running a query similar to the following:&lt;br/&gt;
&quot;insert overwrite table outputtable select a, b, cast(sum(counter) as INT) from tablea join tableb on (tablea.username=tableb.username) join tablec on (tablec.userid = tablea.userid) join tabled on (tablec.id=tabled.id) where insertdate &amp;gt;= &apos;somedate&apos; and insertdate &amp;lt;= &apos;someotherdate&apos; group by a, b;&quot;&lt;br/&gt;
Where one table is ~40gb or so and the others are a couple of hundred mb. The error happens in the first mapred job that processes the 40gb.&lt;/p&gt;

&lt;p&gt;I get the following exception (see attached file for full stack trace):&lt;br/&gt;
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: Stream closed.&lt;br/&gt;
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:162)&lt;/p&gt;

&lt;p&gt;It happens in one reduce task and is reproducible, running the same query gives the error.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Hive from trunk, hadoop 0.18.2, ~20 machines&lt;/p&gt;</environment>
        <key id="12411879">HIVE-217</key>
            <summary>Stream closed exception</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="johanoskarsson">Johan Oskarsson</assignee>
                                    <reporter username="johanoskarsson">Johan Oskarsson</reporter>
                        <labels>
                    </labels>
                <created>Wed, 7 Jan 2009 19:05:32 +0000</created>
                <updated>Sat, 17 Dec 2011 00:08:54 +0000</updated>
                            <resolved>Wed, 21 Jan 2009 19:04:30 +0000</resolved>
                                                    <fixVersion>0.3.0</fixVersion>
                                    <component>Query Processor</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>0</watches>
                                                                                                                <comments>
                            <comment id="12661662" author="johanoskarsson" created="Wed, 7 Jan 2009 19:06:52 +0000"  >&lt;p&gt;The  full stack trace&lt;/p&gt;</comment>
                            <comment id="12661666" author="johanoskarsson" created="Wed, 7 Jan 2009 19:15:50 +0000"  >&lt;p&gt;It is also worth mentioning that the namenode was under hardly any load when this happened. I have not seen a reproducible problem like this with normal map reduce jobs on that cluster.&lt;/p&gt;</comment>
                            <comment id="12661993" author="johanoskarsson" created="Thu, 8 Jan 2009 15:15:11 +0000"  >&lt;p&gt;Added some debug logging to the close and process methods in FileSinkOperator and the close method is not called before the process method fails as one could guess from the stack trace.&lt;/p&gt;</comment>
                            <comment id="12662074" author="johanoskarsson" created="Thu, 8 Jan 2009 19:22:42 +0000"  >&lt;p&gt;Looking into this a bit further the exception is actually caused by the reduce task timing out:&lt;br/&gt;
&quot;Task attempt_200901071012_0373_r_000031_0 failed to report status for 616 seconds. Killing!&quot;&lt;/p&gt;

&lt;p&gt;This in turn closes the FileSystem/stream and the FileSink keeps trying to write, causing the exception seen in the log.&lt;br/&gt;
Now the question is why the task fails to report status.&lt;/p&gt;</comment>
                            <comment id="12662078" author="jsensarma" created="Thu, 8 Jan 2009 19:31:05 +0000"  >&lt;p&gt;one question - which side was the big table on?&lt;/p&gt;

&lt;p&gt;regardless though - perhaps the issue is that when we iterate through large reduce groups - hadoop is not reporting progress - maybe that&apos;s something that the application has to do.&lt;/p&gt;

&lt;p&gt;what version of hadoop are u using?&lt;/p&gt;</comment>
                            <comment id="12662081" author="johanoskarsson" created="Thu, 8 Jan 2009 19:37:02 +0000"  >&lt;p&gt;The big table in the example query above is tablec, is this what you were referring to?&lt;br/&gt;
I&apos;m using Hadoop 0.18.2.&lt;/p&gt;</comment>
                            <comment id="12662085" author="johanoskarsson" created="Thu, 8 Jan 2009 19:41:11 +0000"  >&lt;p&gt;Just had a look and it doesn&apos;t seem like the reducer that fails gets any more data to it compared to the others.&lt;/p&gt;</comment>
                            <comment id="12662111" author="jsensarma" created="Thu, 8 Jan 2009 20:35:09 +0000"  >&lt;p&gt;ok - got it. hadoop is ok - it&apos;s already reporting progress whenever any data is consumed or any data is emitted to output collector.&lt;/p&gt;

&lt;p&gt;the issue is that we are not sending data to the output collector - rather we write out data to file system ourselves. the stack trace indicates that we have consumed the entire reduce group and we are writing to the filesystem. that means that hadoop gets no opportunity to report progress (it would have reported progress if we were writing to the output collector).&lt;/p&gt;

&lt;p&gt;I am not sure why we haven&apos;t seen the problem in our environment yet - perhaps DFS IO is slow in ur environment. the fix is simple (we can report progress either in the filesinkoperator or the joinoperator). i think we should do this in the joinoperator (for the particular case where we have already gone through the entire reduce group) - since that&apos;s the only place we are vulnerable right now.&lt;/p&gt;

&lt;p&gt;i can post a patch u can try out in a couple of hours .. &lt;/p&gt;</comment>
                            <comment id="12662303" author="johanoskarsson" created="Fri, 9 Jan 2009 10:15:43 +0000"  >&lt;p&gt;That sounds like a very reasonable explanation. We have fewer machines meaning fewer disks in total then Facebook, that could explain why we&apos;re seeing this error.&lt;br/&gt;
Out of curiosity, how come you&apos;re not using the standard output collector?&lt;/p&gt;</comment>
                            <comment id="12663314" author="johanoskarsson" created="Tue, 13 Jan 2009 11:52:54 +0000"  >&lt;p&gt;To verify that this was indeed the issue I have slapped together a patch. Tried it on the same cluster and the same query as above and it worked fine this time. It&apos;s not an ideal patch, it changes the initialize method etc. I&apos;ll leave a real solution up to someone with more detailed knowledge about this area.&lt;/p&gt;</comment>
                            <comment id="12663535" author="jsensarma" created="Tue, 13 Jan 2009 23:25:29 +0000"  >&lt;p&gt;For the change to get the reporter reference into the operator structure - the interface change looks good. However - why don&apos;t we just store the reporter reference in the base Operator class rather than the FileSinkOperator specifically? If we run into other cases where we have to add progress indicators - this will make it easier.&lt;/p&gt;

&lt;p&gt;+1 otherwise. &lt;/p&gt;</comment>
                            <comment id="12663776" author="johanoskarsson" created="Wed, 14 Jan 2009 15:21:52 +0000"  >&lt;p&gt;Updated with the changes mentioned.&lt;/p&gt;</comment>
                            <comment id="12663788" author="jsensarma" created="Wed, 14 Jan 2009 16:11:44 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;thanks Johan!&lt;/p&gt;</comment>
                            <comment id="12665786" author="johanoskarsson" created="Wed, 21 Jan 2009 10:37:42 +0000"  >&lt;p&gt;If one of the committers have time to look at the patch it would be much appreciated, thanks.&lt;/p&gt;</comment>
                            <comment id="12665916" author="athusoo" created="Wed, 21 Jan 2009 19:04:30 +0000"  >&lt;p&gt;committed. Thanks Johan!!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12397314" name="HIVE-217.log" size="3767" author="johanoskarsson" created="Wed, 7 Jan 2009 19:06:52 +0000"/>
                            <attachment id="12397890" name="HIVE-217.patch" size="18893" author="johanoskarsson" created="Wed, 14 Jan 2009 15:21:52 +0000"/>
                            <attachment id="12397771" name="HIVE-217.patch" size="18212" author="johanoskarsson" created="Tue, 13 Jan 2009 11:52:54 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>73701</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            16 years, 44 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0l8jz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>122036</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>