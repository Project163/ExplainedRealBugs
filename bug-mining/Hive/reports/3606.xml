<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:34:46 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-10746]  Hive 1.2.0+Tez produces 1-byte FileSplits from mapred.TextInputFormat</title>
                <link>https://issues.apache.org/jira/browse/HIVE-10746</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;The following query: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-sql&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;SELECT&lt;/span&gt; appl_user_id, arsn_cd, &lt;span class=&quot;code-keyword&quot;&gt;COUNT&lt;/span&gt;(*) &lt;span class=&quot;code-keyword&quot;&gt;as&lt;/span&gt; RecordCount &lt;span class=&quot;code-keyword&quot;&gt;FROM&lt;/span&gt; adw.crc_arsn &lt;span class=&quot;code-keyword&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;BY&lt;/span&gt; appl_user_id,arsn_cd &lt;span class=&quot;code-keyword&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;BY&lt;/span&gt; appl_user_id;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; runs consistently fast in Spark and Mapreduce on Hive 1.2.0. When attempting to run this same query against Tez as the execution engine it consistently runs for over 300-500 seconds this seems extremely long. This is a basic external table delimited by tabs and is a single file in a folder. In Hive 0.13 this query with Tez runs fast and I tested with Hive 0.14, 0.14.1/1.0.0 and now Hive 1.2.0 and there clearly is something going awry with Hive w/Tez as an execution engine with Single or small file tables. I can attach further logs if someone needs them for deeper analysis.&lt;/p&gt;

&lt;p&gt;HDFS Output:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;hadoop fs -ls /example_dw/crc/arsn
Found 2 items
-rwxr-x---   6 loaduser hadoopusers          0 2015-05-17 20:03 /example_dw/crc/arsn/_SUCCESS
-rwxr-x---   6 loaduser hadoopusers    3883880 2015-05-17 20:03 /example_dw/crc/arsn/part-m-00000
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hive Table Describe:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;hive&amp;gt; describe formatted crc_arsn;
OK
# col_name              data_type               comment             
                 
arsn_cd                 string                                      
clmlvl_cd               string                                      
arclss_cd               string                                      
arclssg_cd              string                                      
arsn_prcsr_rmk_ind      string                                      
arsn_mbr_rspns_ind      string                                      
savtyp_cd               string                                      
arsn_eff_dt             string                                      
arsn_exp_dt             string                                      
arsn_pstd_dts           string                                      
arsn_lstupd_dts         string                                      
arsn_updrsn_txt         string                                      
appl_user_id            string                                      
arsntyp_cd              string                                      
pre_d_indicator         string                                      
arsn_display_txt        string                                      
arstat_cd               string                                      
arsn_tracking_no        string                                      
arsn_cstspcfc_ind       string                                      
arsn_mstr_rcrd_ind      string                                      
state_specific_ind      string                                      
region_specific_in      string                                      
arsn_dpndnt_cd          string                                      
unit_adjustment_in      string                                      
arsn_mbr_only_ind       string                                      
arsn_qrmb_ind           string                                      
                 
# Detailed Table Information             
Database:               adw                      
Owner:                  LOADUSER@EXA.EXAMPLE.COM   
CreateTime:             Mon Apr 28 13:28:05 EDT 2014     
LastAccessTime:         UNKNOWN                  
Protect Mode:           None                     
Retention:              0                        
Location:               hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn        
Table Type:             EXTERNAL_TABLE           
Table Parameters:                
        EXTERNAL                TRUE                
        transient_lastDdlTime   1398706085          
                 
# Storage Information            
SerDe Library:          org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe       
InputFormat:            org.apache.hadoop.mapred.TextInputFormat         
OutputFormat:           org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat       
Compressed:             No                       
Num Buckets:            -1                       
Bucket Columns:         []                       
Sort Columns:           []                       
Storage Desc Params:             
        field.delim             \t                  
        line.delim              \n                  
        serialization.format    \t                  
Time taken: 1.245 seconds, Fetched: 54 row(s)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Explain Hive 1.2.0 w/Tez:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE)
        Reducer 3 &amp;lt;- Reducer 2 (SIMPLE_EDGE)


Explain Hive 0.13 w/Tez:
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Reducer 2 &amp;lt;- Map 1 (SIMPLE_EDGE)
        Reducer 3 &amp;lt;- Reducer 2 (SIMPLE_EDGE) 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;	Hive 1.2.0 w/Spark 1.3.1:
 		Finished successfully in 7.09 seconds

	Hive 1.2.0 w/Mapreduce:
		Stage 1: 32 Seconds
		Stage 2: 35 Seconds

	Hive 1.2.0 w/Tez 0.5.3:
		Time taken: 565.025 seconds, Fetched: 11516 row(s)
	
	Hive 0.13 w/Tez 0.4.0:
		Time taken: 13.552 seconds, Fetched: 11516 row(s)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally looking at the Dag Attempt that is stuck for 500 seconds or so in Tez it looks to be stuck running the same method over and over again:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;8 duration=2561 from=org.apache.hadoop.hive.ql.exec.tez.RecordProcessor&amp;gt;
2015-05-18 19:58:41,719 INFO [TezChild] exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/dbc4b0b5-7859-4487-a56d-969440bc5e90/hive_2015-05-18_19-58-25_951_5497535752804149087-1/gss2002/_tez_scratch_dir/4e635121-c4cd-4e3f-b96b-9f08a6a7bf5d/map.xml
2015-05-18 19:58:41,822 INFO [TezChild] exec.MapOperator: MAP[4]: records read - 1
2015-05-18 19:58:41,835 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000
2015-05-18 19:58:41,848 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000

......


2015-05-18 20:07:46,560 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000
2015-05-18 20:07:46,574 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000
2015-05-18 20:07:46,587 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000
2015-05-18 20:07:46,603 INFO [TezChild] io.HiveContextAwareRecordReader: Processing file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000
2015-05-18 20:07:46,603 INFO [TezChild] log.PerfLogger: &amp;lt;/PERFLOG method=TezRunProcessor start=1431993518764 end=1431994066603 duration=547839 from=org.apache.hadoop.hive.ql.exec.tez.TezProcessor&amp;gt;
2015-05-18 20:07:46,603 INFO [TezChild] exec.MapOperator: 4 finished. closing... 
2015-05-18 20:07:46,603 INFO [TezChild] exec.MapOperator: RECORDS_IN_Map_1:13440
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12830899">HIVE-10746</key>
            <summary> Hive 1.2.0+Tez produces 1-byte FileSplits from mapred.TextInputFormat</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gopalv">Gopal Vijayaraghavan</assignee>
                                    <reporter username="gss2002">Greg Senia</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 May 2015 02:25:35 +0000</created>
                <updated>Tue, 16 Feb 2016 23:50:17 +0000</updated>
                            <resolved>Thu, 18 Jun 2015 20:39:08 +0000</resolved>
                                    <version>0.14.0</version>
                    <version>0.14.1</version>
                    <version>1.1.0</version>
                    <version>1.1.1</version>
                    <version>1.2.0</version>
                                    <fixVersion>1.2.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>Hive</component>
                    <component>Tez</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14550669" author="mmokhtar" created="Tue, 19 May 2015 16:00:04 +0000"  >&lt;p&gt;Can you please run &quot;set hive.tez.exec.print.summary=true;&quot; in the session and attach the output?&lt;br/&gt;
Another thing to check is the value of &quot;hive.exec.reducers.bytes.per.reducer&quot; this controls the parallelism in the reducer phase.&lt;/p&gt;</comment>
                            <comment id="14550862" author="gopalv" created="Tue, 19 May 2015 17:48:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gss2002&quot; class=&quot;user-hover&quot; rel=&quot;gss2002&quot;&gt;gss2002&lt;/a&gt;: the only difference in codepath should be the split-generation - everything else is identical between all 3.&lt;/p&gt;

&lt;p&gt;Both the MR and Spark impls ignore the actual FileInputFormat implementation and do not call FileInputFormat::getSplits().&lt;/p&gt;

&lt;p&gt;Tez seems to be actually calling the FileInputFormat::getSplits() for TextInputFormat.&lt;/p&gt;

&lt;p&gt;Can you run the same query with compressed input data?&lt;/p&gt;</comment>
                            <comment id="14550979" author="gss2002" created="Tue, 19 May 2015 18:40:13 +0000"  >&lt;p&gt;Mostafa Mokhtar: This isn&apos;t happening in the reducer phase. It&apos;s occurring in the Map phase attempt. I&apos;ve attached the Map Phase attempt output from the TezChild task.&lt;/p&gt;</comment>
                            <comment id="14550985" author="gss2002" created="Tue, 19 May 2015 18:42:11 +0000"  >&lt;p&gt;hive.exec.reducers.bytes.per.reducer=67108864; default?&lt;/p&gt;</comment>
                            <comment id="14552508" author="gss2002" created="Wed, 20 May 2015 15:40:58 +0000"  >&lt;p&gt;Seems to be that single file with a group by/order by is generating 40040 splits... I think the map file is needed at this point to determine why this is happening correct?&lt;/p&gt;

&lt;p&gt;2015-05-19 16:20:32,462 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;AsyncDispatcher event handler&amp;#93;&lt;/span&gt; impl.VertexImpl: Num tasks is -1. Expecting VertexManager/InputInitializers/1-1 split to set #tasks for the vertex vertex_1426958683478_171530_1_00&lt;br/&gt;
2015-05-19 16:20:32,707 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; security.UserGroupInformation: PrivilegedAction as:gss2002 (auth:SIMPLE) from:org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:239)&lt;br/&gt;
2015-05-19 16:20:32,708 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; dag.RootInputInitializerManager: Starting InputInitializer for Input: crc_arsn on vertex vertex_1426958683478_171530_1_00 &lt;span class=&quot;error&quot;&gt;&amp;#91;Map 1&amp;#93;&lt;/span&gt;&lt;br/&gt;
2015-05-19 16:20:32,722 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.HiveInputFormat&amp;gt;&lt;br/&gt;
2015-05-19 16:20:32,723 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml&lt;br/&gt;
2015-05-19 16:20:32,723 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Found plan in cache for name: map.xml&lt;br/&gt;
2015-05-19 16:20:32,744 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Processing alias crc_arsn&lt;br/&gt;
2015-05-19 16:20:32,744 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Adding input file hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn&lt;br/&gt;
2015-05-19 16:20:32,747 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: hive.io.file.readcolumn.ids=&lt;br/&gt;
2015-05-19 16:20:32,747 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: hive.io.file.readcolumn.names=,arsn_cd,appl_user_id&lt;br/&gt;
2015-05-19 16:20:32,747 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: Generating splits&lt;br/&gt;
2015-05-19 16:20:32,780 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-19 16:20:32,780 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-19 16:20:32,780 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-19 16:20:32,780 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-19 16:20:32,781 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-19 16:20:32,782 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7879a53d&lt;br/&gt;
2015-05-19 16:20:32,785 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-19 16:20:32,785 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-19 16:20:32,785 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-19 16:20:32,785 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-19 16:20:32,786 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-19 16:20:32,786 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7879a53d&lt;br/&gt;
2015-05-19 16:20:32,876 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Time taken to get FileStatuses: 87&lt;br/&gt;
2015-05-19 16:20:32,876 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total input paths to process : 1&lt;br/&gt;
2015-05-19 16:20:32,881 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-19 16:20:32,881 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-19 16:20:32,881 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-19 16:20:32,881 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-19 16:20:32,882 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-19 16:20:32,883 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@7879a53d&lt;br/&gt;
2015-05-19 16:20:32,907 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total # of splits generated by getSplits: 40040, TimeTaken: 124&lt;br/&gt;
2015-05-19 16:20:32,916 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: number of splits 40040&lt;br/&gt;
2015-05-19 16:20:32,916 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=getSplits start=1432066832722 end=1432066832916 duration=194 from=org.apache.hadoop.hive.ql.io.HiveInputFormat&amp;gt;&lt;br/&gt;
2015-05-19 16:20:32,917 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.HiveSplitGenerator: Number of input splits: 40040. 23542 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;br/&gt;
2015-05-19 16:20:32,917 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml&lt;br/&gt;
2015-05-19 16:20:32,918 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: **************&lt;b&gt;non-local mode&lt;/b&gt;**************&lt;br/&gt;
2015-05-19 16:20:32,918 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: local path = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml&lt;br/&gt;
2015-05-19 16:20:32,918 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Loading plan from string: /tmp/hive/gss2002/431ae2bc-ebc9-48e7-bbb3-f03144198009/hive_2015-05-19_16-20-28_783_5570914503219655045-1/gss2002/_tez_scratch_dir/9da6870e-7388-40b1-bab6-9d0f242b1702/map.xml&lt;br/&gt;
2015-05-19 16:20:32,919 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;&lt;br/&gt;
2015-05-19 16:20:32,919 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Deserializing MapWork via kryo&lt;br/&gt;
2015-05-19 16:20:32,940 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=deserializePlan start=1432066832919 end=1432066832940 duration=21 from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;&lt;br/&gt;
2015-05-19 16:20:32,941 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: Adding split hdfs://xhadnnm1p.example.com:8020/example_dw/crc/arsn/part-m-00000 to src new group? true&lt;/p&gt;</comment>
                            <comment id="14552793" author="gss2002" created="Wed, 20 May 2015 18:07:34 +0000"  >&lt;p&gt;I am guessing this JIRA could be the root of this issue: &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-7156&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-7156&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&quot;gss2002_20150520132600_e4199888_c149_4394_8231_238d9d9dee98_1.Map_1_crc_arsn&quot; -&amp;gt; &quot;gss2002_20150520132600_e4199888_c149_4394_8231_238d9d9dee98_1.Map_1&quot; [ label = &quot;Input &lt;span class=&quot;error&quot;&gt;&amp;#91;inputClass=MRInputLegacy,\n initializer=HiveSplitGenerator&amp;#93;&lt;/span&gt;&quot; ]&lt;/p&gt;




&lt;p&gt;2015-05-20 13:26:03,760 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; app.DAGAppMaster: JSON dump for submitted DAG, dagId=dag_1426958683478_173250_1, json={&quot;dagName&quot;:&quot;gss2002_20150520132600_e4199888-c149-4394-8231-238d9d9dee98:1&quot;,&quot;dagInfo&quot;:&quot;&lt;/p&gt;
{\&quot;description\&quot;:\&quot;\\nSELECT appl_user_id, arsn_cd, COUNT(*) as RecordCount FROM adw.crc_arsn GROUP BY appl_user_id,arsn_cd ORDER BY appl_user_id\&quot;,\&quot;context\&quot;:\&quot;Hive\&quot;}
&lt;p&gt;&quot;,&quot;version&quot;:1,&quot;vertices&quot;:[{&quot;vertexName&quot;:&quot;Map 1&quot;,&quot;processorClass&quot;:&quot;org.apache.hadoop.hive.ql.exec.tez.MapTezProcessor&quot;,&quot;outEdgeIds&quot;:&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;196588160&amp;quot;&amp;#93;&lt;/span&gt;,&quot;additionalInputs&quot;:[&lt;/p&gt;
{&quot;name&quot;:&quot;crc_arsn&quot;,&quot;class&quot;:&quot;org.apache.tez.mapreduce.input.MRInputLegacy&quot;,&quot;initializer&quot;:&quot;org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator&quot;}
&lt;p&gt;]},&lt;/p&gt;
{&quot;vertexName&quot;:&quot;Reducer 2&quot;,&quot;processorClass&quot;:&quot;org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor&quot;,&quot;inEdgeIds&quot;:[&quot;196588160&quot;],&quot;outEdgeIds&quot;:[&quot;1320926067&quot;]}
&lt;p&gt;,{&quot;vertexName&quot;:&quot;Reducer 3&quot;,&quot;processorClass&quot;:&quot;org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor&quot;,&quot;inEdgeIds&quot;:&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;1320926067&amp;quot;&amp;#93;&lt;/span&gt;,&quot;additionalOutputs&quot;:[&lt;/p&gt;
{&quot;name&quot;:&quot;out_Reducer 3&quot;,&quot;class&quot;:&quot;org.apache.tez.mapreduce.output.MROutput&quot;}
&lt;p&gt;]}],&quot;edges&quot;:[&lt;/p&gt;
{&quot;edgeId&quot;:&quot;196588160&quot;,&quot;inputVertexName&quot;:&quot;Map 1&quot;,&quot;outputVertexName&quot;:&quot;Reducer 2&quot;,&quot;dataMovementType&quot;:&quot;SCATTER_GATHER&quot;,&quot;dataSourceType&quot;:&quot;PERSISTED&quot;,&quot;schedulingType&quot;:&quot;SEQUENTIAL&quot;,&quot;edgeSourceClass&quot;:&quot;org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput&quot;,&quot;edgeDestinationClass&quot;:&quot;org.apache.tez.runtime.library.input.OrderedGroupedKVInput&quot;}
&lt;p&gt;,&lt;/p&gt;
{&quot;edgeId&quot;:&quot;1320926067&quot;,&quot;inputVertexName&quot;:&quot;Reducer 2&quot;,&quot;outputVertexName&quot;:&quot;Reducer 3&quot;,&quot;dataMovementType&quot;:&quot;SCATTER_GATHER&quot;,&quot;dataSourceType&quot;:&quot;PERSISTED&quot;,&quot;schedulingType&quot;:&quot;SEQUENTIAL&quot;,&quot;edgeSourceClass&quot;:&quot;org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput&quot;,&quot;edgeDestinationClass&quot;:&quot;org.apache.tez.runtime.library.input.OrderedGroupedKVInput&quot;}
&lt;p&gt;]}&lt;br/&gt;
2015-05-20 13:26:03,762 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; app.DAGAppMaster: Generating DAG graphviz file, dagId=dag_1426958683478_173250_1, filePath=/u01/hadoop/yarn/log/application_1426958683478_173250/container_1426958683478_173250_01_000001/dag_1426958683478_173250_1.dot&lt;/p&gt;


&lt;p&gt;2015-05-20 13:26:05,142 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total # of splits generated by getSplits: 40040, TimeTaken: 168&lt;br/&gt;
2015-05-20 13:26:05,144 DEBUG &lt;a href=&quot;#1 for port 33574&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Socket Reader #1 for port 33574&lt;/a&gt; ipc.Server:  got #159&lt;br/&gt;
2015-05-20 13:26:05,145 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; ipc.Server: IPC Server handler 0 on 33574: org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 167.69.200.206:54162 Call#159 Retry#0 for RpcKind RPC_PROTOCOL_BUFFER&lt;br/&gt;
2015-05-20 13:26:05,145 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; security.UserGroupInformation: PrivilegedAction as:gss2002@exa.example.COM (auth:TOKEN) from:org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)&lt;br/&gt;
2015-05-20 13:26:05,147 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; ipc.Server: Served: getDAGStatus queueTime= 1 procesingTime= 2&lt;br/&gt;
2015-05-20 13:26:05,147 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; ipc.Server: IPC Server handler 0 on 33574: responding to org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 167.69.200.206:54162 Call#159 Retry#0&lt;br/&gt;
2015-05-20 13:26:05,147 DEBUG &lt;span class=&quot;error&quot;&gt;&amp;#91;IPC Server handler 0 on 33574&amp;#93;&lt;/span&gt; ipc.Server: IPC Server handler 0 on 33574: responding to org.apache.tez.dag.api.client.rpc.DAGClientAMProtocolBlockingPB.getDAGStatus from 167.69.200.206:54162 Call#159 Retry#0 Wrote 145 bytes.&lt;br/&gt;
2015-05-20 13:26:05,154 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: number of splits 40040&lt;br/&gt;
2015-05-20 13:26:05,154 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=getSplits start=1432142764918 end=1432142765154 duration=236 from=org.apache.hadoop.hive.ql.io.HiveInputFormat&amp;gt;&lt;br/&gt;
2015-05-20 13:26:05,155 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.HiveSplitGenerator: Number of input splits: 40040. 23542 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;/p&gt;</comment>
                            <comment id="14552826" author="gopalv" created="Wed, 20 May 2015 18:20:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gss2002&quot; class=&quot;user-hover&quot; rel=&quot;gss2002&quot;&gt;gss2002&lt;/a&gt;: No, that JIRA is irrelevant to this issue - that has to do with column statistics, which your job does not have.&lt;/p&gt;

&lt;p&gt;The issue you&apos;re hitting has its origins in the hadoop-1 TextInputFormat::getSplits(), which seems to generate 1 split per row.&lt;/p&gt;

&lt;p&gt;Can you please test with a compressed input file &amp;amp; confirm if compressing the input makes a query faster?&lt;/p&gt;</comment>
                            <comment id="14552831" author="gss2002" created="Wed, 20 May 2015 18:24:42 +0000"  >&lt;p&gt;So it looks like it turned a 3MB file into 5GB worth of reads since it did 40040 read ops due to the SPLITS?? &lt;/p&gt;

&lt;p&gt;BYTES_READ=5,149,746,588, BYTES_WRITTEN=0, READ_OPS=40040&lt;/p&gt;

&lt;p&gt;hadoop fs -ls /example_dw/crc/arsn&lt;br/&gt;
Found 2 items&lt;br/&gt;
&lt;del&gt;rwxr-x&lt;/del&gt;--   6 loaduser hadoopusers          0 2015-05-17 20:03 /example_dw/crc/arsn/_SUCCESS&lt;br/&gt;
&lt;del&gt;rwxr-x&lt;/del&gt;--   6 loaduser hadoopusers    3883880 2015-05-17 20:03 /example_dw/crc/arsn/part-m-00000&lt;/p&gt;

&lt;p&gt;2015-05-20 13:35:27,017 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;AsyncDispatcher event handler&amp;#93;&lt;/span&gt; history.HistoryEventHandler: &lt;span class=&quot;error&quot;&gt;&amp;#91;HISTORY&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;DAG:dag_1426958683478_173250_1&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Event:TASK_FINISHED&amp;#93;&lt;/span&gt;: vertexName=Map 1, taskId=task_1426958683478_173250_1_00_000000, startTime=1432142771521, finishTime=1432143327014, timeTaken=555493, status=SUCCEEDED, successfulAttemptID=attempt_1426958683478_173250_1_00_000000_0, diagnostics=, counters=Counters: 28, org.apache.tez.common.counters.DAGCounter, RACK_LOCAL_TASKS=1, File System Counters, BYTES_READ=32, BYTES_WRITTEN=59817, READ_OPS=0, LARGE_READ_OPS=0, WRITE_OPS=0, BYTES_READ=5149746588, BYTES_WRITTEN=0, READ_OPS=40040, LARGE_READ_OPS=0, WRITE_OPS=0, org.apache.tez.common.counters.TaskCounter, SPILLED_RECORDS=11516, GC_TIME_MILLIS=19923, CPU_MILLISECONDS=890510, PHYSICAL_MEMORY_BYTES=1285681152, VIRTUAL_MEMORY_BYTES=5264326656, COMMITTED_HEAP_BYTES=3007840256, INPUT_RECORDS_PROCESSED=13440, OUTPUT_RECORDS=11516, OUTPUT_BYTES=218808, OUTPUT_BYTES_WITH_OVERHEAD=241846, OUTPUT_BYTES_PHYSICAL=59785, ADDITIONAL_SPILLS_BYTES_WRITTEN=0, ADDITIONAL_SPILLS_BYTES_READ=0, ADDITIONAL_SPILL_COUNT=0, HIVE, DESERIALIZE_ERRORS=0, RECORDS_IN_Map_1=13440, RECORDS_OUT_INTERMEDIATE_Map_1=11516&lt;/p&gt;</comment>
                            <comment id="14552833" author="gss2002" created="Wed, 20 May 2015 18:28:46 +0000"  >&lt;p&gt;hadoop fs -cat /example_dw/crc/arsn/part-m-00000| wc -l &lt;br/&gt;
13440&lt;/p&gt;


&lt;p&gt;so 13440 rows... with a repfactor of 6...&lt;/p&gt;

&lt;p&gt;What compression format are you looking for gz snappy etc?&lt;/p&gt;</comment>
                            <comment id="14552843" author="gopalv" created="Wed, 20 May 2015 18:38:18 +0000"  >&lt;p&gt;&quot;gzip -1 &amp;gt; part.gz&quot;, which should do the trick.&lt;/p&gt;

&lt;p&gt;Also, you might want to do &quot;hive --hiveconf hive.prewarm.enabled=true;&quot; if you&apos;re worried about latency within the CLI - the default config is for throughput.&lt;/p&gt;</comment>
                            <comment id="14552976" author="gss2002" created="Wed, 20 May 2015 19:48:35 +0000"  >&lt;p&gt;With Compression with Snappy it ran in 7 seconds...&lt;/p&gt;

&lt;p&gt;Status: DAG finished successfully in 7.93 seconds&lt;/p&gt;


&lt;p&gt;METHOD                         DURATION(ms) &lt;br/&gt;
parse                                1,081&lt;br/&gt;
semanticAnalyze                      1,488&lt;br/&gt;
TezBuildDag                            490&lt;br/&gt;
TezSubmitToRunningDag                  374&lt;br/&gt;
TotalPrepTime                        4,958&lt;/p&gt;

&lt;p&gt;VERTICES         TOTAL_TASKS  FAILED_ATTEMPTS KILLED_TASKS DURATION_SECONDS    CPU_TIME_MILLIS     GC_TIME_MILLIS  INPUT_RECORDS   OUTPUT_RECORDS &lt;br/&gt;
Map 1                      1                0            0             2.23              3,790                 29         13,440           11,516&lt;br/&gt;
Reducer 2                  1                0            0             0.81              2,150                  0         11,516           11,516&lt;br/&gt;
Reducer 3                  1                0            0             0.61              1,110                  0         11,516                0&lt;br/&gt;
OK&lt;br/&gt;
BB166674         P16     1&lt;/p&gt;</comment>
                            <comment id="14553018" author="gss2002" created="Wed, 20 May 2015 20:05:05 +0000"  >&lt;p&gt;Just to clarify this data is tab delimited being loaded from SqoopV1... What is the difference between compressed vs uncompressed at this point?&lt;/p&gt;

&lt;p&gt;Map 1: 0(+1)/1  Reducer 2: 0/1  Reducer 3: 0/1  &lt;br/&gt;
Map 1: 0(+1)/1  Reducer 2: 0/1  Reducer 3: 0/1  &lt;br/&gt;
Map 1: 0(+1)/1  Reducer 2: 0/1  Reducer 3: 0/1  &lt;br/&gt;
Map 1: 1/1      Reducer 2: 0/1  Reducer 3: 0/1  &lt;br/&gt;
Map 1: 1/1      Reducer 2: 0(+1)/1      Reducer 3: 0/1  &lt;br/&gt;
Map 1: 1/1      Reducer 2: 1/1  Reducer 3: 0(+1)/1      &lt;br/&gt;
Map 1: 1/1      Reducer 2: 1/1  Reducer 3: 1/1  &lt;br/&gt;
Status: DAG finished successfully in 523.42 seconds&lt;/p&gt;


&lt;p&gt;METHOD                         DURATION(ms) &lt;br/&gt;
parse                                   17&lt;br/&gt;
semanticAnalyze                      1,593&lt;br/&gt;
TezBuildDag                            585&lt;br/&gt;
TezSubmitToRunningDag                  187&lt;br/&gt;
TotalPrepTime                        3,522&lt;/p&gt;

&lt;p&gt;VERTICES         TOTAL_TASKS  FAILED_ATTEMPTS KILLED_TASKS DURATION_SECONDS    CPU_TIME_MILLIS     GC_TIME_MILLIS  INPUT_RECORDS   OUTPUT_RECORDS &lt;br/&gt;
Map 1                      1                0            0           516.72            752,950             15,318         13,440           11,516&lt;br/&gt;
Reducer 2                  1                0            0             0.81              1,890                 24         11,516           11,516&lt;br/&gt;
Reducer 3                  1                0            0             0.61              1,460                 19         11,516                0&lt;br/&gt;
OK&lt;br/&gt;
BB166674         P16     1&lt;/p&gt;</comment>
                            <comment id="14553061" author="gss2002" created="Wed, 20 May 2015 20:36:39 +0000"  >&lt;p&gt;Debug logs from DAG with compressed it sets 1 split.. so how do we fix this issue?&lt;/p&gt;


&lt;p&gt;2015-05-20 16:15:12,041 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Found plan in cache for name: map.xml&lt;br/&gt;
2015-05-20 16:15:12,055 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Processing alias gss_rsn2&lt;br/&gt;
2015-05-20 16:15:12,055 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Adding input file hdfs://xhadnnm1p.example.com:8020/apps/hive/warehouse/hue_debug.db/gss_rsn2&lt;br/&gt;
2015-05-20 16:15:12,057 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: hive.io.file.readcolumn.ids=&lt;br/&gt;
2015-05-20 16:15:12,058 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: hive.io.file.readcolumn.names=,arsn_cd,appl_user_id&lt;br/&gt;
2015-05-20 16:15:12,058 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: Generating splits&lt;br/&gt;
2015-05-20 16:15:12,087 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-20 16:15:12,087 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-20 16:15:12,087 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-20 16:15:12,087 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-20 16:15:12,088 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-20 16:15:12,088 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-20 16:15:12,092 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a&lt;br/&gt;
2015-05-20 16:15:12,216 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Time taken to get FileStatuses: 112&lt;br/&gt;
2015-05-20 16:15:12,216 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total input paths to process : 1&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-20 16:15:12,220 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-20 16:15:12,220 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a&lt;br/&gt;
2015-05-20 16:15:12,222 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total # of splits generated by getSplits: 1, TimeTaken: 132&lt;br/&gt;
2015-05-20 16:15:12,222 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: number of splits 1&lt;br/&gt;
2015-05-20 16:15:12,222 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=getSplits start=1432152912040 end=1432152912222 duration=182 from=org.apache.hadoop.hive.ql.io.HiveInputFormat&amp;gt;&lt;br/&gt;
2015-05-20 16:15:12,222 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.HiveSplitGenerator: Number of input splits: 1. 23542 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: **************&lt;b&gt;non-local mode&lt;/b&gt;**************&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: local path = hdfs://xhadnnm1p.example.com:8020/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml&lt;br/&gt;
2015-05-20 16:15:12,223 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Loading plan from string: /tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Deserializing MapWork via kryo&lt;br/&gt;
2015-05-20 16:15:12,239 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=deserializePlan start=1432152912223 end=1432152912239 duration=16 from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;&lt;br/&gt;
2015-05-20 16:15:12,240 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: Adding split hdfs://xhadnnm1p.example.com:8020/apps/hive/warehouse/hue_debug.db/gss_rsn2/000000_0.snappy to src new group? true&lt;br/&gt;
2015-05-20 16:15:12,240 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: # Src groups for split generation: 2&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_&lt;br/&gt;
socket&lt;br/&gt;
2015-05-20 16:15:12,091 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-20 16:15:12,092 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c&lt;br/&gt;
93595a&lt;br/&gt;
2015-05-20 16:15:12,216 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Time taken to get FileStatuses: 112&lt;br/&gt;
2015-05-20 16:15:12,216 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total input paths to process : 1&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-20 16:15:12,219 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_&lt;br/&gt;
socket&lt;br/&gt;
2015-05-20 16:15:12,220 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-20 16:15:12,220 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c&lt;br/&gt;
93595a&lt;br/&gt;
2015-05-20 16:15:12,222 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; mapred.FileInputFormat: Total # of splits generated by getSplits: 1, Tim&lt;br/&gt;
eTaken: 132&lt;br/&gt;
2015-05-20 16:15:12,222 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; io.HiveInputFormat: number of splits 1&lt;br/&gt;
2015-05-20 16:15:12,222 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=getSplits start=1432152912040 end=143215&lt;br/&gt;
2912222 duration=182 from=org.apache.hadoop.hive.ql.io.HiveInputFormat&amp;gt;&lt;br/&gt;
2015-05-20 16:15:12,222 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.HiveSplitGenerator: Number of input splits: 1. 23542 available slots,&lt;br/&gt;
 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: PLAN PATH = hdfs://xhadnnm1p.example.com:8020/tmp/hive/a760&lt;br/&gt;
104/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4&lt;br/&gt;
805-90a5-84b8c38fe1f4/map.xml&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: **************&lt;b&gt;non-local mode&lt;/b&gt;**************&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: local path = hdfs://xhadnnm1p.example.com:8020/tmp/hive/a76&lt;br/&gt;
0104/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-&lt;br/&gt;
4805-90a5-84b8c38fe1f4/map.xml&lt;br/&gt;
2015-05-20 16:15:12,223 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Loading plan from string: /tmp/hive/gss2002/646469af-0a8&lt;br/&gt;
7-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38&lt;br/&gt;
fe1f4/map.xml&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;PERFLOG method=deserializePlan from=org.apache.hadoop.hi&lt;br/&gt;
ve.ql.exec.Utilities&amp;gt;&lt;br/&gt;
2015-05-20 16:15:12,223 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; exec.Utilities: Deserializing MapWork via kryo&lt;br/&gt;
2015-05-20 16:15:12,239 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; log.PerfLogger: &amp;lt;/PERFLOG method=deserializePlan start=1432152912223 end=&lt;br/&gt;
1432152912239 duration=16 from=org.apache.hadoop.hive.ql.exec.Utilities&amp;gt;&lt;br/&gt;
2015-05-20 16:15:12,240 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: Adding split hdfs://xhadnnm1p.example.com:8020/apps/hive&lt;br/&gt;
/warehouse/hue_debug.db/gss_rsn2/000000_0.snappy to src new group? true&lt;br/&gt;
2015-05-20 16:15:12,240 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: # Src groups for split generation: 2&lt;br/&gt;
2015-05-20 16:15:12,241 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: Estimated number of tasks: 40021 for bucket 1&lt;br/&gt;
2015-05-20 16:15:12,241 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; split.TezMapredSplitsGrouper: Grouping splits in Tez&lt;br/&gt;
2015-05-20 16:15:12,241 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; split.TezMapredSplitsGrouper: Desired splits: 40021 too large.  Desired splitLength: 20 Min splitLength: 16777216 New desired splits: 1 Total length: 807489 Original splits: 1&lt;br/&gt;
2015-05-20 16:15:12,242 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; split.TezMapredSplitsGrouper: Using original number of splits: 1 desired splits: 1&lt;br/&gt;
2015-05-20 16:15:12,242 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.SplitGrouper: Original split size is 1 grouped split size is 1, for bucket: 1&lt;br/&gt;
2015-05-20 16:15:12,244 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; tez.HiveSplitGenerator: Number of grouped splits: 1&lt;br/&gt;
2015-05-20 16:15:12,251 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false&lt;br/&gt;
2015-05-20 16:15:12,251 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = true&lt;br/&gt;
2015-05-20 16:15:12,251 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false&lt;br/&gt;
2015-05-20 16:15:12,251 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; hdfs.BlockReaderLocal: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket&lt;br/&gt;
2015-05-20 16:15:12,251 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; retry.RetryUtils: multipleLinearRandomRetry = null&lt;br/&gt;
2015-05-20 16:15:12,251 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6c93595a&lt;br/&gt;
2015-05-20 16:15:12,252 TRACE &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.ProtobufRpcEngine: 85: Call -&amp;gt; xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo &lt;/p&gt;
{src: &quot;/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/map.xml&quot;}
&lt;p&gt;2015-05-20 16:15:12,253 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms&lt;br/&gt;
2015-05-20 16:15:12,253 TRACE &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.ProtobufRpcEngine: 85: Response &amp;lt;- xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo {}&lt;br/&gt;
2015-05-20 16:15:12,254 TRACE &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.ProtobufRpcEngine: 85: Call -&amp;gt; xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo &lt;/p&gt;
{src: &quot;/tmp/hive/gss2002/646469af-0a87-4080-9d2b-e40af4a34c0e/hive_2015-05-20_16-15-06_565_5281905327000741927-1/gss2002/_tez_scratch_dir/049d6a0d-aea4-4805-90a5-84b8c38fe1f4/reduce.xml&quot;}
&lt;p&gt;2015-05-20 16:15:12,255 DEBUG &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms&lt;br/&gt;
2015-05-20 16:15:12,255 TRACE &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; ipc.ProtobufRpcEngine: 85: Response &amp;lt;- xhadnnm1p.example.com/167.69.200.200:8020: getFileInfo {}&lt;br/&gt;
2015-05-20 16:15:12,255 INFO &lt;a href=&quot;#0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InputInitializer [Map 1] #0&lt;/a&gt; dag.RootInputInitializerManager: Succeeded InputInitializer for Input: gss_rsn2 on vertex vertex_1426958683478_173564_1_00 &lt;span class=&quot;error&quot;&gt;&amp;#91;Map 1&amp;#93;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="14555456" author="gss2002" created="Fri, 22 May 2015 02:10:53 +0000"  >&lt;p&gt;After having offline discussion with Gopal V he determined the cause of this problem is that starting in Hive 0.14 org.apache.hadoop.mapred.TextInputFormat  uses whatever is defined in property: mapreduce.input.fileinputformat.split.minsize; In my case this was defined to &quot;1&quot;... Unfortunately that is 1 byte so it created 40040 splits creating 40400 reads of the single 3MB file...&lt;/p&gt;

&lt;p&gt;Hope this helps someone else out.&lt;/p&gt;

&lt;p&gt;Should be around half of the HDFS block size in my case 64MB since my block size is 128MB..&lt;br/&gt;
mapreduce.input.fileinputformat.split.minsize=67108864&lt;/p&gt;


&lt;p&gt;Gopal V if no fix is coming should we resolve/close this JIRA?&lt;/p&gt;</comment>
                            <comment id="14585306" author="gopalv" created="Sun, 14 Jun 2015 23:06:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gss2002&quot; class=&quot;user-hover&quot; rel=&quot;gss2002&quot;&gt;gss2002&lt;/a&gt;: Talking to the MRv2 folks to change the defaults to be saner than 1 byte.&lt;/p&gt;

&lt;p&gt;Until that issue is resolved, I&apos;ll keep this open as a critical issue.&lt;/p&gt;</comment>
                            <comment id="14590903" author="gopalv" created="Thu, 18 Jun 2015 00:15:49 +0000"  >&lt;p&gt;Fixing legacy &lt;tt&gt;Mapred.TextInputFormat&lt;/tt&gt; is fraught with issues.&lt;/p&gt;

&lt;p&gt;Allow Tez split generation to set a sane default if the min-size is misconfigured during execution.&lt;/p&gt;</comment>
                            <comment id="14590912" author="hagleitn" created="Thu, 18 Jun 2015 00:21:32 +0000"  >&lt;p&gt;+1. (Nit: ws issue, the closing brace for the if statement is not correctly aligned).&lt;/p&gt;</comment>
                            <comment id="14591479" author="hiveqa" created="Thu, 18 Jun 2015 08:37:25 +0000"  >

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;Overall&lt;/font&gt;: -1 at least one tests failed&lt;/p&gt;

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12740251/HIVE-10746.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12740251/HIVE-10746.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 2 failed/errored test(s), 9009 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_corr
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_join28
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4299/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4299/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4299/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4299/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4299/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-4299/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12740251 - PreCommit-HIVE-TRUNK-Build&lt;/p&gt;</comment>
                            <comment id="14592102" author="gopalv" created="Thu, 18 Jun 2015 17:00:37 +0000"  >&lt;p&gt;Test failures look unrelated.&lt;/p&gt;

&lt;p&gt;Reformat before commit.&lt;/p&gt;</comment>
                            <comment id="14592471" author="hagleitn" created="Thu, 18 Jun 2015 20:39:08 +0000"  >&lt;p&gt;Committed to 1.2.1, branch-1 and master.&lt;/p&gt;</comment>
                            <comment id="14592604" author="sushanth" created="Thu, 18 Jun 2015 21:54:09 +0000"  >&lt;p&gt;Please add to the release wiki ( &lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Hive+1.2+Release+Status&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/Hive/Hive+1.2+Release+Status&lt;/a&gt; ) when you commit any patch to branch-1.2. I&apos;ll go ahead and add this one in.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12740251" name="HIVE-10746.1.patch" size="2926" author="gopalv" created="Thu, 18 Jun 2015 00:15:49 +0000"/>
                            <attachment id="12740431" name="HIVE-10746.2.patch" size="2931" author="gopalv" created="Thu, 18 Jun 2015 17:00:37 +0000"/>
                            <attachment id="12733910" name="slow_query_output.zip" size="457404" author="gss2002" created="Tue, 19 May 2015 18:39:11 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 22 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2ewon:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Use sane split min-sizes when using legacy mapred.InputFormat::getSplits(job, num)</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>