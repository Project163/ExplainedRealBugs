<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:49:09 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-15237] Propagate Spark job failure to Hive</title>
                <link>https://issues.apache.org/jira/browse/HIVE-15237</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;If a Spark job failed for some reason, Hive doesn&apos;t get any additional error message, which makes it very hard for user to figure out why. Here is an example:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Status: Running (Hive on Spark job[0])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost]
2016-11-17 21:32:53,134	Stage-0_0: 0/23	Stage-1_0: 0/28	
2016-11-17 21:32:55,156	Stage-0_0: 0(+1)/23	Stage-1_0: 0/28	
2016-11-17 21:32:57,167	Stage-0_0: 0(+3)/23	Stage-1_0: 0/28	
2016-11-17 21:33:00,216	Stage-0_0: 0(+3)/23	Stage-1_0: 0/28	
2016-11-17 21:33:03,251	Stage-0_0: 0(+3)/23	Stage-1_0: 0/28	
2016-11-17 21:33:06,286	Stage-0_0: 0(+4)/23	Stage-1_0: 0/28	
2016-11-17 21:33:09,308	Stage-0_0: 0(+2,-3)/23	Stage-1_0: 0/28	
2016-11-17 21:33:12,332	Stage-0_0: 0(+2,-3)/23	Stage-1_0: 0/28	
2016-11-17 21:33:13,338	Stage-0_0: 0(+21,-3)/23	Stage-1_0: 0/28	
2016-11-17 21:33:15,349	Stage-0_0: 0(+21,-5)/23	Stage-1_0: 0/28	
2016-11-17 21:33:16,358	Stage-0_0: 0(+18,-8)/23	Stage-1_0: 0/28	
2016-11-17 21:33:19,373	Stage-0_0: 0(+21,-8)/23	Stage-1_0: 0/28	
2016-11-17 21:33:22,400	Stage-0_0: 0(+18,-14)/23	Stage-1_0: 0/28	
2016-11-17 21:33:23,404	Stage-0_0: 0(+15,-20)/23	Stage-1_0: 0/28	
2016-11-17 21:33:24,408	Stage-0_0: 0(+12,-23)/23	Stage-1_0: 0/28	
2016-11-17 21:33:25,417	Stage-0_0: 0(+9,-26)/23	Stage-1_0: 0/28	
2016-11-17 21:33:26,420	Stage-0_0: 0(+12,-26)/23	Stage-1_0: 0/28	
2016-11-17 21:33:28,427	Stage-0_0: 0(+9,-29)/23	Stage-1_0: 0/28	
2016-11-17 21:33:29,432	Stage-0_0: 0(+12,-29)/23	Stage-1_0: 0/28	
2016-11-17 21:33:31,444	Stage-0_0: 0(+18,-29)/23	Stage-1_0: 0/28	
2016-11-17 21:33:34,464	Stage-0_0: 0(+18,-29)/23	Stage-1_0: 0/28	
Status: Failed
FAILED: Execution Error, &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It would be better if we can propagate Spark error to Hive.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13021630">HIVE-15237</key>
            <summary>Propagate Spark job failure to Hive</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lirui">Rui Li</assignee>
                                    <reporter username="xuefuz">Xuefu Zhang</reporter>
                        <labels>
                    </labels>
                <created>Fri, 18 Nov 2016 00:05:27 +0000</created>
                <updated>Fri, 21 Jul 2017 18:26:25 +0000</updated>
                            <resolved>Thu, 24 Nov 2016 08:29:23 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15675223" author="xuefuz" created="Fri, 18 Nov 2016 00:06:02 +0000"  >&lt;p&gt;fyi: &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15675299" author="lirui" created="Fri, 18 Nov 2016 00:42:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, thanks for raising the issue. I&apos;ll take a look at this.&lt;/p&gt;</comment>
                            <comment id="15675417" author="xuefuz" created="Fri, 18 Nov 2016 01:38:15 +0000"  >&lt;p&gt;Thanks, Rui. For your reference, I attached a POC patch that was based on CDH 5.7. With the patch, I got the following on the console, which seems having too much and duplicated info. Please feel free to adapt and modify.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2016-11-18 00:10:17,732	Stage-0_0: 1(+12,-41)/23	Stage-1_0: 0/28	
Status: Failed: org.apache.spark.SparkException: java.util.concurrent.ExecutionException: Exception thrown by job
	at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:311)
	at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)
	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:382)
	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:335)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 4 times, most recent failure: Lost task 9.3 in stage 0.0 (TID 39, hadoopworker863-sjc1.prod.uber.internal): java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:265)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&amp;lt;init&amp;gt;(HadoopShimsSecure.java:212)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:332)
	at com.uber.hoodie.hadoop.hive.HoodieCombineHiveInputFormat.getRecordReader(HoodieCombineHiveInputFormat.java:741)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:237)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:251)
	... 18 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:778)
	at parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:129)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:212)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:123)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:85)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&amp;lt;init&amp;gt;(CombineHiveRecordReader.java:67)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:265)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&amp;lt;init&amp;gt;(HadoopShimsSecure.java:212)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:332)
	at com.uber.hoodie.hadoop.hive.HoodieCombineHiveInputFormat.getRecordReader(HoodieCombineHiveInputFormat.java:741)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:237)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:251)
	... 18 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:778)
	at parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:129)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:212)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:123)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:85)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&amp;lt;init&amp;gt;(CombineHiveRecordReader.java:67)
	... 23 more

FAILED: Execution Error, &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. java.util.concurrent.ExecutionException: Exception thrown by job
	at org.apache.spark.JavaFutureActionWrapper.getImpl(FutureAction.scala:311)
	at org.apache.spark.JavaFutureActionWrapper.get(FutureAction.scala:316)
	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:382)
	at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:335)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 4 times, most recent failure: Lost task 9.3 in stage 0.0 (TID 39, hadoopworker863-sjc1.prod.uber.internal): java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:265)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&amp;lt;init&amp;gt;(HadoopShimsSecure.java:212)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:332)
	at com.uber.hoodie.hadoop.hive.HoodieCombineHiveInputFormat.getRecordReader(HoodieCombineHiveInputFormat.java:741)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:237)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:251)
	... 18 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:778)
	at parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:129)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:212)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:123)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:85)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&amp;lt;init&amp;gt;(CombineHiveRecordReader.java:67)
	... 23 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:265)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.&amp;lt;init&amp;gt;(HadoopShimsSecure.java:212)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:332)
	at com.uber.hoodie.hadoop.hive.HoodieCombineHiveInputFormat.getRecordReader(HoodieCombineHiveInputFormat.java:741)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.&amp;lt;init&amp;gt;(HadoopRDD.scala:237)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:208)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:251)
	... 18 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at parquet.hadoop.ParquetFileReader$ConsecutiveChunkList.readAll(ParquetFileReader.java:778)
	at parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:511)
	at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:129)
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:212)
	at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:227)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:123)
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&amp;lt;init&amp;gt;(ParquetRecordReaderWrapper.java:85)
	at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&amp;lt;init&amp;gt;(CombineHiveRecordReader.java:67)
	... 23 more

hive&amp;gt; quit;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15686234" author="lirui" created="Tue, 22 Nov 2016 09:51:23 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt; for the patch. I made some modifications based on your work. Tried to parse the root cause from the Throwable. Then print the root cause to console and leave the detailed message to the log.&lt;br/&gt;
Please try it and let me know your opinions. Thanks.&lt;/p&gt;</comment>
                            <comment id="15686449" author="hiveqa" created="Tue, 22 Nov 2016 11:18:44 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12839982/HIVE-15237.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12839982/HIVE-15237.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10701 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=102)
	[skewjoinopt3.q,smb_mapjoin_4.q,timestamp_comparison.q,union_remove_10.q,mapreduce2.q,bucketmapjoin_negative.q,udf_in_file.q,auto_join12.q,skewjoin.q,vector_left_outer_join.q,semijoin.q,skewjoinopt9.q,smb_mapjoin_3.q,stats10.q,nullgroup4.q]
TestSparkCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=112)
	[escape_distributeby1.q,join9.q,groupby2.q,groupby4_map.q,udf_max.q,vectorization_pushdown.q,cbo_gby_empty.q,join_cond_pushdown_unqual3.q,vectorization_short_regress.q,join8.q,stats5.q,sample10.q,cross_product_check_1.q,auto_join_stats.q,input_part2.q]
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_4] (batchId=91)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/2240/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/2240/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/2240/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/2240/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-2240/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-2240/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12839982 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15688770" author="lirui" created="Wed, 23 Nov 2016 03:33:28 +0000"  >&lt;p&gt;Cannot reproduce the failures. Try again.&lt;/p&gt;</comment>
                            <comment id="15688835" author="xuefuz" created="Wed, 23 Nov 2016 04:01:41 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, thanks for the patch. It looks good. +1. Let&apos;s see how the test goes.&lt;/p&gt;</comment>
                            <comment id="15688957" author="hiveqa" created="Wed, 23 Nov 2016 05:16:46 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12840195/HIVE-15237.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12840195/HIVE-15237.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 4 failed/errored test(s), 10732 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[transform_ppr2] (batchId=133)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=150)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=145)
org.apache.hive.spark.client.TestSparkClient.testJobSubmission (batchId=272)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/2253/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/2253/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/2253/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/2253/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-2253/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-2253/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 4 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12840195 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="15688985" author="lirui" created="Wed, 23 Nov 2016 05:28:07 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;. The latest failures are not related. I&apos;ll commit this if no one has further comments.&lt;/p&gt;</comment>
                            <comment id="15692631" author="lirui" created="Thu, 24 Nov 2016 08:29:24 +0000"  >&lt;p&gt;Committed to master. Thanks Xuefu for the review.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12840195" name="HIVE-15237.2.patch" size="4841" author="lirui" created="Wed, 23 Nov 2016 03:33:28 +0000"/>
                            <attachment id="12839982" name="HIVE-15237.2.patch" size="4841" author="lirui" created="Tue, 22 Nov 2016 09:51:23 +0000"/>
                            <attachment id="12839483" name="HIVE-15237.patch" size="6830" author="xuefuz" created="Fri, 18 Nov 2016 01:38:15 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 51 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i36hf3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>