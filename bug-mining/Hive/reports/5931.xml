<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:57:59 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-15104] Hive on Spark generate more shuffle data than hive on mr</title>
                <link>https://issues.apache.org/jira/browse/HIVE-15104</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;the same sql,  running on spark  and mr engine, will generate different size of shuffle data.&lt;/p&gt;

&lt;p&gt;i think it is because of hive on mr just serialize part of HiveKey, but hive on spark which using kryo will serialize full of Hivekey object.  &lt;/p&gt;

&lt;p&gt;what is your opionion?&lt;/p&gt;</description>
                <environment></environment>
        <key id="13016935">HIVE-15104</key>
            <summary>Hive on Spark generate more shuffle data than hive on mr</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lirui">Rui Li</assignee>
                                    <reporter username="wenli">wangwenli</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Nov 2016 16:08:42 +0000</created>
                <updated>Tue, 22 May 2018 23:58:02 +0000</updated>
                            <resolved>Wed, 25 Oct 2017 03:11:47 +0000</resolved>
                                    <version>1.2.1</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>Spark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="15626085" author="aihuaxu" created="Tue, 1 Nov 2016 17:32:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wenli&quot; class=&quot;user-hover&quot; rel=&quot;wenli&quot;&gt;wenli&lt;/a&gt; Can you give an example that I can run and compare? &lt;/p&gt;</comment>
                            <comment id="15627812" author="wenli" created="Wed, 2 Nov 2016 05:34:40 +0000"  >&lt;p&gt;try select count(distinct col1), count (distinct col2) from table,   and see the statistic for shuffle data size.&lt;/p&gt;

&lt;p&gt;if you cann&apos;t reproduce, let me know.    i will find one table in tpch, and reproduce , then tell the details step.  &lt;/p&gt;

&lt;p&gt;thank you~&lt;/p&gt;</comment>
                            <comment id="15628605" author="lirui" created="Wed, 2 Nov 2016 11:02:02 +0000"  >&lt;p&gt;Seems MR can just serialize the key as BytesWritable instead of HiveKey. We once hit some problem when only serializing the BytesWritable part. But I think it&apos;s worth investigating whether we can improve.&lt;/p&gt;</comment>
                            <comment id="15630435" author="aihuaxu" created="Wed, 2 Nov 2016 20:48:21 +0000"  >&lt;p&gt;This is changed by &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-8017&quot; title=&quot;Use HiveKey instead of BytesWritable as key type of the pair RDD [Spark Branch]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-8017&quot;&gt;&lt;del&gt;HIVE-8017&lt;/del&gt;&lt;/a&gt;. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; Do you recall what kind of issues it caused?&lt;/p&gt;</comment>
                            <comment id="15631053" author="lirui" created="Thu, 3 Nov 2016 00:42:47 +0000"  >&lt;p&gt;We need to use HiveKey because it holds the proper hash code to be used for partitioning. MR also uses HiveKey, but in OutputCollector, seems it only serializes the BytesWritable part. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wenli&quot; class=&quot;user-hover&quot; rel=&quot;wenli&quot;&gt;wenli&lt;/a&gt;, is this what you mean?&lt;br/&gt;
I suspect we&apos;ll need help from Spark if we want to do something similar.&lt;/p&gt;</comment>
                            <comment id="15631376" author="xuefuz" created="Thu, 3 Nov 2016 03:14:14 +0000"  >&lt;p&gt;This is rather interesting. I know I originally reviewed &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-8017&quot; title=&quot;Use HiveKey instead of BytesWritable as key type of the pair RDD [Spark Branch]&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-8017&quot;&gt;&lt;del&gt;HIVE-8017&lt;/del&gt;&lt;/a&gt;, but I didn&apos;t really know why ByteWritable works for MR while we need HiveKey for Spark. Since Spark is stable now, it would be interesting to find out at least why, whether we can optimize or not.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ruili&quot; class=&quot;user-hover&quot; rel=&quot;ruili&quot;&gt;ruili&lt;/a&gt;, since you originally discovered the problem, could you revisit the issue? Thanks.&lt;/p&gt;</comment>
                            <comment id="15632244" author="lirui" created="Thu, 3 Nov 2016 09:52:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, here&apos;s what I find so far.&lt;br/&gt;
Firstly, MR uses HiveKey as the &lt;a href=&quot;https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java#L253&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;key type&lt;/a&gt;. So we&apos;re inline with MR. HiveKey extends BytesWritable. I think the main reason we need HiveKey is we don&apos;t want the hash code simply computed from the internal bytes. Instead, we somehow compute the hash code elsewhere and set it into HiveKey.&lt;/p&gt;

&lt;p&gt;During shuffle, MR passes the HiveKey to OutputCollector. OutputCollector computes the proper partition for HiveKey (using the hash code), and uses &lt;a href=&quot;https://github.com/apache/hadoop/blob/release-2.7.2-RC2/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/WritableSerialization.java#L97&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;WritableSerialization&lt;/a&gt; to serialize it. At this point, it&apos;s OK to just serialize the BytesWritable part since the partition has already been figured out.&lt;/p&gt;

&lt;p&gt;On reduce side, WritableSerialization is used again to deserialize input key as HiveKey, then feed it to ExecReducer. Of course at this point, the HiveKey&apos;s hash code is just 0. But it doesn&apos;t matter because it&apos;s not needed any more. And ExecReducer just cast the input key as BytesWritable.&lt;/p&gt;

&lt;p&gt;Therefore, I think whether we can achieve the same depends on how Spark deals with the HiveKey we pass to it. If that&apos;s possible, we can register a custom Serializer for HiveKey and only ser/de the BytesWritable part. Here&apos;re some docs regarding how to do that:&lt;br/&gt;
&lt;a href=&quot;http://spark.apache.org/docs/latest/configuration.html#compression-and-serialization&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;spark.kryo.registrator&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://github.com/EsotericSoftware/kryo#registration&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;kryo registration&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15632623" author="aihuaxu" created="Thu, 3 Nov 2016 12:44:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; So what you are saying is, it depends on how spark shuffles the data and whether spark relies on such hashCode to shuffle the data? &lt;/p&gt;</comment>
                            <comment id="15633820" author="xuefuz" created="Thu, 3 Nov 2016 18:52:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, thanks for sharing your findings. Can you confirm that Spark also uses BytesWritable.hashcode() to partition the RS output rows? If this is true, then there is no difference for Spark because the actual object Hive passed to Spark by RS is HiveKey, whose hashcode will be used for partitioning. &lt;/p&gt;

&lt;p&gt;If this is the case, then we should be able to define the output of our map function and reduce function just as &amp;lt;BytesWritable, BytesWritable&amp;gt;, for which we don&apos;t need a custom serializer because we don&apos;t need to declare the type as &amp;lt;HiveKey, BytesWritable&amp;gt;. It seems that there is still a gap in our understanding.&lt;/p&gt;
</comment>
                            <comment id="15634112" author="xuefuz" created="Thu, 3 Nov 2016 20:18:48 +0000"  >&lt;p&gt;I checked the source code and it seems that both Spark (Partitioner.scala) and MapReduce (HashPartitioner.java) calls key.hashCode() to get the partition number.&lt;/p&gt;</comment>
                            <comment id="15634914" author="lirui" created="Fri, 4 Nov 2016 01:48:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;aihuaxu&lt;/a&gt;, both MR and Spark need HiveKey.hashCode to compute the partition number. HiveKey&apos;s &lt;a href=&quot;https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/HiveKey.java#L58&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;hashCode&lt;/a&gt; is not computed on demand. Instead it&apos;s a field we set to the HiveKey.&lt;br/&gt;
I think what we need to investigate is, whether the hash code is still needed after the HiveKey is serialized, e.g. Spark somehow deserialize the HiveKey and access the hash code again. If not, we can just ser/de the BytesWritable part of HiveKey because the hash code is not needed any more.&lt;/p&gt;</comment>
                            <comment id="15644296" author="aihuaxu" created="Mon, 7 Nov 2016 14:20:15 +0000"  >&lt;p&gt;I will take a look at Spark to see if it&apos;s needed after it&apos;s serialized.&lt;/p&gt;</comment>
                            <comment id="15960146" author="lirui" created="Fri, 7 Apr 2017 01:45:55 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aihuaxu&quot; class=&quot;user-hover&quot; rel=&quot;aihuaxu&quot;&gt;aihuaxu&lt;/a&gt;, are you still working on this? If not, do you mind if I take over?&lt;/p&gt;</comment>
                            <comment id="15960801" author="aihuaxu" created="Fri, 7 Apr 2017 13:34:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; I didn&apos;t have time to work on that . Feel free to take it over. &lt;/p&gt;</comment>
                            <comment id="15998177" author="lirui" created="Fri, 5 May 2017 11:18:29 +0000"  >&lt;p&gt;I looked at the shuffle writers of Spark and none of them seem to need the hashCode/partitionId after the HiveKey is serialized. But I got a problem during implementation. The plan is to implement this Spark trait:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;trait KryoRegistrator {
  def registerClasses(kryo: Kryo): Unit
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then we set this implementing class to &lt;tt&gt;spark.kryo.registrator&lt;/tt&gt;. At runtime, Spark will use reflection to instantiate our class and call its registerClasses to register the optimized SerDe for HiveKey.&lt;br/&gt;
However, Kryo is relocated in Hive. After build, the method signature of our class will actually be:&lt;br/&gt;
&lt;tt&gt;public void registerClasses(org.apache.hive.com.esotericsoftware.kryo.Kryo kryo)&lt;/tt&gt;.&lt;br/&gt;
When Spark calls the method, we get an &lt;tt&gt;AbstractMethodError&lt;/tt&gt;. I suppose this is because the &lt;tt&gt;public void registerClasses(com.esotericsoftware.kryo.Kryo kryo)&lt;/tt&gt; method is not really implemented.&lt;br/&gt;
Does anybody know how this can be resolved?&lt;/p&gt;</comment>
                            <comment id="15998437" author="lirui" created="Fri, 5 May 2017 14:56:28 +0000"  >&lt;p&gt;Tried disabling relocation locally. It does solve the AbstractMethodError. However, seems Spark still needs the hashCode on reducer side. Will dig more ...&lt;/p&gt;</comment>
                            <comment id="16007788" author="lirui" created="Fri, 12 May 2017 08:26:13 +0000"  >&lt;p&gt;Spark needs the hash code on reducer side for the groupBy shuffling. Since groupBy does no ordering, reducer needs to put the shuffled data into a map to combine values by key, thus needing the hash code. We just need to keep the hash code during SerDe if groupBy shuffle is used.&lt;/p&gt;

&lt;p&gt;Upload a PoC patch to demonstrate the idea. It disables kryo relocation which should not be acceptable.&lt;/p&gt;

&lt;p&gt;Also did simple test to see the improvement. The test is to run a query: &lt;tt&gt;select key, count ( * ) from A group by key order by key;&lt;/tt&gt;, where A contains 40000000 records with 20 distinct keys. The measurement is the number of bytes written during shuffle. I tested optimize HiveKey alone, as well as optimize HiveKey and BytesWritable. We can see even for simple classes like BytesWritable, the custom SerDe does better than a generic one.&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Opt(N)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Opt(Y, Key)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Opt(Y, Key + Value)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;GBY(Y)&lt;/th&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2269&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1953&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1699&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;GBY(N)&lt;/th&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2269&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1713&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1460&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="16008107" author="xuefuz" created="Fri, 12 May 2017 13:16:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, great progress! Thanks for keeping up the effort.&lt;/p&gt;

&lt;p&gt;As to Kryo class relocation, I think Hive did that to avoid version difference between Spark and Hive. (git history might confirm this.) I&apos;m concerned that class conflicts might come back if we stop relocating Kryo. Any thoughts?&lt;/p&gt;</comment>
                            <comment id="16008502" author="hiveqa" created="Fri, 12 May 2017 18:10:54 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12867727/HIVE-15104.1.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12867727/HIVE-15104.1.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10688 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_3] (batchId=97)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=97)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=89)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testSparkQuery (batchId=225)
org.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testTempTable (batchId=225)
org.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery (batchId=225)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5226/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5226/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5226/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5226/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5226/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5226/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12867727 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16009026" author="lirui" created="Sat, 13 May 2017 01:34:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, kryo was relocated in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-5915&quot; title=&quot;Shade Kryo dependency&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-5915&quot;&gt;&lt;del&gt;HIVE-5915&lt;/del&gt;&lt;/a&gt;. So it&apos;s not intended for Spark. Actually, we&apos;re on the same version as Spark-2.0.0: kryo-shaded-3.0.3.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I&apos;m concerned that class conflicts might come back if we stop relocating Kryo&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You&apos;re right. I&apos;m not sure whether it&apos;s a conflict or loading issue, but when I tried to run some TPC-H benchmark, I got a ClassNotFoundException, although the class is there in hive-exec.jar. I&apos;ll see how to workaround this.&lt;/p&gt;

&lt;p&gt;BTW, the test in my last comment shuffles very little data. That&apos;s why optimizing the overhead can have a significant improvement. I guess this won&apos;t be the case in real world query. That&apos;s why I want to run some more serious benchmark.&lt;/p&gt;</comment>
                            <comment id="16010348" author="lirui" created="Mon, 15 May 2017 11:29:43 +0000"  >&lt;p&gt;The CNF is due to how kryo is loaded in &lt;tt&gt;KryoMessageCodec&lt;/tt&gt;. W/ relocation, kryo is in package &lt;tt&gt;org.apache.hive.com.esotericsoftware&lt;/tt&gt;. So it&apos;s loaded from hive-exec.jar. Spark adds hive-exec.jar at runtime with some URL class loader. W/o relocation, we&apos;re using same kryo as Spark. Kryo&apos;s class loader is by default the one that loads it - therefore the AppClassLoader. However, AppClassLoader cannot load classes from hive-exec.jar and thus the CNF.&lt;br/&gt;
To solve it, we can make &lt;tt&gt;KryoMessageCodec&lt;/tt&gt; use the current context loader.&lt;/p&gt;</comment>
                            <comment id="16010433" author="hiveqa" created="Mon, 15 May 2017 12:49:08 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12868055/HIVE-15104.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12868055/HIVE-15104.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 3 failed/errored test(s), 10698 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[parquet_ppd_decimal] (batchId=9)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=144)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testConcurrentStatements (batchId=225)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5260/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5260/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5260/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5260/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5260/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5260/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12868055 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16017527" author="hiveqa" created="Fri, 19 May 2017 15:17:58 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12868935/HIVE-15104.3.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12868935/HIVE-15104.3.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 1 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 3 failed/errored test(s), 10738 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_3] (batchId=97)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainuser_3] (batchId=97)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query24] (batchId=231)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5349/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5349/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5349/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5349/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5349/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5349/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12868935 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16017621" author="lirui" created="Fri, 19 May 2017 16:27:10 +0000"  >&lt;p&gt;Patch v3 compiles the registrators at runtime, so that we don&apos;t have to disable kryo relocation. I&apos;ve also put it on RB. Will upload a benchmark result later.&lt;/p&gt;</comment>
                            <comment id="16018496" author="lirui" created="Sat, 20 May 2017 15:12:05 +0000"  >&lt;p&gt;Attaching TPC-H benchmark result. It shows the improvement is more obvious for long queries when we need to shuffle a lot of data. And it&apos;s better to use with groupBy shuffle disabled.&lt;/p&gt;</comment>
                            <comment id="16018507" author="hiveqa" created="Sat, 20 May 2017 15:31:34 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12869112/TPC-H%20100G.xlsx&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12869112/TPC-H%20100G.xlsx&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5367/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5367/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5367/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5367/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5367/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5367/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command &apos;bash /data/hiveptest/working/scratch/source-prep.sh&apos; failed with exit status 1 and output &apos;+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-05-20 15:31:32.562
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export &apos;ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m &apos;
+ ANT_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m &apos;
+ export &apos;MAVEN_OPTS=-Xmx1g &apos;
+ MAVEN_OPTS=&apos;-Xmx1g &apos;
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-5367/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-05-20 15:31:32.564
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 7429f5f HIVE-16717: Extend shared scan optimizer to handle partitions (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
+ git clean -f -d
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimal.txt
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgDecimalMerge.txt
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgMerge.txt
Removing ql/src/gen/vectorization/UDAFTemplates/VectorUDAFAvgTimestamp.txt
Removing ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumTimestamp.java
+ git checkout master
Already on &apos;master&apos;
Your branch is up-to-date with &apos;origin/master&apos;.
+ git reset --hard origin/master
HEAD is now at 7429f5f HIVE-16717: Extend shared scan optimizer to handle partitions (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-05-20 15:31:33.510
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
patch: **** Only garbage was found in the patch input.
patch: **** Only garbage was found in the patch input.
patch: **** Only garbage was found in the patch input.
fatal: unrecognized input
The patch does not appear to apply with p0, p1, or p2
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12869112 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16052006" author="lirui" created="Fri, 16 Jun 2017 14:59:55 +0000"  >&lt;p&gt;The approach here can cause problem when we cache RDDs, e.g. combining equivalent works. The cached RDDs will be serialized when stored to disk or transferred via network, then we need the hash code after the data is deserialized. I think we have to ser/de the hash code anyway to be safe.&lt;/p&gt;</comment>
                            <comment id="16085271" author="lirui" created="Thu, 13 Jul 2017 06:44:46 +0000"  >&lt;p&gt;Update patch v4:&lt;br/&gt;
1. Moved the registrator code to a resource file. Hopefully the patch is more readable.&lt;br/&gt;
2. To be safe, we still have to store the hash code. But that&apos;s still better than the generic serializer.&lt;/p&gt;</comment>
                            <comment id="16085276" author="lirui" created="Thu, 13 Jul 2017 06:51:11 +0000"  >&lt;p&gt;I also run another round of TPC-DS. The overall shuffle data is reduced by 12%. The query time improvement is however negligible - about 1.5%.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt; do you think it&apos;s worth the effort?&lt;/p&gt;</comment>
                            <comment id="16085324" author="xuefuz" created="Thu, 13 Jul 2017 07:53:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, I&apos;m wondering if there is anything new (other than moving code around). Last time we benchmarked and found there was actual performance degradation. We can do that again, and if the perf degradation still exists, we may not want this at lest not by default. I wasn&apos;t able to figure out why this degradation might happen.&lt;/p&gt;</comment>
                            <comment id="16085384" author="lirui" created="Thu, 13 Jul 2017 08:41:42 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, I can&apos;t reproduce the perf degradation on my side. Some case runs slower with the patch which I think is just variations. The overall perf (total time taken to run the benchmark) is still slightly improved. I don&apos;t have physical nodes to run the benchmark so it&apos;s done on VMs. It&apos;d be great if you could rerun your benchmark, and we can take a closer look at the degradation. Thanks.&lt;/p&gt;</comment>
                            <comment id="16085465" author="hiveqa" created="Thu, 13 Jul 2017 09:46:24 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12877028/HIVE-15104.4.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12877028/HIVE-15104.4.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 1 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10889 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6004/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6004/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6004/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6004/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-6004/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-6004/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12877028 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16129871" author="lirui" created="Thu, 17 Aug 2017 04:13:59 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, with &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-17114&quot; title=&quot;HoS: Possible skew in shuffling when data is not really skewed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-17114&quot;&gt;&lt;del&gt;HIVE-17114&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-17321&quot; title=&quot;HoS: analyze ORC table doesn&amp;#39;t compute raw data size when noscan/partialscan is not specified&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-17321&quot;&gt;&lt;del&gt;HIVE-17321&lt;/del&gt;&lt;/a&gt; the benchmark results become more stable and the improvement is a little higher. Here&apos;s the latest &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1ba-AbUpJOHNb0_5PZyWQHzrH4wfRljMxQP9vA9JACHg/edit?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;100GB TPC-DS result&lt;/a&gt;.&lt;br/&gt;
Would you mind share your benchmark tool so that I can look into the perf degradation? Thanks.&lt;/p&gt;</comment>
                            <comment id="16129895" author="xuefuz" created="Thu, 17 Aug 2017 04:26:12 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, thanks for continuing the work here. The improvement is impressive and not much perf degradation is observed. Let me get back my old benchmarks and see if those patches help.&lt;/p&gt;</comment>
                            <comment id="16133290" author="xuefuz" created="Fri, 18 Aug 2017 17:10:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, I found it difficulty to backport &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-17114&quot; title=&quot;HoS: Possible skew in shuffling when data is not really skewed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-17114&quot;&gt;&lt;del&gt;HIVE-17114&lt;/del&gt;&lt;/a&gt; to our code base, so I had to give up. However, since you have a configuration to turn this on/off, I think it&apos;s find to have this and postpone the verification on my side later until we upgrade our Hive.&lt;/p&gt;

&lt;p&gt;I need some time to review your latest patch as it&apos;s different from the previous has some low-level class/jar manipulations.&lt;/p&gt;</comment>
                            <comment id="16134639" author="lirui" created="Mon, 21 Aug 2017 02:28:06 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt; and take your time. I guess we can also run a round of QA test with the switch turned on.&lt;/p&gt;</comment>
                            <comment id="16134983" author="lirui" created="Mon, 21 Aug 2017 10:01:26 +0000"  >&lt;p&gt;Run tests with the switch on.&lt;/p&gt;</comment>
                            <comment id="16141431" author="hiveqa" created="Fri, 25 Aug 2017 09:47:29 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12883637/HIVE-15104.5.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12883637/HIVE-15104.5.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 1 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 3 failed/errored test(s), 11001 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=61)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=169)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testHttpRetryOnServerIdleTimeout (batchId=228)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6535/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6535/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/6535/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/6535/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-6535/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-6535/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 3 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12883637 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16148174" author="xuefuz" created="Wed, 30 Aug 2017 22:47:59 +0000"  >&lt;p&gt;The patch looks good to me. My only concern is about the reliability of the runtime compilation and jar creating. I&apos;d think it&apos;s best if we can avoid that.&lt;/p&gt;

&lt;p&gt;I&apos;m not 100% sure of the class loading problem we faced. If we define class HiveKryoRegistrator in Hive, with relocation, Spark&apos;s unrelocated kryo isn&apos;t able to find it?&lt;/p&gt;</comment>
                            <comment id="16148340" author="lirui" created="Thu, 31 Aug 2017 02:03:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, my previous &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15104?focusedCommentId=15998177&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15998177&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;comment&lt;/a&gt; has some explanations about the relocation problem. Basically, the problem is we need to implement some method defined by Spark, and the method accepts a kryo parameter. With relocation, Hive&apos;s kryo and Spark&apos;s kryo are in different packages. If we compile the class in Hive and runs it in Spark, Spark will find the method not implemented because it has a different signature.&lt;/p&gt;</comment>
                            <comment id="16148351" author="xuefuz" created="Thu, 31 Aug 2017 02:17:34 +0000"  >&lt;p&gt;I see. It might be possible to put this class in a new package (jar), for which we don&apos;t relocate kryo? &lt;/p&gt;</comment>
                            <comment id="16148485" author="lirui" created="Thu, 31 Aug 2017 05:46:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, I&apos;ll try if that&apos;s feasible. Do you think it&apos;s OK to create a package just for one single class?&lt;/p&gt;</comment>
                            <comment id="16149777" author="xuefuz" created="Thu, 31 Aug 2017 23:27:59 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, I think creating a trivial package is still better than the runtime compilation/packaging. Plus, non-hos developers doesn&apos;t need to bother with that package. I don&apos;t foresee any problem with a separate project.&lt;/p&gt;</comment>
                            <comment id="16201405" author="lirui" created="Thu, 12 Oct 2017 03:57:57 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, sorry for taking so long to update. I tried out your proposal. The idea is to build the trivial package and add it with the &lt;tt&gt;--jars&lt;/tt&gt; config when we launch the Spark app. So we need to locate the jar at runtime. To locate the jar, we can use reflection to get the class and call &lt;tt&gt;SparkContext.jarOfClass&lt;/tt&gt; to get the URI. But since kryo is shaded, we don&apos;t have &lt;tt&gt;com.esotericsoftware.kryo.Kryo&lt;/tt&gt; in our class path on Hive side. When I try to get the registrator class, I get a &lt;tt&gt;NoClassDefFoundError&lt;/tt&gt; for &lt;tt&gt;com.esotericsoftware.kryo.Kryo&lt;/tt&gt;.&lt;br/&gt;
I guess one workaround is to let user specify the path to the jar, but that seems not very friendly. Any suggestions?&lt;/p&gt;</comment>
                            <comment id="16201695" author="lirui" created="Thu, 12 Oct 2017 09:32:00 +0000"  >&lt;p&gt;One correction: the &lt;tt&gt;NoClassDefFoundError&lt;/tt&gt; is for &lt;tt&gt;com.esotericsoftware.kryo.Serializer&lt;/tt&gt;. That&apos;s because our HiveKey and BytesWritable serializer extend kryo&apos;s Serializer. When loading our classes, the super class also needs to be loaded and thus the error.&lt;/p&gt;

&lt;p&gt;Since the serializers are static nested classes of HiveKryoRegistrator, I tried loading the class w/o linking it, i.e. by calling &lt;tt&gt;ClassLoader.loadClass()&lt;/tt&gt;. And that can avoid the NoClassDefFoundError. But not sure whether this is reliable and independent from JVM implementations.&lt;/p&gt;</comment>
                            <comment id="16202897" author="xuefuz" created="Fri, 13 Oct 2017 01:01:03 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, to locate the jar, can we assume that the jar is located somewhere in Hive&apos;s installation path? I&apos;m not sure where (Hive, spark-submit, or remote driver) we need to find the location of the jar.&lt;/p&gt;</comment>
                            <comment id="16203332" author="lirui" created="Fri, 13 Oct 2017 09:59:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, we need to locate the jar on Hive side, before we call spark-submit. I made Hive include it in the &lt;tt&gt;HIVE_HOME/lib&lt;/tt&gt; directory. I guess we can find the path to hive-exec.jar (which is also under lib) and search for the registrator jar under the same path (or relative ones). But that will totally depend on how Hive is installed.&lt;/p&gt;</comment>
                            <comment id="16203867" author="xuefuz" created="Fri, 13 Oct 2017 17:07:30 +0000"  >&lt;p&gt;I think it&apos;s fairly safe to assume that hive-exec.jar and the new jar are in the same location. We can error out if the jar cannot be found in that location.&lt;/p&gt;</comment>
                            <comment id="16205846" author="lirui" created="Mon, 16 Oct 2017 12:54:28 +0000"  >&lt;p&gt;Update patch v6 based on Xuefu&apos;s suggestions.&lt;/p&gt;</comment>
                            <comment id="16205863" author="hiveqa" created="Mon, 16 Oct 2017 13:13:40 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12892380/HIVE-15104.6.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12892380/HIVE-15104.6.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7323/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7323/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7323/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7323/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-7323/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-7323/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command &apos;bash /data/hiveptest/working/scratch/source-prep.sh&apos; failed with exit status 1 and output &apos;+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-10-16 13:13:35.520
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export &apos;ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m &apos;
+ ANT_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m &apos;
+ export &apos;MAVEN_OPTS=-Xmx1g &apos;
+ MAVEN_OPTS=&apos;-Xmx1g &apos;
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-7323/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-10-16 13:13:35.522
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   6339936..da304ef  master     -&amp;gt; origin/master
+ git reset --hard HEAD
HEAD is now at 6339936 HIVE-17749: Multiple class have missed the ASF header (Saijin Huang via Rui)
+ git clean -f -d
Removing standalone-metastore/src/gen/org/
+ git checkout master
Already on &apos;master&apos;
Your branch is behind &apos;origin/master&apos; by 2 commits, and can be fast-forwarded.
  (use &quot;git pull&quot; to update your local branch)
+ git reset --hard origin/master
HEAD is now at da304ef HIVE-17798: When replacing the src table names in BeeLine testing, the table names shouldn&apos;t be changed to lower case (Marta Kuczora, via Peter Vary)
+ git merge --ff-only origin/master
Already up-to-date.
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-10-16 13:13:39.565
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
error: a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java: No such file or directory
error: a/itests/src/test/resources/testconfiguration.properties: No such file or directory
error: a/packaging/pom.xml: No such file or directory
error: a/pom.xml: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java: No such file or directory
error: a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java: No such file or directory
error: a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java: No such file or directory
The patch does not appear to apply with p0, p1, or p2
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12892380 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16207028" author="hiveqa" created="Tue, 17 Oct 2017 05:23:56 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12892511/HIVE-15104.7.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12892511/HIVE-15104.7.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to build exiting with an error&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7341/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7341/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7341/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7341/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-7341/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-7341/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;**** This message was trimmed, see log for full details ****
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export &apos;ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m &apos;
+ ANT_OPTS=&apos;-Xmx1g -XX:MaxPermSize=256m &apos;
+ export &apos;MAVEN_OPTS=-Xmx1g &apos;
+ MAVEN_OPTS=&apos;-Xmx1g &apos;
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-7341/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-10-17 05:22:03.427
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 8c3f0e4 HIVE-17815: prevent OOM with Atlas Hive hook (Anishek Agarwal reviewed by Thejas Nair)
+ git clean -f -d
+ git checkout master
Already on &apos;master&apos;
Your branch is up-to-date with &apos;origin/master&apos;.
+ git reset --hard origin/master
HEAD is now at 8c3f0e4 HIVE-17815: prevent OOM with Atlas Hive hook (Anishek Agarwal reviewed by Thejas Nair)
+ git merge --ff-only origin/master
Already up-to-date.
+ date &apos;+%Y-%m-%d %T.%3N&apos;
2017-10-17 05:22:03.911
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: patch -p0
patching file common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
patching file hive-kryo-registrator/pom.xml
patching file hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java
patching file itests/src/test/resources/testconfiguration.properties
patching file packaging/pom.xml
patching file pom.xml
patching file ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java
patching file ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java
patching file ql/src/test/queries/clientpositive/spark_opt_shuffle_serde.q
patching file ql/src/test/results/clientpositive/spark/spark_opt_shuffle_serde.q.out
patching file spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
patching file spark-client/src/main/java/org/apache/hive/spark/client/SparkClientUtilities.java
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
protoc-jar: protoc version: 250, detected platform: linux/amd64
protoc-jar: executing: [/tmp/protoc9130095883787762036.exe, -I/data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources, /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/standalone-metastore/target/generated-sources/antlr3/org/apache/hadoop/hive/metastore/parser/FilterParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/parser/Filter.g
org/apache/hadoop/hive/metastore/parser/Filter.g
DataNucleus Enhancer (version 4.1.17) for API &quot;JDO&quot;
DataNucleus Enhancer : Classpath
&amp;gt;&amp;gt;  /usr/share/maven/boot/plexus-classworlds-2.x.jar
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDatabase
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFieldSchema
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MType
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MConstraint
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MSerDeInfo
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MOrder
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MColumnDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStringList
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MStorageDescriptor
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartition
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MIndex
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRole
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MRoleMap
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MGlobalPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDBPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTablePrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnPrivilege
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionEvent
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMasterKey
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MDelegationToken
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MTableColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MVersionTable
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MMetastoreDBProperties
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MResourceUri
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MFunction
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationLog
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MNotificationNextId
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMResourcePlan
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMPool
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMTrigger
ENHANCED (Persistable) : org.apache.hadoop.hive.metastore.model.MWMMapping
DataNucleus Enhancer completed with success for 35 classes. Timings : input=193 ms, enhance=178 ms, total=371 ms. Consult the log for full details
ANTLR Parser Generator  Version 3.5.2
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveLexer.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveLexer.g
org/apache/hadoop/hive/ql/parse/HiveLexer.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HiveParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g
org/apache/hadoop/hive/ql/parse/HiveParser.g
Output file /data/hiveptest/working/apache-github-source-source/ql/target/generated-sources/antlr3/org/apache/hadoop/hive/ql/parse/HintParser.java does not exist: must build /data/hiveptest/working/apache-github-source-source/ql/src/java/org/apache/hadoop/hive/ql/parse/HintParser.g
org/apache/hadoop/hive/ql/parse/HintParser.g
Generating vector expression code
Generating vector expression test code
ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.
[ERROR] COMPILATION ERROR : 
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[20,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[21,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[22,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[23,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[26,35] package org.apache.spark.serializer does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[34,45] cannot find symbol
  symbol: class KryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[36,31] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[41,50] cannot find symbol
  symbol:   class Serializer
  location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,23] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,34] cannot find symbol
  symbol:   class Output
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,25] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,36] cannot find symbol
  symbol:   class Input
  location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[57,56] cannot find symbol
  symbol:   class Serializer
  location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,23] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,34] cannot find symbol
  symbol:   class Output
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,31] cannot find symbol
  symbol:   class Kryo
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,42] cannot find symbol
  symbol:   class Input
  location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.1:compile (default-compile) on project hive-kryo-registrator: Compilation failure: Compilation failure:
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[20,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[21,33] package com.esotericsoftware.kryo does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[22,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[23,36] package com.esotericsoftware.kryo.io does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[26,35] package org.apache.spark.serializer does not exist
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[34,45] cannot find symbol
[ERROR] symbol: class KryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[36,31] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[41,50] cannot find symbol
[ERROR] symbol:   class Serializer
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,23] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[43,34] cannot find symbol
[ERROR] symbol:   class Output
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,25] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[49,36] cannot find symbol
[ERROR] symbol:   class Input
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.HiveKeySerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[57,56] cannot find symbol
[ERROR] symbol:   class Serializer
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,23] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[59,34] cannot find symbol
[ERROR] symbol:   class Output
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,31] cannot find symbol
[ERROR] symbol:   class Kryo
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] /data/hiveptest/working/apache-github-source-source/hive-kryo-registrator/src/main/java/org/apache/hive/spark/HiveKryoRegistrator.java:[64,42] cannot find symbol
[ERROR] symbol:   class Input
[ERROR] location: class org.apache.hive.spark.HiveKryoRegistrator.BytesWritableSerializer
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :hive-kryo-registrator
+ exit 1
&apos;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12892511 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16207094" author="lirui" created="Tue, 17 Oct 2017 06:59:33 +0000"  >&lt;p&gt;Fix dependencies&lt;/p&gt;</comment>
                            <comment id="16207563" author="hiveqa" created="Tue, 17 Oct 2017 12:26:06 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12892549/HIVE-15104.9.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12892549/HIVE-15104.9.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 1 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 13 failed/errored test(s), 11276 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[unionDistinct_1] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[optimize_nullscan] (batchId=163)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=110)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_notin] (batchId=133)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_scalar] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_select] (batchId=119)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_views] (batchId=108)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query16] (batchId=243)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query94] (batchId=243)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query16] (batchId=241)
org.apache.hadoop.hive.cli.TestTezPerfCliDriver.testCliDriver[query94] (batchId=241)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=204)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighShuffleBytes (batchId=229)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7348/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7348/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7348/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7348/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-7348/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-7348/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12892549 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16208904" author="lirui" created="Wed, 18 Oct 2017 07:05:23 +0000"  >&lt;p&gt;The sub-query failures are tracked by &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-17823&quot; title=&quot;Fix subquery Qtest of Hive on Spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-17823&quot;&gt;&lt;del&gt;HIVE-17823&lt;/del&gt;&lt;/a&gt;. Others are not related.&lt;br/&gt;
I&apos;ve put the 9th patch on RB. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;, could you take another look? Thanks.&lt;/p&gt;</comment>
                            <comment id="16216271" author="lirui" created="Tue, 24 Oct 2017 04:16:04 +0000"  >&lt;p&gt;Update to address review comments. Also changed the default switch back to false.&lt;/p&gt;</comment>
                            <comment id="16217431" author="xuefuz" created="Tue, 24 Oct 2017 18:42:51 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="16217849" author="hiveqa" created="Tue, 24 Oct 2017 23:01:03 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12893666/HIVE-15104.10.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12893666/HIVE-15104.10.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;green&quot;&gt;SUCCESS:&lt;/font&gt; +1 due to 1 test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 5 failed/errored test(s), 11319 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=110)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=205)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=222)
org.apache.hadoop.hive.ql.parse.authorization.plugin.sqlstd.TestOperation2Privilege.checkHiveOperationTypeMatch (batchId=270)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighShuffleBytes (batchId=229)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7456/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7456/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/7456/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/7456/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-7456/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-7456/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12893666 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16218038" author="lirui" created="Wed, 25 Oct 2017 03:11:47 +0000"  >&lt;p&gt;Pushed to master. Thanks Xuefu for the review!&lt;/p&gt;</comment>
                            <comment id="16219861" author="lefty@hortonworks.com" created="Thu, 26 Oct 2017 02:22:39 +0000"  >&lt;p&gt;Doc note:  This adds &lt;b&gt;hive.spark.optimize.shuffle.serde&lt;/b&gt; to HiveConf.java, so it needs to be documented in the wiki.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-Spark&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Configuration Properties &amp;#8211; Spark &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Added a TODOC3.0 label.&lt;/p&gt;</comment>
                            <comment id="16224326" author="lirui" created="Mon, 30 Oct 2017 02:36:19 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=leftylev&quot; class=&quot;user-hover&quot; rel=&quot;leftylev&quot;&gt;leftylev&lt;/a&gt; for the reminder. I&apos;ve updated the wiki.&lt;/p&gt;</comment>
                            <comment id="16233602" author="lefty@hortonworks.com" created="Wed, 1 Nov 2017 03:15:07 +0000"  >&lt;p&gt;Good doc, thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;.  I removed the TODOC3.0 label.&lt;/p&gt;

&lt;p&gt;Here&apos;s a direct link to the doc:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-hive.spark.optimize.shuffle.serde&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;hive.spark.optimize.shuffle.serde &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16388551" author="stakiar" created="Tue, 6 Mar 2018 21:23:10 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt; I found some time to do some internal testing of this patch. I ran a 1 TB Parquet TPC-DS benchmark (subset of 49 queries, run three times each) on a physical cluster and found similar results to your TPC-DS benchmark.&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&#160;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Baseline Run (default configuration)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Optimized Serde&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Optimized Serde + No GroupBy&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Shuffle Bytes Read&lt;/th&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;699.5 GB&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;530.7 GB&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;531.6 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Shuffle Bytes Written&lt;/th&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;690 GB&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;529.5 GB&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;530.3 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Total Latency (min)&lt;/th&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;202&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;191&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;190&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;So about a 25% improvement on shuffle data and 5% performance improvement. I think the improvement for the shuffle data is significant and is a good&#160;improvement.&lt;/p&gt;

&lt;p&gt;A few questions on the implementation.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The &lt;tt&gt;HiveKryoRegistrator&lt;/tt&gt; still seems to be serializing the &lt;tt&gt;hashCode&lt;/tt&gt; so where are the actual savings coming from?&lt;/li&gt;
	&lt;li&gt;I&apos;m not sure I understand why the performance should improve when &lt;tt&gt;hive.spark.use.groupby.shuffle&lt;/tt&gt; is set to &lt;tt&gt;false&lt;/tt&gt;. It&apos;s still using the same registrator right?&lt;/li&gt;
	&lt;li&gt;You said that we need to serialize the &lt;tt&gt;hashCode&lt;/tt&gt; because &quot;&lt;tt&gt;The cached RDDs will be serialized when stored to disk or transferred via network, then we need the hash code after the data is deserialized&quot;&lt;/tt&gt; - why do we need the &lt;tt&gt;hashCode&lt;/tt&gt; after deserializing the data?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16388915" author="lirui" created="Wed, 7 Mar 2018 02:37:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stakiar&quot; class=&quot;user-hover&quot; rel=&quot;stakiar&quot;&gt;stakiar&lt;/a&gt;, thanks for trying this out.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The HiveKryoRegistrator still seems to be serializing the hashCode so where are the actual savings coming from?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I didn&apos;t look deeply into kryo, but I think the reason is generic kryo SerDe has some overhead to store class meta info, while &lt;br/&gt;
 in &lt;tt&gt;HiveKryoRegistrator&lt;/tt&gt; we just store the data. My earlier &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15104?focusedCommentId=16007788&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16007788&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;comment&lt;/a&gt; shows custom SerDe can bring improvements for BytesWritable too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m not sure I understand why the performance should improve when hive.spark.use.groupby.shuffle is set to false.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I guess the difference is due to the different shuffle we used &amp;#8211; if &lt;tt&gt;hive.spark.use.groupby.shuffle&lt;/tt&gt; is false, group-by-key shuffle is replaced with repartition-and-sort-within-partition shuffle. And yes, the registrator is same for the two cases.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;why do we need the hashCode after deserializing the data?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For MR, the hash code is not needed for deserialized HiveKey (see HiveKey::hashCode), because when HiveKey is deserialized, it&apos;s already been distributed to the proper reducer. For Spark, RDDs may get cached during the execution. So if we deserialize a cached RDD and try to partition it to a downstream reducer, we&apos;ll need the hash code available after deserialization.&lt;/p&gt;</comment>
                            <comment id="16485871" author="vgarg" created="Tue, 22 May 2018 23:58:02 +0000"  >&lt;p&gt;Hive 3.0.0 has been released so closing this jira.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12867727" name="HIVE-15104.1.patch" size="7779" author="lirui" created="Fri, 12 May 2017 08:26:13 +0000"/>
                            <attachment id="12893666" name="HIVE-15104.10.patch" size="20208" author="lirui" created="Tue, 24 Oct 2017 04:14:57 +0000"/>
                            <attachment id="12868055" name="HIVE-15104.2.patch" size="8456" author="lirui" created="Mon, 15 May 2017 11:29:43 +0000"/>
                            <attachment id="12868935" name="HIVE-15104.3.patch" size="23631" author="lirui" created="Fri, 19 May 2017 12:37:37 +0000"/>
                            <attachment id="12877028" name="HIVE-15104.4.patch" size="21416" author="lirui" created="Thu, 13 Jul 2017 06:42:31 +0000"/>
                            <attachment id="12883637" name="HIVE-15104.5.patch" size="21425" author="lirui" created="Fri, 25 Aug 2017 01:24:54 +0000"/>
                            <attachment id="12892380" name="HIVE-15104.6.patch" size="20488" author="lirui" created="Mon, 16 Oct 2017 12:54:12 +0000"/>
                            <attachment id="12892511" name="HIVE-15104.7.patch" size="20027" author="lirui" created="Tue, 17 Oct 2017 02:28:51 +0000"/>
                            <attachment id="12892544" name="HIVE-15104.8.patch" size="20187" author="lirui" created="Tue, 17 Oct 2017 06:59:22 +0000"/>
                            <attachment id="12892549" name="HIVE-15104.9.patch" size="20248" author="lirui" created="Tue, 17 Oct 2017 07:41:03 +0000"/>
                            <attachment id="12869112" name="TPC-H 100G.xlsx" size="30724" author="lirui" created="Sat, 20 May 2017 15:12:05 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 26 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35ogf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>