<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 18:54:36 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HIVE-16780] Case &quot;multiple sources, single key&quot; in spark_dynamic_pruning.q fails </title>
                <link>https://issues.apache.org/jira/browse/HIVE-16780</link>
                <project id="12310843" key="HIVE">Hive</project>
                    <description>&lt;p&gt;script.q&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;set hive.optimize.ppd=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
set hive.ppd.remove.duplicatefilters=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
set hive.spark.dynamic.partition.pruning=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
set hive.optimize.metadataonly=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
set hive.optimize.index.filter=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
set hive.strict.checks.cartesian.product=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
set hive.spark.dynamic.partition.pruning=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;

-- multiple sources, single key
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if disabling &quot;hive.optimize.index.filter&quot;, case passes otherwise it always hang out in the first job. Exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;17/05/27 23:39:45 DEBUG Executor task launch worker-0 PerfLogger: &amp;lt;/PERFLOG method=SparkInitializeOperators start=1495899585574 end=1495899585933 duration=359 from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler&amp;gt;
17/05/27 23:39:45 INFO Executor task launch worker-0 Utilities: PLAN PATH = hdfs:&lt;span class=&quot;code-comment&quot;&gt;//bdpe41:8020/tmp/hive/root/029a2d8a-c6e5-4ea9-adea-ef8fbea3cde2/hive_2017-05-27_23-39-06_464_5915518562441677640-1/-mr-10007/617d9dd6-9f9a-4786-8131-a7b98e8abc3e/map.xml
&lt;/span&gt;17/05/27 23:39:45 DEBUG Executor task launch worker-0 Utilities: Found plan in cache &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; name: map.xml
17/05/27 23:39:45 DEBUG Executor task launch worker-0 DFSClient: Connecting to datanode 10.239.47.162:50010
17/05/27 23:39:45 DEBUG Executor task launch worker-0 MapOperator: Processing alias(es) srcpart_hour &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; file hdfs:&lt;span class=&quot;code-comment&quot;&gt;//bdpe41:8020/user/hive/warehouse/srcpart_hour/000008_0
&lt;/span&gt;17/05/27 23:39:45 DEBUG Executor task launch worker-0 ObjectCache: Creating root_20170527233906_ac2934e1-2e58-4116-9f0d-35dee302d689_DynamicValueRegistry
17/05/27 23:39:45 ERROR Executor task launch worker-0 SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)
     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)
     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)
     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
     at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:893)
     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
     at org.apache.spark.scheduler.Task.run(Task.scala:85)
     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; RS_7_srcpart__col3_min
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)
     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead._evaluate(ExprNodeEvaluatorHead.java:44)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)
     at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:112)
     at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
     at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
     at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)
     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547)
     ... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:62)
     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:51)
     at org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.retrieve(ObjectCacheWrapper.java:40)
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:119)
     ... 41 more
Caused by: java.lang.NullPointerException
     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:60)
     ... 44 more
17/05/27 23:39:45 ERROR Executor task launch worker-0 Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:149)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)
     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)
     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
     at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:893)
     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
     at org.apache.spark.scheduler.Task.run(Task.scala:85)
     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)
     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)
     ... 16 more
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; RS_7_srcpart__col3_min
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)
     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; </description>
                <environment></environment>
        <key id="13075336">HIVE-16780</key>
            <summary>Case &quot;multiple sources, single key&quot; in spark_dynamic_pruning.q fails </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kellyzly">liyunzhang</assignee>
                                    <reporter username="kellyzly">liyunzhang</reporter>
                        <labels>
                    </labels>
                <created>Sat, 27 May 2017 07:51:20 +0000</created>
                <updated>Tue, 22 May 2018 23:58:32 +0000</updated>
                            <resolved>Fri, 9 Jun 2017 22:23:54 +0000</resolved>
                                                    <fixVersion>3.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16027342" author="kellyzly" created="Sat, 27 May 2017 07:54:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;:  I found that if i disable &quot;hive.optimize.index.filter&quot;, the case pass. &lt;br/&gt;
if enable hive.optimize.index.filter, case fail, the exception is &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;17/05/27 23:39:45 DEBUG Executor task launch worker-0 PerfLogger: &amp;lt;/PERFLOG method=SparkInitializeOperators start=1495899585574 end=1495899585933 duration=359 from=org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler&amp;gt;
17/05/27 23:39:45 INFO Executor task launch worker-0 Utilities: PLAN PATH = hdfs:&lt;span class=&quot;code-comment&quot;&gt;//bdpe41:8020/tmp/hive/root/029a2d8a-c6e5-4ea9-adea-ef8fbea3cde2/hive_2017-05-27_23-39-06_464_5915518562441677640-1/-mr-10007/617d9dd6-9f9a-4786-8131-a7b98e8abc3e/map.xml
&lt;/span&gt;17/05/27 23:39:45 DEBUG Executor task launch worker-0 Utilities: Found plan in cache &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; name: map.xml
17/05/27 23:39:45 DEBUG Executor task launch worker-0 DFSClient: Connecting to datanode 10.239.47.162:50010
17/05/27 23:39:45 DEBUG Executor task launch worker-0 MapOperator: Processing alias(es) srcpart_hour &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; file hdfs:&lt;span class=&quot;code-comment&quot;&gt;//bdpe41:8020/user/hive/warehouse/srcpart_hour/000008_0
&lt;/span&gt;17/05/27 23:39:45 DEBUG Executor task launch worker-0 ObjectCache: Creating root_20170527233906_ac2934e1-2e58-4116-9f0d-35dee302d689_DynamicValueRegistry
17/05/27 23:39:45 ERROR Executor task launch worker-0 SparkMapRecordHandler: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)
     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)
     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)
     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
     at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:893)
     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
     at org.apache.spark.scheduler.Task.run(Task.scala:85)
     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; RS_7_srcpart__col3_min
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)
     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorHead._evaluate(ExprNodeEvaluatorHead.java:44)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68)
     at org.apache.hadoop.hive.ql.exec.FilterOperator.process(FilterOperator.java:112)
     at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
     at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
     at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)
     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547)
     ... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:62)
     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:51)
     at org.apache.hadoop.hive.ql.exec.ObjectCacheWrapper.retrieve(ObjectCacheWrapper.java:40)
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:119)
     ... 41 more
Caused by: java.lang.NullPointerException
     at org.apache.hadoop.hive.ql.exec.mr.ObjectCache.retrieve(ObjectCache.java:60)
     ... 44 more
17/05/27 23:39:45 ERROR Executor task launch worker-0 Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.RuntimeException: Error processing row: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:149)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:48)
     at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunctionResultList.processNextRecord(HiveMapFunctionResultList.java:27)
     at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.hasNext(HiveBaseFunctionResultList.java:85)
     at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)
     at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:893)
     at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.rdd.AsyncRDDActions$$anonfun$foreachAsync$1$$anonfun$apply$12.apply(AsyncRDDActions.scala:127)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
     at org.apache.spark.scheduler.Task.run(Task.scala:85)
     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; processing row {&lt;span class=&quot;code-quote&quot;&gt;&quot;hr&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;hour&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt;}
     at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562)
     at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.processRow(SparkMapRecordHandler.java:136)
     ... 16 more
Caused by: java.lang.IllegalStateException: Failed to retrieve dynamic value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; RS_7_srcpart__col3_min
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getValue(DynamicValue.java:126)
     at org.apache.hadoop.hive.ql.plan.DynamicValue.getWritableValue(DynamicValue.java:101)
     at org.apache.hadoop.hive.ql.exec.ExprNodeDynamicValueEvaluator._evaluate(ExprNodeDynamicValueEvaluator.java:51)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPEqualOrGreaterThan.evaluate(GenericUDFOPEqualOrGreaterThan.java:108)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFBetween.evaluate(GenericUDFBetween.java:57)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive.ql.udf.&lt;span class=&quot;code-keyword&quot;&gt;generic&lt;/span&gt;.GenericUDFOPAnd.evaluate(GenericUDFOPAnd.java:63)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187)
     at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80)
     at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator$DeferredExprObject.get(ExprNodeGenericFuncEvaluator.java:88)
     at org.apache.hadoop.hive
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Can you help to verify whether this pass or not in your environment?  in my enviroment, hive version:54dbca69c9ea630b9cccd5550bdb455b9bbc240c  spark:2.0.0.&lt;/p&gt;</comment>
                            <comment id="16030853" author="kellyzly" created="Wed, 31 May 2017 08:51:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xuefuz&quot; class=&quot;user-hover&quot; rel=&quot;xuefuz&quot;&gt;xuefuz&lt;/a&gt;,&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;:  I guess this exception is caused by the modification of &lt;a href=&quot;https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java#L266&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;DynamicPartitonPruning&lt;/a&gt; after &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-15269&quot; title=&quot;Dynamic Min-Max/BloomFilter runtime-filtering for Tez&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-15269&quot;&gt;&lt;del&gt;HIVE-15269&lt;/del&gt;&lt;/a&gt;.  NPE is caused by&lt;br/&gt;
org.apache.hadoop.hive.ql.exec.mr.ObjectCache#retrieve(java.lang.String)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;@Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &amp;lt;T&amp;gt; T retrieve(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; HiveException {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; retrieve(key, &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);
  }

  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &amp;lt;T&amp;gt; T retrieve(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key, Callable&amp;lt;T&amp;gt; fn) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; HiveException {
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isDebugEnabled) {
        LOG.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Creating &quot;&lt;/span&gt; + key);
      }
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; fn.call();  &lt;span class=&quot;code-comment&quot;&gt;// NPE is thrown here
&lt;/span&gt;    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HiveException(e);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Comparing with org.apache.hadoop.hive.ql.exec.tez.ObjectCache#retrieve(java.lang.String)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &amp;lt;T&amp;gt; T retrieve(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; key) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; HiveException {
    T value = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      value = (T) registry.get(key);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ( value != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Found &quot;&lt;/span&gt; + key + &lt;span class=&quot;code-quote&quot;&gt;&quot; in cache with value: &quot;&lt;/span&gt; + value);
      }
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {
      &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HiveException(e);
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; value;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; if we want to fix this, we need add a similar code as what hive on tez does( DynamicValueRegistryTez, RegistryConfTez), appreciate if you can give some suggestions.&lt;/p&gt;</comment>
                            <comment id="16036520" author="csun" created="Mon, 5 Jun 2017 04:37:58 +0000"  >&lt;p&gt;Thanks for the findings &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kellyzly&quot; class=&quot;user-hover&quot; rel=&quot;kellyzly&quot;&gt;kellyzly&lt;/a&gt;! I wonder if the issue still happen when &lt;tt&gt;hive.tez.dynamic.semijoin.reduction&lt;/tt&gt; is set to false.&lt;/p&gt;

&lt;p&gt;It seems this config affects Spark branch too, which should not happen. Maybe we should first disable this optimization for Spark in &lt;tt&gt;DynamicPartitionPruningOptimization&lt;/tt&gt;, which is shared by both engines. In future we can investigate on how to enable this optimization for Spark.&lt;/p&gt;</comment>
                            <comment id="16036670" author="kellyzly" created="Mon, 5 Jun 2017 08:24:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;:  case &quot;multiple sources, single key&quot; pass if hive.tez.dynamic.semijoin.reduction is false.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Maybe we should first disable this optimization for Spark in DynamicPartitionPruningOptimization&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;agree, update &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-16780&quot; title=&quot;Case &amp;quot;multiple sources, single key&amp;quot; in spark_dynamic_pruning.q fails &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-16780&quot;&gt;&lt;del&gt;HIVE-16780&lt;/del&gt;&lt;/a&gt;.1.patch.&lt;/p&gt;

&lt;p&gt;the explain when enabling hive.tez.dynamic.semijoin.reduction&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-3 depends on stages: Stage-2
  Stage-1 depends on stages: Stage-3
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      DagName: root_20170605152828_4c4f4f82-d08f-41e9-9a07-4147b8529dd0:2
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: srcpart_date
                  filterExpr: ds is not null (type: boolean)
                  Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: ds is not null (type: boolean)
                    Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 ds (type: string)
                        1 ds (type: string)
                    Select Operator
                      expressions: ds (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: ds
                          Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                          target column name: ds
                          target work: Map 1
            Local Work:
              Map Reduce Local Work
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: srcpart_hour
                  filterExpr: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)
                  Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)
                    Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col3 (type: string)
                        1 hr (type: string)
                    Select Operator
                      expressions: hr (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: hr
                          Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                          target column name: hr
                          target work: Map 1
            Local Work:
              Map Reduce Local Work

  Stage: Stage-3
    Spark
      DagName: root_20170605152828_4c4f4f82-d08f-41e9-9a07-4147b8529dd0:3
      Vertices:
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: srcpart_date
                  filterExpr: ds is not null (type: boolean)
                  Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: ds is not null (type: boolean)
                    Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 ds (type: string)
                        1 ds (type: string)
                    Select Operator
                      expressions: ds (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: ds
                          Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                          target column name: ds
                          target work: Map 1
            Local Work:
              Map Reduce Local Work
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: srcpart_hour
                  filterExpr: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)
                  Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (hr is not null and (hr BETWEEN DynamicValue(RS_7_srcpart__col3_min) AND DynamicValue(RS_7_srcpart__col3_max) and in_bloom_filter(hr, DynamicValue(RS_7_srcpart__col3_bloom_filter)))) (type: boolean)
                    Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col3 (type: string)
                        1 hr (type: string)
                    Select Operator
                      expressions: hr (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: hr
                          Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                          target column name: hr
                          target work: Map 1
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 &amp;lt;- Map 6 (GROUP, 1)
        Reducer 3 &amp;lt;- Map 7 (GROUP, 1)
      DagName: root_20170605152828_4c4f4f82-d08f-41e9-9a07-4147b8529dd0:1
      Vertices:
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 1 Data size: 23248 Basic stats: PARTIAL Column stats: NONE
                  Map Join Operator
                    condition map:
                         Inner Join 0 to 1
                    keys:
                      0 ds (type: string)
                      1 ds (type: string)
                    outputColumnNames: _col3
                    input vertices:
                      1 Map 4
                    Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: _col3 (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        aggregations: min(_col0), max(_col0), bloom_filter(_col0, expectedEntries=1000000)
                        mode: hash
                        outputColumnNames: _col0, _col1, _col2
                        Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          sort order: 
                          Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE
                          value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)
            Local Work:
              Map Reduce Local Work
        Map 7 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 1 Data size: 23248 Basic stats: PARTIAL Column stats: NONE
                  Map Join Operator
                    condition map:
                         Inner Join 0 to 1
                    keys:
                      0 ds (type: string)
                      1 ds (type: string)
                    outputColumnNames: _col3
                    input vertices:
                      1 Map 4
                    Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col3 (type: string)
                        1 hr (type: string)
                      input vertices:
                        1 Map 5
                      Statistics: Num rows: 2 Data size: 50 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        aggregations: count()
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          sort order: 
                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                          value expressions: _col0 (type: bigint)
            Local Work:
              Map Reduce Local Work
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=1000000)
                mode: final
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)
        Reducer 3 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;the explain when disabling hive.tez.dynamic.semijoin.reduction&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;STAGE DEPENDENCIES:
  Stage-2 is a root stage
  Stage-1 depends on stages: Stage-2
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-2
    Spark
      DagName: root_20170605153113_6d0bbb45-8a4a-4b10-9b2c-4585abdf2a79:2
      Vertices:
        Map 3 
            Map Operator Tree:
                TableScan
                  alias: srcpart_date
                  filterExpr: ds is not null (type: boolean)
                  Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: ds is not null (type: boolean)
                    Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 ds (type: string)
                        1 ds (type: string)
                    Select Operator
                      expressions: ds (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: ds
                          Statistics: Num rows: 2 Data size: 42 Basic stats: COMPLETE Column stats: NONE
                          target column name: ds
                          target work: Map 1
            Local Work:
              Map Reduce Local Work
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: srcpart_hour
                  filterExpr: hr is not null (type: boolean)
                  Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: hr is not null (type: boolean)
                    Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                    Spark HashTable Sink Operator
                      keys:
                        0 _col3 (type: string)
                        1 hr (type: string)
                    Select Operator
                      expressions: hr (type: string)
                      outputColumnNames: _col0
                      Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        keys: _col0 (type: string)
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                        Spark Partition Pruning Sink Operator
                          partition key expr: hr
                          Statistics: Num rows: 2 Data size: 10 Basic stats: COMPLETE Column stats: NONE
                          target column name: hr
                          target work: Map 1
            Local Work:
              Map Reduce Local Work

  Stage: Stage-1
    Spark
      Edges:
        Reducer 2 &amp;lt;- Map 1 (GROUP, 1)
      DagName: root_20170605153113_6d0bbb45-8a4a-4b10-9b2c-4585abdf2a79:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: srcpart
                  Statistics: Num rows: 1 Data size: 23248 Basic stats: PARTIAL Column stats: NONE
                  Map Join Operator
                    condition map:
                         Inner Join 0 to 1
                    keys:
                      0 ds (type: string)
                      1 ds (type: string)
                    outputColumnNames: _col3
                    input vertices:
                      1 Map 3
                    Statistics: Num rows: 2 Data size: 46 Basic stats: COMPLETE Column stats: NONE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      keys:
                        0 _col3 (type: string)
                        1 hr (type: string)
                      input vertices:
                        1 Map 4
                      Statistics: Num rows: 2 Data size: 50 Basic stats: COMPLETE Column stats: NONE
                      Group By Operator
                        aggregations: count()
                        mode: hash
                        outputColumnNames: _col0
                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                        Reduce Output Operator
                          sort order: 
                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                          value expressions: _col0 (type: bigint)
            Local Work:
              Map Reduce Local Work
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One interesting thing is when enabling &lt;tt&gt;hive.tez.dynamic.semijoin.reduction&lt;/tt&gt;, there is an extra reduce Reducer 2 &amp;lt;- Map 6 (GROUP, 1).  But what&apos;s purpose of Reducer 2?&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;   Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=1000000)
                mode: final
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  sort order: 
                  Statistics: Num rows: 1 Data size: 552 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: binary)
     
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16036722" author="hiveqa" created="Mon, 5 Jun 2017 09:35:47 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12871199/HIVE-16780.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12871199/HIVE-16780.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 6 failed/errored test(s), 10820 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udf_reverse] (batchId=83)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5529/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5529/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5529/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5529/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5529/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5529/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12871199 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16037480" author="csun" created="Mon, 5 Jun 2017 20:03:30 +0000"  >&lt;blockquote&gt;
&lt;p&gt;One interesting thing is when enabling hive.tez.dynamic.semijoin.reduction, there is an extra reduce Reducer 2 &amp;lt;- Map 6 (GROUP, 1). But what&apos;s purpose of Reducer 2?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think that&apos;s for the aggregation of min/max and bloom filter. See &lt;a href=&quot;https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java#L489&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16038380" author="kellyzly" created="Tue, 6 Jun 2017 08:37:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;: Reduce2 is calculate the aggregation(mix/max) of specified key? then save the result to the temp file or source file?  If have time, please help review &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12871199/HIVE-16780.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;HIVE-16780.patch&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16042240" author="csun" created="Thu, 8 Jun 2017 05:55:47 +0000"  >&lt;p&gt;Looks OK. Did tests pass? Can you add a TODO to enable this optimization for Spark in future?&lt;/p&gt;</comment>
                            <comment id="16043537" author="kellyzly" created="Thu, 8 Jun 2017 22:20:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt;:&lt;br/&gt;
the test in jira description pass.&lt;br/&gt;
 have filed jira &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-16862&quot; title=&quot;Implement a similar feature like &amp;quot;hive.tez.dynamic.semijoin.reduction&amp;quot; in hive on spark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-16862&quot;&gt;&lt;del&gt;HIVE-16862&lt;/del&gt;&lt;/a&gt; to track a similar feature like &quot;hive.tez.dynamic.semijoin.reduction&quot; in hos. and add a TODO in &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-16780&quot; title=&quot;Case &amp;quot;multiple sources, single key&amp;quot; in spark_dynamic_pruning.q fails &quot; class=&quot;issue-link&quot; data-issue-key=&quot;HIVE-16780&quot;&gt;&lt;del&gt;HIVE-16780&lt;/del&gt;&lt;/a&gt;.2.patch. help review, thanks!&lt;/p&gt;</comment>
                            <comment id="16043818" author="hiveqa" created="Fri, 9 Jun 2017 02:08:19 +0000"  >

&lt;p&gt;Here are the results of testing the latest attachment:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12872156/HIVE-16780.2.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12872156/HIVE-16780.2.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to no test(s) being added or modified.&lt;/p&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;ERROR:&lt;/font&gt; -1 due to 7 failed/errored test(s), 10816 tests executed&lt;br/&gt;
&lt;b&gt;Failed tests:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[orc_ppd_basic] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query78] (batchId=232)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=239)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5591/testReport&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5591/testReport&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HIVE-Build/5591/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HIVE-Build/5591/console&lt;/a&gt;&lt;br/&gt;
Test logs: &lt;a href=&quot;http://104.198.109.242/logs/PreCommit-HIVE-Build-5591/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://104.198.109.242/logs/PreCommit-HIVE-Build-5591/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Messages:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;

&lt;p&gt;ATTACHMENT ID: 12872156 - PreCommit-HIVE-Build&lt;/p&gt;</comment>
                            <comment id="16043878" author="csun" created="Fri, 9 Jun 2017 03:35:16 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="16043914" author="kellyzly" created="Fri, 9 Jun 2017 04:28:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt; : thanks for review, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ferd&quot; class=&quot;user-hover&quot; rel=&quot;Ferd&quot;&gt;Ferd&lt;/a&gt;: can you help commit it to upstream?  &lt;/p&gt;</comment>
                            <comment id="16043955" author="ferd" created="Fri, 9 Jun 2017 05:25:07 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kellyzly&quot; class=&quot;user-hover&quot; rel=&quot;kellyzly&quot;&gt;kellyzly&lt;/a&gt; for the patch. We need to wait for 24 hrs to see if any other people have further comments.&lt;/p&gt;</comment>
                            <comment id="16045129" author="ferd" created="Fri, 9 Jun 2017 22:23:54 +0000"  >&lt;p&gt;Committed to the upstream. Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kellyzly&quot; class=&quot;user-hover&quot; rel=&quot;kellyzly&quot;&gt;kellyzly&lt;/a&gt; for the patch and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=csun&quot; class=&quot;user-hover&quot; rel=&quot;csun&quot;&gt;csun&lt;/a&gt; for the reivew.&lt;/p&gt;</comment>
                            <comment id="16485979" author="vgarg" created="Tue, 22 May 2018 23:58:32 +0000"  >&lt;p&gt;Hive 3.0.0 has been released so closing this jira.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13078451">HIVE-16862</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12872156" name="HIVE-16780.2.patch" size="1130" author="kellyzly" created="Thu, 8 Jun 2017 22:20:48 +0000"/>
                            <attachment id="12871199" name="HIVE-16780.patch" size="1016" author="kellyzly" created="Mon, 5 Jun 2017 08:24:28 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 26 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3fjs7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>