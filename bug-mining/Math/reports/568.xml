<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Sat Nov 08 20:24:56 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[MATH-1246] Kolmogorov-Smirnov 2-sample test does not correctly handle ties</title>
                <link>https://issues.apache.org/jira/browse/MATH-1246</link>
                <project id="12310485" key="MATH">Commons Math</project>
                    <description>&lt;p&gt;For small samples, KolmogorovSmirnovTest(double[], double[]) computes the distribution of a D-statistic for m-n sets with no ties.  No warning or special handling is delivered in the presence of ties.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12842758">MATH-1246</key>
            <summary>Kolmogorov-Smirnov 2-sample test does not correctly handle ties</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="psteitz">Phil Steitz</reporter>
                        <labels>
                    </labels>
                <created>Mon, 6 Jul 2015 00:45:26 +0000</created>
                <updated>Mon, 25 Jan 2016 20:28:02 +0000</updated>
                            <resolved>Thu, 31 Dec 2015 19:30:20 +0000</resolved>
                                                    <fixVersion>3.6</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="14616134" author="psteitz" created="Tue, 7 Jul 2015 03:57:55 +0000"  >&lt;p&gt;I think the current implementation can be fixed as follows.  If we move to a faster implementation, the strategy below may not work.&lt;/p&gt;

&lt;p&gt;What exactP does now is to exhaustively compute all possible D-statistics for all m-set / n-set partitions of m+n and simply tally the number that exceed (strict) or are as large as (not strict) the observed D.  If there are ties in the data, it is not correct to look at partitions of m+n, since not all partitions of an m+n set with duplicates are distinct and the set of possible D values is different in the presence of ties.  I think we can correctly handle ties in the data if we compute and tally D statistics based on a combined multi-set sample with duplicates in the positions corresponding to what is observed in the data.  For example, suppose that the two samples are x = &lt;span class=&quot;error&quot;&gt;&amp;#91;0, 3, 6, 9, 9, 10&amp;#93;&lt;/span&gt; and y = &lt;span class=&quot;error&quot;&gt;&amp;#91;1, 3, 4, 8, 11&amp;#93;&lt;/span&gt;.  then the multi-set universe is  U = &lt;span class=&quot;error&quot;&gt;&amp;#91;0, 1, 3, 3, 4, 6, 8, 9, 9, 10, 11&amp;#93;&lt;/span&gt;.  As before, we generate partitions of 11 into a 6-set and a 5-set, but instead of computing the D-statistics on the subsets of 11, we use indexes into U instead.  So if a generated split is mSet = &lt;span class=&quot;error&quot;&gt;&amp;#91;0, 2, 3, 7, 8, 9&amp;#93;&lt;/span&gt;, nSet = &lt;span class=&quot;error&quot;&gt;&amp;#91;1, 4, 5, 6, 10&amp;#93;&lt;/span&gt;, we compute D for &lt;span class=&quot;error&quot;&gt;&amp;#91;0, 3, 3, 9, 9, 10&amp;#93;&lt;/span&gt; and &lt;span class=&quot;error&quot;&gt;&amp;#91;1, 4, 6, 8, 11&amp;#93;&lt;/span&gt;.  The rationale here is that the p-value is the probability that if U is split randomly into a 5-set and a 6-set, the D-value exceeds the observed d.&lt;/p&gt;</comment>
                            <comment id="14735930" author="psteitz" created="Wed, 9 Sep 2015 01:04:56 +0000"  >&lt;p&gt;Pushed changes to exactP per outline above in ce98d00852e21ce34d8d247db7f6be138967b559.&lt;/p&gt;

&lt;p&gt;The same problem applies to monteCarloP.  It is not obvious to me how to make the necessary changes to the improved implementation there to accommodate ties.  I suspect it is not possible, so unless others have better ideas, I will add back a modified version of the earlier naive implementation that samples the multi-set.&lt;/p&gt;</comment>
                            <comment id="14737500" author="otmar ertl" created="Wed, 9 Sep 2015 20:10:45 +0000"  >&lt;p&gt;I am thinking of another way to treat ties:&lt;/p&gt;

&lt;p&gt;The probability that two values sampled from a continuous distribution are equal is equal to 0. One of them is always greater than the other. However, represented as doubles we cannot distinguish them. Therefore, the best what we can do is to treat both cases equally likely. For example, if we have x = (0, 3, 5) and y = (5, 6, 7) we get two different values for the observed D-statistic. If we assume value 5 in x to be smaller than that in y, we would get D=1. Otherwise, we would get D=2/3, both with probability 0.5. In the general case, we can determine a discrete distribution describing all possible values of the observed D-statistics. Finally, we calculate the p-value for each of those possible values and calculate the weighted average which we take as the final p-value.&lt;/p&gt;

&lt;p&gt;Does this make sense? If yes, I think there is a way to adapt the new Monte Carlo approach.&lt;/p&gt;</comment>
                            <comment id="14737874" author="psteitz" created="Thu, 10 Sep 2015 00:27:45 +0000"  >&lt;p&gt;Otmar - that is a cool idea.  So for a given sample with ties, the D statistic would be the average of the D&apos;s you could make by differently ordering the tied values, right?  That is an alternative to what I did.  I wonder how different the p-values would end up.   I think they will definitely be different, but I wonder by how much.  Note also things get a little complicated when you have more than two of the same value, which we have to assume could happen.&lt;/p&gt;

&lt;p&gt;How would you modify the current Monte Carlo approach to do this?&lt;/p&gt;

&lt;p&gt;I tried to find references to how others handle this; but unfortunately most packages just say you can&apos;t compute exact p values in the presence of ties.  Given that we have two, defensible and different definitions of how the p-value should be defined, I can see why.  I will look some more to see if I can find some math stat references to help us settle on the right definition.&lt;/p&gt;

&lt;p&gt;Thanks for looking into this.&lt;/p&gt;</comment>
                            <comment id="14739536" author="otmar ertl" created="Thu, 10 Sep 2015 20:29:14 +0000"  >&lt;p&gt;The Monte Carlo approach can be modified by simultaneously sampling D. Here is an outline how this sampling  could be achieved:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;First determine set of points P = (p_i) for which equal values exist in both samples.&lt;/li&gt;
	&lt;li&gt;Determine maximum difference of CDFs over all values not included in P&lt;/li&gt;
	&lt;li&gt;Determine for each point p_i if it is possible at all to get a CDF difference that is larger than the calculated maximum. If not, those points can be excluded from P. Otherwise, remember the difference of the CDF d_i just before that point and the number of equal values in both samples n_i and m_i, respectively.&lt;/li&gt;
	&lt;li&gt;Within each Monte Carlo iteration, generate for each point p_i a random ordering of the n_i and m_i equal values (using a function similar to fillBooleanArrayRandomlyWithFixedNumberTrueValues). Determine the maximum differences of the CDFs at all points p_i using the random ordering and d_i, and take the maximum of them and the maximum calculated in 2) which gives us the sampled (observed) D-statistic that is finally compared to curD.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Anyway, we should find the right definition first before implementing anything.&lt;/p&gt;</comment>
                            <comment id="14742726" author="psteitz" created="Sun, 13 Sep 2015 23:36:12 +0000"  >&lt;p&gt;I have done some research and I am convinced that the definition based on the empirical distributions as given is correct.  In other words&lt;/p&gt;

&lt;p&gt;1.  The statistic that we should use is that given by comparing the empirical distributions with ties contributing the mass that they do.  This is the Kolmogorov metric that is part of the definition of the test.  Distributions with point masses should be allowed and empirical distributions based on data including repeated values should be taken as presented by the data.&lt;br/&gt;
2.  The correct definition of p-value (with or without ties) is the probability that when an n-set and m-set are randomly selected from the combined dataset the associated D-value is greater than (resp greater than or equal to) the observed D value (with ties included).  Equivalently, it is the probability that when group assignment is done randomly, the resulting empirical distributions are separated by Kolmogorov distance as large as the observed D.&lt;/p&gt;

&lt;p&gt;This is supported theoretically in &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, recommended in &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; and implemented in the R-package ks.boot, which the R community recommends when ties are present in the data.&lt;/p&gt;

&lt;p&gt;The current small sample, exactP method computes the probability defined above by actually enumerating all n-m splits and agrees with tabulated data and R for samples with no ties.  As explained above, in the presence of ties, exact computation requires that the partition enumeration be over the actual combined data (including the ties).  The fix committed in ce98d00852e21ce34d8d247db7f6be138967b559 does that, so I think it is correct.  I will run some comparisons with ks.boot to check consistency / find errors in the implementation.&lt;/p&gt;

&lt;p&gt;Happily, in &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; I found a much more efficient way to compute exactP in the no-ties case.  Unfortunately, I can&apos;t find &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; or the algorithm presented freely available anywhere.  I am going to try to implement it and once that is done, we can likely use Monte Carlo only for moderate size samples with ties (since the faster algorithm should work for the non-tied case up to the level where the asymptotic approximation is fine - this is basically what R does).  I think in any case, our Monte Carlo implementation should use the combined sample semantics (as in &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;), which means in the presence of ties, it will have to use the multi-set as sampling universe.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Abadie, Alberto. 2002.  &quot;Bootstrap Tests for Distributional Treatment Effects in Instrumental Variable Models.&apos;&apos; Journal of the American Statistical Association, 97:457 (March) 284-292.  Currently available online at &lt;a href=&quot;http://hks.harvard.edu/fs/aabadie/dtep.pdf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hks.harvard.edu/fs/aabadie/dtep.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; Wilcox, Rand. 2012. &lt;cite&gt;Introduction to Robust Estimation and Hypothesis Testing&lt;/cite&gt;, 3rd Ed. Academic Press.&lt;/p&gt;</comment>
                            <comment id="14744096" author="otmar ertl" created="Mon, 14 Sep 2015 19:35:32 +0000"  >&lt;p&gt;I still have doubts:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;There is a difference, if there are ties within one sample, or if the same value exists at least once in both samples. In the first case the D-statistic is well defined. In the latter case the D-statistic is undefined. For example, if x = (1, 3, 3, 5) and y = (2, 4, 4, 6) D = 0.5. On the other hand, if  x = (1, 3, 3, 5) and y = (2, 3, 3, 6) the D-statistic could be any value  between 0.25 and 0.75. The current implementation returns the minimum (0.25 in this case), but this seems to be a quite arbitrary choice. Furthermore, the implementation does not distinguish between these two cases (see hasTies() method).&lt;/li&gt;
	&lt;li&gt;If the current implementation of exactP() follows the definition you described, I do not really understand why the following two statements return different values:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KolmogorovSmirnovTest().exactP(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;[]{0.9, 1.0, 1.1}, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;[]{0.0, 0.0}, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; KolmogorovSmirnovTest().exactP(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;[]{1.0, 1.0, 1.0}, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;[]{0.0, 0.0}, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The D-statistic is well-defined and equal to 1 in both cases.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;[1] describes estimating the p-Value using bootstrapping. I am not sure, if an exact definition can be derived from there, since bootstrapping in general is not a consistent  estimation method.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="14744680" author="psteitz" created="Tue, 15 Sep 2015 01:51:10 +0000"  >&lt;p&gt;Thanks again, Otmar for looking carefully at this.  It is a little painful to try to do this in JIRA comments, but since we started here and it will be best to keep the comments together, I will try to respond to each of your points above.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;I must be missing something here.  In the case x = (1, 3, 3, 5) and y = (2, 3, 3, 6), I don&apos;t see how there is ambiguity in the D statistic, which looks correct to me at .25.  The D statistic is the maximum difference in the empirical distributions.  In this case, the max is .25, attained at two domain values: 1 and 5.&lt;/li&gt;
	&lt;li&gt;The D statistics are the same, but the empirical distributions and underlying datasets are different.  The p-value depends on both the D-statistic and the empirical distributions.  When there are no ties, D_n,m is has the same distribution regardless of the underlying sample data.  When ties are present, the distribution is still discrete, but it depends on the number and location of the ties.&lt;/li&gt;
	&lt;li&gt;What is proven is that bootstrapping gives asymptotically correct results.  The bootstrapping is over the combined dataset, including ties (as ks.boot does).  Exact computation using enumeration of all possible splits will give the same result as what will be expected from bootstrapping.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;It could be that we are not agreeing on the core definition of what the p-value is supposed to be.  To me, ties in the data just add mass to the empirical distributions where they fall and the 2-sample test is really just assessing the null hypothesis that the distributions represent draws from the same underlying population distribution.   The common underlying distribution under the null hypothesis is best represented by the pooled data.  This is the interpretation that &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; appears to agree with, and &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; (sadly not free) as well.  &lt;/p&gt;</comment>
                            <comment id="14745495" author="otmar ertl" created="Tue, 15 Sep 2015 13:59:24 +0000"  >&lt;p&gt;After some research I have the feeling we are discussing how to define zero divided by zero. There are at least two methods to calculate a reasonable p-value in the presence of ties:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The method you have proposed which seems to be also known as permutation method. Averaging only over some permutations and averaging over all possible permutations correspond to the bootstrap method and the current exactP() implementation, respectively.&lt;/li&gt;
	&lt;li&gt;Another method is to add some jitter to the sampled values to break ties. (This google search &lt;a href=&quot;https://www.google.com/?gfe_rd=cr&amp;amp;ei=qCL4VaKvNIWI8QfLibD4Bg&amp;amp;gws_rd=cr&amp;amp;fg=1#q=jitter+kolmogorov+smirnov&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.google.com/?gfe_rd=cr&amp;amp;ei=qCL4VaKvNIWI8QfLibD4Bg&amp;amp;gws_rd=cr&amp;amp;fg=1#q=jitter+kolmogorov+smirnov&lt;/a&gt; immediately gives you a couple of references.) This method corresponds to the method I have proposed. Adding small random values to ties to get a strict ordering corresponds to choosing any random ordering. Averaging over all possible orderings would also lead to a well-defined p-value.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Maybe, the user should be able to choose the method how to resolve ties?&lt;/p&gt;</comment>
                            <comment id="14746500" author="psteitz" created="Tue, 15 Sep 2015 23:37:25 +0000"  >&lt;p&gt;I don&apos;t think there is really a question about the definition of the p-value - I can&apos;t find any reference that does not confirm it to be what I described above.  And it is well-defined in the presence of ties - just messy to compute, as the distribution of D depends on not just n and m but the location of the ties.  The permutation method does correspond to ks.boot and what I would propose for the monte carlo impl in the presence of ties.  The method currently implemented for exactP(x,y,b) computes p-values based on full enumeration of the underlying sample space sampled by ks.boot (resp. the permutation method).&lt;/p&gt;

&lt;p&gt;I understand, though, the inefficiency of doing full enumeration and the convenience of working with D statistics that depend only on n and m.  So I think it may be best to do as you suggest and make the behavior in the presence of ties configurable.  I like the idea of introducing a (public or protected) jitter method that just randomly perturbs combined samples with ties.  Then if you configure ties handling to use jitter, the implementation just applies the jitter and uses the (not yet implemented) fast method for exact computation without ties and the current monteCarlo implementation (that does not handle ties) for monteCarloP.  [An interesting theorem to prove is that the expected p-value computed using random jitter in the presence of ties equals the true p-value equals the expectation of the permutation method (the last part is what &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; shows)].  Once we have the fast no-ties method implemented, we may be able to dispense with the version of monteCarloP that does not handle ties, as the fast, exact method should be usable up to the sample sizes where the K-S distribution based method is OK (and more accurate).&lt;/p&gt;

&lt;p&gt;Assuming you are OK with this, I will proceed with a) the fast implementation of the no ties exactP b) a version of monteCarloP that basically does what ks.boot does (resamples the combined dataset with ties included).  &lt;/p&gt;

&lt;p&gt;If we agree on this approach we need to decide&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;How configuration should work&lt;/li&gt;
	&lt;li&gt;How (if at all) we signal to the user that there are ties in the data&lt;/li&gt;
	&lt;li&gt;What the default behavior should be&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="14746924" author="otmar ertl" created="Wed, 16 Sep 2015 05:48:50 +0000"  >&lt;p&gt;The p-value is the probability that the observed KS-statistic is smaller than the KS-statistic that I get if two random samples of same sizes are drawn from the underlying distribution. In the no-ties case this value can be calculated exactly without knowing the underlying distribution. In case of ties, the p-value cannot be calculated exactly. There are different approaches how to calculate some approximation of the p-value for the tie-case:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Approximation of the underlying distribution by the observed data, which definitely makes sense for bootstrapping where the sample sizes are usually large. However, in our case the underlying distribution is estimated from small sample sizes, since this is the domain for the exactP method. Therefore, I doubt that the calculate p-value deserves the label &quot;exact&quot; in this case.&lt;/li&gt;
	&lt;li&gt;Assumption that orderings of observed equal values are equally likely, which of course is also an approximation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I still do not understand why the first approach should be the true one.&lt;/p&gt;</comment>
                            <comment id="14790915" author="psteitz" created="Wed, 16 Sep 2015 18:49:47 +0000"  >&lt;p&gt;I could be wrong on this and I am OK with reverting the current exactP ties handling code and replacing with the random jitter approach.  I still think the exact p can in fact be computed with ties present; but to do so you have to view the combined sample as the empirical distribution representing the (combined) population.   You make a good point above about that being dubious for small samples.   I will continue to research this, but given lack of consensus, I will remove the implementation from the code.&lt;/p&gt;

&lt;p&gt;So let&apos;s see if we can agree on &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Add non-naive exactP to handle no ties small sample.  Extend it to n * m = 10000 as default behavior (this is the cut that R uses).  Beyond this point, use the K-S distribution, so we no longer need MonteCarloP for moderate size samples.&lt;/li&gt;
	&lt;li&gt;Implement jitter method and use this by default in the small sample case to break ties.  Until we  have eliminated the need for MonteCarloP as a default, use jitter to break ties for moderate sample sizes and use monteCarloP as is post-jitter.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Optionally, implement a ks.boot-like monteCarloP that works with tied data.&lt;/p&gt;

</comment>
                            <comment id="14790993" author="otmar ertl" created="Wed, 16 Sep 2015 19:26:04 +0000"  >&lt;p&gt;Sounds ok to me, due to its slow convergence avoiding Monte Carlo as much as possible is generally a good idea.&lt;/p&gt;</comment>
                            <comment id="14997372" author="psteitz" created="Mon, 9 Nov 2015 21:14:38 +0000"  >&lt;p&gt;I did some more extensive testing against R&apos;s ks.boot and found significant differences from the code in ce98d00852e21ce34d8d247db7f6be138967b559.  I have determined the reason why the results are different and that my initial approach was incorrect.  The difference is due to the fact that ks.boot samples &quot;with replacement&quot; from the combined empirical distribution while my approach constrains the n-m split to be a split that can be achieved using the combined dataset.  I interpreted the p-value to be essentially the same as in the no ties case - what is the probability that when the combined set of values is split into an n-set and an m-set, the KS statistic is greater than or equal to what we observe in the data.  The theoretical development in &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; and the implementation in ks.boot define the p-value to be the probability that when an m-set and n-set are drawn independently from the combined empirical distribution, the p-value exceeds what we see in the data.  This is not the same and when there are a lot of ties the estimates diverge.  Apologies for being a little dense on this.&lt;/p&gt;</comment>
                            <comment id="15021126" author="psteitz" created="Sun, 22 Nov 2015 18:42:05 +0000"  >&lt;p&gt;Flawed exactP implementation removed in &lt;br/&gt;
fd37b5dd02bbce93f6f4fceb6bc3e6aa4641c5a7 (master)&lt;br/&gt;
3d055c620c7132b0fbe61ffeba87fd9f2391826f (MATH_3_X)&lt;/p&gt;</comment>
                            <comment id="15021164" author="psteitz" created="Sun, 22 Nov 2015 19:46:01 +0000"  >&lt;p&gt;Added boostrap method (like ks.boot) in&lt;br/&gt;
7851a3e2bf24c58eea66da70fa42a32f03532620 (master)&lt;br/&gt;
fbc327e9e30093acdc0fc325b1719cae4ea8bac1 (MATH_3_X)&lt;/p&gt;</comment>
                            <comment id="15115968" author="luc" created="Mon, 25 Jan 2016 20:28:02 +0000"  >&lt;p&gt;Closing all resolved issues that were included in 3.6 release.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 42 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2gv6f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>