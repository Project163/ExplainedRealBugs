{"url":"https://api.github.com/repos/rclone/rclone/issues/8460","repository_url":"https://api.github.com/repos/rclone/rclone","labels_url":"https://api.github.com/repos/rclone/rclone/issues/8460/labels{/name}","comments_url":"https://api.github.com/repos/rclone/rclone/issues/8460/comments","events_url":"https://api.github.com/repos/rclone/rclone/issues/8460/events","html_url":"https://github.com/rclone/rclone/issues/8460","id":2933214003,"node_id":"I_kwDOAQ-n5M6u1Usz","number":8460,"title":"Backblaze needs the equivalent of `--s3-max-upload-parts` which defaults to 10000","user":{"login":"zackees","id":6856673,"node_id":"MDQ6VXNlcjY4NTY2NzM=","avatar_url":"https://avatars.githubusercontent.com/u/6856673?v=4","gravatar_id":"","url":"https://api.github.com/users/zackees","html_url":"https://github.com/zackees","followers_url":"https://api.github.com/users/zackees/followers","following_url":"https://api.github.com/users/zackees/following{/other_user}","gists_url":"https://api.github.com/users/zackees/gists{/gist_id}","starred_url":"https://api.github.com/users/zackees/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zackees/subscriptions","organizations_url":"https://api.github.com/users/zackees/orgs","repos_url":"https://api.github.com/users/zackees/repos","events_url":"https://api.github.com/users/zackees/events{/privacy}","received_events_url":"https://api.github.com/users/zackees/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[{"id":86399005,"node_id":"MDU6TGFiZWw4NjM5OTAwNQ==","url":"https://api.github.com/repos/rclone/rclone/labels/bug","name":"bug","color":"fc2929","default":true,"description":null},{"id":306063207,"node_id":"MDU6TGFiZWwzMDYwNjMyMDc=","url":"https://api.github.com/repos/rclone/rclone/labels/Remote:%20Backblaze%20B2","name":"Remote: Backblaze B2","color":"fbca04","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":{"url":"https://api.github.com/repos/rclone/rclone/milestones/60","html_url":"https://github.com/rclone/rclone/milestone/60","labels_url":"https://api.github.com/repos/rclone/rclone/milestones/60/labels","id":13114399,"node_id":"MI_kwDOAQ-n5M4AyBwf","number":60,"title":"v1.72","description":"","creator":{"login":"ncw","id":536803,"node_id":"MDQ6VXNlcjUzNjgwMw==","avatar_url":"https://avatars.githubusercontent.com/u/536803?v=4","gravatar_id":"","url":"https://api.github.com/users/ncw","html_url":"https://github.com/ncw","followers_url":"https://api.github.com/users/ncw/followers","following_url":"https://api.github.com/users/ncw/following{/other_user}","gists_url":"https://api.github.com/users/ncw/gists{/gist_id}","starred_url":"https://api.github.com/users/ncw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ncw/subscriptions","organizations_url":"https://api.github.com/users/ncw/orgs","repos_url":"https://api.github.com/users/ncw/repos","events_url":"https://api.github.com/users/ncw/events{/privacy}","received_events_url":"https://api.github.com/users/ncw/received_events","type":"User","user_view_type":"public","site_admin":false},"open_issues":14,"closed_issues":16,"state":"open","created_at":"2025-06-19T17:12:26Z","updated_at":"2025-11-04T12:09:49Z","due_on":"2025-11-21T08:00:00Z","closed_at":null},"comments":6,"created_at":"2025-03-19T21:21:25Z","updated_at":"2025-09-15T12:06:01Z","closed_at":"2025-09-15T12:05:21Z","author_association":"CONTRIBUTOR","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"It's entirely possible that I'm wrong on this, but I think I've figured out why extremely large (9TB) uploads were failing to back blaze. The chunk size was producing > 10000 parts. I see that on the `s3` backend you have logic that will increase the size of the chunks to maintain the 10,000 part limit by s3.\n\nOn Backblaze this doesn't seem to work. I was uploading chunks much smaller than this resulting in > 10000 parts. For a 9TB file, the minium chunk size is about 9.5 GB per chunk. Rclone was reporting that 96MB chunks were being uploaded.\n\nThis would explain why I was failing to upload at around the 10% mark.\n\nThis request is to copy the logic of chunk size promotion in the B2 service to match S3","closed_by":{"login":"ncw","id":536803,"node_id":"MDQ6VXNlcjUzNjgwMw==","avatar_url":"https://avatars.githubusercontent.com/u/536803?v=4","gravatar_id":"","url":"https://api.github.com/users/ncw","html_url":"https://github.com/ncw","followers_url":"https://api.github.com/users/ncw/followers","following_url":"https://api.github.com/users/ncw/following{/other_user}","gists_url":"https://api.github.com/users/ncw/gists{/gist_id}","starred_url":"https://api.github.com/users/ncw/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ncw/subscriptions","organizations_url":"https://api.github.com/users/ncw/orgs","repos_url":"https://api.github.com/users/ncw/repos","events_url":"https://api.github.com/users/ncw/events{/privacy}","received_events_url":"https://api.github.com/users/ncw/received_events","type":"User","user_view_type":"public","site_admin":false},"reactions":{"url":"https://api.github.com/repos/rclone/rclone/issues/8460/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/rclone/rclone/issues/8460/timeline","performed_via_github_app":null,"state_reason":"completed"}